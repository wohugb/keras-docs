{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Keras \u6587\u6863 \u57fa\u4e8e Python \u7684\u6df1\u5ea6\u5b66\u4e60\u5e93 \u4f60\u6070\u597d\u53d1\u73b0\u4e86 Keras\u3002 Keras \u662f\u4e00\u4e2a\u7528 Python \u7f16\u5199\u7684\u9ad8\u7ea7\u795e\u7ecf\u7f51\u7edc API\uff0c\u5b83\u80fd\u591f\u4ee5 TensorFlow , CNTK , \u6216\u8005 Theano \u4f5c\u4e3a\u540e\u7aef\u8fd0\u884c\u3002Keras \u7684\u5f00\u53d1\u91cd\u70b9\u662f\u652f\u6301\u5feb\u901f\u7684\u5b9e\u9a8c\u3002 \u80fd\u591f\u4ee5\u6700\u5c0f\u7684\u65f6\u5ef6\u628a\u4f60\u7684\u60f3\u6cd5\u8f6c\u6362\u4e3a\u5b9e\u9a8c\u7ed3\u679c\uff0c\u662f\u505a\u597d\u7814\u7a76\u7684\u5173\u952e\u3002 \u5982\u679c\u4f60\u5728\u4ee5\u4e0b\u60c5\u51b5\u4e0b\u9700\u8981\u6df1\u5ea6\u5b66\u4e60\u5e93\uff0c\u8bf7\u4f7f\u7528 Keras\uff1a \u5141\u8bb8\u7b80\u5355\u800c\u5feb\u901f\u7684\u539f\u578b\u8bbe\u8ba1\uff08\u7531\u4e8e\u7528\u6237\u53cb\u597d\uff0c\u9ad8\u5ea6\u6a21\u5757\u5316\uff0c\u53ef\u6269\u5c55\u6027\uff09\u3002 \u540c\u65f6\u652f\u6301\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff0c\u4ee5\u53ca\u4e24\u8005\u7684\u7ec4\u5408\u3002 \u5728 CPU \u548c GPU \u4e0a\u65e0\u7f1d\u8fd0\u884c\u3002 \u67e5\u770b\u6587\u6863\uff0c\u8bf7\u8bbf\u95ee Keras.io \u3002 Keras \u517c\u5bb9\u7684 Python \u7248\u672c: Python 2.7-3.6 \u3002 \u6307\u5bfc\u539f\u5219 \u7528\u6237\u53cb\u597d\u3002 Keras \u662f\u4e3a\u4eba\u7c7b\u800c\u4e0d\u662f\u4e3a\u673a\u5668\u8bbe\u8ba1\u7684 API\u3002\u5b83\u628a\u7528\u6237\u4f53\u9a8c\u653e\u5728\u9996\u8981\u548c\u4e2d\u5fc3\u4f4d\u7f6e\u3002Keras \u9075\u5faa\u51cf\u5c11\u8ba4\u77e5\u56f0\u96be\u7684\u6700\u4f73\u5b9e\u8df5\uff1a\u5b83\u63d0\u4f9b\u4e00\u81f4\u4e14\u7b80\u5355\u7684 API\uff0c\u5c06\u5e38\u89c1\u7528\u4f8b\u6240\u9700\u7684\u7528\u6237\u64cd\u4f5c\u6570\u91cf\u964d\u81f3\u6700\u4f4e\uff0c\u5e76\u4e14\u5728\u7528\u6237\u9519\u8bef\u65f6\u63d0\u4f9b\u6e05\u6670\u548c\u53ef\u64cd\u4f5c\u7684\u53cd\u9988\u3002 \u6a21\u5757\u5316\u3002 \u6a21\u578b\u88ab\u7406\u89e3\u4e3a\u7531\u72ec\u7acb\u7684\u3001\u5b8c\u5168\u53ef\u914d\u7f6e\u7684\u6a21\u5757\u6784\u6210\u7684\u5e8f\u5217\u6216\u56fe\u3002\u8fd9\u4e9b\u6a21\u5757\u53ef\u4ee5\u4ee5\u5c3d\u53ef\u80fd\u5c11\u7684\u9650\u5236\u7ec4\u88c5\u5728\u4e00\u8d77\u3002\u7279\u522b\u662f\u795e\u7ecf\u7f51\u7edc\u5c42\u3001\u635f\u5931\u51fd\u6570\u3001\u4f18\u5316\u5668\u3001\u521d\u59cb\u5316\u65b9\u6cd5\u3001\u6fc0\u6d3b\u51fd\u6570\u3001\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u5b83\u4eec\u90fd\u662f\u53ef\u4ee5\u7ed3\u5408\u8d77\u6765\u6784\u5efa\u65b0\u6a21\u578b\u7684\u6a21\u5757\u3002 \u6613\u6269\u5c55\u6027\u3002 \u65b0\u7684\u6a21\u5757\u662f\u5f88\u5bb9\u6613\u6dfb\u52a0\u7684\uff08\u4f5c\u4e3a\u65b0\u7684\u7c7b\u548c\u51fd\u6570\uff09\uff0c\u73b0\u6709\u7684\u6a21\u5757\u5df2\u7ecf\u63d0\u4f9b\u4e86\u5145\u8db3\u7684\u793a\u4f8b\u3002\u7531\u4e8e\u80fd\u591f\u8f7b\u677e\u5730\u521b\u5efa\u53ef\u4ee5\u63d0\u9ad8\u8868\u73b0\u529b\u7684\u65b0\u6a21\u5757\uff0cKeras \u66f4\u52a0\u9002\u5408\u9ad8\u7ea7\u7814\u7a76\u3002 \u57fa\u4e8e Python \u5b9e\u73b0\u3002 Keras \u6ca1\u6709\u7279\u5b9a\u683c\u5f0f\u7684\u5355\u72ec\u914d\u7f6e\u6587\u4ef6\u3002\u6a21\u578b\u5b9a\u4e49\u5728 Python \u4ee3\u7801\u4e2d\uff0c\u8fd9\u4e9b\u4ee3\u7801\u7d27\u51d1\uff0c\u6613\u4e8e\u8c03\u8bd5\uff0c\u5e76\u4e14\u6613\u4e8e\u6269\u5c55\u3002 \u5feb\u901f\u5f00\u59cb\uff1a30 \u79d2\u4e0a\u624b Keras Keras \u7684\u6838\u5fc3\u6570\u636e\u7ed3\u6784\u662f model \uff0c\u4e00\u79cd\u7ec4\u7ec7\u7f51\u7edc\u5c42\u7684\u65b9\u5f0f\u3002\u6700\u7b80\u5355\u7684\u6a21\u578b\u662f Sequential \u987a\u5e8f\u6a21\u578b \uff0c\u5b83\u7531\u591a\u4e2a\u7f51\u7edc\u5c42\u7ebf\u6027\u5806\u53e0\u3002\u5bf9\u4e8e\u66f4\u590d\u6742\u7684\u7ed3\u6784\uff0c\u4f60\u5e94\u8be5\u4f7f\u7528 Keras \u51fd\u6570\u5f0f API \uff0c\u5b83\u5141\u8bb8\u6784\u5efa\u4efb\u610f\u7684\u795e\u7ecf\u7f51\u7edc\u56fe\u3002 Sequential \u6a21\u578b\u5982\u4e0b\u6240\u793a\uff1a from keras.models import Sequential model = Sequential() \u53ef\u4ee5\u7b80\u5355\u5730\u4f7f\u7528 .add() \u6765\u5806\u53e0\u6a21\u578b\uff1a from keras.layers import Dense model.add(Dense(units=64, activation='relu', input_dim=100)) model.add(Dense(units=10, activation='softmax')) \u5728\u5b8c\u6210\u4e86\u6a21\u578b\u7684\u6784\u5efa\u540e, \u53ef\u4ee5\u4f7f\u7528 .compile() \u6765\u914d\u7f6e\u5b66\u4e60\u8fc7\u7a0b\uff1a model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy']) \u5982\u679c\u9700\u8981\uff0c\u4f60\u8fd8\u53ef\u4ee5\u8fdb\u4e00\u6b65\u5730\u914d\u7f6e\u4f60\u7684\u4f18\u5316\u5668\u3002Keras \u7684\u6838\u5fc3\u539f\u5219\u662f\u4f7f\u4e8b\u60c5\u53d8\u5f97\u76f8\u5f53\u7b80\u5355\uff0c\u540c\u65f6\u53c8\u5141\u8bb8\u7528\u6237\u5728\u9700\u8981\u7684\u65f6\u5019\u80fd\u591f\u8fdb\u884c\u5b8c\u5168\u7684\u63a7\u5236\uff08\u7ec8\u6781\u7684\u63a7\u5236\u662f\u6e90\u4ee3\u7801\u7684\u6613\u6269\u5c55\u6027\uff09\u3002 model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)) \u73b0\u5728\uff0c\u4f60\u53ef\u4ee5\u6279\u91cf\u5730\u5728\u8bad\u7ec3\u6570\u636e\u4e0a\u8fdb\u884c\u8fed\u4ee3\u4e86\uff1a # x_train \u548c y_train \u662f Numpy \u6570\u7ec4 -- \u5c31\u50cf\u5728 Scikit-Learn API \u4e2d\u4e00\u6837\u3002 model.fit(x_train, y_train, epochs=5, batch_size=32) \u6216\u8005\uff0c\u4f60\u53ef\u4ee5\u624b\u52a8\u5730\u5c06\u6279\u6b21\u7684\u6570\u636e\u63d0\u4f9b\u7ed9\u6a21\u578b\uff1a model.train_on_batch(x_batch, y_batch) \u53ea\u9700\u4e00\u884c\u4ee3\u7801\u5c31\u80fd\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff1a loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128) \u6216\u8005\u5bf9\u65b0\u7684\u6570\u636e\u751f\u6210\u9884\u6d4b\uff1a classes = model.predict(x_test, batch_size=128) \u6784\u5efa\u4e00\u4e2a\u95ee\u7b54\u7cfb\u7edf\uff0c\u4e00\u4e2a\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\uff0c\u4e00\u4e2a\u795e\u7ecf\u56fe\u7075\u673a\uff0c\u6216\u8005\u5176\u4ed6\u7684\u4efb\u4f55\u6a21\u578b\uff0c\u5c31\u662f\u8fd9\u4e48\u7684\u5feb\u3002\u6df1\u5ea6\u5b66\u4e60\u80cc\u540e\u7684\u601d\u60f3\u5f88\u7b80\u5355\uff0c\u90a3\u4e48\u5b83\u4eec\u7684\u5b9e\u73b0\u53c8\u4f55\u5fc5\u8981\u90a3\u4e48\u75db\u82e6\u5462\uff1f \u6709\u5173 Keras \u66f4\u6df1\u5165\u7684\u6559\u7a0b\uff0c\u8bf7\u67e5\u770b\uff1a \u5f00\u59cb\u4f7f\u7528 Sequential \u6a21\u578b \u5f00\u59cb\u4f7f\u7528\u51fd\u6570\u5f0f API \u5728\u4ee3\u7801\u4ed3\u5e93\u7684 examples \u76ee\u5f55 \u4e2d\uff0c\u4f60\u4f1a\u627e\u5230\u66f4\u591a\u9ad8\u7ea7\u6a21\u578b\uff1a\u57fa\u4e8e\u8bb0\u5fc6\u7f51\u7edc\u7684\u95ee\u7b54\u7cfb\u7edf\u3001\u57fa\u4e8e\u6808\u5f0f LSTM \u7684\u6587\u672c\u751f\u6210\u7b49\u7b49\u3002 \u5b89\u88c5\u6307\u5f15 \u5728\u5b89\u88c5 Keras \u4e4b\u524d\uff0c\u8bf7\u5b89\u88c5\u4ee5\u4e0b\u540e\u7aef\u5f15\u64ce\u4e4b\u4e00\uff1aTensorFlow\uff0cTheano\uff0c\u6216\u8005 CNTK\u3002\u6211\u4eec\u63a8\u8350 TensorFlow \u540e\u7aef\u3002 TensorFlow \u5b89\u88c5\u6307\u5f15 \u3002 Theano \u5b89\u88c5\u6307\u5f15 \u3002 CNTK \u5b89\u88c5\u6307\u5f15 \u3002 \u4f60\u4e5f\u53ef\u4ee5\u8003\u8651\u5b89\u88c5\u4ee5\u4e0b \u53ef\u9009\u4f9d\u8d56 \uff1a cuDNN (\u5982\u679c\u4f60\u8ba1\u5212\u5728 GPU \u4e0a\u8fd0\u884c Keras\uff0c\u5efa\u8bae\u5b89\u88c5)\u3002 HDF5 \u548c h5py (\u5982\u679c\u4f60\u9700\u8981\u5c06 Keras \u6a21\u578b\u4fdd\u5b58\u5230\u78c1\u76d8\uff0c\u5219\u9700\u8981\u8fd9\u4e9b)\u3002 graphviz \u548c pydot (\u7528\u4e8e \u53ef\u89c6\u5316\u5de5\u5177 \u7ed8\u5236\u6a21\u578b\u56fe)\u3002 \u7136\u540e\u4f60\u5c31\u53ef\u4ee5\u5b89\u88c5 Keras \u672c\u8eab\u4e86\u3002\u6709\u4e24\u79cd\u65b9\u6cd5\u5b89\u88c5 Keras\uff1a \u4f7f\u7528 PyPI \u5b89\u88c5 Keras (\u63a8\u8350)\uff1a sudo pip install keras \u5982\u679c\u4f60\u4f7f\u7528 virtualenv \u865a\u62df\u73af\u5883, \u4f60\u53ef\u4ee5\u907f\u514d\u4f7f\u7528 sudo\uff1a pip install keras \u6216\u8005\uff1a\u4f7f\u7528 GitHub \u6e90\u7801\u5b89\u88c5 Keras\uff1a \u9996\u5148\uff0c\u4f7f\u7528 git \u6765\u514b\u9686 Keras\uff1a git clone https://github.com/keras-team/keras.git \u7136\u540e\uff0c cd \u5230 Keras \u76ee\u5f55\u5e76\u4e14\u8fd0\u884c\u5b89\u88c5\u547d\u4ee4\uff1a cd keras sudo python setup.py install \u914d\u7f6e\u4f60\u7684 Keras \u540e\u7aef \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cKeras \u5c06\u4f7f\u7528 TensorFlow \u4f5c\u4e3a\u5176\u5f20\u91cf\u64cd\u4f5c\u5e93\u3002\u8bf7 \u8ddf\u968f\u8fd9\u4e9b\u6307\u5f15 \u6765\u914d\u7f6e\u5176\u4ed6 Keras \u540e\u7aef\u3002 \u6280\u672f\u652f\u6301 \u4f60\u53ef\u4ee5\u63d0\u51fa\u95ee\u9898\u5e76\u53c2\u4e0e\u5f00\u53d1\u8ba8\u8bba\uff1a Keras Google group \u3002 Keras Slack channel \u3002 \u4f7f\u7528 \u8fd9\u4e2a\u94fe\u63a5 \u5411\u8be5\u9891\u9053\u8bf7\u6c42\u9080\u8bf7\u51fd\u3002 \u6216\u8005\u52a0\u5165 Keras \u6df1\u5ea6\u5b66\u4e60\u4ea4\u6d41\u7fa4\uff0c\u534f\u52a9\u6587\u6863\u7684\u7ffb\u8bd1\u5de5\u4f5c\uff0c\u7fa4\u53f7\u4e3a 951623081\u3002 \u4f60\u4e5f\u53ef\u4ee5\u5728 GitHub issues \u4e2d\u53d1\u5e03 \u6f0f\u6d1e\u62a5\u544a\u548c\u65b0\u529f\u80fd\u8bf7\u6c42 \uff08\u4ec5\u9650\u4e8e\u6b64\uff09\u3002\u6ce8\u610f\u8bf7\u5148\u9605\u8bfb \u89c4\u8303\u6587\u6863 \u3002 \u4e3a\u4ec0\u4e48\u53d6\u540d\u4e3a Keras? Keras (\u03ba\u03ad\u03c1\u03b1\u03c2) \u5728\u5e0c\u814a\u8bed\u4e2d\u610f\u4e3a \u53f7\u89d2 \u3002\u5b83\u6765\u81ea\u53e4\u5e0c\u814a\u548c\u62c9\u4e01\u6587\u5b66\u4e2d\u7684\u4e00\u4e2a\u6587\u5b66\u5f62\u8c61\uff0c\u9996\u5148\u51fa\u73b0\u4e8e \u300a\u5965\u5fb7\u8d5b\u300b \u4e2d\uff0c \u68a6\u795e ( Oneiroi , singular Oneiros ) \u4ece\u8fd9\u4e24\u7c7b\u4eba\u4e2d\u5206\u79bb\u51fa\u6765\uff1a\u90a3\u4e9b\u7528\u865a\u5e7b\u7684\u666f\u8c61\u6b3a\u9a97\u4eba\u7c7b\uff0c\u901a\u8fc7\u8c61\u7259\u4e4b\u95e8\u62b5\u8fbe\u5730\u7403\u4e4b\u4eba\uff0c\u4ee5\u53ca\u90a3\u4e9b\u5ba3\u544a\u672a\u6765\u5373\u5c06\u5230\u6765\uff0c\u901a\u8fc7\u53f7\u89d2\u4e4b\u95e8\u62b5\u8fbe\u4e4b\u4eba\u3002 \u5b83\u7c7b\u4f3c\u4e8e\u6587\u5b57\u5bd3\u610f\uff0c\u03ba\u03ad\u03c1\u03b1\u03c2 (\u53f7\u89d2) / \u03ba\u03c1\u03b1\u03af\u03bd\u03c9 (\u5c65\u884c)\uff0c\u4ee5\u53ca \u1f10\u03bb\u03ad\u03c6\u03b1\u03c2 (\u8c61\u7259) / \u1f10\u03bb\u03b5\u03c6\u03b1\u03af\u03c1\u03bf\u03bc\u03b1\u03b9 (\u6b3a\u9a97)\u3002 Keras \u6700\u521d\u662f\u4f5c\u4e3a ONEIROS \u9879\u76ee\uff08\u5f00\u653e\u5f0f\u795e\u7ecf\u7535\u5b50\u667a\u80fd\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\uff09\u7814\u7a76\u5de5\u4f5c\u7684\u4e00\u90e8\u5206\u800c\u5f00\u53d1\u7684\u3002 \"Oneiroi \u8d85\u51fa\u4e86\u6211\u4eec\u7684\u7406\u89e3 - \u8c01\u80fd\u786e\u5b9a\u5b83\u4eec\u8bb2\u8ff0\u4e86\u4ec0\u4e48\u6545\u4e8b\uff1f\u5e76\u4e0d\u662f\u6240\u6709\u4eba\u90fd\u80fd\u627e\u5230\u3002\u90a3\u91cc\u6709\u4e24\u6247\u95e8\uff0c\u5c31\u662f\u901a\u5f80\u77ed\u6682\u7684 Oneiroi \u7684\u901a\u9053\uff1b\u4e00\u4e2a\u662f\u7528\u53f7\u89d2\u5236\u9020\u7684\uff0c\u4e00\u4e2a\u662f\u7528\u8c61\u7259\u5236\u9020\u7684\u3002\u7a7f\u8fc7\u5c16\u9510\u7684\u8c61\u7259\u7684 Oneiroi \u662f\u8be1\u8ba1\u591a\u7aef\u7684\uff0c\u4ed6\u4eec\u5e26\u6709\u4e00\u4e9b\u4e0d\u4f1a\u5b9e\u73b0\u7684\u4fe1\u606f\uff1b \u90a3\u4e9b\u7a7f\u8fc7\u629b\u5149\u7684\u5587\u53ed\u51fa\u6765\u7684\u4eba\u80cc\u540e\u5177\u6709\u771f\u7406\uff0c\u5bf9\u4e8e\u770b\u5230\u4ed6\u4eec\u7684\u4eba\u6765\u8bf4\u662f\u5b8c\u6210\u7684\u3002\" Homer, Odyssey 19. 562 ff (Shewring translation).","title":"Keras \u6587\u6863"},{"location":"#keras","text":"\u57fa\u4e8e Python \u7684\u6df1\u5ea6\u5b66\u4e60\u5e93","title":"Keras \u6587\u6863"},{"location":"#keras_1","text":"Keras \u662f\u4e00\u4e2a\u7528 Python \u7f16\u5199\u7684\u9ad8\u7ea7\u795e\u7ecf\u7f51\u7edc API\uff0c\u5b83\u80fd\u591f\u4ee5 TensorFlow , CNTK , \u6216\u8005 Theano \u4f5c\u4e3a\u540e\u7aef\u8fd0\u884c\u3002Keras \u7684\u5f00\u53d1\u91cd\u70b9\u662f\u652f\u6301\u5feb\u901f\u7684\u5b9e\u9a8c\u3002 \u80fd\u591f\u4ee5\u6700\u5c0f\u7684\u65f6\u5ef6\u628a\u4f60\u7684\u60f3\u6cd5\u8f6c\u6362\u4e3a\u5b9e\u9a8c\u7ed3\u679c\uff0c\u662f\u505a\u597d\u7814\u7a76\u7684\u5173\u952e\u3002 \u5982\u679c\u4f60\u5728\u4ee5\u4e0b\u60c5\u51b5\u4e0b\u9700\u8981\u6df1\u5ea6\u5b66\u4e60\u5e93\uff0c\u8bf7\u4f7f\u7528 Keras\uff1a \u5141\u8bb8\u7b80\u5355\u800c\u5feb\u901f\u7684\u539f\u578b\u8bbe\u8ba1\uff08\u7531\u4e8e\u7528\u6237\u53cb\u597d\uff0c\u9ad8\u5ea6\u6a21\u5757\u5316\uff0c\u53ef\u6269\u5c55\u6027\uff09\u3002 \u540c\u65f6\u652f\u6301\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff0c\u4ee5\u53ca\u4e24\u8005\u7684\u7ec4\u5408\u3002 \u5728 CPU \u548c GPU \u4e0a\u65e0\u7f1d\u8fd0\u884c\u3002 \u67e5\u770b\u6587\u6863\uff0c\u8bf7\u8bbf\u95ee Keras.io \u3002 Keras \u517c\u5bb9\u7684 Python \u7248\u672c: Python 2.7-3.6 \u3002","title":"\u4f60\u6070\u597d\u53d1\u73b0\u4e86 Keras\u3002"},{"location":"#_1","text":"\u7528\u6237\u53cb\u597d\u3002 Keras \u662f\u4e3a\u4eba\u7c7b\u800c\u4e0d\u662f\u4e3a\u673a\u5668\u8bbe\u8ba1\u7684 API\u3002\u5b83\u628a\u7528\u6237\u4f53\u9a8c\u653e\u5728\u9996\u8981\u548c\u4e2d\u5fc3\u4f4d\u7f6e\u3002Keras \u9075\u5faa\u51cf\u5c11\u8ba4\u77e5\u56f0\u96be\u7684\u6700\u4f73\u5b9e\u8df5\uff1a\u5b83\u63d0\u4f9b\u4e00\u81f4\u4e14\u7b80\u5355\u7684 API\uff0c\u5c06\u5e38\u89c1\u7528\u4f8b\u6240\u9700\u7684\u7528\u6237\u64cd\u4f5c\u6570\u91cf\u964d\u81f3\u6700\u4f4e\uff0c\u5e76\u4e14\u5728\u7528\u6237\u9519\u8bef\u65f6\u63d0\u4f9b\u6e05\u6670\u548c\u53ef\u64cd\u4f5c\u7684\u53cd\u9988\u3002 \u6a21\u5757\u5316\u3002 \u6a21\u578b\u88ab\u7406\u89e3\u4e3a\u7531\u72ec\u7acb\u7684\u3001\u5b8c\u5168\u53ef\u914d\u7f6e\u7684\u6a21\u5757\u6784\u6210\u7684\u5e8f\u5217\u6216\u56fe\u3002\u8fd9\u4e9b\u6a21\u5757\u53ef\u4ee5\u4ee5\u5c3d\u53ef\u80fd\u5c11\u7684\u9650\u5236\u7ec4\u88c5\u5728\u4e00\u8d77\u3002\u7279\u522b\u662f\u795e\u7ecf\u7f51\u7edc\u5c42\u3001\u635f\u5931\u51fd\u6570\u3001\u4f18\u5316\u5668\u3001\u521d\u59cb\u5316\u65b9\u6cd5\u3001\u6fc0\u6d3b\u51fd\u6570\u3001\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u5b83\u4eec\u90fd\u662f\u53ef\u4ee5\u7ed3\u5408\u8d77\u6765\u6784\u5efa\u65b0\u6a21\u578b\u7684\u6a21\u5757\u3002 \u6613\u6269\u5c55\u6027\u3002 \u65b0\u7684\u6a21\u5757\u662f\u5f88\u5bb9\u6613\u6dfb\u52a0\u7684\uff08\u4f5c\u4e3a\u65b0\u7684\u7c7b\u548c\u51fd\u6570\uff09\uff0c\u73b0\u6709\u7684\u6a21\u5757\u5df2\u7ecf\u63d0\u4f9b\u4e86\u5145\u8db3\u7684\u793a\u4f8b\u3002\u7531\u4e8e\u80fd\u591f\u8f7b\u677e\u5730\u521b\u5efa\u53ef\u4ee5\u63d0\u9ad8\u8868\u73b0\u529b\u7684\u65b0\u6a21\u5757\uff0cKeras \u66f4\u52a0\u9002\u5408\u9ad8\u7ea7\u7814\u7a76\u3002 \u57fa\u4e8e Python \u5b9e\u73b0\u3002 Keras \u6ca1\u6709\u7279\u5b9a\u683c\u5f0f\u7684\u5355\u72ec\u914d\u7f6e\u6587\u4ef6\u3002\u6a21\u578b\u5b9a\u4e49\u5728 Python \u4ee3\u7801\u4e2d\uff0c\u8fd9\u4e9b\u4ee3\u7801\u7d27\u51d1\uff0c\u6613\u4e8e\u8c03\u8bd5\uff0c\u5e76\u4e14\u6613\u4e8e\u6269\u5c55\u3002","title":"\u6307\u5bfc\u539f\u5219"},{"location":"#30-keras","text":"Keras \u7684\u6838\u5fc3\u6570\u636e\u7ed3\u6784\u662f model \uff0c\u4e00\u79cd\u7ec4\u7ec7\u7f51\u7edc\u5c42\u7684\u65b9\u5f0f\u3002\u6700\u7b80\u5355\u7684\u6a21\u578b\u662f Sequential \u987a\u5e8f\u6a21\u578b \uff0c\u5b83\u7531\u591a\u4e2a\u7f51\u7edc\u5c42\u7ebf\u6027\u5806\u53e0\u3002\u5bf9\u4e8e\u66f4\u590d\u6742\u7684\u7ed3\u6784\uff0c\u4f60\u5e94\u8be5\u4f7f\u7528 Keras \u51fd\u6570\u5f0f API \uff0c\u5b83\u5141\u8bb8\u6784\u5efa\u4efb\u610f\u7684\u795e\u7ecf\u7f51\u7edc\u56fe\u3002 Sequential \u6a21\u578b\u5982\u4e0b\u6240\u793a\uff1a from keras.models import Sequential model = Sequential() \u53ef\u4ee5\u7b80\u5355\u5730\u4f7f\u7528 .add() \u6765\u5806\u53e0\u6a21\u578b\uff1a from keras.layers import Dense model.add(Dense(units=64, activation='relu', input_dim=100)) model.add(Dense(units=10, activation='softmax')) \u5728\u5b8c\u6210\u4e86\u6a21\u578b\u7684\u6784\u5efa\u540e, \u53ef\u4ee5\u4f7f\u7528 .compile() \u6765\u914d\u7f6e\u5b66\u4e60\u8fc7\u7a0b\uff1a model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy']) \u5982\u679c\u9700\u8981\uff0c\u4f60\u8fd8\u53ef\u4ee5\u8fdb\u4e00\u6b65\u5730\u914d\u7f6e\u4f60\u7684\u4f18\u5316\u5668\u3002Keras \u7684\u6838\u5fc3\u539f\u5219\u662f\u4f7f\u4e8b\u60c5\u53d8\u5f97\u76f8\u5f53\u7b80\u5355\uff0c\u540c\u65f6\u53c8\u5141\u8bb8\u7528\u6237\u5728\u9700\u8981\u7684\u65f6\u5019\u80fd\u591f\u8fdb\u884c\u5b8c\u5168\u7684\u63a7\u5236\uff08\u7ec8\u6781\u7684\u63a7\u5236\u662f\u6e90\u4ee3\u7801\u7684\u6613\u6269\u5c55\u6027\uff09\u3002 model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)) \u73b0\u5728\uff0c\u4f60\u53ef\u4ee5\u6279\u91cf\u5730\u5728\u8bad\u7ec3\u6570\u636e\u4e0a\u8fdb\u884c\u8fed\u4ee3\u4e86\uff1a # x_train \u548c y_train \u662f Numpy \u6570\u7ec4 -- \u5c31\u50cf\u5728 Scikit-Learn API \u4e2d\u4e00\u6837\u3002 model.fit(x_train, y_train, epochs=5, batch_size=32) \u6216\u8005\uff0c\u4f60\u53ef\u4ee5\u624b\u52a8\u5730\u5c06\u6279\u6b21\u7684\u6570\u636e\u63d0\u4f9b\u7ed9\u6a21\u578b\uff1a model.train_on_batch(x_batch, y_batch) \u53ea\u9700\u4e00\u884c\u4ee3\u7801\u5c31\u80fd\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff1a loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128) \u6216\u8005\u5bf9\u65b0\u7684\u6570\u636e\u751f\u6210\u9884\u6d4b\uff1a classes = model.predict(x_test, batch_size=128) \u6784\u5efa\u4e00\u4e2a\u95ee\u7b54\u7cfb\u7edf\uff0c\u4e00\u4e2a\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\uff0c\u4e00\u4e2a\u795e\u7ecf\u56fe\u7075\u673a\uff0c\u6216\u8005\u5176\u4ed6\u7684\u4efb\u4f55\u6a21\u578b\uff0c\u5c31\u662f\u8fd9\u4e48\u7684\u5feb\u3002\u6df1\u5ea6\u5b66\u4e60\u80cc\u540e\u7684\u601d\u60f3\u5f88\u7b80\u5355\uff0c\u90a3\u4e48\u5b83\u4eec\u7684\u5b9e\u73b0\u53c8\u4f55\u5fc5\u8981\u90a3\u4e48\u75db\u82e6\u5462\uff1f \u6709\u5173 Keras \u66f4\u6df1\u5165\u7684\u6559\u7a0b\uff0c\u8bf7\u67e5\u770b\uff1a \u5f00\u59cb\u4f7f\u7528 Sequential \u6a21\u578b \u5f00\u59cb\u4f7f\u7528\u51fd\u6570\u5f0f API \u5728\u4ee3\u7801\u4ed3\u5e93\u7684 examples \u76ee\u5f55 \u4e2d\uff0c\u4f60\u4f1a\u627e\u5230\u66f4\u591a\u9ad8\u7ea7\u6a21\u578b\uff1a\u57fa\u4e8e\u8bb0\u5fc6\u7f51\u7edc\u7684\u95ee\u7b54\u7cfb\u7edf\u3001\u57fa\u4e8e\u6808\u5f0f LSTM \u7684\u6587\u672c\u751f\u6210\u7b49\u7b49\u3002","title":"\u5feb\u901f\u5f00\u59cb\uff1a30 \u79d2\u4e0a\u624b Keras"},{"location":"#_2","text":"\u5728\u5b89\u88c5 Keras \u4e4b\u524d\uff0c\u8bf7\u5b89\u88c5\u4ee5\u4e0b\u540e\u7aef\u5f15\u64ce\u4e4b\u4e00\uff1aTensorFlow\uff0cTheano\uff0c\u6216\u8005 CNTK\u3002\u6211\u4eec\u63a8\u8350 TensorFlow \u540e\u7aef\u3002 TensorFlow \u5b89\u88c5\u6307\u5f15 \u3002 Theano \u5b89\u88c5\u6307\u5f15 \u3002 CNTK \u5b89\u88c5\u6307\u5f15 \u3002 \u4f60\u4e5f\u53ef\u4ee5\u8003\u8651\u5b89\u88c5\u4ee5\u4e0b \u53ef\u9009\u4f9d\u8d56 \uff1a cuDNN (\u5982\u679c\u4f60\u8ba1\u5212\u5728 GPU \u4e0a\u8fd0\u884c Keras\uff0c\u5efa\u8bae\u5b89\u88c5)\u3002 HDF5 \u548c h5py (\u5982\u679c\u4f60\u9700\u8981\u5c06 Keras \u6a21\u578b\u4fdd\u5b58\u5230\u78c1\u76d8\uff0c\u5219\u9700\u8981\u8fd9\u4e9b)\u3002 graphviz \u548c pydot (\u7528\u4e8e \u53ef\u89c6\u5316\u5de5\u5177 \u7ed8\u5236\u6a21\u578b\u56fe)\u3002 \u7136\u540e\u4f60\u5c31\u53ef\u4ee5\u5b89\u88c5 Keras \u672c\u8eab\u4e86\u3002\u6709\u4e24\u79cd\u65b9\u6cd5\u5b89\u88c5 Keras\uff1a \u4f7f\u7528 PyPI \u5b89\u88c5 Keras (\u63a8\u8350)\uff1a sudo pip install keras \u5982\u679c\u4f60\u4f7f\u7528 virtualenv \u865a\u62df\u73af\u5883, \u4f60\u53ef\u4ee5\u907f\u514d\u4f7f\u7528 sudo\uff1a pip install keras \u6216\u8005\uff1a\u4f7f\u7528 GitHub \u6e90\u7801\u5b89\u88c5 Keras\uff1a \u9996\u5148\uff0c\u4f7f\u7528 git \u6765\u514b\u9686 Keras\uff1a git clone https://github.com/keras-team/keras.git \u7136\u540e\uff0c cd \u5230 Keras \u76ee\u5f55\u5e76\u4e14\u8fd0\u884c\u5b89\u88c5\u547d\u4ee4\uff1a cd keras sudo python setup.py install","title":"\u5b89\u88c5\u6307\u5f15"},{"location":"#keras_2","text":"\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cKeras \u5c06\u4f7f\u7528 TensorFlow \u4f5c\u4e3a\u5176\u5f20\u91cf\u64cd\u4f5c\u5e93\u3002\u8bf7 \u8ddf\u968f\u8fd9\u4e9b\u6307\u5f15 \u6765\u914d\u7f6e\u5176\u4ed6 Keras \u540e\u7aef\u3002","title":"\u914d\u7f6e\u4f60\u7684 Keras \u540e\u7aef"},{"location":"#_3","text":"\u4f60\u53ef\u4ee5\u63d0\u51fa\u95ee\u9898\u5e76\u53c2\u4e0e\u5f00\u53d1\u8ba8\u8bba\uff1a Keras Google group \u3002 Keras Slack channel \u3002 \u4f7f\u7528 \u8fd9\u4e2a\u94fe\u63a5 \u5411\u8be5\u9891\u9053\u8bf7\u6c42\u9080\u8bf7\u51fd\u3002 \u6216\u8005\u52a0\u5165 Keras \u6df1\u5ea6\u5b66\u4e60\u4ea4\u6d41\u7fa4\uff0c\u534f\u52a9\u6587\u6863\u7684\u7ffb\u8bd1\u5de5\u4f5c\uff0c\u7fa4\u53f7\u4e3a 951623081\u3002 \u4f60\u4e5f\u53ef\u4ee5\u5728 GitHub issues \u4e2d\u53d1\u5e03 \u6f0f\u6d1e\u62a5\u544a\u548c\u65b0\u529f\u80fd\u8bf7\u6c42 \uff08\u4ec5\u9650\u4e8e\u6b64\uff09\u3002\u6ce8\u610f\u8bf7\u5148\u9605\u8bfb \u89c4\u8303\u6587\u6863 \u3002","title":"\u6280\u672f\u652f\u6301"},{"location":"#keras_3","text":"Keras (\u03ba\u03ad\u03c1\u03b1\u03c2) \u5728\u5e0c\u814a\u8bed\u4e2d\u610f\u4e3a \u53f7\u89d2 \u3002\u5b83\u6765\u81ea\u53e4\u5e0c\u814a\u548c\u62c9\u4e01\u6587\u5b66\u4e2d\u7684\u4e00\u4e2a\u6587\u5b66\u5f62\u8c61\uff0c\u9996\u5148\u51fa\u73b0\u4e8e \u300a\u5965\u5fb7\u8d5b\u300b \u4e2d\uff0c \u68a6\u795e ( Oneiroi , singular Oneiros ) \u4ece\u8fd9\u4e24\u7c7b\u4eba\u4e2d\u5206\u79bb\u51fa\u6765\uff1a\u90a3\u4e9b\u7528\u865a\u5e7b\u7684\u666f\u8c61\u6b3a\u9a97\u4eba\u7c7b\uff0c\u901a\u8fc7\u8c61\u7259\u4e4b\u95e8\u62b5\u8fbe\u5730\u7403\u4e4b\u4eba\uff0c\u4ee5\u53ca\u90a3\u4e9b\u5ba3\u544a\u672a\u6765\u5373\u5c06\u5230\u6765\uff0c\u901a\u8fc7\u53f7\u89d2\u4e4b\u95e8\u62b5\u8fbe\u4e4b\u4eba\u3002 \u5b83\u7c7b\u4f3c\u4e8e\u6587\u5b57\u5bd3\u610f\uff0c\u03ba\u03ad\u03c1\u03b1\u03c2 (\u53f7\u89d2) / \u03ba\u03c1\u03b1\u03af\u03bd\u03c9 (\u5c65\u884c)\uff0c\u4ee5\u53ca \u1f10\u03bb\u03ad\u03c6\u03b1\u03c2 (\u8c61\u7259) / \u1f10\u03bb\u03b5\u03c6\u03b1\u03af\u03c1\u03bf\u03bc\u03b1\u03b9 (\u6b3a\u9a97)\u3002 Keras \u6700\u521d\u662f\u4f5c\u4e3a ONEIROS \u9879\u76ee\uff08\u5f00\u653e\u5f0f\u795e\u7ecf\u7535\u5b50\u667a\u80fd\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\uff09\u7814\u7a76\u5de5\u4f5c\u7684\u4e00\u90e8\u5206\u800c\u5f00\u53d1\u7684\u3002 \"Oneiroi \u8d85\u51fa\u4e86\u6211\u4eec\u7684\u7406\u89e3 - \u8c01\u80fd\u786e\u5b9a\u5b83\u4eec\u8bb2\u8ff0\u4e86\u4ec0\u4e48\u6545\u4e8b\uff1f\u5e76\u4e0d\u662f\u6240\u6709\u4eba\u90fd\u80fd\u627e\u5230\u3002\u90a3\u91cc\u6709\u4e24\u6247\u95e8\uff0c\u5c31\u662f\u901a\u5f80\u77ed\u6682\u7684 Oneiroi \u7684\u901a\u9053\uff1b\u4e00\u4e2a\u662f\u7528\u53f7\u89d2\u5236\u9020\u7684\uff0c\u4e00\u4e2a\u662f\u7528\u8c61\u7259\u5236\u9020\u7684\u3002\u7a7f\u8fc7\u5c16\u9510\u7684\u8c61\u7259\u7684 Oneiroi \u662f\u8be1\u8ba1\u591a\u7aef\u7684\uff0c\u4ed6\u4eec\u5e26\u6709\u4e00\u4e9b\u4e0d\u4f1a\u5b9e\u73b0\u7684\u4fe1\u606f\uff1b \u90a3\u4e9b\u7a7f\u8fc7\u629b\u5149\u7684\u5587\u53ed\u51fa\u6765\u7684\u4eba\u80cc\u540e\u5177\u6709\u771f\u7406\uff0c\u5bf9\u4e8e\u770b\u5230\u4ed6\u4eec\u7684\u4eba\u6765\u8bf4\u662f\u5b8c\u6210\u7684\u3002\" Homer, Odyssey 19. 562 ff (Shewring translation).","title":"\u4e3a\u4ec0\u4e48\u53d6\u540d\u4e3a Keras?"},{"location":"1.why-use-keras/","text":"\u4e3a\u4ec0\u4e48\u9009\u62e9 Keras\uff1f \u5728\u5982\u4eca\u65e0\u6570\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u4e3a\u4ec0\u4e48\u8981\u4f7f\u7528 Keras \u800c\u975e\u5176\u4ed6\uff1f\u4ee5\u4e0b\u662f Keras \u4e0e\u73b0\u6709\u66ff\u4ee3\u54c1\u7684\u4e00\u4e9b\u6bd4\u8f83\u3002 Keras \u4f18\u5148\u8003\u8651\u5f00\u53d1\u4eba\u5458\u7684\u7ecf\u9a8c Keras \u662f\u4e3a\u4eba\u7c7b\u800c\u975e\u673a\u5668\u8bbe\u8ba1\u7684 API\u3002 Keras \u9075\u5faa\u51cf\u5c11\u8ba4\u77e5\u56f0\u96be\u7684\u6700\u4f73\u5b9e\u8df5 : \u5b83\u63d0\u4f9b\u4e00\u81f4\u4e14\u7b80\u5355\u7684 API\uff0c\u5b83\u5c06\u5e38\u89c1\u7528\u4f8b\u6240\u9700\u7684\u7528\u6237\u64cd\u4f5c\u6570\u91cf\u964d\u81f3\u6700\u4f4e\uff0c\u5e76\u4e14\u5728\u7528\u6237\u9519\u8bef\u65f6\u63d0\u4f9b\u6e05\u6670\u548c\u53ef\u64cd\u4f5c\u7684\u53cd\u9988\u3002 \u8fd9\u4f7f Keras \u6613\u4e8e\u5b66\u4e60\u548c\u4f7f\u7528\u3002\u4f5c\u4e3a Keras \u7528\u6237\uff0c\u4f60\u7684\u5de5\u4f5c\u6548\u7387\u66f4\u9ad8\uff0c\u80fd\u591f\u6bd4\u7ade\u4e89\u5bf9\u624b\u66f4\u5feb\u5730\u5c1d\u8bd5\u66f4\u591a\u521b\u610f\uff0c\u4ece\u800c \u5e2e\u52a9\u4f60\u8d62\u5f97\u673a\u5668\u5b66\u4e60\u7ade\u8d5b \u3002 \u8fd9\u79cd\u6613\u7528\u6027\u5e76\u4e0d\u4ee5\u964d\u4f4e\u7075\u6d3b\u6027\u4e3a\u4ee3\u4ef7\uff1a\u56e0\u4e3a Keras \u4e0e\u5e95\u5c42\u6df1\u5ea6\u5b66\u4e60\u8bed\u8a00\uff08\u7279\u522b\u662f TensorFlow\uff09\u96c6\u6210\u5728\u4e00\u8d77\uff0c\u6240\u4ee5\u5b83\u53ef\u4ee5\u8ba9\u4f60\u5b9e\u73b0\u4efb\u4f55\u4f60\u53ef\u4ee5\u7528\u57fa\u7840\u8bed\u8a00\u7f16\u5199\u7684\u4e1c\u897f\u3002\u7279\u522b\u662f\uff0c tf.keras \u4f5c\u4e3a Keras API \u53ef\u4ee5\u4e0e TensorFlow \u5de5\u4f5c\u6d41\u65e0\u7f1d\u96c6\u6210\u3002 Keras \u88ab\u5de5\u4e1a\u754c\u548c\u5b66\u672f\u754c\u5e7f\u6cdb\u91c7\u7528 Deep learning \u6846\u67b6\u6392\u540d\uff0c\u7531 Jeff Hale \u57fa\u4e8e 7 \u4e2a\u5206\u7c7b\u7684 11 \u4e2a\u6570\u636e\u6e90\u8ba1\u7b97\u5f97\u51fa \u622a\u81f3 2018 \u5e74\u4e2d\u671f\uff0cKeras \u62e5\u6709\u8d85\u8fc7 250,000 \u540d\u4e2a\u4eba\u7528\u6237\u3002\u4e0e\u5176\u4ed6\u4efb\u4f55\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u76f8\u6bd4\uff0cKeras \u5728\u884c\u4e1a\u548c\u7814\u7a76\u9886\u57df\u7684\u5e94\u7528\u7387\u66f4\u9ad8\uff08\u9664 TensorFlow \u4e4b\u5916\uff0c\u4e14 Keras API \u662f TensorFlow \u7684\u5b98\u65b9\u524d\u7aef\uff0c\u901a\u8fc7 tf.keras \u6a21\u5757\u4f7f\u7528\uff09\u3002 \u60a8\u5df2\u7ecf\u4e0d\u65ad\u4e0e\u4f7f\u7528 Keras \u6784\u5efa\u7684\u529f\u80fd\u8fdb\u884c\u4ea4\u4e92 - \u5b83\u5728 Netflix, Uber, Yelp, Instacart, Zocdoc, Square \u7b49\u4f17\u591a\u7f51\u7ad9\u4e0a\u4f7f\u7528\u3002\u5b83\u5c24\u5176\u53d7\u4ee5\u6df1\u5ea6\u5b66\u4e60\u4f5c\u4e3a\u4ea7\u54c1\u6838\u5fc3\u7684\u521b\u4e1a\u516c\u53f8\u7684\u6b22\u8fce\u3002 Keras\u4e5f\u662f\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u4eba\u5458\u7684\u6700\u7231\uff0c\u5728\u4e0a\u8f7d\u5230\u9884\u5370\u672c\u670d\u52a1\u5668 arXiv.org \u7684\u79d1\u5b66\u8bba\u6587\u4e2d\u88ab\u63d0\u53ca\u7684\u6b21\u6570\u4f4d\u5c45\u7b2c\u4e8c\u3002Keras \u8fd8\u88ab\u5927\u578b\u79d1\u5b66\u7ec4\u7ec7\u7684\u7814\u7a76\u4eba\u5458\u91c7\u7528\uff0c\u7279\u522b\u662f CERN \u548c NASA\u3002 Keras \u53ef\u4ee5\u8f7b\u677e\u5c06\u6a21\u578b\u8f6c\u5316\u4e3a\u4ea7\u54c1 \u4e0e\u4efb\u4f55\u5176\u4ed6\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u76f8\u6bd4\uff0c\u4f60\u7684 Keras \u6a21\u578b\u53ef\u4ee5\u5728\u66f4\u5e7f\u6cdb\u7684\u5e73\u53f0\u4e0a\u8f7b\u677e\u90e8\u7f72\uff1a \u5728 iOS \u4e0a\uff0c\u901a\u8fc7 Apple\u2019s CoreML \uff08\u82f9\u679c\u4e3a Keras \u63d0\u4f9b\u5b98\u65b9\u652f\u6301\uff09\u3002\u8fd9\u91cc\u6709\u4e00\u4e2a \u6559\u7a0b \u3002 \u5728 Android \u4e0a\uff0c\u901a\u8fc7 TensorFlow Android runtime\uff0c\u4f8b\u5982\uff1a Not Hotdog app \u3002 \u5728\u6d4f\u89c8\u5668\u4e2d\uff0c\u901a\u8fc7 GPU \u52a0\u901f\u7684 JavaScript \u8fd0\u884c\u65f6\uff0c\u4f8b\u5982\uff1a Keras.js \u548c WebDNN \u3002 \u5728 Google Cloud \u4e0a\uff0c\u901a\u8fc7 TensorFlow-Serving \u3002 \u5728 Python webapp \u540e\u7aef\uff08\u6bd4\u5982 Flask app\uff09\u4e2d \u3002 \u5728 JVM \u4e0a\uff0c\u901a\u8fc7 SkyMind \u63d0\u4f9b\u7684 DL4J \u6a21\u578b\u5bfc\u5165 \u3002 \u5728 Raspberry Pi \u6811\u8393\u6d3e\u4e0a\u3002 Keras \u652f\u6301\u591a\u4e2a\u540e\u7aef\u5f15\u64ce\uff0c\u4e0d\u4f1a\u5c06\u4f60\u9501\u5b9a\u5230\u4e00\u4e2a\u751f\u6001\u7cfb\u7edf\u4e2d \u4f60\u7684 Keras \u6a21\u578b\u53ef\u4ee5\u57fa\u4e8e\u4e0d\u540c\u7684 \u6df1\u5ea6\u5b66\u4e60\u540e\u7aef \u5f00\u53d1\u3002\u91cd\u8981\u7684\u662f\uff0c\u4efb\u4f55\u4ec5\u5229\u7528\u5185\u7f6e\u5c42\u6784\u5efa\u7684 Keras \u6a21\u578b\uff0c\u90fd\u53ef\u4ee5\u5728\u6240\u6709\u8fd9\u4e9b\u540e\u7aef\u4e2d\u79fb\u690d\uff1a\u4f60\u53ef\u4ee5\u7528\u4e00\u79cd\u540e\u7aef\u8bad\u7ec3\u6a21\u578b\uff0c\u518d\u5c06\u5b83\u8f7d\u5165\u53e6\u4e00\u79cd\u540e\u7aef\u4e2d\uff08\u4f8b\u5982\u4e3a\u4e86\u53d1\u5e03\u7684\u9700\u8981\uff09\u3002\u652f\u6301\u7684\u540e\u7aef\u6709\uff1a \u8c37\u6b4c\u7684 TensorFlow \u540e\u7aef \u5fae\u8f6f\u7684 CNTK \u540e\u7aef Theano \u540e\u7aef \u4e9a\u9a6c\u900a\u4e5f\u6b63\u5728\u4e3a Keras \u5f00\u53d1 MXNet \u540e\u7aef\u3002 \u5982\u6b64\u4e00\u6765\uff0c\u4f60\u7684 Keras \u6a21\u578b\u53ef\u4ee5\u5728 CPU \u4e4b\u5916\u7684\u4e0d\u540c\u786c\u4ef6\u5e73\u53f0\u4e0a\u8bad\u7ec3\uff1a NVIDIA GPU Google TPU \uff0c\u901a\u8fc7 TensorFlow \u540e\u7aef\u548c Google Cloud OpenCL \u652f\u6301\u7684 GPU, \u6bd4\u5982 AMD, \u901a\u8fc7 PlaidML Keras \u540e\u7aef Keras \u62e5\u6709\u5f3a\u5927\u7684\u591a GPU \u548c\u5206\u5e03\u5f0f\u8bad\u7ec3\u652f\u6301 Keras \u5185\u7f6e\u5bf9\u591a GPU \u6570\u636e\u5e76\u884c\u7684\u652f\u6301 \u3002 \u4f18\u6b65\u7684 Horovod \u5bf9 Keras \u6a21\u578b\u62e5\u6709\u4e00\u6d41\u7684\u652f\u6301\u3002 Keras \u6a21\u578b \u53ef\u4ee5\u88ab\u8f6c\u6362\u4e3a TensorFlow Estimators \u5e76\u5728 Google Cloud \u7684 GPU \u96c6\u7fa4 \u4e0a\u8bad\u7ec3\u3002 Keras \u53ef\u4ee5\u5728 Spark\uff08\u901a\u8fc7 CERN \u7684 Dist-Keras \uff09\u548c Elephas \u4e0a\u8fd0\u884c\u3002 Keras \u7684\u53d1\u5c55\u5f97\u5230\u6df1\u5ea6\u5b66\u4e60\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5173\u952e\u516c\u53f8\u7684\u652f\u6301 Keras \u7684\u5f00\u53d1\u4e3b\u8981\u7531\u8c37\u6b4c\u652f\u6301\uff0cKeras API \u4ee5 tf.keras \u7684\u5f62\u5f0f\u5305\u88c5\u5728 TensorFlow \u4e2d\u3002\u6b64\u5916\uff0c\u5fae\u8f6f\u7ef4\u62a4\u7740 Keras \u7684 CNTK \u540e\u7aef\u3002\u4e9a\u9a6c\u900a AWS \u6b63\u5728\u5f00\u53d1 MXNet \u652f\u6301\u3002\u5176\u4ed6\u63d0\u4f9b\u652f\u6301\u7684\u516c\u53f8\u5305\u62ec NVIDIA\u3001\u4f18\u6b65\u3001\u82f9\u679c\uff08\u901a\u8fc7 CoreML\uff09\u7b49\u3002","title":"\u4e3a\u4ec0\u4e48\u9009\u62e9 Keras\uff1f"},{"location":"1.why-use-keras/#keras","text":"\u5728\u5982\u4eca\u65e0\u6570\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u4e3a\u4ec0\u4e48\u8981\u4f7f\u7528 Keras \u800c\u975e\u5176\u4ed6\uff1f\u4ee5\u4e0b\u662f Keras \u4e0e\u73b0\u6709\u66ff\u4ee3\u54c1\u7684\u4e00\u4e9b\u6bd4\u8f83\u3002","title":"\u4e3a\u4ec0\u4e48\u9009\u62e9 Keras\uff1f"},{"location":"1.why-use-keras/#keras_1","text":"Keras \u662f\u4e3a\u4eba\u7c7b\u800c\u975e\u673a\u5668\u8bbe\u8ba1\u7684 API\u3002 Keras \u9075\u5faa\u51cf\u5c11\u8ba4\u77e5\u56f0\u96be\u7684\u6700\u4f73\u5b9e\u8df5 : \u5b83\u63d0\u4f9b\u4e00\u81f4\u4e14\u7b80\u5355\u7684 API\uff0c\u5b83\u5c06\u5e38\u89c1\u7528\u4f8b\u6240\u9700\u7684\u7528\u6237\u64cd\u4f5c\u6570\u91cf\u964d\u81f3\u6700\u4f4e\uff0c\u5e76\u4e14\u5728\u7528\u6237\u9519\u8bef\u65f6\u63d0\u4f9b\u6e05\u6670\u548c\u53ef\u64cd\u4f5c\u7684\u53cd\u9988\u3002 \u8fd9\u4f7f Keras \u6613\u4e8e\u5b66\u4e60\u548c\u4f7f\u7528\u3002\u4f5c\u4e3a Keras \u7528\u6237\uff0c\u4f60\u7684\u5de5\u4f5c\u6548\u7387\u66f4\u9ad8\uff0c\u80fd\u591f\u6bd4\u7ade\u4e89\u5bf9\u624b\u66f4\u5feb\u5730\u5c1d\u8bd5\u66f4\u591a\u521b\u610f\uff0c\u4ece\u800c \u5e2e\u52a9\u4f60\u8d62\u5f97\u673a\u5668\u5b66\u4e60\u7ade\u8d5b \u3002 \u8fd9\u79cd\u6613\u7528\u6027\u5e76\u4e0d\u4ee5\u964d\u4f4e\u7075\u6d3b\u6027\u4e3a\u4ee3\u4ef7\uff1a\u56e0\u4e3a Keras \u4e0e\u5e95\u5c42\u6df1\u5ea6\u5b66\u4e60\u8bed\u8a00\uff08\u7279\u522b\u662f TensorFlow\uff09\u96c6\u6210\u5728\u4e00\u8d77\uff0c\u6240\u4ee5\u5b83\u53ef\u4ee5\u8ba9\u4f60\u5b9e\u73b0\u4efb\u4f55\u4f60\u53ef\u4ee5\u7528\u57fa\u7840\u8bed\u8a00\u7f16\u5199\u7684\u4e1c\u897f\u3002\u7279\u522b\u662f\uff0c tf.keras \u4f5c\u4e3a Keras API \u53ef\u4ee5\u4e0e TensorFlow \u5de5\u4f5c\u6d41\u65e0\u7f1d\u96c6\u6210\u3002","title":"Keras \u4f18\u5148\u8003\u8651\u5f00\u53d1\u4eba\u5458\u7684\u7ecf\u9a8c"},{"location":"1.why-use-keras/#keras_2","text":"Deep learning \u6846\u67b6\u6392\u540d\uff0c\u7531 Jeff Hale \u57fa\u4e8e 7 \u4e2a\u5206\u7c7b\u7684 11 \u4e2a\u6570\u636e\u6e90\u8ba1\u7b97\u5f97\u51fa \u622a\u81f3 2018 \u5e74\u4e2d\u671f\uff0cKeras \u62e5\u6709\u8d85\u8fc7 250,000 \u540d\u4e2a\u4eba\u7528\u6237\u3002\u4e0e\u5176\u4ed6\u4efb\u4f55\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u76f8\u6bd4\uff0cKeras \u5728\u884c\u4e1a\u548c\u7814\u7a76\u9886\u57df\u7684\u5e94\u7528\u7387\u66f4\u9ad8\uff08\u9664 TensorFlow \u4e4b\u5916\uff0c\u4e14 Keras API \u662f TensorFlow \u7684\u5b98\u65b9\u524d\u7aef\uff0c\u901a\u8fc7 tf.keras \u6a21\u5757\u4f7f\u7528\uff09\u3002 \u60a8\u5df2\u7ecf\u4e0d\u65ad\u4e0e\u4f7f\u7528 Keras \u6784\u5efa\u7684\u529f\u80fd\u8fdb\u884c\u4ea4\u4e92 - \u5b83\u5728 Netflix, Uber, Yelp, Instacart, Zocdoc, Square \u7b49\u4f17\u591a\u7f51\u7ad9\u4e0a\u4f7f\u7528\u3002\u5b83\u5c24\u5176\u53d7\u4ee5\u6df1\u5ea6\u5b66\u4e60\u4f5c\u4e3a\u4ea7\u54c1\u6838\u5fc3\u7684\u521b\u4e1a\u516c\u53f8\u7684\u6b22\u8fce\u3002 Keras\u4e5f\u662f\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u4eba\u5458\u7684\u6700\u7231\uff0c\u5728\u4e0a\u8f7d\u5230\u9884\u5370\u672c\u670d\u52a1\u5668 arXiv.org \u7684\u79d1\u5b66\u8bba\u6587\u4e2d\u88ab\u63d0\u53ca\u7684\u6b21\u6570\u4f4d\u5c45\u7b2c\u4e8c\u3002Keras \u8fd8\u88ab\u5927\u578b\u79d1\u5b66\u7ec4\u7ec7\u7684\u7814\u7a76\u4eba\u5458\u91c7\u7528\uff0c\u7279\u522b\u662f CERN \u548c NASA\u3002","title":"Keras \u88ab\u5de5\u4e1a\u754c\u548c\u5b66\u672f\u754c\u5e7f\u6cdb\u91c7\u7528"},{"location":"1.why-use-keras/#keras_3","text":"\u4e0e\u4efb\u4f55\u5176\u4ed6\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u76f8\u6bd4\uff0c\u4f60\u7684 Keras \u6a21\u578b\u53ef\u4ee5\u5728\u66f4\u5e7f\u6cdb\u7684\u5e73\u53f0\u4e0a\u8f7b\u677e\u90e8\u7f72\uff1a \u5728 iOS \u4e0a\uff0c\u901a\u8fc7 Apple\u2019s CoreML \uff08\u82f9\u679c\u4e3a Keras \u63d0\u4f9b\u5b98\u65b9\u652f\u6301\uff09\u3002\u8fd9\u91cc\u6709\u4e00\u4e2a \u6559\u7a0b \u3002 \u5728 Android \u4e0a\uff0c\u901a\u8fc7 TensorFlow Android runtime\uff0c\u4f8b\u5982\uff1a Not Hotdog app \u3002 \u5728\u6d4f\u89c8\u5668\u4e2d\uff0c\u901a\u8fc7 GPU \u52a0\u901f\u7684 JavaScript \u8fd0\u884c\u65f6\uff0c\u4f8b\u5982\uff1a Keras.js \u548c WebDNN \u3002 \u5728 Google Cloud \u4e0a\uff0c\u901a\u8fc7 TensorFlow-Serving \u3002 \u5728 Python webapp \u540e\u7aef\uff08\u6bd4\u5982 Flask app\uff09\u4e2d \u3002 \u5728 JVM \u4e0a\uff0c\u901a\u8fc7 SkyMind \u63d0\u4f9b\u7684 DL4J \u6a21\u578b\u5bfc\u5165 \u3002 \u5728 Raspberry Pi \u6811\u8393\u6d3e\u4e0a\u3002","title":"Keras \u53ef\u4ee5\u8f7b\u677e\u5c06\u6a21\u578b\u8f6c\u5316\u4e3a\u4ea7\u54c1"},{"location":"1.why-use-keras/#keras_4","text":"\u4f60\u7684 Keras \u6a21\u578b\u53ef\u4ee5\u57fa\u4e8e\u4e0d\u540c\u7684 \u6df1\u5ea6\u5b66\u4e60\u540e\u7aef \u5f00\u53d1\u3002\u91cd\u8981\u7684\u662f\uff0c\u4efb\u4f55\u4ec5\u5229\u7528\u5185\u7f6e\u5c42\u6784\u5efa\u7684 Keras \u6a21\u578b\uff0c\u90fd\u53ef\u4ee5\u5728\u6240\u6709\u8fd9\u4e9b\u540e\u7aef\u4e2d\u79fb\u690d\uff1a\u4f60\u53ef\u4ee5\u7528\u4e00\u79cd\u540e\u7aef\u8bad\u7ec3\u6a21\u578b\uff0c\u518d\u5c06\u5b83\u8f7d\u5165\u53e6\u4e00\u79cd\u540e\u7aef\u4e2d\uff08\u4f8b\u5982\u4e3a\u4e86\u53d1\u5e03\u7684\u9700\u8981\uff09\u3002\u652f\u6301\u7684\u540e\u7aef\u6709\uff1a \u8c37\u6b4c\u7684 TensorFlow \u540e\u7aef \u5fae\u8f6f\u7684 CNTK \u540e\u7aef Theano \u540e\u7aef \u4e9a\u9a6c\u900a\u4e5f\u6b63\u5728\u4e3a Keras \u5f00\u53d1 MXNet \u540e\u7aef\u3002 \u5982\u6b64\u4e00\u6765\uff0c\u4f60\u7684 Keras \u6a21\u578b\u53ef\u4ee5\u5728 CPU \u4e4b\u5916\u7684\u4e0d\u540c\u786c\u4ef6\u5e73\u53f0\u4e0a\u8bad\u7ec3\uff1a NVIDIA GPU Google TPU \uff0c\u901a\u8fc7 TensorFlow \u540e\u7aef\u548c Google Cloud OpenCL \u652f\u6301\u7684 GPU, \u6bd4\u5982 AMD, \u901a\u8fc7 PlaidML Keras \u540e\u7aef","title":"Keras \u652f\u6301\u591a\u4e2a\u540e\u7aef\u5f15\u64ce\uff0c\u4e0d\u4f1a\u5c06\u4f60\u9501\u5b9a\u5230\u4e00\u4e2a\u751f\u6001\u7cfb\u7edf\u4e2d"},{"location":"1.why-use-keras/#keras-gpu","text":"Keras \u5185\u7f6e\u5bf9\u591a GPU \u6570\u636e\u5e76\u884c\u7684\u652f\u6301 \u3002 \u4f18\u6b65\u7684 Horovod \u5bf9 Keras \u6a21\u578b\u62e5\u6709\u4e00\u6d41\u7684\u652f\u6301\u3002 Keras \u6a21\u578b \u53ef\u4ee5\u88ab\u8f6c\u6362\u4e3a TensorFlow Estimators \u5e76\u5728 Google Cloud \u7684 GPU \u96c6\u7fa4 \u4e0a\u8bad\u7ec3\u3002 Keras \u53ef\u4ee5\u5728 Spark\uff08\u901a\u8fc7 CERN \u7684 Dist-Keras \uff09\u548c Elephas \u4e0a\u8fd0\u884c\u3002","title":"Keras \u62e5\u6709\u5f3a\u5927\u7684\u591a GPU \u548c\u5206\u5e03\u5f0f\u8bad\u7ec3\u652f\u6301"},{"location":"1.why-use-keras/#keras_5","text":"Keras \u7684\u5f00\u53d1\u4e3b\u8981\u7531\u8c37\u6b4c\u652f\u6301\uff0cKeras API \u4ee5 tf.keras \u7684\u5f62\u5f0f\u5305\u88c5\u5728 TensorFlow \u4e2d\u3002\u6b64\u5916\uff0c\u5fae\u8f6f\u7ef4\u62a4\u7740 Keras \u7684 CNTK \u540e\u7aef\u3002\u4e9a\u9a6c\u900a AWS \u6b63\u5728\u5f00\u53d1 MXNet \u652f\u6301\u3002\u5176\u4ed6\u63d0\u4f9b\u652f\u6301\u7684\u516c\u53f8\u5305\u62ec NVIDIA\u3001\u4f18\u6b65\u3001\u82f9\u679c\uff08\u901a\u8fc7 CoreML\uff09\u7b49\u3002","title":"Keras \u7684\u53d1\u5c55\u5f97\u5230\u6df1\u5ea6\u5b66\u4e60\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5173\u952e\u516c\u53f8\u7684\u652f\u6301"},{"location":"2.losses/","text":"\u635f\u5931\u51fd\u6570 Losses \u635f\u5931\u51fd\u6570\u7684\u4f7f\u7528 \u635f\u5931\u51fd\u6570\uff08\u6216\u79f0\u76ee\u6807\u51fd\u6570\u3001\u4f18\u5316\u8bc4\u5206\u51fd\u6570\uff09\u662f\u7f16\u8bd1\u6a21\u578b\u65f6\u6240\u9700\u7684\u4e24\u4e2a\u53c2\u6570\u4e4b\u4e00\uff1a model.compile(loss='mean_squared_error', optimizer='sgd') from keras import losses model.compile(loss=losses.mean_squared_error, optimizer='sgd') \u4f60\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u73b0\u6709\u7684\u635f\u5931\u51fd\u6570\u540d\uff0c\u6216\u8005\u4e00\u4e2a TensorFlow/Theano \u7b26\u53f7\u51fd\u6570\u3002 \u8be5\u7b26\u53f7\u51fd\u6570\u4e3a\u6bcf\u4e2a\u6570\u636e\u70b9\u8fd4\u56de\u4e00\u4e2a\u6807\u91cf\uff0c\u6709\u4ee5\u4e0b\u4e24\u4e2a\u53c2\u6570: y_true : \u771f\u5b9e\u6807\u7b7e\u3002TensorFlow/Theano \u5f20\u91cf\u3002 y_pred : \u9884\u6d4b\u503c\u3002TensorFlow/Theano \u5f20\u91cf\uff0c\u5176 shape \u4e0e y_true \u76f8\u540c\u3002 \u5b9e\u9645\u7684\u4f18\u5316\u76ee\u6807\u662f\u6240\u6709\u6570\u636e\u70b9\u7684\u8f93\u51fa\u6570\u7ec4\u7684\u5e73\u5747\u503c\u3002 \u6709\u5173\u8fd9\u4e9b\u51fd\u6570\u7684\u51e0\u4e2a\u4f8b\u5b50\uff0c\u8bf7\u67e5\u770b losses source \u3002 \u53ef\u7528\u635f\u5931\u51fd\u6570 mean_squared_error mean_squared_error(y_true, y_pred) mean_absolute_error mean_absolute_error(y_true, y_pred) mean_absolute_percentage_error mean_absolute_percentage_error(y_true, y_pred) mean_squared_logarithmic_error mean_squared_logarithmic_error(y_true, y_pred) squared_hinge squared_hinge(y_true, y_pred) hinge hinge(y_true, y_pred) categorical_hinge categorical_hinge(y_true, y_pred) logcosh logcosh(y_true, y_pred) \u9884\u6d4b\u8bef\u5dee\u7684\u53cc\u66f2\u4f59\u5f26\u7684\u5bf9\u6570\u3002 \u5bf9\u4e8e\u5c0f\u7684 x \uff0c log(cosh(x)) \u8fd1\u4f3c\u7b49\u4e8e (x ** 2) / 2 \u3002\u5bf9\u4e8e\u5927\u7684 x \uff0c\u8fd1\u4f3c\u4e8e abs(x) - log(2) \u3002\u8fd9\u8868\u793a 'logcosh' \u4e0e\u5747\u65b9\u8bef\u5dee\u5927\u81f4\u76f8\u540c\uff0c\u4f46\u662f\u4e0d\u4f1a\u53d7\u5230\u5076\u5c14\u75af\u72c2\u7684\u9519\u8bef\u9884\u6d4b\u7684\u5f3a\u70c8\u5f71\u54cd\u3002 \u53c2\u6570 y_true : \u76ee\u6807\u771f\u5b9e\u503c\u7684\u5f20\u91cf\u3002 y_pred : \u76ee\u6807\u9884\u6d4b\u503c\u7684\u5f20\u91cf\u3002 \u8fd4\u56de \u6bcf\u4e2a\u6837\u672c\u90fd\u6709\u4e00\u4e2a\u6807\u91cf\u635f\u5931\u7684\u5f20\u91cf\u3002 categorical_crossentropy categorical_crossentropy(y_true, y_pred) sparse_categorical_crossentropy sparse_categorical_crossentropy(y_true, y_pred) binary_crossentropy binary_crossentropy(y_true, y_pred) kullback_leibler_divergence kullback_leibler_divergence(y_true, y_pred) poisson poisson(y_true, y_pred) cosine_proximity cosine_proximity(y_true, y_pred) \u6ce8\u610f : \u5f53\u4f7f\u7528 categorical_crossentropy \u635f\u5931\u65f6\uff0c\u4f60\u7684\u76ee\u6807\u503c\u5e94\u8be5\u662f\u5206\u7c7b\u683c\u5f0f (\u5373\uff0c\u5982\u679c\u4f60\u6709 10 \u4e2a\u7c7b\uff0c\u6bcf\u4e2a\u6837\u672c\u7684\u76ee\u6807\u503c\u5e94\u8be5\u662f\u4e00\u4e2a 10 \u7ef4\u7684\u5411\u91cf\uff0c\u8fd9\u4e2a\u5411\u91cf\u9664\u4e86\u8868\u793a\u7c7b\u522b\u7684\u90a3\u4e2a\u7d22\u5f15\u4e3a 1\uff0c\u5176\u4ed6\u5747\u4e3a 0)\u3002 \u4e3a\u4e86\u5c06 \u6574\u6570\u76ee\u6807\u503c \u8f6c\u6362\u4e3a \u5206\u7c7b\u76ee\u6807\u503c \uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528 Keras \u5b9e\u7528\u51fd\u6570 to_categorical \uff1a from keras.utils.np_utils import to_categorical categorical_labels = to_categorical(int_labels, num_classes=None)","title":"\u635f\u5931\u51fd\u6570 Losses"},{"location":"2.losses/#losses","text":"","title":"\u635f\u5931\u51fd\u6570 Losses"},{"location":"2.losses/#_1","text":"\u635f\u5931\u51fd\u6570\uff08\u6216\u79f0\u76ee\u6807\u51fd\u6570\u3001\u4f18\u5316\u8bc4\u5206\u51fd\u6570\uff09\u662f\u7f16\u8bd1\u6a21\u578b\u65f6\u6240\u9700\u7684\u4e24\u4e2a\u53c2\u6570\u4e4b\u4e00\uff1a model.compile(loss='mean_squared_error', optimizer='sgd') from keras import losses model.compile(loss=losses.mean_squared_error, optimizer='sgd') \u4f60\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u73b0\u6709\u7684\u635f\u5931\u51fd\u6570\u540d\uff0c\u6216\u8005\u4e00\u4e2a TensorFlow/Theano \u7b26\u53f7\u51fd\u6570\u3002 \u8be5\u7b26\u53f7\u51fd\u6570\u4e3a\u6bcf\u4e2a\u6570\u636e\u70b9\u8fd4\u56de\u4e00\u4e2a\u6807\u91cf\uff0c\u6709\u4ee5\u4e0b\u4e24\u4e2a\u53c2\u6570: y_true : \u771f\u5b9e\u6807\u7b7e\u3002TensorFlow/Theano \u5f20\u91cf\u3002 y_pred : \u9884\u6d4b\u503c\u3002TensorFlow/Theano \u5f20\u91cf\uff0c\u5176 shape \u4e0e y_true \u76f8\u540c\u3002 \u5b9e\u9645\u7684\u4f18\u5316\u76ee\u6807\u662f\u6240\u6709\u6570\u636e\u70b9\u7684\u8f93\u51fa\u6570\u7ec4\u7684\u5e73\u5747\u503c\u3002 \u6709\u5173\u8fd9\u4e9b\u51fd\u6570\u7684\u51e0\u4e2a\u4f8b\u5b50\uff0c\u8bf7\u67e5\u770b losses source \u3002","title":"\u635f\u5931\u51fd\u6570\u7684\u4f7f\u7528"},{"location":"2.losses/#_2","text":"","title":"\u53ef\u7528\u635f\u5931\u51fd\u6570"},{"location":"2.losses/#mean_squared_error","text":"mean_squared_error(y_true, y_pred)","title":"mean_squared_error"},{"location":"2.losses/#mean_absolute_error","text":"mean_absolute_error(y_true, y_pred)","title":"mean_absolute_error"},{"location":"2.losses/#mean_absolute_percentage_error","text":"mean_absolute_percentage_error(y_true, y_pred)","title":"mean_absolute_percentage_error"},{"location":"2.losses/#mean_squared_logarithmic_error","text":"mean_squared_logarithmic_error(y_true, y_pred)","title":"mean_squared_logarithmic_error"},{"location":"2.losses/#squared_hinge","text":"squared_hinge(y_true, y_pred)","title":"squared_hinge"},{"location":"2.losses/#hinge","text":"hinge(y_true, y_pred)","title":"hinge"},{"location":"2.losses/#categorical_hinge","text":"categorical_hinge(y_true, y_pred)","title":"categorical_hinge"},{"location":"2.losses/#logcosh","text":"logcosh(y_true, y_pred) \u9884\u6d4b\u8bef\u5dee\u7684\u53cc\u66f2\u4f59\u5f26\u7684\u5bf9\u6570\u3002 \u5bf9\u4e8e\u5c0f\u7684 x \uff0c log(cosh(x)) \u8fd1\u4f3c\u7b49\u4e8e (x ** 2) / 2 \u3002\u5bf9\u4e8e\u5927\u7684 x \uff0c\u8fd1\u4f3c\u4e8e abs(x) - log(2) \u3002\u8fd9\u8868\u793a 'logcosh' \u4e0e\u5747\u65b9\u8bef\u5dee\u5927\u81f4\u76f8\u540c\uff0c\u4f46\u662f\u4e0d\u4f1a\u53d7\u5230\u5076\u5c14\u75af\u72c2\u7684\u9519\u8bef\u9884\u6d4b\u7684\u5f3a\u70c8\u5f71\u54cd\u3002 \u53c2\u6570 y_true : \u76ee\u6807\u771f\u5b9e\u503c\u7684\u5f20\u91cf\u3002 y_pred : \u76ee\u6807\u9884\u6d4b\u503c\u7684\u5f20\u91cf\u3002 \u8fd4\u56de \u6bcf\u4e2a\u6837\u672c\u90fd\u6709\u4e00\u4e2a\u6807\u91cf\u635f\u5931\u7684\u5f20\u91cf\u3002","title":"logcosh"},{"location":"2.losses/#categorical_crossentropy","text":"categorical_crossentropy(y_true, y_pred)","title":"categorical_crossentropy"},{"location":"2.losses/#sparse_categorical_crossentropy","text":"sparse_categorical_crossentropy(y_true, y_pred)","title":"sparse_categorical_crossentropy"},{"location":"2.losses/#binary_crossentropy","text":"binary_crossentropy(y_true, y_pred)","title":"binary_crossentropy"},{"location":"2.losses/#kullback_leibler_divergence","text":"kullback_leibler_divergence(y_true, y_pred)","title":"kullback_leibler_divergence"},{"location":"2.losses/#poisson","text":"poisson(y_true, y_pred)","title":"poisson"},{"location":"2.losses/#cosine_proximity","text":"cosine_proximity(y_true, y_pred) \u6ce8\u610f : \u5f53\u4f7f\u7528 categorical_crossentropy \u635f\u5931\u65f6\uff0c\u4f60\u7684\u76ee\u6807\u503c\u5e94\u8be5\u662f\u5206\u7c7b\u683c\u5f0f (\u5373\uff0c\u5982\u679c\u4f60\u6709 10 \u4e2a\u7c7b\uff0c\u6bcf\u4e2a\u6837\u672c\u7684\u76ee\u6807\u503c\u5e94\u8be5\u662f\u4e00\u4e2a 10 \u7ef4\u7684\u5411\u91cf\uff0c\u8fd9\u4e2a\u5411\u91cf\u9664\u4e86\u8868\u793a\u7c7b\u522b\u7684\u90a3\u4e2a\u7d22\u5f15\u4e3a 1\uff0c\u5176\u4ed6\u5747\u4e3a 0)\u3002 \u4e3a\u4e86\u5c06 \u6574\u6570\u76ee\u6807\u503c \u8f6c\u6362\u4e3a \u5206\u7c7b\u76ee\u6807\u503c \uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528 Keras \u5b9e\u7528\u51fd\u6570 to_categorical \uff1a from keras.utils.np_utils import to_categorical categorical_labels = to_categorical(int_labels, num_classes=None)","title":"cosine_proximity"},{"location":"3.metrics/","text":"\u8bc4\u4f30\u6807\u51c6 Metrics \u8bc4\u4ef7\u51fd\u6570\u7684\u7528\u6cd5 \u8bc4\u4ef7\u51fd\u6570\u7528\u4e8e\u8bc4\u4f30\u5f53\u524d\u8bad\u7ec3\u6a21\u578b\u7684\u6027\u80fd\u3002\u5f53\u6a21\u578b\u7f16\u8bd1\u540e\uff08compile\uff09\uff0c\u8bc4\u4ef7\u51fd\u6570\u5e94\u8be5\u4f5c\u4e3a metrics \u7684\u53c2\u6570\u6765\u8f93\u5165\u3002 model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['mae', 'acc']) from keras import metrics model.compile(loss='mean_squared_error', optimizer='sgd', metrics=[metrics.mae, metrics.categorical_accuracy]) \u8bc4\u4ef7\u51fd\u6570\u548c \u635f\u5931\u51fd\u6570 \u76f8\u4f3c\uff0c\u53ea\u4e0d\u8fc7\u8bc4\u4ef7\u51fd\u6570\u7684\u7ed3\u679c\u4e0d\u4f1a\u7528\u4e8e\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u3002 \u6211\u4eec\u53ef\u4ee5\u4f20\u9012\u5df2\u6709\u7684\u8bc4\u4ef7\u51fd\u6570\u540d\u79f0\uff0c\u6216\u8005\u4f20\u9012\u4e00\u4e2a\u81ea\u5b9a\u4e49\u7684 Theano/TensorFlow \u51fd\u6570\u6765\u4f7f\u7528\uff08\u67e5\u9605 \u81ea\u5b9a\u4e49\u8bc4\u4ef7\u51fd\u6570 \uff09\u3002 \u53c2\u6570 y_true : \u771f\u5b9e\u6807\u7b7e\uff0cTheano/Tensorflow \u5f20\u91cf\u3002 y_pred : \u9884\u6d4b\u503c\u3002\u548c y_true \u76f8\u540c\u5c3a\u5bf8\u7684 Theano/TensorFlow \u5f20\u91cf\u3002 \u8fd4\u56de\u503c \u8fd4\u56de\u4e00\u4e2a\u8868\u793a\u5168\u90e8\u6570\u636e\u70b9\u5e73\u5747\u503c\u7684\u5f20\u91cf\u3002 \u53ef\u4f7f\u7528\u7684\u8bc4\u4ef7\u51fd\u6570 binary_accuracy binary_accuracy(y_true, y_pred) categorical_accuracy categorical_accuracy(y_true, y_pred) sparse_categorical_accuracy sparse_categorical_accuracy(y_true, y_pred) top_k_categorical_accuracy top_k_categorical_accuracy(y_true, y_pred, k=5) sparse_top_k_categorical_accuracy sparse_top_k_categorical_accuracy(y_true, y_pred, k=5) \u81ea\u5b9a\u4e49\u8bc4\u4ef7\u51fd\u6570 \u81ea\u5b9a\u4e49\u8bc4\u4ef7\u51fd\u6570\u5e94\u8be5\u5728\u7f16\u8bd1\u7684\u65f6\u5019\uff08compile\uff09\u4f20\u9012\u8fdb\u53bb\u3002\u8be5\u51fd\u6570\u9700\u8981\u4ee5 (y_true, y_pred) \u4f5c\u4e3a\u8f93\u5165\u53c2\u6570\uff0c\u5e76\u8fd4\u56de\u4e00\u4e2a\u5f20\u91cf\u4f5c\u4e3a\u8f93\u51fa\u7ed3\u679c\u3002 import keras.backend as K def mean_pred(y_true, y_pred): return K.mean(y_pred) model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', mean_pred])","title":"\u8bc4\u4f30\u6807\u51c6 Metrics"},{"location":"3.metrics/#metrics","text":"","title":"\u8bc4\u4f30\u6807\u51c6 Metrics"},{"location":"3.metrics/#_1","text":"\u8bc4\u4ef7\u51fd\u6570\u7528\u4e8e\u8bc4\u4f30\u5f53\u524d\u8bad\u7ec3\u6a21\u578b\u7684\u6027\u80fd\u3002\u5f53\u6a21\u578b\u7f16\u8bd1\u540e\uff08compile\uff09\uff0c\u8bc4\u4ef7\u51fd\u6570\u5e94\u8be5\u4f5c\u4e3a metrics \u7684\u53c2\u6570\u6765\u8f93\u5165\u3002 model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['mae', 'acc']) from keras import metrics model.compile(loss='mean_squared_error', optimizer='sgd', metrics=[metrics.mae, metrics.categorical_accuracy]) \u8bc4\u4ef7\u51fd\u6570\u548c \u635f\u5931\u51fd\u6570 \u76f8\u4f3c\uff0c\u53ea\u4e0d\u8fc7\u8bc4\u4ef7\u51fd\u6570\u7684\u7ed3\u679c\u4e0d\u4f1a\u7528\u4e8e\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u3002 \u6211\u4eec\u53ef\u4ee5\u4f20\u9012\u5df2\u6709\u7684\u8bc4\u4ef7\u51fd\u6570\u540d\u79f0\uff0c\u6216\u8005\u4f20\u9012\u4e00\u4e2a\u81ea\u5b9a\u4e49\u7684 Theano/TensorFlow \u51fd\u6570\u6765\u4f7f\u7528\uff08\u67e5\u9605 \u81ea\u5b9a\u4e49\u8bc4\u4ef7\u51fd\u6570 \uff09\u3002 \u53c2\u6570 y_true : \u771f\u5b9e\u6807\u7b7e\uff0cTheano/Tensorflow \u5f20\u91cf\u3002 y_pred : \u9884\u6d4b\u503c\u3002\u548c y_true \u76f8\u540c\u5c3a\u5bf8\u7684 Theano/TensorFlow \u5f20\u91cf\u3002 \u8fd4\u56de\u503c \u8fd4\u56de\u4e00\u4e2a\u8868\u793a\u5168\u90e8\u6570\u636e\u70b9\u5e73\u5747\u503c\u7684\u5f20\u91cf\u3002","title":"\u8bc4\u4ef7\u51fd\u6570\u7684\u7528\u6cd5"},{"location":"3.metrics/#_2","text":"","title":"\u53ef\u4f7f\u7528\u7684\u8bc4\u4ef7\u51fd\u6570"},{"location":"3.metrics/#binary_accuracy","text":"binary_accuracy(y_true, y_pred)","title":"binary_accuracy"},{"location":"3.metrics/#categorical_accuracy","text":"categorical_accuracy(y_true, y_pred)","title":"categorical_accuracy"},{"location":"3.metrics/#sparse_categorical_accuracy","text":"sparse_categorical_accuracy(y_true, y_pred)","title":"sparse_categorical_accuracy"},{"location":"3.metrics/#top_k_categorical_accuracy","text":"top_k_categorical_accuracy(y_true, y_pred, k=5)","title":"top_k_categorical_accuracy"},{"location":"3.metrics/#sparse_top_k_categorical_accuracy","text":"sparse_top_k_categorical_accuracy(y_true, y_pred, k=5)","title":"sparse_top_k_categorical_accuracy"},{"location":"3.metrics/#_3","text":"\u81ea\u5b9a\u4e49\u8bc4\u4ef7\u51fd\u6570\u5e94\u8be5\u5728\u7f16\u8bd1\u7684\u65f6\u5019\uff08compile\uff09\u4f20\u9012\u8fdb\u53bb\u3002\u8be5\u51fd\u6570\u9700\u8981\u4ee5 (y_true, y_pred) \u4f5c\u4e3a\u8f93\u5165\u53c2\u6570\uff0c\u5e76\u8fd4\u56de\u4e00\u4e2a\u5f20\u91cf\u4f5c\u4e3a\u8f93\u51fa\u7ed3\u679c\u3002 import keras.backend as K def mean_pred(y_true, y_pred): return K.mean(y_pred) model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', mean_pred])","title":"\u81ea\u5b9a\u4e49\u8bc4\u4ef7\u51fd\u6570"},{"location":"4.optimizers/","text":"\u4f18\u5316\u5668 Optimizers \u4f18\u5316\u5668\u7684\u7528\u6cd5 \u4f18\u5316\u5668 (optimizer) \u662f\u7f16\u8bd1 Keras \u6a21\u578b\u7684\u6240\u9700\u7684\u4e24\u4e2a\u53c2\u6570\u4e4b\u4e00\uff1a from keras import optimizers model = Sequential() model.add(Dense(64, kernel_initializer='uniform', input_shape=(10,))) model.add(Activation('softmax')) sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss='mean_squared_error', optimizer=sgd) \u4f60\u53ef\u4ee5\u5148\u5b9e\u4f8b\u5316\u4e00\u4e2a\u4f18\u5316\u5668\u5bf9\u8c61\uff0c\u7136\u540e\u5c06\u5b83\u4f20\u5165 model.compile() \uff0c\u50cf\u4e0a\u8ff0\u793a\u4f8b\u4e2d\u4e00\u6837\uff0c \u6216\u8005\u4f60\u53ef\u4ee5\u901a\u8fc7\u540d\u79f0\u6765\u8c03\u7528\u4f18\u5316\u5668\u3002\u5728\u540e\u4e00\u79cd\u60c5\u51b5\u4e0b\uff0c\u5c06\u4f7f\u7528\u4f18\u5316\u5668\u7684\u9ed8\u8ba4\u53c2\u6570\u3002 # \u4f20\u5165\u4f18\u5316\u5668\u540d\u79f0: \u9ed8\u8ba4\u53c2\u6570\u5c06\u88ab\u91c7\u7528 model.compile(loss='mean_squared_error', optimizer='sgd') Keras \u4f18\u5316\u5668\u7684\u516c\u5171\u53c2\u6570 \u53c2\u6570 clipnorm \u548c clipvalue \u80fd\u5728\u6240\u6709\u7684\u4f18\u5316\u5668\u4e2d\u4f7f\u7528\uff0c\u7528\u4e8e\u63a7\u5236\u68af\u5ea6\u88c1\u526a\uff08Gradient Clipping\uff09\uff1a from keras import optimizers # \u6240\u6709\u53c2\u6570\u68af\u5ea6\u5c06\u88ab\u88c1\u526a\uff0c\u8ba9\u5176l2\u8303\u6570\u6700\u5927\u4e3a1\uff1ag * 1 / max(1, l2_norm) sgd = optimizers.SGD(lr=0.01, clipnorm=1.) from keras import optimizers # \u6240\u6709\u53c2\u6570d \u68af\u5ea6\u5c06\u88ab\u88c1\u526a\u5230\u6570\u503c\u8303\u56f4\u5185\uff1a # \u6700\u5927\u503c0.5 # \u6700\u5c0f\u503c-0.5 sgd = optimizers.SGD(lr=0.01, clipvalue=0.5) [source] SGD keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False) \u968f\u673a\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u5668\u3002 \u5305\u542b\u6269\u5c55\u529f\u80fd\u7684\u652f\u6301\uff1a \u52a8\u91cf\uff08momentum\uff09\u4f18\u5316, \u5b66\u4e60\u7387\u8870\u51cf\uff08\u6bcf\u6b21\u53c2\u6570\u66f4\u65b0\u540e\uff09 Nestrov \u52a8\u91cf (NAG) \u4f18\u5316 \u53c2\u6570 lr : float >= 0. \u5b66\u4e60\u7387\u3002 momentum : float >= 0. \u53c2\u6570\uff0c\u7528\u4e8e\u52a0\u901f SGD \u5728\u76f8\u5173\u65b9\u5411\u4e0a\u524d\u8fdb\uff0c\u5e76\u6291\u5236\u9707\u8361\u3002 decay : float >= 0. \u6bcf\u6b21\u53c2\u6570\u66f4\u65b0\u540e\u5b66\u4e60\u7387\u8870\u51cf\u503c\u3002 nesterov : boolean. \u662f\u5426\u4f7f\u7528 Nesterov \u52a8\u91cf\u3002 [source] RMSprop keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0) RMSProp \u4f18\u5316\u5668. \u5efa\u8bae\u4f7f\u7528\u4f18\u5316\u5668\u7684\u9ed8\u8ba4\u53c2\u6570 \uff08\u9664\u4e86\u5b66\u4e60\u7387 lr\uff0c\u5b83\u53ef\u4ee5\u88ab\u81ea\u7531\u8c03\u8282\uff09 \u8fd9\u4e2a\u4f18\u5316\u5668\u901a\u5e38\u662f\u8bad\u7ec3\u5faa\u73af\u795e\u7ecf\u7f51\u7edc RNN \u7684\u4e0d\u9519\u9009\u62e9\u3002 \u53c2\u6570 lr : float >= 0. \u5b66\u4e60\u7387\u3002 rho : float >= 0. RMSProp \u68af\u5ea6\u5e73\u65b9\u7684\u79fb\u52a8\u5747\u503c\u7684\u8870\u51cf\u7387. epsilon : float >= 0. \u6a21\u7cca\u56e0\u5b50. \u82e5\u4e3a None , \u9ed8\u8ba4\u4e3a K.epsilon() \u3002 decay : float >= 0. \u6bcf\u6b21\u53c2\u6570\u66f4\u65b0\u540e\u5b66\u4e60\u7387\u8870\u51cf\u503c\u3002 \u53c2\u8003\u6587\u732e rmsprop: Divide the gradient by a running average of its recent magnitude [source] Adagrad keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0) Adagrad \u4f18\u5316\u5668\u3002 Adagrad \u662f\u4e00\u79cd\u5177\u6709\u7279\u5b9a\u53c2\u6570\u5b66\u4e60\u7387\u7684\u4f18\u5316\u5668\uff0c\u5b83\u6839\u636e\u53c2\u6570\u5728\u8bad\u7ec3\u671f\u95f4\u7684\u66f4\u65b0\u9891\u7387\u8fdb\u884c\u81ea\u9002\u5e94\u8c03\u6574\u3002\u53c2\u6570\u63a5\u6536\u7684\u66f4\u65b0\u8d8a\u591a\uff0c\u66f4\u65b0\u8d8a\u5c0f\u3002 \u5efa\u8bae\u4f7f\u7528\u4f18\u5316\u5668\u7684\u9ed8\u8ba4\u53c2\u6570\u3002 \u53c2\u6570 lr : float >= 0. \u5b66\u4e60\u7387. epsilon : float >= 0. \u82e5\u4e3a None , \u9ed8\u8ba4\u4e3a K.epsilon() . decay : float >= 0. \u6bcf\u6b21\u53c2\u6570\u66f4\u65b0\u540e\u5b66\u4e60\u7387\u8870\u51cf\u503c. \u53c2\u8003\u6587\u732e Adaptive Subgradient Methods for Online Learning and Stochastic Optimization [source] Adadelta keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0) Adadelta \u4f18\u5316\u5668\u3002 Adadelta \u662f Adagrad \u7684\u4e00\u4e2a\u5177\u6709\u66f4\u5f3a\u9c81\u68d2\u6027\u7684\u7684\u6269\u5c55\u7248\u672c\uff0c\u5b83\u4e0d\u662f\u7d2f\u79ef\u6240\u6709\u8fc7\u53bb\u7684\u68af\u5ea6\uff0c\u800c\u662f\u6839\u636e\u6e10\u53d8\u66f4\u65b0\u7684\u79fb\u52a8\u7a97\u53e3\u8c03\u6574\u5b66\u4e60\u901f\u7387\u3002 \u8fd9\u6837\uff0c\u5373\u4f7f\u8fdb\u884c\u4e86\u8bb8\u591a\u66f4\u65b0\uff0cAdadelta \u4ecd\u5728\u7ee7\u7eed\u5b66\u4e60\u3002 \u4e0e Adagrad \u76f8\u6bd4\uff0c\u5728 Adadelta \u7684\u539f\u59cb\u7248\u672c\u4e2d\uff0c\u60a8\u65e0\u9700\u8bbe\u7f6e\u521d\u59cb\u5b66\u4e60\u7387\u3002 \u5728\u6b64\u7248\u672c\u4e2d\uff0c\u4e0e\u5927\u591a\u6570\u5176\u4ed6 Keras \u4f18\u5316\u5668\u4e00\u6837\uff0c\u53ef\u4ee5\u8bbe\u7f6e\u521d\u59cb\u5b66\u4e60\u901f\u7387\u548c\u8870\u51cf\u56e0\u5b50\u3002 \u5efa\u8bae\u4f7f\u7528\u4f18\u5316\u5668\u7684\u9ed8\u8ba4\u53c2\u6570\u3002 \u53c2\u6570 lr : float >= 0. \u5b66\u4e60\u7387\uff0c\u5efa\u8bae\u4fdd\u7559\u9ed8\u8ba4\u503c\u3002 rho : float >= 0. Adadelta \u68af\u5ea6\u5e73\u65b9\u79fb\u52a8\u5747\u503c\u7684\u8870\u51cf\u7387\u3002 epsilon : float >= 0. \u6a21\u7cca\u56e0\u5b50. \u82e5\u4e3a None , \u9ed8\u8ba4\u4e3a K.epsilon() \u3002 decay : float >= 0. \u6bcf\u6b21\u53c2\u6570\u66f4\u65b0\u540e\u5b66\u4e60\u7387\u8870\u51cf\u503c\u3002 \u53c2\u8003\u6587\u732e Adadelta - an adaptive learning rate method [source] Adam keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False) Adam \u4f18\u5316\u5668\u3002 \u9ed8\u8ba4\u53c2\u6570\u9075\u5faa\u539f\u8bba\u6587\u4e2d\u63d0\u4f9b\u7684\u503c\u3002 \u53c2\u6570 lr : float >= 0. \u5b66\u4e60\u7387\u3002 beta_1 : float, 0 < beta < 1. \u901a\u5e38\u63a5\u8fd1\u4e8e 1\u3002 beta_2 : float, 0 < beta < 1. \u901a\u5e38\u63a5\u8fd1\u4e8e 1\u3002 epsilon : float >= 0. \u6a21\u7cca\u56e0\u5b50. \u82e5\u4e3a None , \u9ed8\u8ba4\u4e3a K.epsilon() \u3002 decay : float >= 0. \u6bcf\u6b21\u53c2\u6570\u66f4\u65b0\u540e\u5b66\u4e60\u7387\u8870\u51cf\u503c\u3002 amsgrad : boolean. \u662f\u5426\u5e94\u7528\u6b64\u7b97\u6cd5\u7684 AMSGrad \u53d8\u79cd\uff0c\u6765\u81ea\u8bba\u6587 \"On the Convergence of Adam and Beyond\"\u3002 \u53c2\u8003\u6587\u732e Adam - A Method for Stochastic Optimization On the Convergence of Adam and Beyond [source] Adamax keras.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0) Adamax \u4f18\u5316\u5668\uff0c\u6765\u81ea Adam \u8bba\u6587\u7684\u7b2c\u4e03\u5c0f\u8282. \u5b83\u662f Adam \u7b97\u6cd5\u57fa\u4e8e\u65e0\u7a77\u8303\u6570\uff08infinity norm\uff09\u7684\u53d8\u79cd\u3002 \u9ed8\u8ba4\u53c2\u6570\u9075\u5faa\u8bba\u6587\u4e2d\u63d0\u4f9b\u7684\u503c\u3002 \u53c2\u6570 lr : float >= 0. \u5b66\u4e60\u7387\u3002 beta_1/beta_2 : floats, 0 < beta < 1. \u901a\u5e38\u63a5\u8fd1\u4e8e 1\u3002 epsilon : float >= 0. \u6a21\u7cca\u56e0\u5b50. \u82e5\u4e3a None , \u9ed8\u8ba4\u4e3a K.epsilon() \u3002 decay : float >= 0. \u6bcf\u6b21\u53c2\u6570\u66f4\u65b0\u540e\u5b66\u4e60\u7387\u8870\u51cf\u503c\u3002 \u53c2\u8003\u6587\u732e Adam - A Method for Stochastic Optimization [source] Nadam keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004) Nesterov \u7248\u672c Adam \u4f18\u5316\u5668\u3002 \u6b63\u50cf Adam \u672c\u8d28\u4e0a\u662f RMSProp \u4e0e\u52a8\u91cf momentum \u7684\u7ed3\u5408\uff0c Nadam \u662f\u91c7\u7528 Nesterov momentum \u7248\u672c\u7684 Adam \u4f18\u5316\u5668\u3002 \u9ed8\u8ba4\u53c2\u6570\u9075\u5faa\u8bba\u6587\u4e2d\u63d0\u4f9b\u7684\u503c\u3002 \u5efa\u8bae\u4f7f\u7528\u4f18\u5316\u5668\u7684\u9ed8\u8ba4\u53c2\u6570\u3002 \u53c2\u6570 lr : float >= 0. \u5b66\u4e60\u7387\u3002 beta_1/beta_2 : floats, 0 < beta < 1. \u901a\u5e38\u63a5\u8fd1\u4e8e 1\u3002 epsilon : float >= 0. \u6a21\u7cca\u56e0\u5b50. \u82e5\u4e3a None , \u9ed8\u8ba4\u4e3a K.epsilon() \u3002 \u53c2\u8003\u6587\u732e Nadam report On the importance of initialization and momentum in deep learning","title":"\u4f18\u5316\u5668 Optimizers"},{"location":"4.optimizers/#optimizers","text":"","title":"\u4f18\u5316\u5668 Optimizers"},{"location":"4.optimizers/#_1","text":"\u4f18\u5316\u5668 (optimizer) \u662f\u7f16\u8bd1 Keras \u6a21\u578b\u7684\u6240\u9700\u7684\u4e24\u4e2a\u53c2\u6570\u4e4b\u4e00\uff1a from keras import optimizers model = Sequential() model.add(Dense(64, kernel_initializer='uniform', input_shape=(10,))) model.add(Activation('softmax')) sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss='mean_squared_error', optimizer=sgd) \u4f60\u53ef\u4ee5\u5148\u5b9e\u4f8b\u5316\u4e00\u4e2a\u4f18\u5316\u5668\u5bf9\u8c61\uff0c\u7136\u540e\u5c06\u5b83\u4f20\u5165 model.compile() \uff0c\u50cf\u4e0a\u8ff0\u793a\u4f8b\u4e2d\u4e00\u6837\uff0c \u6216\u8005\u4f60\u53ef\u4ee5\u901a\u8fc7\u540d\u79f0\u6765\u8c03\u7528\u4f18\u5316\u5668\u3002\u5728\u540e\u4e00\u79cd\u60c5\u51b5\u4e0b\uff0c\u5c06\u4f7f\u7528\u4f18\u5316\u5668\u7684\u9ed8\u8ba4\u53c2\u6570\u3002 # \u4f20\u5165\u4f18\u5316\u5668\u540d\u79f0: \u9ed8\u8ba4\u53c2\u6570\u5c06\u88ab\u91c7\u7528 model.compile(loss='mean_squared_error', optimizer='sgd')","title":"\u4f18\u5316\u5668\u7684\u7528\u6cd5"},{"location":"4.optimizers/#keras","text":"\u53c2\u6570 clipnorm \u548c clipvalue \u80fd\u5728\u6240\u6709\u7684\u4f18\u5316\u5668\u4e2d\u4f7f\u7528\uff0c\u7528\u4e8e\u63a7\u5236\u68af\u5ea6\u88c1\u526a\uff08Gradient Clipping\uff09\uff1a from keras import optimizers # \u6240\u6709\u53c2\u6570\u68af\u5ea6\u5c06\u88ab\u88c1\u526a\uff0c\u8ba9\u5176l2\u8303\u6570\u6700\u5927\u4e3a1\uff1ag * 1 / max(1, l2_norm) sgd = optimizers.SGD(lr=0.01, clipnorm=1.) from keras import optimizers # \u6240\u6709\u53c2\u6570d \u68af\u5ea6\u5c06\u88ab\u88c1\u526a\u5230\u6570\u503c\u8303\u56f4\u5185\uff1a # \u6700\u5927\u503c0.5 # \u6700\u5c0f\u503c-0.5 sgd = optimizers.SGD(lr=0.01, clipvalue=0.5) [source]","title":"Keras \u4f18\u5316\u5668\u7684\u516c\u5171\u53c2\u6570"},{"location":"4.optimizers/#sgd","text":"keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False) \u968f\u673a\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u5668\u3002 \u5305\u542b\u6269\u5c55\u529f\u80fd\u7684\u652f\u6301\uff1a \u52a8\u91cf\uff08momentum\uff09\u4f18\u5316, \u5b66\u4e60\u7387\u8870\u51cf\uff08\u6bcf\u6b21\u53c2\u6570\u66f4\u65b0\u540e\uff09 Nestrov \u52a8\u91cf (NAG) \u4f18\u5316 \u53c2\u6570 lr : float >= 0. \u5b66\u4e60\u7387\u3002 momentum : float >= 0. \u53c2\u6570\uff0c\u7528\u4e8e\u52a0\u901f SGD \u5728\u76f8\u5173\u65b9\u5411\u4e0a\u524d\u8fdb\uff0c\u5e76\u6291\u5236\u9707\u8361\u3002 decay : float >= 0. \u6bcf\u6b21\u53c2\u6570\u66f4\u65b0\u540e\u5b66\u4e60\u7387\u8870\u51cf\u503c\u3002 nesterov : boolean. \u662f\u5426\u4f7f\u7528 Nesterov \u52a8\u91cf\u3002 [source]","title":"SGD"},{"location":"4.optimizers/#rmsprop","text":"keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0) RMSProp \u4f18\u5316\u5668. \u5efa\u8bae\u4f7f\u7528\u4f18\u5316\u5668\u7684\u9ed8\u8ba4\u53c2\u6570 \uff08\u9664\u4e86\u5b66\u4e60\u7387 lr\uff0c\u5b83\u53ef\u4ee5\u88ab\u81ea\u7531\u8c03\u8282\uff09 \u8fd9\u4e2a\u4f18\u5316\u5668\u901a\u5e38\u662f\u8bad\u7ec3\u5faa\u73af\u795e\u7ecf\u7f51\u7edc RNN \u7684\u4e0d\u9519\u9009\u62e9\u3002 \u53c2\u6570 lr : float >= 0. \u5b66\u4e60\u7387\u3002 rho : float >= 0. RMSProp \u68af\u5ea6\u5e73\u65b9\u7684\u79fb\u52a8\u5747\u503c\u7684\u8870\u51cf\u7387. epsilon : float >= 0. \u6a21\u7cca\u56e0\u5b50. \u82e5\u4e3a None , \u9ed8\u8ba4\u4e3a K.epsilon() \u3002 decay : float >= 0. \u6bcf\u6b21\u53c2\u6570\u66f4\u65b0\u540e\u5b66\u4e60\u7387\u8870\u51cf\u503c\u3002 \u53c2\u8003\u6587\u732e rmsprop: Divide the gradient by a running average of its recent magnitude [source]","title":"RMSprop"},{"location":"4.optimizers/#adagrad","text":"keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0) Adagrad \u4f18\u5316\u5668\u3002 Adagrad \u662f\u4e00\u79cd\u5177\u6709\u7279\u5b9a\u53c2\u6570\u5b66\u4e60\u7387\u7684\u4f18\u5316\u5668\uff0c\u5b83\u6839\u636e\u53c2\u6570\u5728\u8bad\u7ec3\u671f\u95f4\u7684\u66f4\u65b0\u9891\u7387\u8fdb\u884c\u81ea\u9002\u5e94\u8c03\u6574\u3002\u53c2\u6570\u63a5\u6536\u7684\u66f4\u65b0\u8d8a\u591a\uff0c\u66f4\u65b0\u8d8a\u5c0f\u3002 \u5efa\u8bae\u4f7f\u7528\u4f18\u5316\u5668\u7684\u9ed8\u8ba4\u53c2\u6570\u3002 \u53c2\u6570 lr : float >= 0. \u5b66\u4e60\u7387. epsilon : float >= 0. \u82e5\u4e3a None , \u9ed8\u8ba4\u4e3a K.epsilon() . decay : float >= 0. \u6bcf\u6b21\u53c2\u6570\u66f4\u65b0\u540e\u5b66\u4e60\u7387\u8870\u51cf\u503c. \u53c2\u8003\u6587\u732e Adaptive Subgradient Methods for Online Learning and Stochastic Optimization [source]","title":"Adagrad"},{"location":"4.optimizers/#adadelta","text":"keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0) Adadelta \u4f18\u5316\u5668\u3002 Adadelta \u662f Adagrad \u7684\u4e00\u4e2a\u5177\u6709\u66f4\u5f3a\u9c81\u68d2\u6027\u7684\u7684\u6269\u5c55\u7248\u672c\uff0c\u5b83\u4e0d\u662f\u7d2f\u79ef\u6240\u6709\u8fc7\u53bb\u7684\u68af\u5ea6\uff0c\u800c\u662f\u6839\u636e\u6e10\u53d8\u66f4\u65b0\u7684\u79fb\u52a8\u7a97\u53e3\u8c03\u6574\u5b66\u4e60\u901f\u7387\u3002 \u8fd9\u6837\uff0c\u5373\u4f7f\u8fdb\u884c\u4e86\u8bb8\u591a\u66f4\u65b0\uff0cAdadelta \u4ecd\u5728\u7ee7\u7eed\u5b66\u4e60\u3002 \u4e0e Adagrad \u76f8\u6bd4\uff0c\u5728 Adadelta \u7684\u539f\u59cb\u7248\u672c\u4e2d\uff0c\u60a8\u65e0\u9700\u8bbe\u7f6e\u521d\u59cb\u5b66\u4e60\u7387\u3002 \u5728\u6b64\u7248\u672c\u4e2d\uff0c\u4e0e\u5927\u591a\u6570\u5176\u4ed6 Keras \u4f18\u5316\u5668\u4e00\u6837\uff0c\u53ef\u4ee5\u8bbe\u7f6e\u521d\u59cb\u5b66\u4e60\u901f\u7387\u548c\u8870\u51cf\u56e0\u5b50\u3002 \u5efa\u8bae\u4f7f\u7528\u4f18\u5316\u5668\u7684\u9ed8\u8ba4\u53c2\u6570\u3002 \u53c2\u6570 lr : float >= 0. \u5b66\u4e60\u7387\uff0c\u5efa\u8bae\u4fdd\u7559\u9ed8\u8ba4\u503c\u3002 rho : float >= 0. Adadelta \u68af\u5ea6\u5e73\u65b9\u79fb\u52a8\u5747\u503c\u7684\u8870\u51cf\u7387\u3002 epsilon : float >= 0. \u6a21\u7cca\u56e0\u5b50. \u82e5\u4e3a None , \u9ed8\u8ba4\u4e3a K.epsilon() \u3002 decay : float >= 0. \u6bcf\u6b21\u53c2\u6570\u66f4\u65b0\u540e\u5b66\u4e60\u7387\u8870\u51cf\u503c\u3002 \u53c2\u8003\u6587\u732e Adadelta - an adaptive learning rate method [source]","title":"Adadelta"},{"location":"4.optimizers/#adam","text":"keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False) Adam \u4f18\u5316\u5668\u3002 \u9ed8\u8ba4\u53c2\u6570\u9075\u5faa\u539f\u8bba\u6587\u4e2d\u63d0\u4f9b\u7684\u503c\u3002 \u53c2\u6570 lr : float >= 0. \u5b66\u4e60\u7387\u3002 beta_1 : float, 0 < beta < 1. \u901a\u5e38\u63a5\u8fd1\u4e8e 1\u3002 beta_2 : float, 0 < beta < 1. \u901a\u5e38\u63a5\u8fd1\u4e8e 1\u3002 epsilon : float >= 0. \u6a21\u7cca\u56e0\u5b50. \u82e5\u4e3a None , \u9ed8\u8ba4\u4e3a K.epsilon() \u3002 decay : float >= 0. \u6bcf\u6b21\u53c2\u6570\u66f4\u65b0\u540e\u5b66\u4e60\u7387\u8870\u51cf\u503c\u3002 amsgrad : boolean. \u662f\u5426\u5e94\u7528\u6b64\u7b97\u6cd5\u7684 AMSGrad \u53d8\u79cd\uff0c\u6765\u81ea\u8bba\u6587 \"On the Convergence of Adam and Beyond\"\u3002 \u53c2\u8003\u6587\u732e Adam - A Method for Stochastic Optimization On the Convergence of Adam and Beyond [source]","title":"Adam"},{"location":"4.optimizers/#adamax","text":"keras.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0) Adamax \u4f18\u5316\u5668\uff0c\u6765\u81ea Adam \u8bba\u6587\u7684\u7b2c\u4e03\u5c0f\u8282. \u5b83\u662f Adam \u7b97\u6cd5\u57fa\u4e8e\u65e0\u7a77\u8303\u6570\uff08infinity norm\uff09\u7684\u53d8\u79cd\u3002 \u9ed8\u8ba4\u53c2\u6570\u9075\u5faa\u8bba\u6587\u4e2d\u63d0\u4f9b\u7684\u503c\u3002 \u53c2\u6570 lr : float >= 0. \u5b66\u4e60\u7387\u3002 beta_1/beta_2 : floats, 0 < beta < 1. \u901a\u5e38\u63a5\u8fd1\u4e8e 1\u3002 epsilon : float >= 0. \u6a21\u7cca\u56e0\u5b50. \u82e5\u4e3a None , \u9ed8\u8ba4\u4e3a K.epsilon() \u3002 decay : float >= 0. \u6bcf\u6b21\u53c2\u6570\u66f4\u65b0\u540e\u5b66\u4e60\u7387\u8870\u51cf\u503c\u3002 \u53c2\u8003\u6587\u732e Adam - A Method for Stochastic Optimization [source]","title":"Adamax"},{"location":"4.optimizers/#nadam","text":"keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004) Nesterov \u7248\u672c Adam \u4f18\u5316\u5668\u3002 \u6b63\u50cf Adam \u672c\u8d28\u4e0a\u662f RMSProp \u4e0e\u52a8\u91cf momentum \u7684\u7ed3\u5408\uff0c Nadam \u662f\u91c7\u7528 Nesterov momentum \u7248\u672c\u7684 Adam \u4f18\u5316\u5668\u3002 \u9ed8\u8ba4\u53c2\u6570\u9075\u5faa\u8bba\u6587\u4e2d\u63d0\u4f9b\u7684\u503c\u3002 \u5efa\u8bae\u4f7f\u7528\u4f18\u5316\u5668\u7684\u9ed8\u8ba4\u53c2\u6570\u3002 \u53c2\u6570 lr : float >= 0. \u5b66\u4e60\u7387\u3002 beta_1/beta_2 : floats, 0 < beta < 1. \u901a\u5e38\u63a5\u8fd1\u4e8e 1\u3002 epsilon : float >= 0. \u6a21\u7cca\u56e0\u5b50. \u82e5\u4e3a None , \u9ed8\u8ba4\u4e3a K.epsilon() \u3002 \u53c2\u8003\u6587\u732e Nadam report On the importance of initialization and momentum in deep learning","title":"Nadam"},{"location":"5.activations/","text":"\u6fc0\u6d3b\u51fd\u6570 Activations \u6fc0\u6d3b\u51fd\u6570\u7684\u7528\u6cd5 \u6fc0\u6d3b\u51fd\u6570\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e\u5355\u72ec\u7684\u6fc0\u6d3b\u5c42\u5b9e\u73b0\uff0c\u4e5f\u53ef\u4ee5\u5728\u6784\u9020\u5c42\u5bf9\u8c61\u65f6\u901a\u8fc7\u4f20\u9012 activation \u53c2\u6570\u5b9e\u73b0\uff1a from keras.layers import Activation, Dense model.add(Dense(64)) model.add(Activation('tanh')) \u7b49\u4ef7\u4e8e\uff1a model.add(Dense(64, activation='tanh')) \u4f60\u4e5f\u53ef\u4ee5\u901a\u8fc7\u4f20\u9012\u4e00\u4e2a\u9010\u5143\u7d20\u8fd0\u7b97\u7684 Theano/TensorFlow/CNTK \u51fd\u6570\u6765\u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\uff1a from keras import backend as K model.add(Dense(64, activation=K.tanh)) model.add(Activation(K.tanh)) \u9884\u5b9a\u4e49\u6fc0\u6d3b\u51fd\u6570 softmax keras.activations.softmax(x, axis=-1) Softmax \u6fc0\u6d3b\u51fd\u6570\u3002 \u53c2\u6570 x \uff1a\u5f20\u91cf\u3002 axis \uff1a\u6574\u6570\uff0c\u4ee3\u8868 softmax \u6240\u4f5c\u7528\u7684\u7ef4\u5ea6\u3002 \u8fd4\u56de softmax \u53d8\u6362\u540e\u7684\u5f20\u91cf\u3002 \u5f02\u5e38 ValueError \uff1a\u5982\u679c dim(x) == 1 \u3002 elu keras.activations.elu(x, alpha=1.0) \u6307\u6570\u7ebf\u6027\u5355\u5143\u3002 \u53c2\u6570 x \uff1a\u5f20\u91cf\u3002 alpha \uff1a\u4e00\u4e2a\u6807\u91cf\uff0c\u8868\u793a\u8d1f\u6570\u90e8\u5206\u7684\u659c\u7387\u3002 \u8fd4\u56de \u7ebf\u6027\u6307\u6570\u6fc0\u6d3b\uff1a\u5982\u679c x > 0 \uff0c\u8fd4\u56de\u503c\u4e3a x \uff1b\u5982\u679c x < 0 \u8fd4\u56de\u503c\u4e3a alpha * (exp(x)-1) \u53c2\u8003\u6587\u732e Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) selu keras.activations.selu(x) \u53ef\u4f38\u7f29\u7684\u6307\u6570\u7ebf\u6027\u5355\u5143\uff08SELU\uff09\u3002 SELU \u7b49\u540c\u4e8e\uff1a scale * elu(x, alpha) \uff0c\u5176\u4e2d alpha \u548c scale \u662f\u9884\u5b9a\u4e49\u7684\u5e38\u91cf\u3002\u53ea\u8981\u6b63\u786e\u521d\u59cb\u5316\u6743\u91cd\uff08\u53c2\u89c1 lecun_normal \u521d\u59cb\u5316\u65b9\u6cd5\uff09\u5e76\u4e14\u8f93\u5165\u7684\u6570\u91cf\u300c\u8db3\u591f\u5927\u300d\uff08\u53c2\u89c1\u53c2\u8003\u6587\u732e\u83b7\u5f97\u66f4\u591a\u4fe1\u606f\uff09\uff0c\u9009\u62e9\u5408\u9002\u7684 alpha \u548c scale \u7684\u503c\uff0c\u5c31\u53ef\u4ee5\u5728\u4e24\u4e2a\u8fde\u7eed\u5c42\u4e4b\u95f4\u4fdd\u7559\u8f93\u5165\u7684\u5747\u503c\u548c\u65b9\u5dee\u3002 \u53c2\u6570 x : \u4e00\u4e2a\u7528\u6765\u7528\u4e8e\u8ba1\u7b97\u6fc0\u6d3b\u51fd\u6570\u7684\u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u53ef\u4f38\u7f29\u7684\u6307\u6570\u7ebf\u6027\u6fc0\u6d3b\uff1a scale * elu(x, alpha) \u3002 \u6ce8\u610f \u4e0e\u300clecun_normal\u300d\u521d\u59cb\u5316\u65b9\u6cd5\u4e00\u8d77\u4f7f\u7528\u3002 \u4e0e dropout \u7684\u53d8\u79cd\u300cAlphaDropout\u300d\u4e00\u8d77\u4f7f\u7528\u3002 \u53c2\u8003\u6587\u732e Self-Normalizing Neural Networks softplus keras.activations.softplus(x) Softplus \u6fc0\u6d3b\u51fd\u6570\u3002 \u53c2\u6570 x : \u5f20\u91cf\u3002 \u8fd4\u56de Softplus \u6fc0\u6d3b\uff1a log(exp(x) + 1) \u3002 softsign keras.activations.softsign(x) Softsign \u6fc0\u6d3b\u51fd\u6570\u3002 \u53c2\u6570 x : \u5f20\u91cf\u3002 \u8fd4\u56de Softsign \u6fc0\u6d3b\uff1a x / (abs(x) + 1) \u3002 relu keras.activations.relu(x, alpha=0.0, max_value=None, threshold=0.0) \u6574\u6d41\u7ebf\u6027\u5355\u5143\u3002 \u4f7f\u7528\u9ed8\u8ba4\u503c\u65f6\uff0c\u5b83\u8fd4\u56de\u9010\u5143\u7d20\u7684 max(x, 0) \u3002 \u5426\u5219\uff0c\u5b83\u9075\u5faa\uff1a \u5982\u679c x >= max_value \uff1a f(x) = max_value \uff0c \u5982\u679c threshold <= x < max_value \uff1a f(x) = x \uff0c \u5426\u5219\uff1a f(x) = alpha * (x - threshold) \u3002 \u53c2\u6570 x : \u5f20\u91cf\u3002 alpha \uff1a\u8d1f\u6570\u90e8\u5206\u7684\u659c\u7387\u3002\u9ed8\u8ba4\u4e3a 0\u3002 max_value \uff1a\u8f93\u51fa\u7684\u6700\u5927\u503c\u3002 threshold : \u6d6e\u70b9\u6570\u3002Thresholded activation \u7684\u9608\u503c\u503c\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 tanh keras.activations.tanh(x) \u53cc\u66f2\u6b63\u5207\u6fc0\u6d3b\u51fd\u6570\u3002 sigmoid sigmoid(x) Sigmoid \u6fc0\u6d3b\u51fd\u6570\u3002 hard_sigmoid hard_sigmoid(x) Hard sigmoid \u6fc0\u6d3b\u51fd\u6570\u3002 \u8ba1\u7b97\u901f\u5ea6\u6bd4 sigmoid \u6fc0\u6d3b\u51fd\u6570\u66f4\u5feb\u3002 \u53c2\u6570 x : \u5f20\u91cf\u3002 \u8fd4\u56de Hard sigmoid \u6fc0\u6d3b\uff1a \u5982\u679c x < -2.5 \uff0c\u8fd4\u56de 0\u3002 \u5982\u679c x > 2.5 \uff0c\u8fd4\u56de 1\u3002 \u5982\u679c -2.5 <= x <= 2.5 \uff0c\u8fd4\u56de 0.2 * x + 0.5 \u3002 exponential keras.activations.exponential(x) \u81ea\u7136\u6570\u6307\u6570\u6fc0\u6d3b\u51fd\u6570\u3002 linear keras.activations.linear(x) \u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\uff08\u5373\u4e0d\u505a\u4efb\u4f55\u6539\u53d8\uff09 \u9ad8\u7ea7\u6fc0\u6d3b\u51fd\u6570 \u5bf9\u4e8e Theano/TensorFlow/CNTK \u4e0d\u80fd\u8868\u8fbe\u7684\u590d\u6742\u6fc0\u6d3b\u51fd\u6570\uff0c\u5982\u542b\u6709\u53ef\u5b66\u4e60\u53c2\u6570\u7684\u6fc0\u6d3b\u51fd\u6570\uff0c\u53ef\u901a\u8fc7 \u9ad8\u7ea7\u6fc0\u6d3b\u51fd\u6570 \u5b9e\u73b0\uff0c\u53ef\u4ee5\u5728 keras.layers.advanced_activations \u6a21\u5757\u4e2d\u627e\u5230\u3002 \u8fd9\u4e9b\u9ad8\u7ea7\u6fc0\u6d3b\u51fd\u6570\u5305\u62ec PReLU \u548c LeakyReLU \u3002","title":"\u6fc0\u6d3b\u51fd\u6570 Activations"},{"location":"5.activations/#activations","text":"","title":"\u6fc0\u6d3b\u51fd\u6570 Activations"},{"location":"5.activations/#_1","text":"\u6fc0\u6d3b\u51fd\u6570\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e\u5355\u72ec\u7684\u6fc0\u6d3b\u5c42\u5b9e\u73b0\uff0c\u4e5f\u53ef\u4ee5\u5728\u6784\u9020\u5c42\u5bf9\u8c61\u65f6\u901a\u8fc7\u4f20\u9012 activation \u53c2\u6570\u5b9e\u73b0\uff1a from keras.layers import Activation, Dense model.add(Dense(64)) model.add(Activation('tanh')) \u7b49\u4ef7\u4e8e\uff1a model.add(Dense(64, activation='tanh')) \u4f60\u4e5f\u53ef\u4ee5\u901a\u8fc7\u4f20\u9012\u4e00\u4e2a\u9010\u5143\u7d20\u8fd0\u7b97\u7684 Theano/TensorFlow/CNTK \u51fd\u6570\u6765\u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\uff1a from keras import backend as K model.add(Dense(64, activation=K.tanh)) model.add(Activation(K.tanh))","title":"\u6fc0\u6d3b\u51fd\u6570\u7684\u7528\u6cd5"},{"location":"5.activations/#_2","text":"","title":"\u9884\u5b9a\u4e49\u6fc0\u6d3b\u51fd\u6570"},{"location":"5.activations/#softmax","text":"keras.activations.softmax(x, axis=-1) Softmax \u6fc0\u6d3b\u51fd\u6570\u3002 \u53c2\u6570 x \uff1a\u5f20\u91cf\u3002 axis \uff1a\u6574\u6570\uff0c\u4ee3\u8868 softmax \u6240\u4f5c\u7528\u7684\u7ef4\u5ea6\u3002 \u8fd4\u56de softmax \u53d8\u6362\u540e\u7684\u5f20\u91cf\u3002 \u5f02\u5e38 ValueError \uff1a\u5982\u679c dim(x) == 1 \u3002","title":"softmax"},{"location":"5.activations/#elu","text":"keras.activations.elu(x, alpha=1.0) \u6307\u6570\u7ebf\u6027\u5355\u5143\u3002 \u53c2\u6570 x \uff1a\u5f20\u91cf\u3002 alpha \uff1a\u4e00\u4e2a\u6807\u91cf\uff0c\u8868\u793a\u8d1f\u6570\u90e8\u5206\u7684\u659c\u7387\u3002 \u8fd4\u56de \u7ebf\u6027\u6307\u6570\u6fc0\u6d3b\uff1a\u5982\u679c x > 0 \uff0c\u8fd4\u56de\u503c\u4e3a x \uff1b\u5982\u679c x < 0 \u8fd4\u56de\u503c\u4e3a alpha * (exp(x)-1) \u53c2\u8003\u6587\u732e Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)","title":"elu"},{"location":"5.activations/#selu","text":"keras.activations.selu(x) \u53ef\u4f38\u7f29\u7684\u6307\u6570\u7ebf\u6027\u5355\u5143\uff08SELU\uff09\u3002 SELU \u7b49\u540c\u4e8e\uff1a scale * elu(x, alpha) \uff0c\u5176\u4e2d alpha \u548c scale \u662f\u9884\u5b9a\u4e49\u7684\u5e38\u91cf\u3002\u53ea\u8981\u6b63\u786e\u521d\u59cb\u5316\u6743\u91cd\uff08\u53c2\u89c1 lecun_normal \u521d\u59cb\u5316\u65b9\u6cd5\uff09\u5e76\u4e14\u8f93\u5165\u7684\u6570\u91cf\u300c\u8db3\u591f\u5927\u300d\uff08\u53c2\u89c1\u53c2\u8003\u6587\u732e\u83b7\u5f97\u66f4\u591a\u4fe1\u606f\uff09\uff0c\u9009\u62e9\u5408\u9002\u7684 alpha \u548c scale \u7684\u503c\uff0c\u5c31\u53ef\u4ee5\u5728\u4e24\u4e2a\u8fde\u7eed\u5c42\u4e4b\u95f4\u4fdd\u7559\u8f93\u5165\u7684\u5747\u503c\u548c\u65b9\u5dee\u3002 \u53c2\u6570 x : \u4e00\u4e2a\u7528\u6765\u7528\u4e8e\u8ba1\u7b97\u6fc0\u6d3b\u51fd\u6570\u7684\u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u53ef\u4f38\u7f29\u7684\u6307\u6570\u7ebf\u6027\u6fc0\u6d3b\uff1a scale * elu(x, alpha) \u3002 \u6ce8\u610f \u4e0e\u300clecun_normal\u300d\u521d\u59cb\u5316\u65b9\u6cd5\u4e00\u8d77\u4f7f\u7528\u3002 \u4e0e dropout \u7684\u53d8\u79cd\u300cAlphaDropout\u300d\u4e00\u8d77\u4f7f\u7528\u3002 \u53c2\u8003\u6587\u732e Self-Normalizing Neural Networks","title":"selu"},{"location":"5.activations/#softplus","text":"keras.activations.softplus(x) Softplus \u6fc0\u6d3b\u51fd\u6570\u3002 \u53c2\u6570 x : \u5f20\u91cf\u3002 \u8fd4\u56de Softplus \u6fc0\u6d3b\uff1a log(exp(x) + 1) \u3002","title":"softplus"},{"location":"5.activations/#softsign","text":"keras.activations.softsign(x) Softsign \u6fc0\u6d3b\u51fd\u6570\u3002 \u53c2\u6570 x : \u5f20\u91cf\u3002 \u8fd4\u56de Softsign \u6fc0\u6d3b\uff1a x / (abs(x) + 1) \u3002","title":"softsign"},{"location":"5.activations/#relu","text":"keras.activations.relu(x, alpha=0.0, max_value=None, threshold=0.0) \u6574\u6d41\u7ebf\u6027\u5355\u5143\u3002 \u4f7f\u7528\u9ed8\u8ba4\u503c\u65f6\uff0c\u5b83\u8fd4\u56de\u9010\u5143\u7d20\u7684 max(x, 0) \u3002 \u5426\u5219\uff0c\u5b83\u9075\u5faa\uff1a \u5982\u679c x >= max_value \uff1a f(x) = max_value \uff0c \u5982\u679c threshold <= x < max_value \uff1a f(x) = x \uff0c \u5426\u5219\uff1a f(x) = alpha * (x - threshold) \u3002 \u53c2\u6570 x : \u5f20\u91cf\u3002 alpha \uff1a\u8d1f\u6570\u90e8\u5206\u7684\u659c\u7387\u3002\u9ed8\u8ba4\u4e3a 0\u3002 max_value \uff1a\u8f93\u51fa\u7684\u6700\u5927\u503c\u3002 threshold : \u6d6e\u70b9\u6570\u3002Thresholded activation \u7684\u9608\u503c\u503c\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002","title":"relu"},{"location":"5.activations/#tanh","text":"keras.activations.tanh(x) \u53cc\u66f2\u6b63\u5207\u6fc0\u6d3b\u51fd\u6570\u3002","title":"tanh"},{"location":"5.activations/#sigmoid","text":"sigmoid(x) Sigmoid \u6fc0\u6d3b\u51fd\u6570\u3002","title":"sigmoid"},{"location":"5.activations/#hard_sigmoid","text":"hard_sigmoid(x) Hard sigmoid \u6fc0\u6d3b\u51fd\u6570\u3002 \u8ba1\u7b97\u901f\u5ea6\u6bd4 sigmoid \u6fc0\u6d3b\u51fd\u6570\u66f4\u5feb\u3002 \u53c2\u6570 x : \u5f20\u91cf\u3002 \u8fd4\u56de Hard sigmoid \u6fc0\u6d3b\uff1a \u5982\u679c x < -2.5 \uff0c\u8fd4\u56de 0\u3002 \u5982\u679c x > 2.5 \uff0c\u8fd4\u56de 1\u3002 \u5982\u679c -2.5 <= x <= 2.5 \uff0c\u8fd4\u56de 0.2 * x + 0.5 \u3002","title":"hard_sigmoid"},{"location":"5.activations/#exponential","text":"keras.activations.exponential(x) \u81ea\u7136\u6570\u6307\u6570\u6fc0\u6d3b\u51fd\u6570\u3002","title":"exponential"},{"location":"5.activations/#linear","text":"keras.activations.linear(x) \u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\uff08\u5373\u4e0d\u505a\u4efb\u4f55\u6539\u53d8\uff09","title":"linear"},{"location":"5.activations/#_3","text":"\u5bf9\u4e8e Theano/TensorFlow/CNTK \u4e0d\u80fd\u8868\u8fbe\u7684\u590d\u6742\u6fc0\u6d3b\u51fd\u6570\uff0c\u5982\u542b\u6709\u53ef\u5b66\u4e60\u53c2\u6570\u7684\u6fc0\u6d3b\u51fd\u6570\uff0c\u53ef\u901a\u8fc7 \u9ad8\u7ea7\u6fc0\u6d3b\u51fd\u6570 \u5b9e\u73b0\uff0c\u53ef\u4ee5\u5728 keras.layers.advanced_activations \u6a21\u5757\u4e2d\u627e\u5230\u3002 \u8fd9\u4e9b\u9ad8\u7ea7\u6fc0\u6d3b\u51fd\u6570\u5305\u62ec PReLU \u548c LeakyReLU \u3002","title":"\u9ad8\u7ea7\u6fc0\u6d3b\u51fd\u6570"},{"location":"6.callbacks/","text":"\u56de\u8c03\u51fd\u6570 Callbacks \u56de\u8c03\u51fd\u6570\u4f7f\u7528 \u56de\u8c03\u51fd\u6570\u662f\u4e00\u4e2a\u51fd\u6570\u7684\u5408\u96c6\uff0c\u4f1a\u5728\u8bad\u7ec3\u7684\u9636\u6bb5\u4e2d\u6240\u4f7f\u7528\u3002\u4f60\u53ef\u4ee5\u4f7f\u7528\u56de\u8c03\u51fd\u6570\u6765\u67e5\u770b\u8bad\u7ec3\u6a21\u578b\u7684\u5185\u5728\u72b6\u6001\u548c\u7edf\u8ba1\u3002\u4f60\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5217\u8868\u7684\u56de\u8c03\u51fd\u6570\uff08\u4f5c\u4e3a callbacks \u5173\u952e\u5b57\u53c2\u6570\uff09\u5230 Sequential \u6216 Model \u7c7b\u578b\u7684 .fit() \u65b9\u6cd5\u3002\u5728\u8bad\u7ec3\u65f6\uff0c\u76f8\u5e94\u7684\u56de\u8c03\u51fd\u6570\u7684\u65b9\u6cd5\u5c31\u4f1a\u88ab\u5728\u5404\u81ea\u7684\u9636\u6bb5\u88ab\u8c03\u7528\u3002 [source] Callback keras.callbacks.Callback() \u7528\u6765\u7ec4\u5efa\u65b0\u7684\u56de\u8c03\u51fd\u6570\u7684\u62bd\u8c61\u57fa\u7c7b\u3002 \u5c5e\u6027 params : \u5b57\u5178\u3002\u8bad\u7ec3\u53c2\u6570\uff0c (\u4f8b\u5982\uff0cverbosity, batch size, number of epochs...)\u3002 model : keras.models.Model \u7684\u5b9e\u4f8b\u3002 \u6307\u4ee3\u88ab\u8bad\u7ec3\u6a21\u578b\u3002 \u88ab\u56de\u8c03\u51fd\u6570\u4f5c\u4e3a\u53c2\u6570\u7684 logs \u5b57\u5178\uff0c\u5b83\u4f1a\u542b\u6709\u4e8e\u5f53\u524d\u6279\u91cf\u6216\u8bad\u7ec3\u8f6e\u76f8\u5173\u6570\u636e\u7684\u952e\u3002 \u76ee\u524d\uff0c Sequential \u6a21\u578b\u7c7b\u7684 .fit() \u65b9\u6cd5\u4f1a\u5728\u4f20\u5165\u5230\u56de\u8c03\u51fd\u6570\u7684 logs \u91cc\u9762\u5305\u542b\u4ee5\u4e0b\u7684\u6570\u636e\uff1a on_epoch_end : \u5305\u62ec acc \u548c loss \u7684\u65e5\u5fd7\uff0c \u4e5f\u53ef\u4ee5\u9009\u62e9\u6027\u7684\u5305\u62ec val_loss \uff08\u5982\u679c\u5728 fit \u4e2d\u542f\u7528\u9a8c\u8bc1\uff09\uff0c\u548c val_acc \uff08\u5982\u679c\u542f\u7528\u9a8c\u8bc1\u548c\u76d1\u6d4b\u7cbe\u786e\u503c\uff09\u3002 on_batch_begin : \u5305\u62ec size \u7684\u65e5\u5fd7\uff0c\u5728\u5f53\u524d\u6279\u91cf\u5185\u7684\u6837\u672c\u6570\u91cf\u3002 on_batch_end : \u5305\u62ec loss \u7684\u65e5\u5fd7\uff0c\u4e5f\u53ef\u4ee5\u9009\u62e9\u6027\u7684\u5305\u62ec acc \uff08\u5982\u679c\u542f\u7528\u76d1\u6d4b\u7cbe\u786e\u503c\uff09\u3002 [source] BaseLogger keras.callbacks.BaseLogger(stateful_metrics=None) \u4f1a\u79ef\u7d2f\u8bad\u7ec3\u8f6e\u5e73\u5747\u8bc4\u4f30\u7684\u56de\u8c03\u51fd\u6570\u3002 \u8fd9\u4e2a\u56de\u8c03\u51fd\u6570\u88ab\u81ea\u52a8\u5e94\u7528\u5230\u6bcf\u4e00\u4e2a Keras \u6a21\u578b\u4e0a\u9762\u3002 \u53c2\u6570 stateful_metrics : \u53ef\u91cd\u590d\u4f7f\u7528\u4e0d\u5e94\u5728\u4e00\u4e2a epoch \u4e0a\u5e73\u5747\u7684\u6307\u6807\u7684\u5b57\u7b26\u4e32\u540d\u79f0\u3002 \u6b64\u5217\u8868\u4e2d\u7684\u5ea6\u91cf\u6807\u51c6\u5c06\u6309\u539f\u6837\u8bb0\u5f55\u5728 on_epoch_end \u4e2d\u3002 \u6240\u6709\u5176\u4ed6\u6307\u6807\u5c06\u5728 on_epoch_end \u4e2d\u53d6\u5e73\u5747\u503c\u3002 [source] TerminateOnNaN keras.callbacks.TerminateOnNaN() \u5f53\u9047\u5230 NaN \u635f\u5931\u4f1a\u505c\u6b62\u8bad\u7ec3\u7684\u56de\u8c03\u51fd\u6570\u3002 [source] ProgbarLogger keras.callbacks.ProgbarLogger(count_mode='samples', stateful_metrics=None) \u4f1a\u628a\u8bc4\u4f30\u4ee5\u6807\u51c6\u8f93\u51fa\u6253\u5370\u7684\u56de\u8c03\u51fd\u6570\u3002 \u53c2\u6570 count_mode : \"steps\" \u6216\u8005 \"samples\"\u3002 \u8fdb\u5ea6\u6761\u662f\u5426\u5e94\u8be5\u8ba1\u6570\u770b\u89c1\u7684\u6837\u672c\u6216\u6b65\u9aa4\uff08\u6279\u91cf\uff09\u3002 stateful_metrics : \u53ef\u91cd\u590d\u4f7f\u7528\u4e0d\u5e94\u5728\u4e00\u4e2a epoch \u4e0a\u5e73\u5747\u7684\u6307\u6807\u7684\u5b57\u7b26\u4e32\u540d\u79f0\u3002 \u6b64\u5217\u8868\u4e2d\u7684\u5ea6\u91cf\u6807\u51c6\u5c06\u6309\u539f\u6837\u8bb0\u5f55\u5728 on_epoch_end \u4e2d\u3002 \u6240\u6709\u5176\u4ed6\u6307\u6807\u5c06\u5728 on_epoch_end \u4e2d\u53d6\u5e73\u5747\u503c\u3002 \u5f02\u5e38 ValueError : \u5982\u679c count_mode [source] History keras.callbacks.History() \u628a\u6240\u6709\u4e8b\u4ef6\u90fd\u8bb0\u5f55\u5230 History \u5bf9\u8c61\u7684\u56de\u8c03\u51fd\u6570\u3002 \u8fd9\u4e2a\u56de\u8c03\u51fd\u6570\u88ab\u81ea\u52a8\u542f\u7528\u5230\u6bcf\u4e00\u4e2a Keras \u6a21\u578b\u3002 History \u5bf9\u8c61\u4f1a\u88ab\u6a21\u578b\u7684 fit \u65b9\u6cd5\u8fd4\u56de\u3002 [source] ModelCheckpoint keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1) \u5728\u6bcf\u4e2a\u8bad\u7ec3\u671f\u4e4b\u540e\u4fdd\u5b58\u6a21\u578b\u3002 filepath \u53ef\u4ee5\u5305\u62ec\u547d\u540d\u683c\u5f0f\u9009\u9879\uff0c\u53ef\u4ee5\u7531 epoch \u7684\u503c\u548c logs \u7684\u952e\uff08\u7531 on_epoch_end \u53c2\u6570\u4f20\u9012\uff09\u6765\u586b\u5145\u3002 \u4f8b\u5982\uff1a\u5982\u679c filepath \u662f weights.{epoch:02d}-{val_loss:.2f}.hdf5 \uff0c \u90a3\u4e48\u6a21\u578b\u88ab\u4fdd\u5b58\u7684\u7684\u6587\u4ef6\u540d\u5c31\u4f1a\u6709\u8bad\u7ec3\u8f6e\u6570\u548c\u9a8c\u8bc1\u635f\u5931\u3002 \u53c2\u6570 filepath : \u5b57\u7b26\u4e32\uff0c\u4fdd\u5b58\u6a21\u578b\u7684\u8def\u5f84\u3002 monitor : \u88ab\u76d1\u6d4b\u7684\u6570\u636e\u3002 verbose : \u8be6\u7ec6\u4fe1\u606f\u6a21\u5f0f\uff0c0 \u6216\u8005 1 \u3002 save_best_only : \u5982\u679c save_best_only=True \uff0c \u88ab\u76d1\u6d4b\u6570\u636e\u7684\u6700\u4f73\u6a21\u578b\u5c31\u4e0d\u4f1a\u88ab\u8986\u76d6\u3002 mode : {auto, min, max} \u7684\u5176\u4e2d\u4e4b\u4e00\u3002 \u5982\u679c save_best_only=True \uff0c\u90a3\u4e48\u662f\u5426\u8986\u76d6\u4fdd\u5b58\u6587\u4ef6\u7684\u51b3\u5b9a\u5c31\u53d6\u51b3\u4e8e\u88ab\u76d1\u6d4b\u6570\u636e\u7684\u6700\u5927\u6216\u8005\u6700\u5c0f\u503c\u3002 \u5bf9\u4e8e val_acc \uff0c\u6a21\u5f0f\u5c31\u4f1a\u662f max \uff0c\u800c\u5bf9\u4e8e val_loss \uff0c\u6a21\u5f0f\u5c31\u9700\u8981\u662f min \uff0c\u7b49\u7b49\u3002 \u5728 auto \u6a21\u5f0f\u4e2d\uff0c\u65b9\u5411\u4f1a\u81ea\u52a8\u4ece\u88ab\u76d1\u6d4b\u7684\u6570\u636e\u7684\u540d\u5b57\u4e2d\u5224\u65ad\u51fa\u6765\u3002 save_weights_only : \u5982\u679c True\uff0c\u90a3\u4e48\u53ea\u6709\u6a21\u578b\u7684\u6743\u91cd\u4f1a\u88ab\u4fdd\u5b58 ( model.save_weights(filepath) )\uff0c \u5426\u5219\u7684\u8bdd\uff0c\u6574\u4e2a\u6a21\u578b\u4f1a\u88ab\u4fdd\u5b58 ( model.save(filepath) )\u3002 period : \u6bcf\u4e2a\u68c0\u67e5\u70b9\u4e4b\u95f4\u7684\u95f4\u9694\uff08\u8bad\u7ec3\u8f6e\u6570\uff09\u3002 [source] EarlyStopping keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False) \u5f53\u88ab\u76d1\u6d4b\u7684\u6570\u91cf\u4e0d\u518d\u63d0\u5347\uff0c\u5219\u505c\u6b62\u8bad\u7ec3\u3002 \u53c2\u6570 monitor : \u88ab\u76d1\u6d4b\u7684\u6570\u636e\u3002 min_delta : \u5728\u88ab\u76d1\u6d4b\u7684\u6570\u636e\u4e2d\u88ab\u8ba4\u4e3a\u662f\u63d0\u5347\u7684\u6700\u5c0f\u53d8\u5316\uff0c \u4f8b\u5982\uff0c\u5c0f\u4e8e min_delta \u7684\u7edd\u5bf9\u53d8\u5316\u4f1a\u88ab\u8ba4\u4e3a\u6ca1\u6709\u63d0\u5347\u3002 patience : \u6ca1\u6709\u8fdb\u6b65\u7684\u8bad\u7ec3\u8f6e\u6570\uff0c\u5728\u8fd9\u4e4b\u540e\u8bad\u7ec3\u5c31\u4f1a\u88ab\u505c\u6b62\u3002 verbose : \u8be6\u7ec6\u4fe1\u606f\u6a21\u5f0f\u3002 mode : {auto, min, max} \u5176\u4e2d\u4e4b\u4e00\u3002 \u5728 min \u6a21\u5f0f\u4e2d\uff0c \u5f53\u88ab\u76d1\u6d4b\u7684\u6570\u636e\u505c\u6b62\u4e0b\u964d\uff0c\u8bad\u7ec3\u5c31\u4f1a\u505c\u6b62\uff1b\u5728 max \u6a21\u5f0f\u4e2d\uff0c\u5f53\u88ab\u76d1\u6d4b\u7684\u6570\u636e\u505c\u6b62\u4e0a\u5347\uff0c\u8bad\u7ec3\u5c31\u4f1a\u505c\u6b62\uff1b\u5728 auto \u6a21\u5f0f\u4e2d\uff0c\u65b9\u5411\u4f1a\u81ea\u52a8\u4ece\u88ab\u76d1\u6d4b\u7684\u6570\u636e\u7684\u540d\u5b57\u4e2d\u5224\u65ad\u51fa\u6765\u3002 baseline : \u8981\u76d1\u63a7\u7684\u6570\u91cf\u7684\u57fa\u51c6\u503c\u3002 \u5982\u679c\u6a21\u578b\u6ca1\u6709\u663e\u793a\u57fa\u51c6\u7684\u6539\u5584\uff0c\u8bad\u7ec3\u5c06\u505c\u6b62\u3002 restore_best_weights : \u662f\u5426\u4ece\u5177\u6709\u76d1\u6d4b\u6570\u91cf\u7684\u6700\u4f73\u503c\u7684\u65f6\u671f\u6062\u590d\u6a21\u578b\u6743\u91cd\u3002 \u5982\u679c\u4e3a False\uff0c\u5219\u4f7f\u7528\u5728\u8bad\u7ec3\u7684\u6700\u540e\u4e00\u6b65\u83b7\u5f97\u7684\u6a21\u578b\u6743\u91cd\u3002 [source] RemoteMonitor keras.callbacks.RemoteMonitor(root='http://localhost:9000', path='/publish/epoch/end/', field='data', headers=None, send_as_json=False) \u5c06\u4e8b\u4ef6\u6570\u636e\u6d41\u5230\u670d\u52a1\u5668\u7684\u56de\u8c03\u51fd\u6570\u3002 \u9700\u8981 requests \u5e93\u3002 \u4e8b\u4ef6\u88ab\u9ed8\u8ba4\u53d1\u9001\u5230 root + '/publish/epoch/end/' \u3002 \u91c7\u7528 HTTP POST \uff0c\u5176\u4e2d\u7684 data \u53c2\u6570\u662f\u4ee5 JSON \u7f16\u7801\u7684\u4e8b\u4ef6\u6570\u636e\u5b57\u5178\u3002 \u5982\u679c send_as_json \u8bbe\u7f6e\u4e3a True\uff0c\u8bf7\u6c42\u7684 content type \u662f application/json\u3002\u5426\u5219\uff0c\u5c06\u5728\u8868\u5355\u4e2d\u53d1\u9001\u5e8f\u5217\u5316\u7684 JSON\u3002 \u53c2\u6570 root : \u5b57\u7b26\u4e32\uff1b\u76ee\u6807\u670d\u52a1\u5668\u7684\u6839\u5730\u5740\u3002 path : \u5b57\u7b26\u4e32\uff1b\u76f8\u5bf9\u4e8e root \u7684\u8def\u5f84\uff0c\u4e8b\u4ef6\u6570\u636e\u88ab\u9001\u8fbe\u7684\u5730\u5740\u3002 field : \u5b57\u7b26\u4e32\uff1bJSON \uff0c\u6570\u636e\u88ab\u4fdd\u5b58\u7684\u9886\u57df\u3002 headers : \u5b57\u5178\uff1b\u53ef\u9009\u81ea\u5b9a\u4e49\u7684 HTTP \u7684\u5934\u5b57\u6bb5\u3002 send_as_json : \u5e03\u5c14\u503c\uff1b\u8bf7\u6c42\u662f\u5426\u5e94\u8be5\u4ee5 application/json \u683c\u5f0f\u53d1\u9001\u3002 [source] LearningRateScheduler keras.callbacks.LearningRateScheduler(schedule, verbose=0) \u5b66\u4e60\u901f\u7387\u5b9a\u65f6\u5668\u3002 \u53c2\u6570 schedule : \u4e00\u4e2a\u51fd\u6570\uff0c\u63a5\u53d7\u8f6e\u7d22\u5f15\u6570\u4f5c\u4e3a\u8f93\u5165\uff08\u6574\u6570\uff0c\u4ece 0 \u5f00\u59cb\u8fed\u4ee3\uff09 \u7136\u540e\u8fd4\u56de\u4e00\u4e2a\u5b66\u4e60\u901f\u7387\u4f5c\u4e3a\u8f93\u51fa\uff08\u6d6e\u70b9\u6570\uff09\u3002 verbose : \u6574\u6570\u3002 0\uff1a\u5b89\u9759\uff0c1\uff1a\u66f4\u65b0\u4fe1\u606f\u3002 [source] TensorBoard keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch') Tensorboard \u57fa\u672c\u53ef\u89c6\u5316\u3002 TensorBoard \u662f\u7531 Tensorflow \u63d0\u4f9b\u7684\u4e00\u4e2a\u53ef\u89c6\u5316\u5de5\u5177\u3002 \u8fd9\u4e2a\u56de\u8c03\u51fd\u6570\u4e3a Tensorboard \u7f16\u5199\u4e00\u4e2a\u65e5\u5fd7\uff0c \u8fd9\u6837\u4f60\u53ef\u4ee5\u53ef\u89c6\u5316\u6d4b\u8bd5\u548c\u8bad\u7ec3\u7684\u6807\u51c6\u8bc4\u4f30\u7684\u52a8\u6001\u56fe\u50cf\uff0c \u4e5f\u53ef\u4ee5\u53ef\u89c6\u5316\u6a21\u578b\u4e2d\u4e0d\u540c\u5c42\u7684\u6fc0\u6d3b\u503c\u76f4\u65b9\u56fe\u3002 \u5982\u679c\u4f60\u5df2\u7ecf\u4f7f\u7528 pip \u5b89\u88c5\u4e86 Tensorflow\uff0c\u4f60\u5e94\u8be5\u53ef\u4ee5\u4ece\u547d\u4ee4\u884c\u542f\u52a8 Tensorflow\uff1a tensorboard --logdir=/full_path_to_your_logs \u53c2\u6570 log_dir : \u7528\u6765\u4fdd\u5b58\u88ab TensorBoard \u5206\u6790\u7684\u65e5\u5fd7\u6587\u4ef6\u7684\u6587\u4ef6\u540d\u3002 histogram_freq : \u5bf9\u4e8e\u6a21\u578b\u4e2d\u5404\u4e2a\u5c42\u8ba1\u7b97\u6fc0\u6d3b\u503c\u548c\u6a21\u578b\u6743\u91cd\u76f4\u65b9\u56fe\u7684\u9891\u7387\uff08\u8bad\u7ec3\u8f6e\u6570\u4e2d\uff09\u3002 \u5982\u679c\u8bbe\u7f6e\u6210 0 \uff0c\u76f4\u65b9\u56fe\u4e0d\u4f1a\u88ab\u8ba1\u7b97\u3002\u5bf9\u4e8e\u76f4\u65b9\u56fe\u53ef\u89c6\u5316\u7684\u9a8c\u8bc1\u6570\u636e\uff08\u6216\u5206\u79bb\u6570\u636e\uff09\u4e00\u5b9a\u8981\u660e\u786e\u7684\u6307\u51fa\u3002 write_graph : \u662f\u5426\u5728 TensorBoard \u4e2d\u53ef\u89c6\u5316\u56fe\u50cf\u3002 \u5982\u679c write_graph \u88ab\u8bbe\u7f6e\u4e3a True\uff0c\u65e5\u5fd7\u6587\u4ef6\u4f1a\u53d8\u5f97\u975e\u5e38\u5927\u3002 write_grads : \u662f\u5426\u5728 TensorBoard \u4e2d\u53ef\u89c6\u5316\u68af\u5ea6\u503c\u76f4\u65b9\u56fe\u3002 histogram_freq \u5fc5\u987b\u8981\u5927\u4e8e 0 \u3002 batch_size : \u7528\u4ee5\u76f4\u65b9\u56fe\u8ba1\u7b97\u7684\u4f20\u5165\u795e\u7ecf\u5143\u7f51\u7edc\u8f93\u5165\u6279\u7684\u5927\u5c0f\u3002 write_images : \u662f\u5426\u5728 TensorBoard \u4e2d\u5c06\u6a21\u578b\u6743\u91cd\u4ee5\u56fe\u7247\u53ef\u89c6\u5316\u3002 embeddings_freq : \u88ab\u9009\u4e2d\u7684\u5d4c\u5165\u5c42\u4f1a\u88ab\u4fdd\u5b58\u7684\u9891\u7387\uff08\u5728\u8bad\u7ec3\u8f6e\u4e2d\uff09\u3002 embeddings_layer_names : \u4e00\u4e2a\u5217\u8868\uff0c\u4f1a\u88ab\u76d1\u6d4b\u5c42\u7684\u540d\u5b57\u3002 \u5982\u679c\u662f None \u6216\u7a7a\u5217\u8868\uff0c\u90a3\u4e48\u6240\u6709\u7684\u5d4c\u5165\u5c42\u90fd\u4f1a\u88ab\u76d1\u6d4b\u3002 embeddings_metadata : \u4e00\u4e2a\u5b57\u5178\uff0c\u5bf9\u5e94\u5c42\u7684\u540d\u5b57\u5230\u4fdd\u5b58\u6709\u8fd9\u4e2a\u5d4c\u5165\u5c42\u5143\u6570\u636e\u6587\u4ef6\u7684\u540d\u5b57\u3002 \u67e5\u770b \u8be6\u60c5 \u5173\u4e8e\u5143\u6570\u636e\u7684\u6570\u636e\u683c\u5f0f\u3002 \u4ee5\u9632\u540c\u6837\u7684\u5143\u6570\u636e\u88ab\u7528\u4e8e\u6240\u7528\u7684\u5d4c\u5165\u5c42\uff0c\u5b57\u7b26\u4e32\u53ef\u4ee5\u88ab\u4f20\u5165\u3002 embeddings_data : \u8981\u5d4c\u5165\u5728 embeddings_layer_names \u6307\u5b9a\u7684\u5c42\u7684\u6570\u636e\u3002 Numpy \u6570\u7ec4\uff08\u5982\u679c\u6a21\u578b\u6709\u5355\u4e2a\u8f93\u5165\uff09\u6216 Numpy \u6570\u7ec4\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u6709\u591a\u4e2a\u8f93\u5165\uff09\u3002 Learn ore about embeddings \u3002 update_freq : 'batch' \u6216 'epoch' \u6216 \u6574\u6570\u3002\u5f53\u4f7f\u7528 'batch' \u65f6\uff0c\u5728\u6bcf\u4e2a batch \u4e4b\u540e\u5c06\u635f\u5931\u548c\u8bc4\u4f30\u503c\u5199\u5165\u5230 TensorBoard \u4e2d\u3002\u540c\u6837\u7684\u60c5\u51b5\u5e94\u7528\u5230 'epoch' \u4e2d\u3002\u5982\u679c\u4f7f\u7528\u6574\u6570\uff0c\u4f8b\u5982 10000 \uff0c\u8fd9\u4e2a\u56de\u8c03\u4f1a\u5728\u6bcf 10000 \u4e2a\u6837\u672c\u4e4b\u540e\u5c06\u635f\u5931\u548c\u8bc4\u4f30\u503c\u5199\u5165\u5230 TensorBoard \u4e2d\u3002\u6ce8\u610f\uff0c\u9891\u7e41\u5730\u5199\u5165\u5230 TensorBoard \u4f1a\u51cf\u7f13\u4f60\u7684\u8bad\u7ec3\u3002 [source] ReduceLROnPlateau keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0) \u5f53\u6807\u51c6\u8bc4\u4f30\u505c\u6b62\u63d0\u5347\u65f6\uff0c\u964d\u4f4e\u5b66\u4e60\u901f\u7387\u3002 \u5f53\u5b66\u4e60\u505c\u6b62\u65f6\uff0c\u6a21\u578b\u603b\u662f\u4f1a\u53d7\u76ca\u4e8e\u964d\u4f4e 2-10 \u500d\u7684\u5b66\u4e60\u901f\u7387\u3002 \u8fd9\u4e2a\u56de\u8c03\u51fd\u6570\u76d1\u6d4b\u4e00\u4e2a\u6570\u636e\u5e76\u4e14\u5f53\u8fd9\u4e2a\u6570\u636e\u5728\u4e00\u5b9a\u300c\u6709\u8010\u5fc3\u300d\u7684\u8bad\u7ec3\u8f6e\u4e4b\u540e\u8fd8\u6ca1\u6709\u8fdb\u6b65\uff0c \u90a3\u4e48\u5b66\u4e60\u901f\u7387\u5c31\u4f1a\u88ab\u964d\u4f4e\u3002 \u4f8b\u5b50 reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001) model.fit(X_train, Y_train, callbacks=[reduce_lr]) \u53c2\u6570 monitor : \u88ab\u76d1\u6d4b\u7684\u6570\u636e\u3002 factor : \u5b66\u4e60\u901f\u7387\u88ab\u964d\u4f4e\u7684\u56e0\u6570\u3002\u65b0\u7684\u5b66\u4e60\u901f\u7387 = \u5b66\u4e60\u901f\u7387 * \u56e0\u6570 patience : \u6ca1\u6709\u8fdb\u6b65\u7684\u8bad\u7ec3\u8f6e\u6570\uff0c\u5728\u8fd9\u4e4b\u540e\u8bad\u7ec3\u901f\u7387\u4f1a\u88ab\u964d\u4f4e\u3002 verbose : \u6574\u6570\u30020\uff1a\u5b89\u9759\uff0c1\uff1a\u66f4\u65b0\u4fe1\u606f\u3002 mode : {auto, min, max} \u5176\u4e2d\u4e4b\u4e00\u3002\u5982\u679c\u662f min \u6a21\u5f0f\uff0c\u5b66\u4e60\u901f\u7387\u4f1a\u88ab\u964d\u4f4e\u5982\u679c\u88ab\u76d1\u6d4b\u7684\u6570\u636e\u5df2\u7ecf\u505c\u6b62\u4e0b\u964d\uff1b \u5728 max \u6a21\u5f0f\uff0c\u5b66\u4e60\u5851\u6599\u4f1a\u88ab\u964d\u4f4e\u5982\u679c\u88ab\u76d1\u6d4b\u7684\u6570\u636e\u5df2\u7ecf\u505c\u6b62\u4e0a\u5347\uff1b \u5728 auto \u6a21\u5f0f\uff0c\u65b9\u5411\u4f1a\u88ab\u4ece\u88ab\u76d1\u6d4b\u7684\u6570\u636e\u4e2d\u81ea\u52a8\u63a8\u65ad\u51fa\u6765\u3002 min_delta : \u5bf9\u4e8e\u6d4b\u91cf\u65b0\u7684\u6700\u4f18\u5316\u7684\u9600\u503c\uff0c\u53ea\u5173\u6ce8\u5de8\u5927\u7684\u6539\u53d8\u3002 cooldown : \u5728\u5b66\u4e60\u901f\u7387\u88ab\u964d\u4f4e\u4e4b\u540e\uff0c\u91cd\u65b0\u6062\u590d\u6b63\u5e38\u64cd\u4f5c\u4e4b\u524d\u7b49\u5f85\u7684\u8bad\u7ec3\u8f6e\u6570\u91cf\u3002 min_lr : \u5b66\u4e60\u901f\u7387\u7684\u4e0b\u8fb9\u754c\u3002 [source] CSVLogger keras.callbacks.CSVLogger(filename, separator=',', append=False) \u628a\u8bad\u7ec3\u8f6e\u7ed3\u679c\u6570\u636e\u6d41\u5230 csv \u6587\u4ef6\u7684\u56de\u8c03\u51fd\u6570\u3002 \u652f\u6301\u6240\u6709\u53ef\u4ee5\u88ab\u4f5c\u4e3a\u5b57\u7b26\u4e32\u8868\u793a\u7684\u503c\uff0c\u5305\u62ec 1D \u53ef\u8fed\u4ee3\u6570\u636e\uff0c\u4f8b\u5982\uff0cnp.ndarray\u3002 \u4f8b\u5b50 csv_logger = CSVLogger('training.log') model.fit(X_train, Y_train, callbacks=[csv_logger]) \u53c2\u6570 filename : csv \u6587\u4ef6\u7684\u6587\u4ef6\u540d\uff0c\u4f8b\u5982 'run/log.csv'\u3002 separator : \u7528\u6765\u9694\u79bb csv \u6587\u4ef6\u4e2d\u5143\u7d20\u7684\u5b57\u7b26\u4e32\u3002 append : True\uff1a\u5982\u679c\u6587\u4ef6\u5b58\u5728\u5219\u589e\u52a0\uff08\u53ef\u4ee5\u88ab\u7528\u4e8e\u7ee7\u7eed\u8bad\u7ec3\uff09\u3002False\uff1a\u8986\u76d6\u5b58\u5728\u7684\u6587\u4ef6\u3002 [source] LambdaCallback keras.callbacks.LambdaCallback(on_epoch_begin=None, on_epoch_end=None, on_batch_begin=None, on_batch_end=None, on_train_begin=None, on_train_end=None) \u5728\u8bad\u7ec3\u8fdb\u884c\u4e2d\u521b\u5efa\u7b80\u5355\uff0c\u81ea\u5b9a\u4e49\u7684\u56de\u8c03\u51fd\u6570\u7684\u56de\u8c03\u51fd\u6570\u3002 \u8fd9\u4e2a\u56de\u8c03\u51fd\u6570\u548c\u533f\u540d\u51fd\u6570\u5728\u5408\u9002\u7684\u65f6\u95f4\u88ab\u521b\u5efa\u3002 \u9700\u8981\u6ce8\u610f\u7684\u662f\u56de\u8c03\u51fd\u6570\u8981\u6c42\u4f4d\u7f6e\u578b\u53c2\u6570\uff0c\u5982\u4e0b\uff1a on_epoch_begin \u548c on_epoch_end \u8981\u6c42\u4e24\u4e2a\u4f4d\u7f6e\u578b\u7684\u53c2\u6570\uff1a epoch , logs on_batch_begin \u548c on_batch_end \u8981\u6c42\u4e24\u4e2a\u4f4d\u7f6e\u578b\u7684\u53c2\u6570\uff1a batch , logs on_train_begin \u548c on_train_end \u8981\u6c42\u4e00\u4e2a\u4f4d\u7f6e\u578b\u7684\u53c2\u6570\uff1a logs \u53c2\u6570 on_epoch_begin : \u5728\u6bcf\u8f6e\u5f00\u59cb\u65f6\u88ab\u8c03\u7528\u3002 on_epoch_end : \u5728\u6bcf\u8f6e\u7ed3\u675f\u65f6\u88ab\u8c03\u7528\u3002 on_batch_begin : \u5728\u6bcf\u6279\u5f00\u59cb\u65f6\u88ab\u8c03\u7528\u3002 on_batch_end : \u5728\u6bcf\u6279\u7ed3\u675f\u65f6\u88ab\u8c03\u7528\u3002 on_train_begin : \u5728\u6a21\u578b\u8bad\u7ec3\u5f00\u59cb\u65f6\u88ab\u8c03\u7528\u3002 on_train_end : \u5728\u6a21\u578b\u8bad\u7ec3\u7ed3\u675f\u65f6\u88ab\u8c03\u7528\u3002 \u4f8b\u5b50 # \u5728\u6bcf\u4e00\u4e2a\u6279\u5f00\u59cb\u65f6\uff0c\u6253\u5370\u51fa\u6279\u6570\u3002 batch_print_callback = LambdaCallback( on_batch_begin=lambda batch,logs: print(batch)) # \u628a\u8bad\u7ec3\u8f6e\u635f\u5931\u6570\u636e\u6d41\u5230 JSON \u683c\u5f0f\u7684\u6587\u4ef6\u3002\u6587\u4ef6\u7684\u5185\u5bb9 # \u4e0d\u662f\u5b8c\u7f8e\u7684 JSON \u683c\u5f0f\uff0c\u4f46\u662f\u65f6\u6bcf\u4e00\u884c\u90fd\u662f JSON \u5bf9\u8c61\u3002 import json json_log = open('loss_log.json', mode='wt', buffering=1) json_logging_callback = LambdaCallback( on_epoch_end=lambda epoch, logs: json_log.write( json.dumps({'epoch': epoch, 'loss': logs['loss']}) + '\\n'), on_train_end=lambda logs: json_log.close() ) # \u5728\u5b8c\u6210\u6a21\u578b\u8bad\u7ec3\u4e4b\u540e\uff0c\u7ed3\u675f\u4e00\u4e9b\u8fdb\u7a0b\u3002 processes = ... cleanup_callback = LambdaCallback( on_train_end=lambda logs: [ p.terminate() for p in processes if p.is_alive()]) model.fit(..., callbacks=[batch_print_callback, json_logging_callback, cleanup_callback]) \u521b\u5efa\u4e00\u4e2a\u56de\u8c03\u51fd\u6570 \u4f60\u53ef\u4ee5\u901a\u8fc7\u6269\u5c55 keras.callbacks.Callback \u57fa\u7c7b\u6765\u521b\u5efa\u4e00\u4e2a\u81ea\u5b9a\u4e49\u7684\u56de\u8c03\u51fd\u6570\u3002 \u901a\u8fc7\u7c7b\u7684\u5c5e\u6027 self.model \uff0c\u56de\u8c03\u51fd\u6570\u53ef\u4ee5\u83b7\u5f97\u5b83\u6240\u8054\u7cfb\u7684\u6a21\u578b\u3002 \u4e0b\u9762\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\uff0c\u5728\u8bad\u7ec3\u65f6\uff0c\u4fdd\u5b58\u4e00\u4e2a\u5217\u8868\u7684\u6279\u91cf\u635f\u5931\u503c\uff1a class LossHistory(keras.callbacks.Callback): def on_train_begin(self, logs={}): self.losses = [] def on_batch_end(self, batch, logs={}): self.losses.append(logs.get('loss')) \u4f8b: \u8bb0\u5f55\u635f\u5931\u5386\u53f2 class LossHistory(keras.callbacks.Callback): def on_train_begin(self, logs={}): self.losses = [] def on_batch_end(self, batch, logs={}): self.losses.append(logs.get('loss')) model = Sequential() model.add(Dense(10, input_dim=784, kernel_initializer='uniform')) model.add(Activation('softmax')) model.compile(loss='categorical_crossentropy', optimizer='rmsprop') history = LossHistory() model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=0, callbacks=[history]) print(history.losses) # \u8f93\u51fa ''' [0.66047596406559383, 0.3547245744908703, ..., 0.25953155204159617, 0.25901699725311789] ''' \u4f8b: \u6a21\u578b\u68c0\u67e5\u70b9 from keras.callbacks import ModelCheckpoint model = Sequential() model.add(Dense(10, input_dim=784, kernel_initializer='uniform')) model.add(Activation('softmax')) model.compile(loss='categorical_crossentropy', optimizer='rmsprop') ''' \u5982\u679c\u9a8c\u8bc1\u635f\u5931\u4e0b\u964d\uff0c \u90a3\u4e48\u5728\u6bcf\u4e2a\u8bad\u7ec3\u8f6e\u4e4b\u540e\u4fdd\u5b58\u6a21\u578b\u3002 ''' checkpointer = ModelCheckpoint(filepath='/tmp/weights.hdf5', verbose=1, save_best_only=True) model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=0, validation_data=(X_test, Y_test), callbacks=[checkpointer])","title":"\u56de\u8c03\u51fd\u6570 Callbacks"},{"location":"6.callbacks/#callbacks","text":"","title":"\u56de\u8c03\u51fd\u6570 Callbacks"},{"location":"6.callbacks/#_1","text":"\u56de\u8c03\u51fd\u6570\u662f\u4e00\u4e2a\u51fd\u6570\u7684\u5408\u96c6\uff0c\u4f1a\u5728\u8bad\u7ec3\u7684\u9636\u6bb5\u4e2d\u6240\u4f7f\u7528\u3002\u4f60\u53ef\u4ee5\u4f7f\u7528\u56de\u8c03\u51fd\u6570\u6765\u67e5\u770b\u8bad\u7ec3\u6a21\u578b\u7684\u5185\u5728\u72b6\u6001\u548c\u7edf\u8ba1\u3002\u4f60\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5217\u8868\u7684\u56de\u8c03\u51fd\u6570\uff08\u4f5c\u4e3a callbacks \u5173\u952e\u5b57\u53c2\u6570\uff09\u5230 Sequential \u6216 Model \u7c7b\u578b\u7684 .fit() \u65b9\u6cd5\u3002\u5728\u8bad\u7ec3\u65f6\uff0c\u76f8\u5e94\u7684\u56de\u8c03\u51fd\u6570\u7684\u65b9\u6cd5\u5c31\u4f1a\u88ab\u5728\u5404\u81ea\u7684\u9636\u6bb5\u88ab\u8c03\u7528\u3002 [source]","title":"\u56de\u8c03\u51fd\u6570\u4f7f\u7528"},{"location":"6.callbacks/#callback","text":"keras.callbacks.Callback() \u7528\u6765\u7ec4\u5efa\u65b0\u7684\u56de\u8c03\u51fd\u6570\u7684\u62bd\u8c61\u57fa\u7c7b\u3002 \u5c5e\u6027 params : \u5b57\u5178\u3002\u8bad\u7ec3\u53c2\u6570\uff0c (\u4f8b\u5982\uff0cverbosity, batch size, number of epochs...)\u3002 model : keras.models.Model \u7684\u5b9e\u4f8b\u3002 \u6307\u4ee3\u88ab\u8bad\u7ec3\u6a21\u578b\u3002 \u88ab\u56de\u8c03\u51fd\u6570\u4f5c\u4e3a\u53c2\u6570\u7684 logs \u5b57\u5178\uff0c\u5b83\u4f1a\u542b\u6709\u4e8e\u5f53\u524d\u6279\u91cf\u6216\u8bad\u7ec3\u8f6e\u76f8\u5173\u6570\u636e\u7684\u952e\u3002 \u76ee\u524d\uff0c Sequential \u6a21\u578b\u7c7b\u7684 .fit() \u65b9\u6cd5\u4f1a\u5728\u4f20\u5165\u5230\u56de\u8c03\u51fd\u6570\u7684 logs \u91cc\u9762\u5305\u542b\u4ee5\u4e0b\u7684\u6570\u636e\uff1a on_epoch_end : \u5305\u62ec acc \u548c loss \u7684\u65e5\u5fd7\uff0c \u4e5f\u53ef\u4ee5\u9009\u62e9\u6027\u7684\u5305\u62ec val_loss \uff08\u5982\u679c\u5728 fit \u4e2d\u542f\u7528\u9a8c\u8bc1\uff09\uff0c\u548c val_acc \uff08\u5982\u679c\u542f\u7528\u9a8c\u8bc1\u548c\u76d1\u6d4b\u7cbe\u786e\u503c\uff09\u3002 on_batch_begin : \u5305\u62ec size \u7684\u65e5\u5fd7\uff0c\u5728\u5f53\u524d\u6279\u91cf\u5185\u7684\u6837\u672c\u6570\u91cf\u3002 on_batch_end : \u5305\u62ec loss \u7684\u65e5\u5fd7\uff0c\u4e5f\u53ef\u4ee5\u9009\u62e9\u6027\u7684\u5305\u62ec acc \uff08\u5982\u679c\u542f\u7528\u76d1\u6d4b\u7cbe\u786e\u503c\uff09\u3002 [source]","title":"Callback"},{"location":"6.callbacks/#baselogger","text":"keras.callbacks.BaseLogger(stateful_metrics=None) \u4f1a\u79ef\u7d2f\u8bad\u7ec3\u8f6e\u5e73\u5747\u8bc4\u4f30\u7684\u56de\u8c03\u51fd\u6570\u3002 \u8fd9\u4e2a\u56de\u8c03\u51fd\u6570\u88ab\u81ea\u52a8\u5e94\u7528\u5230\u6bcf\u4e00\u4e2a Keras \u6a21\u578b\u4e0a\u9762\u3002 \u53c2\u6570 stateful_metrics : \u53ef\u91cd\u590d\u4f7f\u7528\u4e0d\u5e94\u5728\u4e00\u4e2a epoch \u4e0a\u5e73\u5747\u7684\u6307\u6807\u7684\u5b57\u7b26\u4e32\u540d\u79f0\u3002 \u6b64\u5217\u8868\u4e2d\u7684\u5ea6\u91cf\u6807\u51c6\u5c06\u6309\u539f\u6837\u8bb0\u5f55\u5728 on_epoch_end \u4e2d\u3002 \u6240\u6709\u5176\u4ed6\u6307\u6807\u5c06\u5728 on_epoch_end \u4e2d\u53d6\u5e73\u5747\u503c\u3002 [source]","title":"BaseLogger"},{"location":"6.callbacks/#terminateonnan","text":"keras.callbacks.TerminateOnNaN() \u5f53\u9047\u5230 NaN \u635f\u5931\u4f1a\u505c\u6b62\u8bad\u7ec3\u7684\u56de\u8c03\u51fd\u6570\u3002 [source]","title":"TerminateOnNaN"},{"location":"6.callbacks/#progbarlogger","text":"keras.callbacks.ProgbarLogger(count_mode='samples', stateful_metrics=None) \u4f1a\u628a\u8bc4\u4f30\u4ee5\u6807\u51c6\u8f93\u51fa\u6253\u5370\u7684\u56de\u8c03\u51fd\u6570\u3002 \u53c2\u6570 count_mode : \"steps\" \u6216\u8005 \"samples\"\u3002 \u8fdb\u5ea6\u6761\u662f\u5426\u5e94\u8be5\u8ba1\u6570\u770b\u89c1\u7684\u6837\u672c\u6216\u6b65\u9aa4\uff08\u6279\u91cf\uff09\u3002 stateful_metrics : \u53ef\u91cd\u590d\u4f7f\u7528\u4e0d\u5e94\u5728\u4e00\u4e2a epoch \u4e0a\u5e73\u5747\u7684\u6307\u6807\u7684\u5b57\u7b26\u4e32\u540d\u79f0\u3002 \u6b64\u5217\u8868\u4e2d\u7684\u5ea6\u91cf\u6807\u51c6\u5c06\u6309\u539f\u6837\u8bb0\u5f55\u5728 on_epoch_end \u4e2d\u3002 \u6240\u6709\u5176\u4ed6\u6307\u6807\u5c06\u5728 on_epoch_end \u4e2d\u53d6\u5e73\u5747\u503c\u3002 \u5f02\u5e38 ValueError : \u5982\u679c count_mode [source]","title":"ProgbarLogger"},{"location":"6.callbacks/#history","text":"keras.callbacks.History() \u628a\u6240\u6709\u4e8b\u4ef6\u90fd\u8bb0\u5f55\u5230 History \u5bf9\u8c61\u7684\u56de\u8c03\u51fd\u6570\u3002 \u8fd9\u4e2a\u56de\u8c03\u51fd\u6570\u88ab\u81ea\u52a8\u542f\u7528\u5230\u6bcf\u4e00\u4e2a Keras \u6a21\u578b\u3002 History \u5bf9\u8c61\u4f1a\u88ab\u6a21\u578b\u7684 fit \u65b9\u6cd5\u8fd4\u56de\u3002 [source]","title":"History"},{"location":"6.callbacks/#modelcheckpoint","text":"keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1) \u5728\u6bcf\u4e2a\u8bad\u7ec3\u671f\u4e4b\u540e\u4fdd\u5b58\u6a21\u578b\u3002 filepath \u53ef\u4ee5\u5305\u62ec\u547d\u540d\u683c\u5f0f\u9009\u9879\uff0c\u53ef\u4ee5\u7531 epoch \u7684\u503c\u548c logs \u7684\u952e\uff08\u7531 on_epoch_end \u53c2\u6570\u4f20\u9012\uff09\u6765\u586b\u5145\u3002 \u4f8b\u5982\uff1a\u5982\u679c filepath \u662f weights.{epoch:02d}-{val_loss:.2f}.hdf5 \uff0c \u90a3\u4e48\u6a21\u578b\u88ab\u4fdd\u5b58\u7684\u7684\u6587\u4ef6\u540d\u5c31\u4f1a\u6709\u8bad\u7ec3\u8f6e\u6570\u548c\u9a8c\u8bc1\u635f\u5931\u3002 \u53c2\u6570 filepath : \u5b57\u7b26\u4e32\uff0c\u4fdd\u5b58\u6a21\u578b\u7684\u8def\u5f84\u3002 monitor : \u88ab\u76d1\u6d4b\u7684\u6570\u636e\u3002 verbose : \u8be6\u7ec6\u4fe1\u606f\u6a21\u5f0f\uff0c0 \u6216\u8005 1 \u3002 save_best_only : \u5982\u679c save_best_only=True \uff0c \u88ab\u76d1\u6d4b\u6570\u636e\u7684\u6700\u4f73\u6a21\u578b\u5c31\u4e0d\u4f1a\u88ab\u8986\u76d6\u3002 mode : {auto, min, max} \u7684\u5176\u4e2d\u4e4b\u4e00\u3002 \u5982\u679c save_best_only=True \uff0c\u90a3\u4e48\u662f\u5426\u8986\u76d6\u4fdd\u5b58\u6587\u4ef6\u7684\u51b3\u5b9a\u5c31\u53d6\u51b3\u4e8e\u88ab\u76d1\u6d4b\u6570\u636e\u7684\u6700\u5927\u6216\u8005\u6700\u5c0f\u503c\u3002 \u5bf9\u4e8e val_acc \uff0c\u6a21\u5f0f\u5c31\u4f1a\u662f max \uff0c\u800c\u5bf9\u4e8e val_loss \uff0c\u6a21\u5f0f\u5c31\u9700\u8981\u662f min \uff0c\u7b49\u7b49\u3002 \u5728 auto \u6a21\u5f0f\u4e2d\uff0c\u65b9\u5411\u4f1a\u81ea\u52a8\u4ece\u88ab\u76d1\u6d4b\u7684\u6570\u636e\u7684\u540d\u5b57\u4e2d\u5224\u65ad\u51fa\u6765\u3002 save_weights_only : \u5982\u679c True\uff0c\u90a3\u4e48\u53ea\u6709\u6a21\u578b\u7684\u6743\u91cd\u4f1a\u88ab\u4fdd\u5b58 ( model.save_weights(filepath) )\uff0c \u5426\u5219\u7684\u8bdd\uff0c\u6574\u4e2a\u6a21\u578b\u4f1a\u88ab\u4fdd\u5b58 ( model.save(filepath) )\u3002 period : \u6bcf\u4e2a\u68c0\u67e5\u70b9\u4e4b\u95f4\u7684\u95f4\u9694\uff08\u8bad\u7ec3\u8f6e\u6570\uff09\u3002 [source]","title":"ModelCheckpoint"},{"location":"6.callbacks/#earlystopping","text":"keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False) \u5f53\u88ab\u76d1\u6d4b\u7684\u6570\u91cf\u4e0d\u518d\u63d0\u5347\uff0c\u5219\u505c\u6b62\u8bad\u7ec3\u3002 \u53c2\u6570 monitor : \u88ab\u76d1\u6d4b\u7684\u6570\u636e\u3002 min_delta : \u5728\u88ab\u76d1\u6d4b\u7684\u6570\u636e\u4e2d\u88ab\u8ba4\u4e3a\u662f\u63d0\u5347\u7684\u6700\u5c0f\u53d8\u5316\uff0c \u4f8b\u5982\uff0c\u5c0f\u4e8e min_delta \u7684\u7edd\u5bf9\u53d8\u5316\u4f1a\u88ab\u8ba4\u4e3a\u6ca1\u6709\u63d0\u5347\u3002 patience : \u6ca1\u6709\u8fdb\u6b65\u7684\u8bad\u7ec3\u8f6e\u6570\uff0c\u5728\u8fd9\u4e4b\u540e\u8bad\u7ec3\u5c31\u4f1a\u88ab\u505c\u6b62\u3002 verbose : \u8be6\u7ec6\u4fe1\u606f\u6a21\u5f0f\u3002 mode : {auto, min, max} \u5176\u4e2d\u4e4b\u4e00\u3002 \u5728 min \u6a21\u5f0f\u4e2d\uff0c \u5f53\u88ab\u76d1\u6d4b\u7684\u6570\u636e\u505c\u6b62\u4e0b\u964d\uff0c\u8bad\u7ec3\u5c31\u4f1a\u505c\u6b62\uff1b\u5728 max \u6a21\u5f0f\u4e2d\uff0c\u5f53\u88ab\u76d1\u6d4b\u7684\u6570\u636e\u505c\u6b62\u4e0a\u5347\uff0c\u8bad\u7ec3\u5c31\u4f1a\u505c\u6b62\uff1b\u5728 auto \u6a21\u5f0f\u4e2d\uff0c\u65b9\u5411\u4f1a\u81ea\u52a8\u4ece\u88ab\u76d1\u6d4b\u7684\u6570\u636e\u7684\u540d\u5b57\u4e2d\u5224\u65ad\u51fa\u6765\u3002 baseline : \u8981\u76d1\u63a7\u7684\u6570\u91cf\u7684\u57fa\u51c6\u503c\u3002 \u5982\u679c\u6a21\u578b\u6ca1\u6709\u663e\u793a\u57fa\u51c6\u7684\u6539\u5584\uff0c\u8bad\u7ec3\u5c06\u505c\u6b62\u3002 restore_best_weights : \u662f\u5426\u4ece\u5177\u6709\u76d1\u6d4b\u6570\u91cf\u7684\u6700\u4f73\u503c\u7684\u65f6\u671f\u6062\u590d\u6a21\u578b\u6743\u91cd\u3002 \u5982\u679c\u4e3a False\uff0c\u5219\u4f7f\u7528\u5728\u8bad\u7ec3\u7684\u6700\u540e\u4e00\u6b65\u83b7\u5f97\u7684\u6a21\u578b\u6743\u91cd\u3002 [source]","title":"EarlyStopping"},{"location":"6.callbacks/#remotemonitor","text":"keras.callbacks.RemoteMonitor(root='http://localhost:9000', path='/publish/epoch/end/', field='data', headers=None, send_as_json=False) \u5c06\u4e8b\u4ef6\u6570\u636e\u6d41\u5230\u670d\u52a1\u5668\u7684\u56de\u8c03\u51fd\u6570\u3002 \u9700\u8981 requests \u5e93\u3002 \u4e8b\u4ef6\u88ab\u9ed8\u8ba4\u53d1\u9001\u5230 root + '/publish/epoch/end/' \u3002 \u91c7\u7528 HTTP POST \uff0c\u5176\u4e2d\u7684 data \u53c2\u6570\u662f\u4ee5 JSON \u7f16\u7801\u7684\u4e8b\u4ef6\u6570\u636e\u5b57\u5178\u3002 \u5982\u679c send_as_json \u8bbe\u7f6e\u4e3a True\uff0c\u8bf7\u6c42\u7684 content type \u662f application/json\u3002\u5426\u5219\uff0c\u5c06\u5728\u8868\u5355\u4e2d\u53d1\u9001\u5e8f\u5217\u5316\u7684 JSON\u3002 \u53c2\u6570 root : \u5b57\u7b26\u4e32\uff1b\u76ee\u6807\u670d\u52a1\u5668\u7684\u6839\u5730\u5740\u3002 path : \u5b57\u7b26\u4e32\uff1b\u76f8\u5bf9\u4e8e root \u7684\u8def\u5f84\uff0c\u4e8b\u4ef6\u6570\u636e\u88ab\u9001\u8fbe\u7684\u5730\u5740\u3002 field : \u5b57\u7b26\u4e32\uff1bJSON \uff0c\u6570\u636e\u88ab\u4fdd\u5b58\u7684\u9886\u57df\u3002 headers : \u5b57\u5178\uff1b\u53ef\u9009\u81ea\u5b9a\u4e49\u7684 HTTP \u7684\u5934\u5b57\u6bb5\u3002 send_as_json : \u5e03\u5c14\u503c\uff1b\u8bf7\u6c42\u662f\u5426\u5e94\u8be5\u4ee5 application/json \u683c\u5f0f\u53d1\u9001\u3002 [source]","title":"RemoteMonitor"},{"location":"6.callbacks/#learningratescheduler","text":"keras.callbacks.LearningRateScheduler(schedule, verbose=0) \u5b66\u4e60\u901f\u7387\u5b9a\u65f6\u5668\u3002 \u53c2\u6570 schedule : \u4e00\u4e2a\u51fd\u6570\uff0c\u63a5\u53d7\u8f6e\u7d22\u5f15\u6570\u4f5c\u4e3a\u8f93\u5165\uff08\u6574\u6570\uff0c\u4ece 0 \u5f00\u59cb\u8fed\u4ee3\uff09 \u7136\u540e\u8fd4\u56de\u4e00\u4e2a\u5b66\u4e60\u901f\u7387\u4f5c\u4e3a\u8f93\u51fa\uff08\u6d6e\u70b9\u6570\uff09\u3002 verbose : \u6574\u6570\u3002 0\uff1a\u5b89\u9759\uff0c1\uff1a\u66f4\u65b0\u4fe1\u606f\u3002 [source]","title":"LearningRateScheduler"},{"location":"6.callbacks/#tensorboard","text":"keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch') Tensorboard \u57fa\u672c\u53ef\u89c6\u5316\u3002 TensorBoard \u662f\u7531 Tensorflow \u63d0\u4f9b\u7684\u4e00\u4e2a\u53ef\u89c6\u5316\u5de5\u5177\u3002 \u8fd9\u4e2a\u56de\u8c03\u51fd\u6570\u4e3a Tensorboard \u7f16\u5199\u4e00\u4e2a\u65e5\u5fd7\uff0c \u8fd9\u6837\u4f60\u53ef\u4ee5\u53ef\u89c6\u5316\u6d4b\u8bd5\u548c\u8bad\u7ec3\u7684\u6807\u51c6\u8bc4\u4f30\u7684\u52a8\u6001\u56fe\u50cf\uff0c \u4e5f\u53ef\u4ee5\u53ef\u89c6\u5316\u6a21\u578b\u4e2d\u4e0d\u540c\u5c42\u7684\u6fc0\u6d3b\u503c\u76f4\u65b9\u56fe\u3002 \u5982\u679c\u4f60\u5df2\u7ecf\u4f7f\u7528 pip \u5b89\u88c5\u4e86 Tensorflow\uff0c\u4f60\u5e94\u8be5\u53ef\u4ee5\u4ece\u547d\u4ee4\u884c\u542f\u52a8 Tensorflow\uff1a tensorboard --logdir=/full_path_to_your_logs \u53c2\u6570 log_dir : \u7528\u6765\u4fdd\u5b58\u88ab TensorBoard \u5206\u6790\u7684\u65e5\u5fd7\u6587\u4ef6\u7684\u6587\u4ef6\u540d\u3002 histogram_freq : \u5bf9\u4e8e\u6a21\u578b\u4e2d\u5404\u4e2a\u5c42\u8ba1\u7b97\u6fc0\u6d3b\u503c\u548c\u6a21\u578b\u6743\u91cd\u76f4\u65b9\u56fe\u7684\u9891\u7387\uff08\u8bad\u7ec3\u8f6e\u6570\u4e2d\uff09\u3002 \u5982\u679c\u8bbe\u7f6e\u6210 0 \uff0c\u76f4\u65b9\u56fe\u4e0d\u4f1a\u88ab\u8ba1\u7b97\u3002\u5bf9\u4e8e\u76f4\u65b9\u56fe\u53ef\u89c6\u5316\u7684\u9a8c\u8bc1\u6570\u636e\uff08\u6216\u5206\u79bb\u6570\u636e\uff09\u4e00\u5b9a\u8981\u660e\u786e\u7684\u6307\u51fa\u3002 write_graph : \u662f\u5426\u5728 TensorBoard \u4e2d\u53ef\u89c6\u5316\u56fe\u50cf\u3002 \u5982\u679c write_graph \u88ab\u8bbe\u7f6e\u4e3a True\uff0c\u65e5\u5fd7\u6587\u4ef6\u4f1a\u53d8\u5f97\u975e\u5e38\u5927\u3002 write_grads : \u662f\u5426\u5728 TensorBoard \u4e2d\u53ef\u89c6\u5316\u68af\u5ea6\u503c\u76f4\u65b9\u56fe\u3002 histogram_freq \u5fc5\u987b\u8981\u5927\u4e8e 0 \u3002 batch_size : \u7528\u4ee5\u76f4\u65b9\u56fe\u8ba1\u7b97\u7684\u4f20\u5165\u795e\u7ecf\u5143\u7f51\u7edc\u8f93\u5165\u6279\u7684\u5927\u5c0f\u3002 write_images : \u662f\u5426\u5728 TensorBoard \u4e2d\u5c06\u6a21\u578b\u6743\u91cd\u4ee5\u56fe\u7247\u53ef\u89c6\u5316\u3002 embeddings_freq : \u88ab\u9009\u4e2d\u7684\u5d4c\u5165\u5c42\u4f1a\u88ab\u4fdd\u5b58\u7684\u9891\u7387\uff08\u5728\u8bad\u7ec3\u8f6e\u4e2d\uff09\u3002 embeddings_layer_names : \u4e00\u4e2a\u5217\u8868\uff0c\u4f1a\u88ab\u76d1\u6d4b\u5c42\u7684\u540d\u5b57\u3002 \u5982\u679c\u662f None \u6216\u7a7a\u5217\u8868\uff0c\u90a3\u4e48\u6240\u6709\u7684\u5d4c\u5165\u5c42\u90fd\u4f1a\u88ab\u76d1\u6d4b\u3002 embeddings_metadata : \u4e00\u4e2a\u5b57\u5178\uff0c\u5bf9\u5e94\u5c42\u7684\u540d\u5b57\u5230\u4fdd\u5b58\u6709\u8fd9\u4e2a\u5d4c\u5165\u5c42\u5143\u6570\u636e\u6587\u4ef6\u7684\u540d\u5b57\u3002 \u67e5\u770b \u8be6\u60c5 \u5173\u4e8e\u5143\u6570\u636e\u7684\u6570\u636e\u683c\u5f0f\u3002 \u4ee5\u9632\u540c\u6837\u7684\u5143\u6570\u636e\u88ab\u7528\u4e8e\u6240\u7528\u7684\u5d4c\u5165\u5c42\uff0c\u5b57\u7b26\u4e32\u53ef\u4ee5\u88ab\u4f20\u5165\u3002 embeddings_data : \u8981\u5d4c\u5165\u5728 embeddings_layer_names \u6307\u5b9a\u7684\u5c42\u7684\u6570\u636e\u3002 Numpy \u6570\u7ec4\uff08\u5982\u679c\u6a21\u578b\u6709\u5355\u4e2a\u8f93\u5165\uff09\u6216 Numpy \u6570\u7ec4\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u6709\u591a\u4e2a\u8f93\u5165\uff09\u3002 Learn ore about embeddings \u3002 update_freq : 'batch' \u6216 'epoch' \u6216 \u6574\u6570\u3002\u5f53\u4f7f\u7528 'batch' \u65f6\uff0c\u5728\u6bcf\u4e2a batch \u4e4b\u540e\u5c06\u635f\u5931\u548c\u8bc4\u4f30\u503c\u5199\u5165\u5230 TensorBoard \u4e2d\u3002\u540c\u6837\u7684\u60c5\u51b5\u5e94\u7528\u5230 'epoch' \u4e2d\u3002\u5982\u679c\u4f7f\u7528\u6574\u6570\uff0c\u4f8b\u5982 10000 \uff0c\u8fd9\u4e2a\u56de\u8c03\u4f1a\u5728\u6bcf 10000 \u4e2a\u6837\u672c\u4e4b\u540e\u5c06\u635f\u5931\u548c\u8bc4\u4f30\u503c\u5199\u5165\u5230 TensorBoard \u4e2d\u3002\u6ce8\u610f\uff0c\u9891\u7e41\u5730\u5199\u5165\u5230 TensorBoard \u4f1a\u51cf\u7f13\u4f60\u7684\u8bad\u7ec3\u3002 [source]","title":"TensorBoard"},{"location":"6.callbacks/#reducelronplateau","text":"keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0) \u5f53\u6807\u51c6\u8bc4\u4f30\u505c\u6b62\u63d0\u5347\u65f6\uff0c\u964d\u4f4e\u5b66\u4e60\u901f\u7387\u3002 \u5f53\u5b66\u4e60\u505c\u6b62\u65f6\uff0c\u6a21\u578b\u603b\u662f\u4f1a\u53d7\u76ca\u4e8e\u964d\u4f4e 2-10 \u500d\u7684\u5b66\u4e60\u901f\u7387\u3002 \u8fd9\u4e2a\u56de\u8c03\u51fd\u6570\u76d1\u6d4b\u4e00\u4e2a\u6570\u636e\u5e76\u4e14\u5f53\u8fd9\u4e2a\u6570\u636e\u5728\u4e00\u5b9a\u300c\u6709\u8010\u5fc3\u300d\u7684\u8bad\u7ec3\u8f6e\u4e4b\u540e\u8fd8\u6ca1\u6709\u8fdb\u6b65\uff0c \u90a3\u4e48\u5b66\u4e60\u901f\u7387\u5c31\u4f1a\u88ab\u964d\u4f4e\u3002 \u4f8b\u5b50 reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001) model.fit(X_train, Y_train, callbacks=[reduce_lr]) \u53c2\u6570 monitor : \u88ab\u76d1\u6d4b\u7684\u6570\u636e\u3002 factor : \u5b66\u4e60\u901f\u7387\u88ab\u964d\u4f4e\u7684\u56e0\u6570\u3002\u65b0\u7684\u5b66\u4e60\u901f\u7387 = \u5b66\u4e60\u901f\u7387 * \u56e0\u6570 patience : \u6ca1\u6709\u8fdb\u6b65\u7684\u8bad\u7ec3\u8f6e\u6570\uff0c\u5728\u8fd9\u4e4b\u540e\u8bad\u7ec3\u901f\u7387\u4f1a\u88ab\u964d\u4f4e\u3002 verbose : \u6574\u6570\u30020\uff1a\u5b89\u9759\uff0c1\uff1a\u66f4\u65b0\u4fe1\u606f\u3002 mode : {auto, min, max} \u5176\u4e2d\u4e4b\u4e00\u3002\u5982\u679c\u662f min \u6a21\u5f0f\uff0c\u5b66\u4e60\u901f\u7387\u4f1a\u88ab\u964d\u4f4e\u5982\u679c\u88ab\u76d1\u6d4b\u7684\u6570\u636e\u5df2\u7ecf\u505c\u6b62\u4e0b\u964d\uff1b \u5728 max \u6a21\u5f0f\uff0c\u5b66\u4e60\u5851\u6599\u4f1a\u88ab\u964d\u4f4e\u5982\u679c\u88ab\u76d1\u6d4b\u7684\u6570\u636e\u5df2\u7ecf\u505c\u6b62\u4e0a\u5347\uff1b \u5728 auto \u6a21\u5f0f\uff0c\u65b9\u5411\u4f1a\u88ab\u4ece\u88ab\u76d1\u6d4b\u7684\u6570\u636e\u4e2d\u81ea\u52a8\u63a8\u65ad\u51fa\u6765\u3002 min_delta : \u5bf9\u4e8e\u6d4b\u91cf\u65b0\u7684\u6700\u4f18\u5316\u7684\u9600\u503c\uff0c\u53ea\u5173\u6ce8\u5de8\u5927\u7684\u6539\u53d8\u3002 cooldown : \u5728\u5b66\u4e60\u901f\u7387\u88ab\u964d\u4f4e\u4e4b\u540e\uff0c\u91cd\u65b0\u6062\u590d\u6b63\u5e38\u64cd\u4f5c\u4e4b\u524d\u7b49\u5f85\u7684\u8bad\u7ec3\u8f6e\u6570\u91cf\u3002 min_lr : \u5b66\u4e60\u901f\u7387\u7684\u4e0b\u8fb9\u754c\u3002 [source]","title":"ReduceLROnPlateau"},{"location":"6.callbacks/#csvlogger","text":"keras.callbacks.CSVLogger(filename, separator=',', append=False) \u628a\u8bad\u7ec3\u8f6e\u7ed3\u679c\u6570\u636e\u6d41\u5230 csv \u6587\u4ef6\u7684\u56de\u8c03\u51fd\u6570\u3002 \u652f\u6301\u6240\u6709\u53ef\u4ee5\u88ab\u4f5c\u4e3a\u5b57\u7b26\u4e32\u8868\u793a\u7684\u503c\uff0c\u5305\u62ec 1D \u53ef\u8fed\u4ee3\u6570\u636e\uff0c\u4f8b\u5982\uff0cnp.ndarray\u3002 \u4f8b\u5b50 csv_logger = CSVLogger('training.log') model.fit(X_train, Y_train, callbacks=[csv_logger]) \u53c2\u6570 filename : csv \u6587\u4ef6\u7684\u6587\u4ef6\u540d\uff0c\u4f8b\u5982 'run/log.csv'\u3002 separator : \u7528\u6765\u9694\u79bb csv \u6587\u4ef6\u4e2d\u5143\u7d20\u7684\u5b57\u7b26\u4e32\u3002 append : True\uff1a\u5982\u679c\u6587\u4ef6\u5b58\u5728\u5219\u589e\u52a0\uff08\u53ef\u4ee5\u88ab\u7528\u4e8e\u7ee7\u7eed\u8bad\u7ec3\uff09\u3002False\uff1a\u8986\u76d6\u5b58\u5728\u7684\u6587\u4ef6\u3002 [source]","title":"CSVLogger"},{"location":"6.callbacks/#lambdacallback","text":"keras.callbacks.LambdaCallback(on_epoch_begin=None, on_epoch_end=None, on_batch_begin=None, on_batch_end=None, on_train_begin=None, on_train_end=None) \u5728\u8bad\u7ec3\u8fdb\u884c\u4e2d\u521b\u5efa\u7b80\u5355\uff0c\u81ea\u5b9a\u4e49\u7684\u56de\u8c03\u51fd\u6570\u7684\u56de\u8c03\u51fd\u6570\u3002 \u8fd9\u4e2a\u56de\u8c03\u51fd\u6570\u548c\u533f\u540d\u51fd\u6570\u5728\u5408\u9002\u7684\u65f6\u95f4\u88ab\u521b\u5efa\u3002 \u9700\u8981\u6ce8\u610f\u7684\u662f\u56de\u8c03\u51fd\u6570\u8981\u6c42\u4f4d\u7f6e\u578b\u53c2\u6570\uff0c\u5982\u4e0b\uff1a on_epoch_begin \u548c on_epoch_end \u8981\u6c42\u4e24\u4e2a\u4f4d\u7f6e\u578b\u7684\u53c2\u6570\uff1a epoch , logs on_batch_begin \u548c on_batch_end \u8981\u6c42\u4e24\u4e2a\u4f4d\u7f6e\u578b\u7684\u53c2\u6570\uff1a batch , logs on_train_begin \u548c on_train_end \u8981\u6c42\u4e00\u4e2a\u4f4d\u7f6e\u578b\u7684\u53c2\u6570\uff1a logs \u53c2\u6570 on_epoch_begin : \u5728\u6bcf\u8f6e\u5f00\u59cb\u65f6\u88ab\u8c03\u7528\u3002 on_epoch_end : \u5728\u6bcf\u8f6e\u7ed3\u675f\u65f6\u88ab\u8c03\u7528\u3002 on_batch_begin : \u5728\u6bcf\u6279\u5f00\u59cb\u65f6\u88ab\u8c03\u7528\u3002 on_batch_end : \u5728\u6bcf\u6279\u7ed3\u675f\u65f6\u88ab\u8c03\u7528\u3002 on_train_begin : \u5728\u6a21\u578b\u8bad\u7ec3\u5f00\u59cb\u65f6\u88ab\u8c03\u7528\u3002 on_train_end : \u5728\u6a21\u578b\u8bad\u7ec3\u7ed3\u675f\u65f6\u88ab\u8c03\u7528\u3002 \u4f8b\u5b50 # \u5728\u6bcf\u4e00\u4e2a\u6279\u5f00\u59cb\u65f6\uff0c\u6253\u5370\u51fa\u6279\u6570\u3002 batch_print_callback = LambdaCallback( on_batch_begin=lambda batch,logs: print(batch)) # \u628a\u8bad\u7ec3\u8f6e\u635f\u5931\u6570\u636e\u6d41\u5230 JSON \u683c\u5f0f\u7684\u6587\u4ef6\u3002\u6587\u4ef6\u7684\u5185\u5bb9 # \u4e0d\u662f\u5b8c\u7f8e\u7684 JSON \u683c\u5f0f\uff0c\u4f46\u662f\u65f6\u6bcf\u4e00\u884c\u90fd\u662f JSON \u5bf9\u8c61\u3002 import json json_log = open('loss_log.json', mode='wt', buffering=1) json_logging_callback = LambdaCallback( on_epoch_end=lambda epoch, logs: json_log.write( json.dumps({'epoch': epoch, 'loss': logs['loss']}) + '\\n'), on_train_end=lambda logs: json_log.close() ) # \u5728\u5b8c\u6210\u6a21\u578b\u8bad\u7ec3\u4e4b\u540e\uff0c\u7ed3\u675f\u4e00\u4e9b\u8fdb\u7a0b\u3002 processes = ... cleanup_callback = LambdaCallback( on_train_end=lambda logs: [ p.terminate() for p in processes if p.is_alive()]) model.fit(..., callbacks=[batch_print_callback, json_logging_callback, cleanup_callback])","title":"LambdaCallback"},{"location":"6.callbacks/#_2","text":"\u4f60\u53ef\u4ee5\u901a\u8fc7\u6269\u5c55 keras.callbacks.Callback \u57fa\u7c7b\u6765\u521b\u5efa\u4e00\u4e2a\u81ea\u5b9a\u4e49\u7684\u56de\u8c03\u51fd\u6570\u3002 \u901a\u8fc7\u7c7b\u7684\u5c5e\u6027 self.model \uff0c\u56de\u8c03\u51fd\u6570\u53ef\u4ee5\u83b7\u5f97\u5b83\u6240\u8054\u7cfb\u7684\u6a21\u578b\u3002 \u4e0b\u9762\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\uff0c\u5728\u8bad\u7ec3\u65f6\uff0c\u4fdd\u5b58\u4e00\u4e2a\u5217\u8868\u7684\u6279\u91cf\u635f\u5931\u503c\uff1a class LossHistory(keras.callbacks.Callback): def on_train_begin(self, logs={}): self.losses = [] def on_batch_end(self, batch, logs={}): self.losses.append(logs.get('loss'))","title":"\u521b\u5efa\u4e00\u4e2a\u56de\u8c03\u51fd\u6570"},{"location":"6.callbacks/#_3","text":"class LossHistory(keras.callbacks.Callback): def on_train_begin(self, logs={}): self.losses = [] def on_batch_end(self, batch, logs={}): self.losses.append(logs.get('loss')) model = Sequential() model.add(Dense(10, input_dim=784, kernel_initializer='uniform')) model.add(Activation('softmax')) model.compile(loss='categorical_crossentropy', optimizer='rmsprop') history = LossHistory() model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=0, callbacks=[history]) print(history.losses) # \u8f93\u51fa ''' [0.66047596406559383, 0.3547245744908703, ..., 0.25953155204159617, 0.25901699725311789] '''","title":"\u4f8b: \u8bb0\u5f55\u635f\u5931\u5386\u53f2"},{"location":"6.callbacks/#_4","text":"from keras.callbacks import ModelCheckpoint model = Sequential() model.add(Dense(10, input_dim=784, kernel_initializer='uniform')) model.add(Activation('softmax')) model.compile(loss='categorical_crossentropy', optimizer='rmsprop') ''' \u5982\u679c\u9a8c\u8bc1\u635f\u5931\u4e0b\u964d\uff0c \u90a3\u4e48\u5728\u6bcf\u4e2a\u8bad\u7ec3\u8f6e\u4e4b\u540e\u4fdd\u5b58\u6a21\u578b\u3002 ''' checkpointer = ModelCheckpoint(filepath='/tmp/weights.hdf5', verbose=1, save_best_only=True) model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=0, validation_data=(X_test, Y_test), callbacks=[checkpointer])","title":"\u4f8b: \u6a21\u578b\u68c0\u67e5\u70b9"},{"location":"7.datasets/","text":"\u5e38\u7528\u6570\u636e\u96c6 Datasets CIFAR10 \u5c0f\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6 50,000 \u5f20 32x32 \u5f69\u8272\u8bad\u7ec3\u56fe\u50cf\u6570\u636e\uff0c\u4ee5\u53ca 10,000 \u5f20\u6d4b\u8bd5\u56fe\u50cf\u6570\u636e\uff0c\u603b\u5171\u5206\u4e3a 10 \u4e2a\u7c7b\u522b\u3002 \u7528\u6cd5\uff1a from keras.datasets import cifar10 (x_train, y_train), (x_test, y_test) = cifar10.load_data() \u8fd4\u56de\uff1a 2 \u4e2a\u5143\u7ec4\uff1a x_train, x_test : uint8 \u6570\u7ec4\u8868\u793a\u7684 RGB \u56fe\u50cf\u6570\u636e\uff0c\u5c3a\u5bf8\u4e3a (num_samples, 3, 32, 32) \u6216 (num_samples, 32, 32, 3)\uff0c\u57fa\u4e8e image_data_format \u540e\u7aef\u8bbe\u5b9a\u7684 channels_first \u6216 channels_last \u3002 y_train, y_test : uint8 \u6570\u7ec4\u8868\u793a\u7684\u7c7b\u522b\u6807\u7b7e\uff08\u8303\u56f4\u5728 0-9 \u4e4b\u95f4\u7684\u6574\u6570\uff09\uff0c\u5c3a\u5bf8\u4e3a (num_samples,)\u3002 CIFAR100 \u5c0f\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6 50,000 \u5f20 32x32 \u5f69\u8272\u8bad\u7ec3\u56fe\u50cf\u6570\u636e\uff0c\u4ee5\u53ca 10,000 \u5f20\u6d4b\u8bd5\u56fe\u50cf\u6570\u636e\uff0c\u603b\u5171\u5206\u4e3a 100 \u4e2a\u7c7b\u522b\u3002 \u7528\u6cd5\uff1a from keras.datasets import cifar100 (x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine') \u8fd4\u56de\uff1a 2 \u4e2a\u5143\u7ec4\uff1a x_train, x_test : uint8 \u6570\u7ec4\u8868\u793a\u7684 RGB \u56fe\u50cf\u6570\u636e\uff0c\u5c3a\u5bf8\u4e3a (num_samples, 3, 32, 32) \u6216 (num_samples, 32, 32, 3)\uff0c\u57fa\u4e8e image_data_format \u540e\u7aef\u8bbe\u5b9a\u7684 channels_first \u6216 channels_last \u3002 y_train, y_test : uint8 \u6570\u7ec4\u8868\u793a\u7684\u7c7b\u522b\u6807\u7b7e\uff0c\u5c3a\u5bf8\u4e3a (num_samples,)\u3002 \u53c2\u6570\uff1a label_mode : \"fine\" \u6216\u8005 \"coarse\" IMDB \u7535\u5f71\u8bc4\u8bba\u60c5\u611f\u5206\u7c7b\u6570\u636e\u96c6 \u6570\u636e\u96c6\u6765\u81ea IMDB \u7684 25,000 \u6761\u7535\u5f71\u8bc4\u8bba\uff0c\u4ee5\u60c5\u7eea\uff08\u6b63\u9762/\u8d1f\u9762\uff09\u6807\u8bb0\u3002\u8bc4\u8bba\u5df2\u7ecf\u8fc7\u9884\u5904\u7406\uff0c\u5e76\u7f16\u7801\u4e3a\u8bcd\u7d22\u5f15\uff08\u6574\u6570\uff09\u7684 \u5e8f\u5217 \u8868\u793a\u3002\u4e3a\u4e86\u65b9\u4fbf\u8d77\u89c1\uff0c\u5c06\u8bcd\u6309\u6570\u636e\u96c6\u4e2d\u51fa\u73b0\u7684\u9891\u7387\u8fdb\u884c\u7d22\u5f15\uff0c\u4f8b\u5982\u6574\u6570 3 \u7f16\u7801\u6570\u636e\u4e2d\u7b2c\u4e09\u4e2a\u6700\u9891\u7e41\u7684\u8bcd\u3002\u8fd9\u5141\u8bb8\u5feb\u901f\u7b5b\u9009\u64cd\u4f5c\uff0c\u4f8b\u5982\uff1a\u300c\u53ea\u8003\u8651\u524d 10,000 \u4e2a\u6700\u5e38\u7528\u7684\u8bcd\uff0c\u4f46\u6392\u9664\u524d 20 \u4e2a\u6700\u5e38\u89c1\u7684\u8bcd\u300d\u3002 \u4f5c\u4e3a\u60ef\u4f8b\uff0c0 \u4e0d\u4ee3\u8868\u7279\u5b9a\u7684\u5355\u8bcd\uff0c\u800c\u662f\u88ab\u7528\u4e8e\u7f16\u7801\u4efb\u4f55\u672a\u77e5\u5355\u8bcd\u3002 \u7528\u6cd5 from keras.datasets import imdb (x_train, y_train), (x_test, y_test) = imdb.load_data(path=\"imdb.npz\", num_words=None, skip_top=0, maxlen=None, seed=113, start_char=1, oov_char=2, index_from=3) \u8fd4\u56de\uff1a 2 \u4e2a\u5143\u7ec4\uff1a x_train, x_test : \u5e8f\u5217\u7684\u5217\u8868\uff0c\u5373\u8bcd\u7d22\u5f15\u7684\u5217\u8868\u3002\u5982\u679c\u6307\u5b9a\u4e86 num_words \u53c2\u6570\uff0c\u5219\u53ef\u80fd\u7684\u6700\u5927\u7d22\u5f15\u503c\u662f num_words-1 \u3002\u5982\u679c\u6307\u5b9a\u4e86 maxlen \u53c2\u6570\uff0c\u5219\u53ef\u80fd\u7684\u6700\u5927\u5e8f\u5217\u957f\u5ea6\u4e3a maxlen \u3002 y_train, y_test : \u6574\u6570\u6807\u7b7e\u5217\u8868 (1 \u6216 0)\u3002 \u53c2\u6570: path : \u5982\u679c\u4f60\u672c\u5730\u6ca1\u6709\u8be5\u6570\u636e\u96c6 (\u5728 '~/.keras/datasets/' + path )\uff0c\u5b83\u5c06\u88ab\u4e0b\u8f7d\u5230\u6b64\u76ee\u5f55\u3002 num_words : \u6574\u6570\u6216 None\u3002\u8981\u8003\u8651\u7684\u6700\u5e38\u7528\u7684\u8bcd\u8bed\u3002\u4efb\u4f55\u4e0d\u592a\u9891\u7e41\u7684\u8bcd\u5c06\u5728\u5e8f\u5217\u6570\u636e\u4e2d\u663e\u793a\u4e3a oov_char \u503c\u3002 skip_top : \u6574\u6570\u3002\u8981\u5ffd\u7565\u7684\u6700\u5e38\u89c1\u7684\u5355\u8bcd\uff08\u5b83\u4eec\u5c06\u5728\u5e8f\u5217\u6570\u636e\u4e2d\u663e\u793a\u4e3a oov_char \u503c\uff09\u3002 maxlen : \u6574\u6570\u3002\u6700\u5927\u5e8f\u5217\u957f\u5ea6\u3002 \u4efb\u4f55\u66f4\u957f\u7684\u5e8f\u5217\u90fd\u5c06\u88ab\u622a\u65ad\u3002 seed : \u6574\u6570\u3002\u7528\u4e8e\u53ef\u91cd\u73b0\u6570\u636e\u6df7\u6d17\u7684\u79cd\u5b50\u3002 start_char : \u6574\u6570\u3002\u5e8f\u5217\u7684\u5f00\u59cb\u5c06\u7528\u8fd9\u4e2a\u5b57\u7b26\u6807\u8bb0\u3002\u8bbe\u7f6e\u4e3a 1\uff0c\u56e0\u4e3a 0 \u901a\u5e38\u4f5c\u4e3a\u586b\u5145\u5b57\u7b26\u3002 oov_char : \u6574\u6570\u3002\u7531\u4e8e num_words \u6216 skip_top \u9650\u5236\u800c\u88ab\u5220\u9664\u7684\u5355\u8bcd\u5c06\u88ab\u66ff\u6362\u4e3a\u6b64\u5b57\u7b26\u3002 index_from : \u6574\u6570\u3002\u4f7f\u7528\u6b64\u6570\u4ee5\u4e0a\u66f4\u9ad8\u7684\u7d22\u5f15\u503c\u5b9e\u9645\u8bcd\u6c47\u7d22\u5f15\u7684\u5f00\u59cb\u3002 \u8def\u900f\u793e\u65b0\u95fb\u4e3b\u9898\u5206\u7c7b \u6570\u636e\u96c6\u6765\u6e90\u4e8e\u8def\u900f\u793e\u7684 11,228 \u6761\u65b0\u95fb\u6587\u672c\uff0c\u603b\u5171\u5206\u4e3a 46 \u4e2a\u4e3b\u9898\u3002\u4e0e IMDB \u6570\u636e\u96c6\u4e00\u6837\uff0c\u6bcf\u6761\u65b0\u95fb\u90fd\u88ab\u7f16\u7801\u4e3a\u4e00\u4e2a\u8bcd\u7d22\u5f15\u7684\u5e8f\u5217\uff08\u76f8\u540c\u7684\u7ea6\u5b9a\uff09\u3002 \u7528\u6cd5\uff1a from keras.datasets import reuters (x_train, y_train), (x_test, y_test) = reuters.load_data(path=\"reuters.npz\", num_words=None, skip_top=0, maxlen=None, test_split=0.2, seed=113, start_char=1, oov_char=2, index_from=3) \u89c4\u683c\u4e0e IMDB \u6570\u636e\u96c6\u7684\u89c4\u683c\u76f8\u540c\uff0c\u4f46\u589e\u52a0\u4e86\uff1a test_split : \u6d6e\u70b9\u578b\u3002\u7528\u4f5c\u6d4b\u8bd5\u96c6\u7684\u6570\u636e\u6bd4\u4f8b\u3002 \u8be5\u6570\u636e\u96c6\u8fd8\u63d0\u4f9b\u4e86\u7528\u4e8e\u7f16\u7801\u5e8f\u5217\u7684\u8bcd\u7d22\u5f15\uff1a word_index = reuters.get_word_index(path=\"reuters_word_index.json\") \u8fd4\u56de\uff1a \u4e00\u4e2a\u5b57\u5178\uff0c\u5176\u4e2d\u952e\u662f\u5355\u8bcd\uff08\u5b57\u7b26\u4e32\uff09\uff0c\u503c\u662f\u7d22\u5f15\uff08\u6574\u6570\uff09\u3002 \u4f8b\u5982\uff0c word_index[\"giraffe\"] \u53ef\u80fd\u4f1a\u8fd4\u56de 1234 \u3002 \u53c2\u6570\uff1a path : \u5982\u679c\u5728\u672c\u5730\u6ca1\u6709\u7d22\u5f15\u6587\u4ef6 (at '~/.keras/datasets/' + path ), \u5b83\u5c06\u88ab\u4e0b\u8f7d\u5230\u8be5\u76ee\u5f55\u3002 MNIST \u624b\u5199\u5b57\u7b26\u6570\u636e\u96c6 \u8bad\u7ec3\u96c6\u4e3a 60,000 \u5f20 28x28 \u50cf\u7d20\u7070\u5ea6\u56fe\u50cf\uff0c\u6d4b\u8bd5\u96c6\u4e3a 10,000 \u540c\u89c4\u683c\u56fe\u50cf\uff0c\u603b\u5171 10 \u7c7b\u6570\u5b57\u6807\u7b7e\u3002 \u7528\u6cd5\uff1a from keras.datasets import mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() \u8fd4\u56de\uff1a 2 \u4e2a\u5143\u7ec4\uff1a x_train, x_test : uint8 \u6570\u7ec4\u8868\u793a\u7684\u7070\u5ea6\u56fe\u50cf\uff0c\u5c3a\u5bf8\u4e3a (num_samples, 28, 28)\u3002 y_train, y_test : uint8 \u6570\u7ec4\u8868\u793a\u7684\u6570\u5b57\u6807\u7b7e\uff08\u8303\u56f4\u5728 0-9 \u4e4b\u95f4\u7684\u6574\u6570\uff09\uff0c\u5c3a\u5bf8\u4e3a (num_samples,)\u3002 \u53c2\u6570\uff1a path : \u5982\u679c\u5728\u672c\u5730\u6ca1\u6709\u7d22\u5f15\u6587\u4ef6 (at '~/.keras/datasets/' + path ), \u5b83\u5c06\u88ab\u4e0b\u8f7d\u5230\u8be5\u76ee\u5f55\u3002 Fashion-MNIST \u65f6\u5c1a\u7269\u54c1\u6570\u636e\u96c6 \u8bad\u7ec3\u96c6\u4e3a 60,000 \u5f20 28x28 \u50cf\u7d20\u7070\u5ea6\u56fe\u50cf\uff0c\u6d4b\u8bd5\u96c6\u4e3a 10,000 \u540c\u89c4\u683c\u56fe\u50cf\uff0c\u603b\u5171 10 \u7c7b\u65f6\u5c1a\u7269\u54c1\u6807\u7b7e\u3002\u8be5\u6570\u636e\u96c6\u53ef\u4ee5\u7528\u4f5c MNIST \u7684\u76f4\u63a5\u66ff\u4ee3\u54c1\u3002\u7c7b\u522b\u6807\u7b7e\u662f\uff1a \u7c7b\u522b \u63cf\u8ff0 \u4e2d\u6587 0 T-shirt/top T\u6064/\u4e0a\u8863 1 Trouser \u88e4\u5b50 2 Pullover \u5957\u5934\u886b 3 Dress \u8fde\u8863\u88d9 4 Coat \u5916\u5957 5 Sandal \u51c9\u978b 6 Shirt \u886c\u886b 7 Sneaker \u8fd0\u52a8\u978b 8 Bag \u80cc\u5305 9 Ankle boot \u77ed\u9774 \u7528\u6cd5\uff1a from keras.datasets import fashion_mnist (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data() \u8fd4\u56de\uff1a 2 \u4e2a\u5143\u7ec4\uff1a x_train, x_test : uint8 \u6570\u7ec4\u8868\u793a\u7684\u7070\u5ea6\u56fe\u50cf\uff0c\u5c3a\u5bf8\u4e3a (num_samples, 28, 28)\u3002 y_train, y_test : uint8 \u6570\u7ec4\u8868\u793a\u7684\u6570\u5b57\u6807\u7b7e\uff08\u8303\u56f4\u5728 0-9 \u4e4b\u95f4\u7684\u6574\u6570\uff09\uff0c\u5c3a\u5bf8\u4e3a (num_samples,)\u3002 Boston \u623f\u4ef7\u56de\u5f52\u6570\u636e\u96c6 \u6570\u636e\u96c6\u6765\u81ea\u5361\u5185\u57fa\u6885\u9686\u5927\u5b66\u7ef4\u62a4\u7684 StatLib \u5e93\u3002 \u6837\u672c\u5305\u542b 1970 \u5e74\u4ee3\u7684\u5728\u6ce2\u58eb\u987f\u90ca\u533a\u4e0d\u540c\u4f4d\u7f6e\u7684\u623f\u5c4b\u4fe1\u606f\uff0c\u603b\u5171\u6709 13 \u79cd\u623f\u5c4b\u5c5e\u6027\u3002 \u76ee\u6807\u503c\u662f\u4e00\u4e2a\u4f4d\u7f6e\u7684\u623f\u5c4b\u7684\u4e2d\u503c\uff08\u5355\u4f4d\uff1ak$\uff09\u3002 \u7528\u6cd5\uff1a from keras.datasets import boston_housing (x_train, y_train), (x_test, y_test) = boston_housing.load_data() \u53c2\u6570\uff1a path : \u7f13\u5b58\u672c\u5730\u6570\u636e\u96c6\u7684\u4f4d\u7f6e (\u76f8\u5bf9\u8def\u5f84 ~/.keras/datasets)\u3002 seed : \u5728\u8ba1\u7b97\u6d4b\u8bd5\u5206\u5272\u4e4b\u524d\u5bf9\u6570\u636e\u8fdb\u884c\u6df7\u6d17\u7684\u968f\u673a\u79cd\u5b50\u3002 test_split : \u9700\u8981\u4fdd\u7559\u4f5c\u4e3a\u6d4b\u8bd5\u6570\u636e\u7684\u6bd4\u4f8b\u3002 \u8fd4\u56de\uff1a Numpy \u6570\u7ec4\u7684\u5143\u7ec4: (x_train, y_train), (x_test, y_test) \u3002","title":"\u5e38\u7528\u6570\u636e\u96c6 Datasets"},{"location":"7.datasets/#datasets","text":"","title":"\u5e38\u7528\u6570\u636e\u96c6 Datasets"},{"location":"7.datasets/#cifar10","text":"50,000 \u5f20 32x32 \u5f69\u8272\u8bad\u7ec3\u56fe\u50cf\u6570\u636e\uff0c\u4ee5\u53ca 10,000 \u5f20\u6d4b\u8bd5\u56fe\u50cf\u6570\u636e\uff0c\u603b\u5171\u5206\u4e3a 10 \u4e2a\u7c7b\u522b\u3002","title":"CIFAR10 \u5c0f\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6"},{"location":"7.datasets/#_1","text":"from keras.datasets import cifar10 (x_train, y_train), (x_test, y_test) = cifar10.load_data() \u8fd4\u56de\uff1a 2 \u4e2a\u5143\u7ec4\uff1a x_train, x_test : uint8 \u6570\u7ec4\u8868\u793a\u7684 RGB \u56fe\u50cf\u6570\u636e\uff0c\u5c3a\u5bf8\u4e3a (num_samples, 3, 32, 32) \u6216 (num_samples, 32, 32, 3)\uff0c\u57fa\u4e8e image_data_format \u540e\u7aef\u8bbe\u5b9a\u7684 channels_first \u6216 channels_last \u3002 y_train, y_test : uint8 \u6570\u7ec4\u8868\u793a\u7684\u7c7b\u522b\u6807\u7b7e\uff08\u8303\u56f4\u5728 0-9 \u4e4b\u95f4\u7684\u6574\u6570\uff09\uff0c\u5c3a\u5bf8\u4e3a (num_samples,)\u3002","title":"\u7528\u6cd5\uff1a"},{"location":"7.datasets/#cifar100","text":"50,000 \u5f20 32x32 \u5f69\u8272\u8bad\u7ec3\u56fe\u50cf\u6570\u636e\uff0c\u4ee5\u53ca 10,000 \u5f20\u6d4b\u8bd5\u56fe\u50cf\u6570\u636e\uff0c\u603b\u5171\u5206\u4e3a 100 \u4e2a\u7c7b\u522b\u3002","title":"CIFAR100 \u5c0f\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6"},{"location":"7.datasets/#_2","text":"from keras.datasets import cifar100 (x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine') \u8fd4\u56de\uff1a 2 \u4e2a\u5143\u7ec4\uff1a x_train, x_test : uint8 \u6570\u7ec4\u8868\u793a\u7684 RGB \u56fe\u50cf\u6570\u636e\uff0c\u5c3a\u5bf8\u4e3a (num_samples, 3, 32, 32) \u6216 (num_samples, 32, 32, 3)\uff0c\u57fa\u4e8e image_data_format \u540e\u7aef\u8bbe\u5b9a\u7684 channels_first \u6216 channels_last \u3002 y_train, y_test : uint8 \u6570\u7ec4\u8868\u793a\u7684\u7c7b\u522b\u6807\u7b7e\uff0c\u5c3a\u5bf8\u4e3a (num_samples,)\u3002 \u53c2\u6570\uff1a label_mode : \"fine\" \u6216\u8005 \"coarse\"","title":"\u7528\u6cd5\uff1a"},{"location":"7.datasets/#imdb","text":"\u6570\u636e\u96c6\u6765\u81ea IMDB \u7684 25,000 \u6761\u7535\u5f71\u8bc4\u8bba\uff0c\u4ee5\u60c5\u7eea\uff08\u6b63\u9762/\u8d1f\u9762\uff09\u6807\u8bb0\u3002\u8bc4\u8bba\u5df2\u7ecf\u8fc7\u9884\u5904\u7406\uff0c\u5e76\u7f16\u7801\u4e3a\u8bcd\u7d22\u5f15\uff08\u6574\u6570\uff09\u7684 \u5e8f\u5217 \u8868\u793a\u3002\u4e3a\u4e86\u65b9\u4fbf\u8d77\u89c1\uff0c\u5c06\u8bcd\u6309\u6570\u636e\u96c6\u4e2d\u51fa\u73b0\u7684\u9891\u7387\u8fdb\u884c\u7d22\u5f15\uff0c\u4f8b\u5982\u6574\u6570 3 \u7f16\u7801\u6570\u636e\u4e2d\u7b2c\u4e09\u4e2a\u6700\u9891\u7e41\u7684\u8bcd\u3002\u8fd9\u5141\u8bb8\u5feb\u901f\u7b5b\u9009\u64cd\u4f5c\uff0c\u4f8b\u5982\uff1a\u300c\u53ea\u8003\u8651\u524d 10,000 \u4e2a\u6700\u5e38\u7528\u7684\u8bcd\uff0c\u4f46\u6392\u9664\u524d 20 \u4e2a\u6700\u5e38\u89c1\u7684\u8bcd\u300d\u3002 \u4f5c\u4e3a\u60ef\u4f8b\uff0c0 \u4e0d\u4ee3\u8868\u7279\u5b9a\u7684\u5355\u8bcd\uff0c\u800c\u662f\u88ab\u7528\u4e8e\u7f16\u7801\u4efb\u4f55\u672a\u77e5\u5355\u8bcd\u3002","title":"IMDB \u7535\u5f71\u8bc4\u8bba\u60c5\u611f\u5206\u7c7b\u6570\u636e\u96c6"},{"location":"7.datasets/#_3","text":"from keras.datasets import imdb (x_train, y_train), (x_test, y_test) = imdb.load_data(path=\"imdb.npz\", num_words=None, skip_top=0, maxlen=None, seed=113, start_char=1, oov_char=2, index_from=3) \u8fd4\u56de\uff1a 2 \u4e2a\u5143\u7ec4\uff1a x_train, x_test : \u5e8f\u5217\u7684\u5217\u8868\uff0c\u5373\u8bcd\u7d22\u5f15\u7684\u5217\u8868\u3002\u5982\u679c\u6307\u5b9a\u4e86 num_words \u53c2\u6570\uff0c\u5219\u53ef\u80fd\u7684\u6700\u5927\u7d22\u5f15\u503c\u662f num_words-1 \u3002\u5982\u679c\u6307\u5b9a\u4e86 maxlen \u53c2\u6570\uff0c\u5219\u53ef\u80fd\u7684\u6700\u5927\u5e8f\u5217\u957f\u5ea6\u4e3a maxlen \u3002 y_train, y_test : \u6574\u6570\u6807\u7b7e\u5217\u8868 (1 \u6216 0)\u3002 \u53c2\u6570: path : \u5982\u679c\u4f60\u672c\u5730\u6ca1\u6709\u8be5\u6570\u636e\u96c6 (\u5728 '~/.keras/datasets/' + path )\uff0c\u5b83\u5c06\u88ab\u4e0b\u8f7d\u5230\u6b64\u76ee\u5f55\u3002 num_words : \u6574\u6570\u6216 None\u3002\u8981\u8003\u8651\u7684\u6700\u5e38\u7528\u7684\u8bcd\u8bed\u3002\u4efb\u4f55\u4e0d\u592a\u9891\u7e41\u7684\u8bcd\u5c06\u5728\u5e8f\u5217\u6570\u636e\u4e2d\u663e\u793a\u4e3a oov_char \u503c\u3002 skip_top : \u6574\u6570\u3002\u8981\u5ffd\u7565\u7684\u6700\u5e38\u89c1\u7684\u5355\u8bcd\uff08\u5b83\u4eec\u5c06\u5728\u5e8f\u5217\u6570\u636e\u4e2d\u663e\u793a\u4e3a oov_char \u503c\uff09\u3002 maxlen : \u6574\u6570\u3002\u6700\u5927\u5e8f\u5217\u957f\u5ea6\u3002 \u4efb\u4f55\u66f4\u957f\u7684\u5e8f\u5217\u90fd\u5c06\u88ab\u622a\u65ad\u3002 seed : \u6574\u6570\u3002\u7528\u4e8e\u53ef\u91cd\u73b0\u6570\u636e\u6df7\u6d17\u7684\u79cd\u5b50\u3002 start_char : \u6574\u6570\u3002\u5e8f\u5217\u7684\u5f00\u59cb\u5c06\u7528\u8fd9\u4e2a\u5b57\u7b26\u6807\u8bb0\u3002\u8bbe\u7f6e\u4e3a 1\uff0c\u56e0\u4e3a 0 \u901a\u5e38\u4f5c\u4e3a\u586b\u5145\u5b57\u7b26\u3002 oov_char : \u6574\u6570\u3002\u7531\u4e8e num_words \u6216 skip_top \u9650\u5236\u800c\u88ab\u5220\u9664\u7684\u5355\u8bcd\u5c06\u88ab\u66ff\u6362\u4e3a\u6b64\u5b57\u7b26\u3002 index_from : \u6574\u6570\u3002\u4f7f\u7528\u6b64\u6570\u4ee5\u4e0a\u66f4\u9ad8\u7684\u7d22\u5f15\u503c\u5b9e\u9645\u8bcd\u6c47\u7d22\u5f15\u7684\u5f00\u59cb\u3002","title":"\u7528\u6cd5"},{"location":"7.datasets/#_4","text":"\u6570\u636e\u96c6\u6765\u6e90\u4e8e\u8def\u900f\u793e\u7684 11,228 \u6761\u65b0\u95fb\u6587\u672c\uff0c\u603b\u5171\u5206\u4e3a 46 \u4e2a\u4e3b\u9898\u3002\u4e0e IMDB \u6570\u636e\u96c6\u4e00\u6837\uff0c\u6bcf\u6761\u65b0\u95fb\u90fd\u88ab\u7f16\u7801\u4e3a\u4e00\u4e2a\u8bcd\u7d22\u5f15\u7684\u5e8f\u5217\uff08\u76f8\u540c\u7684\u7ea6\u5b9a\uff09\u3002","title":"\u8def\u900f\u793e\u65b0\u95fb\u4e3b\u9898\u5206\u7c7b"},{"location":"7.datasets/#_5","text":"from keras.datasets import reuters (x_train, y_train), (x_test, y_test) = reuters.load_data(path=\"reuters.npz\", num_words=None, skip_top=0, maxlen=None, test_split=0.2, seed=113, start_char=1, oov_char=2, index_from=3) \u89c4\u683c\u4e0e IMDB \u6570\u636e\u96c6\u7684\u89c4\u683c\u76f8\u540c\uff0c\u4f46\u589e\u52a0\u4e86\uff1a test_split : \u6d6e\u70b9\u578b\u3002\u7528\u4f5c\u6d4b\u8bd5\u96c6\u7684\u6570\u636e\u6bd4\u4f8b\u3002 \u8be5\u6570\u636e\u96c6\u8fd8\u63d0\u4f9b\u4e86\u7528\u4e8e\u7f16\u7801\u5e8f\u5217\u7684\u8bcd\u7d22\u5f15\uff1a word_index = reuters.get_word_index(path=\"reuters_word_index.json\") \u8fd4\u56de\uff1a \u4e00\u4e2a\u5b57\u5178\uff0c\u5176\u4e2d\u952e\u662f\u5355\u8bcd\uff08\u5b57\u7b26\u4e32\uff09\uff0c\u503c\u662f\u7d22\u5f15\uff08\u6574\u6570\uff09\u3002 \u4f8b\u5982\uff0c word_index[\"giraffe\"] \u53ef\u80fd\u4f1a\u8fd4\u56de 1234 \u3002 \u53c2\u6570\uff1a path : \u5982\u679c\u5728\u672c\u5730\u6ca1\u6709\u7d22\u5f15\u6587\u4ef6 (at '~/.keras/datasets/' + path ), \u5b83\u5c06\u88ab\u4e0b\u8f7d\u5230\u8be5\u76ee\u5f55\u3002","title":"\u7528\u6cd5\uff1a"},{"location":"7.datasets/#mnist","text":"\u8bad\u7ec3\u96c6\u4e3a 60,000 \u5f20 28x28 \u50cf\u7d20\u7070\u5ea6\u56fe\u50cf\uff0c\u6d4b\u8bd5\u96c6\u4e3a 10,000 \u540c\u89c4\u683c\u56fe\u50cf\uff0c\u603b\u5171 10 \u7c7b\u6570\u5b57\u6807\u7b7e\u3002","title":"MNIST \u624b\u5199\u5b57\u7b26\u6570\u636e\u96c6"},{"location":"7.datasets/#_6","text":"from keras.datasets import mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() \u8fd4\u56de\uff1a 2 \u4e2a\u5143\u7ec4\uff1a x_train, x_test : uint8 \u6570\u7ec4\u8868\u793a\u7684\u7070\u5ea6\u56fe\u50cf\uff0c\u5c3a\u5bf8\u4e3a (num_samples, 28, 28)\u3002 y_train, y_test : uint8 \u6570\u7ec4\u8868\u793a\u7684\u6570\u5b57\u6807\u7b7e\uff08\u8303\u56f4\u5728 0-9 \u4e4b\u95f4\u7684\u6574\u6570\uff09\uff0c\u5c3a\u5bf8\u4e3a (num_samples,)\u3002 \u53c2\u6570\uff1a path : \u5982\u679c\u5728\u672c\u5730\u6ca1\u6709\u7d22\u5f15\u6587\u4ef6 (at '~/.keras/datasets/' + path ), \u5b83\u5c06\u88ab\u4e0b\u8f7d\u5230\u8be5\u76ee\u5f55\u3002","title":"\u7528\u6cd5\uff1a"},{"location":"7.datasets/#fashion-mnist","text":"\u8bad\u7ec3\u96c6\u4e3a 60,000 \u5f20 28x28 \u50cf\u7d20\u7070\u5ea6\u56fe\u50cf\uff0c\u6d4b\u8bd5\u96c6\u4e3a 10,000 \u540c\u89c4\u683c\u56fe\u50cf\uff0c\u603b\u5171 10 \u7c7b\u65f6\u5c1a\u7269\u54c1\u6807\u7b7e\u3002\u8be5\u6570\u636e\u96c6\u53ef\u4ee5\u7528\u4f5c MNIST \u7684\u76f4\u63a5\u66ff\u4ee3\u54c1\u3002\u7c7b\u522b\u6807\u7b7e\u662f\uff1a \u7c7b\u522b \u63cf\u8ff0 \u4e2d\u6587 0 T-shirt/top T\u6064/\u4e0a\u8863 1 Trouser \u88e4\u5b50 2 Pullover \u5957\u5934\u886b 3 Dress \u8fde\u8863\u88d9 4 Coat \u5916\u5957 5 Sandal \u51c9\u978b 6 Shirt \u886c\u886b 7 Sneaker \u8fd0\u52a8\u978b 8 Bag \u80cc\u5305 9 Ankle boot \u77ed\u9774","title":"Fashion-MNIST \u65f6\u5c1a\u7269\u54c1\u6570\u636e\u96c6"},{"location":"7.datasets/#_7","text":"from keras.datasets import fashion_mnist (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data() \u8fd4\u56de\uff1a 2 \u4e2a\u5143\u7ec4\uff1a x_train, x_test : uint8 \u6570\u7ec4\u8868\u793a\u7684\u7070\u5ea6\u56fe\u50cf\uff0c\u5c3a\u5bf8\u4e3a (num_samples, 28, 28)\u3002 y_train, y_test : uint8 \u6570\u7ec4\u8868\u793a\u7684\u6570\u5b57\u6807\u7b7e\uff08\u8303\u56f4\u5728 0-9 \u4e4b\u95f4\u7684\u6574\u6570\uff09\uff0c\u5c3a\u5bf8\u4e3a (num_samples,)\u3002","title":"\u7528\u6cd5\uff1a"},{"location":"7.datasets/#boston","text":"\u6570\u636e\u96c6\u6765\u81ea\u5361\u5185\u57fa\u6885\u9686\u5927\u5b66\u7ef4\u62a4\u7684 StatLib \u5e93\u3002 \u6837\u672c\u5305\u542b 1970 \u5e74\u4ee3\u7684\u5728\u6ce2\u58eb\u987f\u90ca\u533a\u4e0d\u540c\u4f4d\u7f6e\u7684\u623f\u5c4b\u4fe1\u606f\uff0c\u603b\u5171\u6709 13 \u79cd\u623f\u5c4b\u5c5e\u6027\u3002 \u76ee\u6807\u503c\u662f\u4e00\u4e2a\u4f4d\u7f6e\u7684\u623f\u5c4b\u7684\u4e2d\u503c\uff08\u5355\u4f4d\uff1ak$\uff09\u3002","title":"Boston \u623f\u4ef7\u56de\u5f52\u6570\u636e\u96c6"},{"location":"7.datasets/#_8","text":"from keras.datasets import boston_housing (x_train, y_train), (x_test, y_test) = boston_housing.load_data() \u53c2\u6570\uff1a path : \u7f13\u5b58\u672c\u5730\u6570\u636e\u96c6\u7684\u4f4d\u7f6e (\u76f8\u5bf9\u8def\u5f84 ~/.keras/datasets)\u3002 seed : \u5728\u8ba1\u7b97\u6d4b\u8bd5\u5206\u5272\u4e4b\u524d\u5bf9\u6570\u636e\u8fdb\u884c\u6df7\u6d17\u7684\u968f\u673a\u79cd\u5b50\u3002 test_split : \u9700\u8981\u4fdd\u7559\u4f5c\u4e3a\u6d4b\u8bd5\u6570\u636e\u7684\u6bd4\u4f8b\u3002 \u8fd4\u56de\uff1a Numpy \u6570\u7ec4\u7684\u5143\u7ec4: (x_train, y_train), (x_test, y_test) \u3002","title":"\u7528\u6cd5\uff1a"},{"location":"8.applications/","text":"\u5e94\u7528 Applications Keras \u7684\u5e94\u7528\u6a21\u5757\uff08keras.applications\uff09\u63d0\u4f9b\u4e86\u5e26\u6709\u9884\u8bad\u7ec3\u6743\u503c\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u53ef\u4ee5\u7528\u6765\u8fdb\u884c\u9884\u6d4b\u3001\u7279\u5f81\u63d0\u53d6\u548c\u5fae\u8c03\uff08fine-tuning\uff09\u3002 \u5f53\u4f60\u521d\u59cb\u5316\u4e00\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\u65f6\uff0c\u4f1a\u81ea\u52a8\u4e0b\u8f7d\u6743\u91cd\u5230 ~/.keras/models/ \u76ee\u5f55\u4e0b\u3002 \u53ef\u7528\u7684\u6a21\u578b \u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u8fc7\u7684\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b\u7684\u6a21\u578b\uff1a Xception VGG16 VGG19 ResNet, ResNetV2, ResNeXt InceptionV3 InceptionResNetV2 MobileNet MobileNetV2 DenseNet NASNet \u6240\u6709\u7684\u8fd9\u4e9b\u67b6\u6784\u90fd\u517c\u5bb9\u6240\u6709\u7684\u540e\u7aef (TensorFlow, Theano \u548c CNTK)\uff0c\u5e76\u4e14\u4f1a\u5728\u5b9e\u4f8b\u5316\u65f6\uff0c\u6839\u636e Keras \u914d\u7f6e\u6587\u4ef6 \u301c/.keras/keras.json \u4e2d\u8bbe\u7f6e\u7684\u56fe\u50cf\u6570\u636e\u683c\u5f0f\u6784\u5efa\u6a21\u578b\u3002\u4e3e\u4e2a\u4f8b\u5b50\uff0c\u5982\u679c\u4f60\u8bbe\u7f6e image_data_format=channels_last \uff0c\u5219\u52a0\u8f7d\u7684\u6a21\u578b\u5c06\u6309\u7167 TensorFlow \u7684\u7ef4\u5ea6\u987a\u5e8f\u6765\u6784\u9020\uff0c\u5373\u300c\u9ad8\u5ea6-\u5bbd\u5ea6-\u6df1\u5ea6\u300d\uff08Height-Width-Depth\uff09\u7684\u987a\u5e8f\u3002 \u6ce8\u610f\uff1a \u5bf9\u4e8e Keras < 2.2.0 \uff0cXception \u6a21\u578b\u4ec5\u9002\u7528\u4e8e TensorFlow\uff0c\u56e0\u4e3a\u5b83\u4f9d\u8d56\u4e8e SeparableConvolution \u5c42\u3002 \u5bf9\u4e8e Keras < 2.1.5 \uff0cMobileNet \u6a21\u578b\u4ec5\u9002\u7528\u4e8e TensorFlow\uff0c\u56e0\u4e3a\u5b83\u4f9d\u8d56\u4e8e DepthwiseConvolution \u5c42\u3002 \u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u7684\u4f7f\u7528\u793a\u4f8b \u4f7f\u7528 ResNet50 \u8fdb\u884c ImageNet \u5206\u7c7b from keras.applications.resnet50 import ResNet50 from keras.preprocessing import image from keras.applications.resnet50 import preprocess_input, decode_predictions import numpy as np model = ResNet50(weights='imagenet') img_path = 'elephant.jpg' img = image.load_img(img_path, target_size=(224, 224)) x = image.img_to_array(img) x = np.expand_dims(x, axis=0) x = preprocess_input(x) preds = model.predict(x) # \u5c06\u7ed3\u679c\u89e3\u7801\u4e3a\u5143\u7ec4\u5217\u8868 (class, description, probability) # (\u4e00\u4e2a\u5217\u8868\u4ee3\u8868\u6279\u6b21\u4e2d\u7684\u4e00\u4e2a\u6837\u672c\uff09 print('Predicted:', decode_predictions(preds, top=3)[0]) # Predicted: [(u'n02504013', u'Indian_elephant', 0.82658225), (u'n01871265', u'tusker', 0.1122357), (u'n02504458', u'African_elephant', 0.061040461)] \u4f7f\u7528 VGG16 \u63d0\u53d6\u7279\u5f81 from keras.applications.vgg16 import VGG16 from keras.preprocessing import image from keras.applications.vgg16 import preprocess_input import numpy as np model = VGG16(weights='imagenet', include_top=False) img_path = 'elephant.jpg' img = image.load_img(img_path, target_size=(224, 224)) x = image.img_to_array(img) x = np.expand_dims(x, axis=0) x = preprocess_input(x) features = model.predict(x) \u4eceVGG19 \u7684\u4efb\u610f\u4e2d\u95f4\u5c42\u4e2d\u62bd\u53d6\u7279\u5f81 from keras.applications.vgg19 import VGG19 from keras.preprocessing import image from keras.applications.vgg19 import preprocess_input from keras.models import Model import numpy as np base_model = VGG19(weights='imagenet') model = Model(inputs=base_model.input, outputs=base_model.get_layer('block4_pool').output) img_path = 'elephant.jpg' img = image.load_img(img_path, target_size=(224, 224)) x = image.img_to_array(img) x = np.expand_dims(x, axis=0) x = preprocess_input(x) block4_pool_features = model.predict(x) \u5728\u65b0\u7c7b\u4e0a\u5fae\u8c03 InceptionV3 from keras.applications.inception_v3 import InceptionV3 from keras.preprocessing import image from keras.models import Model from keras.layers import Dense, GlobalAveragePooling2D from keras import backend as K # \u6784\u5efa\u4e0d\u5e26\u5206\u7c7b\u5668\u7684\u9884\u8bad\u7ec3\u6a21\u578b base_model = InceptionV3(weights='imagenet', include_top=False) # \u6dfb\u52a0\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5c42 x = base_model.output x = GlobalAveragePooling2D()(x) # \u6dfb\u52a0\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42 x = Dense(1024, activation='relu')(x) # \u6dfb\u52a0\u4e00\u4e2a\u5206\u7c7b\u5668\uff0c\u5047\u8bbe\u6211\u4eec\u6709200\u4e2a\u7c7b predictions = Dense(200, activation='softmax')(x) # \u6784\u5efa\u6211\u4eec\u9700\u8981\u8bad\u7ec3\u7684\u5b8c\u6574\u6a21\u578b model = Model(inputs=base_model.input, outputs=predictions) # \u9996\u5148\uff0c\u6211\u4eec\u53ea\u8bad\u7ec3\u9876\u90e8\u7684\u51e0\u5c42\uff08\u968f\u673a\u521d\u59cb\u5316\u7684\u5c42\uff09 # \u9501\u4f4f\u6240\u6709 InceptionV3 \u7684\u5377\u79ef\u5c42 for layer in base_model.layers: layer.trainable = False # \u7f16\u8bd1\u6a21\u578b\uff08\u4e00\u5b9a\u8981\u5728\u9501\u5c42\u4ee5\u540e\u64cd\u4f5c\uff09 model.compile(optimizer='rmsprop', loss='categorical_crossentropy') # \u5728\u65b0\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u51e0\u4ee3 model.fit_generator(...) # \u73b0\u5728\u9876\u5c42\u5e94\u8be5\u8bad\u7ec3\u597d\u4e86\uff0c\u8ba9\u6211\u4eec\u5f00\u59cb\u5fae\u8c03 Inception V3 \u7684\u5377\u79ef\u5c42\u3002 # \u6211\u4eec\u4f1a\u9501\u4f4f\u5e95\u4e0b\u7684\u51e0\u5c42\uff0c\u7136\u540e\u8bad\u7ec3\u5176\u4f59\u7684\u9876\u5c42\u3002 # \u8ba9\u6211\u4eec\u770b\u770b\u6bcf\u4e00\u5c42\u7684\u540d\u5b57\u548c\u5c42\u53f7\uff0c\u770b\u770b\u6211\u4eec\u5e94\u8be5\u9501\u591a\u5c11\u5c42\u5462\uff1a for i, layer in enumerate(base_model.layers): print(i, layer.name) # \u6211\u4eec\u9009\u62e9\u8bad\u7ec3\u6700\u4e0a\u9762\u7684\u4e24\u4e2a Inception block # \u4e5f\u5c31\u662f\u8bf4\u9501\u4f4f\u524d\u9762249\u5c42\uff0c\u7136\u540e\u653e\u5f00\u4e4b\u540e\u7684\u5c42\u3002 for layer in model.layers[:249]: layer.trainable = False for layer in model.layers[249:]: layer.trainable = True # \u6211\u4eec\u9700\u8981\u91cd\u65b0\u7f16\u8bd1\u6a21\u578b\uff0c\u624d\u80fd\u4f7f\u4e0a\u9762\u7684\u4fee\u6539\u751f\u6548 # \u8ba9\u6211\u4eec\u8bbe\u7f6e\u4e00\u4e2a\u5f88\u4f4e\u7684\u5b66\u4e60\u7387\uff0c\u4f7f\u7528 SGD \u6765\u5fae\u8c03 from keras.optimizers import SGD model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy') # \u6211\u4eec\u7ee7\u7eed\u8bad\u7ec3\u6a21\u578b\uff0c\u8fd9\u6b21\u6211\u4eec\u8bad\u7ec3\u6700\u540e\u4e24\u4e2a Inception block # \u548c\u4e24\u4e2a\u5168\u8fde\u63a5\u5c42 model.fit_generator(...) \u901a\u8fc7\u81ea\u5b9a\u4e49\u8f93\u5165\u5f20\u91cf\u6784\u5efa InceptionV3 from keras.applications.inception_v3 import InceptionV3 from keras.layers import Input # \u8fd9\u4e5f\u53ef\u80fd\u662f\u4e0d\u540c\u7684 Keras \u6a21\u578b\u6216\u5c42\u7684\u8f93\u51fa input_tensor = Input(shape=(224, 224, 3)) # \u5047\u5b9a K.image_data_format() == 'channels_last' model = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=True) \u6a21\u578b\u6982\u89c8 \u6a21\u578b \u5927\u5c0f Top-1 \u51c6\u786e\u7387 Top-5 \u51c6\u786e\u7387 \u53c2\u6570\u6570\u91cf \u6df1\u5ea6 Xception 88 MB 0.790 0.945 22,910,480 126 VGG16 528 MB 0.713 0.901 138,357,544 23 VGG19 549 MB 0.713 0.900 143,667,240 26 ResNet50 98 MB 0.749 0.921 25,636,712 - ResNet101 171 MB 0.764 0.928 44,707,176 - ResNet152 232 MB 0.766 0.931 60,419,944 - ResNet50V2 98 MB 0.760 0.930 25,613,800 - ResNet101V2 171 MB 0.772 0.938 44,675,560 - ResNet152V2 232 MB 0.780 0.942 60,380,648 - ResNeXt50 96 MB 0.777 0.938 25,097,128 - ResNeXt101 170 MB 0.787 0.943 44,315,560 - InceptionV3 92 MB 0.779 0.937 23,851,784 159 InceptionResNetV2 215 MB 0.803 0.953 55,873,736 572 MobileNet 16 MB 0.704 0.895 4,253,864 88 MobileNetV2 14 MB 0.713 0.901 3,538,984 88 DenseNet121 33 MB 0.750 0.923 8,062,504 121 DenseNet169 57 MB 0.762 0.932 14,307,880 169 DenseNet201 80 MB 0.773 0.936 20,242,984 201 NASNetMobile 23 MB 0.744 0.919 5,326,716 - NASNetLarge 343 MB 0.825 0.960 88,949,818 - Top-1 \u51c6\u786e\u7387\u548c Top-5 \u51c6\u786e\u7387\u90fd\u662f\u5728 ImageNet \u9a8c\u8bc1\u96c6\u4e0a\u7684\u7ed3\u679c\u3002 Depth \u8868\u793a\u7f51\u7edc\u7684\u62d3\u6251\u6df1\u5ea6\u3002\u8fd9\u5305\u62ec\u6fc0\u6d3b\u5c42\uff0c\u6279\u6807\u51c6\u5316\u5c42\u7b49\u3002 Xception keras.applications.xception.Xception(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) \u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684 Xception V1 \u6a21\u578b\u3002 \u5728 ImageNet \u4e0a\uff0c\u8be5\u6a21\u578b\u53d6\u5f97\u4e86\u9a8c\u8bc1\u96c6 top1 0.790 \u548c top5 0.945 \u7684\u51c6\u786e\u7387\u3002 \u6ce8\u610f\u8be5\u6a21\u578b\u53ea\u652f\u6301 channels_last \u7684\u7ef4\u5ea6\u987a\u5e8f\uff08\u9ad8\u5ea6\u3001\u5bbd\u5ea6\u3001\u901a\u9053\uff09\u3002 \u6a21\u578b\u9ed8\u8ba4\u8f93\u5165\u5c3a\u5bf8\u662f 299x299\u3002 \u53c2\u6570 include_top : \u662f\u5426\u5305\u62ec\u9876\u5c42\u7684\u5168\u8fde\u63a5\u5c42\u3002 weights : None \u4ee3\u8868\u968f\u673a\u521d\u59cb\u5316\uff0c 'imagenet' \u4ee3\u8868\u52a0\u8f7d\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6743\u503c\u3002 input_tensor : \u53ef\u9009\uff0cKeras tensor \u4f5c\u4e3a\u6a21\u578b\u7684\u8f93\u5165\uff08\u5373 layers.Input() \u8f93\u51fa\u7684 tensor\uff09\u3002 input_shape : \u53ef\u9009\uff0c\u8f93\u5165\u5c3a\u5bf8\u5143\u7ec4\uff0c\u4ec5\u5f53 include_top=False \u65f6\u6709\u6548\uff08\u5426\u5219\u8f93\u5165\u5f62\u72b6\u5fc5\u987b\u662f (299, 299, 3) \uff0c\u56e0\u4e3a\u9884\u8bad\u7ec3\u6a21\u578b\u662f\u4ee5\u8fd9\u4e2a\u5927\u5c0f\u8bad\u7ec3\u7684\uff09\u3002\u5b83\u5fc5\u987b\u62e5\u6709 3 \u4e2a\u8f93\u5165\u901a\u9053\uff0c\u4e14\u5bbd\u9ad8\u5fc5\u987b\u4e0d\u5c0f\u4e8e 71\u3002\u4f8b\u5982 (150, 150, 3) \u662f\u4e00\u4e2a\u5408\u6cd5\u7684\u8f93\u5165\u5c3a\u5bf8\u3002 pooling : \u53ef\u9009\uff0c\u5f53 include_top \u4e3a False \u65f6\uff0c\u8be5\u53c2\u6570\u6307\u5b9a\u4e86\u7279\u5f81\u63d0\u53d6\u65f6\u7684\u6c60\u5316\u65b9\u5f0f\u3002 None \u4ee3\u8868\u4e0d\u6c60\u5316\uff0c\u76f4\u63a5\u8f93\u51fa\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u7684\u8f93\u51fa\uff0c\u8be5\u8f93\u51fa\u662f\u4e00\u4e2a 4D \u5f20\u91cf\u3002 'avg' \u4ee3\u8868\u5168\u5c40\u5e73\u5747\u6c60\u5316\uff08GlobalAveragePooling2D\uff09\uff0c\u76f8\u5f53\u4e8e\u5728\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u540e\u9762\u518d\u52a0\u4e00\u5c42\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5c42\uff0c\u8f93\u51fa\u662f\u4e00\u4e2a 2D \u5f20\u91cf\u3002 'max' \u4ee3\u8868\u5168\u5c40\u6700\u5927\u6c60\u5316\u3002 classes : \u53ef\u9009\uff0c\u56fe\u7247\u5206\u7c7b\u7684\u7c7b\u522b\u6570\uff0c\u4ec5\u5f53 include_top \u4e3a True \u5e76\u4e14\u4e0d\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u503c\u65f6\u53ef\u7528\u3002 \u8fd4\u56de\u503c \u4e00\u4e2a Keras Model \u5bf9\u8c61. \u53c2\u8003\u6587\u732e Xception: Deep Learning with Depthwise Separable Convolutions License \u9884\u8bad\u7ec3\u6743\u503c\u7531\u6211\u4eec\u81ea\u5df1\u8bad\u7ec3\u800c\u6765\uff0c\u57fa\u4e8e MIT license \u53d1\u5e03\u3002 VGG16 keras.applications.vgg16.VGG16(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) VGG16 \u6a21\u578b\uff0c\u6743\u503c\u7531 ImageNet \u8bad\u7ec3\u800c\u6765\u3002 \u8be5\u6a21\u578b\u53ef\u540c\u65f6\u6784\u5efa\u4e8e channels_first (\u901a\u9053\uff0c\u9ad8\u5ea6\uff0c\u5bbd\u5ea6) \u548c channels_last \uff08\u9ad8\u5ea6\uff0c\u5bbd\u5ea6\uff0c\u901a\u9053\uff09\u4e24\u79cd\u8f93\u5165\u7ef4\u5ea6\u987a\u5e8f\u3002 \u6a21\u578b\u9ed8\u8ba4\u8f93\u5165\u5c3a\u5bf8\u662f 224x224\u3002 \u53c2\u6570 include_top : \u662f\u5426\u5305\u62ec\u9876\u5c42\u7684\u5168\u8fde\u63a5\u5c42\u3002 weights : None \u4ee3\u8868\u968f\u673a\u521d\u59cb\u5316\uff0c 'imagenet' \u4ee3\u8868\u52a0\u8f7d\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6743\u503c\u3002 input_tensor : \u53ef\u9009\uff0cKeras tensor \u4f5c\u4e3a\u6a21\u578b\u7684\u8f93\u5165\uff08\u5373 layers.Input() \u8f93\u51fa\u7684 tensor\uff09\u3002 input_shape : \u53ef\u9009\uff0c\u8f93\u5165\u5c3a\u5bf8\u5143\u7ec4\uff0c\u4ec5\u5f53 include_top=False \u65f6\u6709\u6548\uff0c\u5426\u5219\u8f93\u5165\u5f62\u72b6\u5fc5\u987b\u662f (244, 244, 3) \uff08\u5bf9\u4e8e channels_last \u6570\u636e\u683c\u5f0f\uff09\uff0c\u6216\u8005 (3, 244, 244) \uff08\u5bf9\u4e8e channels_first \u6570\u636e\u683c\u5f0f\uff09\u3002\u5b83\u5fc5\u987b\u62e5\u6709 3 \u4e2a\u8f93\u5165\u901a\u9053\uff0c\u4e14\u5bbd\u9ad8\u5fc5\u987b\u4e0d\u5c0f\u4e8e 32\u3002\u4f8b\u5982 (200, 200, 3) \u662f\u4e00\u4e2a\u5408\u6cd5\u7684\u8f93\u5165\u5c3a\u5bf8\u3002 pooling : \u53ef\u9009\uff0c\u5f53 include_top \u4e3a False \u65f6\uff0c\u8be5\u53c2\u6570\u6307\u5b9a\u4e86\u7279\u5f81\u63d0\u53d6\u65f6\u7684\u6c60\u5316\u65b9\u5f0f\u3002 None \u4ee3\u8868\u4e0d\u6c60\u5316\uff0c\u76f4\u63a5\u8f93\u51fa\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u7684\u8f93\u51fa\uff0c\u8be5\u8f93\u51fa\u662f\u4e00\u4e2a\u56db\u7ef4\u5f20\u91cf\u3002 'avg' \u4ee3\u8868\u5168\u5c40\u5e73\u5747\u6c60\u5316\uff08GlobalAveragePooling2D\uff09\uff0c\u76f8\u5f53\u4e8e\u5728\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u540e\u9762\u518d\u52a0\u4e00\u5c42\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5c42\uff0c\u8f93\u51fa\u662f\u4e00\u4e2a\u4e8c\u7ef4\u5f20\u91cf\u3002 'max' \u4ee3\u8868\u5168\u5c40\u6700\u5927\u6c60\u5316 classes : \u53ef\u9009\uff0c\u56fe\u7247\u5206\u7c7b\u7684\u7c7b\u522b\u6570\uff0c\u4ec5\u5f53 include_top \u4e3a True \u5e76\u4e14\u4e0d\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u503c\u65f6\u53ef\u7528\u3002 \u8fd4\u56de\u503c \u4e00\u4e2a Keras Model \u5bf9\u8c61\u3002 \u53c2\u8003\u6587\u732e Very Deep Convolutional Networks for Large-Scale Image Recognition \uff1a\u5982\u679c\u5728\u7814\u7a76\u4e2d\u4f7f\u7528\u4e86VGG\uff0c\u8bf7\u5f15\u7528\u8be5\u8bba\u6587\u3002 License \u9884\u8bad\u7ec3\u6743\u503c\u7531 VGG at Oxford \u53d1\u5e03\u7684\u9884\u8bad\u7ec3\u6743\u503c\u79fb\u690d\u800c\u6765\uff0c\u57fa\u4e8e Creative Commons Attribution License \u3002 VGG19 keras.applications.vgg19.VGG19(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) VGG19 \u6a21\u578b\uff0c\u6743\u503c\u7531 ImageNet \u8bad\u7ec3\u800c\u6765\u3002 \u8be5\u6a21\u578b\u53ef\u540c\u65f6\u6784\u5efa\u4e8e channels_first (\u901a\u9053\uff0c\u9ad8\u5ea6\uff0c\u5bbd\u5ea6) \u548c channels_last \uff08\u9ad8\u5ea6\uff0c\u5bbd\u5ea6\uff0c\u901a\u9053\uff09\u4e24\u79cd\u8f93\u5165\u7ef4\u5ea6\u987a\u5e8f\u3002 \u6a21\u578b\u9ed8\u8ba4\u8f93\u5165\u5c3a\u5bf8\u662f 224x224\u3002 \u53c2\u6570 include_top : \u662f\u5426\u5305\u62ec\u9876\u5c42\u7684\u5168\u8fde\u63a5\u5c42\u3002 weights : None \u4ee3\u8868\u968f\u673a\u521d\u59cb\u5316\uff0c 'imagenet' \u4ee3\u8868\u52a0\u8f7d\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6743\u503c\u3002 input_tensor : \u53ef\u9009\uff0cKeras tensor \u4f5c\u4e3a\u6a21\u578b\u7684\u8f93\u5165\uff08\u5373 layers.Input() \u8f93\u51fa\u7684 tensor\uff09\u3002 input_shape : \u53ef\u9009\uff0c\u8f93\u5165\u5c3a\u5bf8\u5143\u7ec4\uff0c\u4ec5\u5f53 include_top=False \u65f6\u6709\u6548\uff0c\u5426\u5219\u8f93\u5165\u5f62\u72b6\u5fc5\u987b\u662f (244, 244, 3) \uff08\u5bf9\u4e8e channels_last \u6570\u636e\u683c\u5f0f\uff09\uff0c\u6216\u8005 (3, 244, 244) \uff08\u5bf9\u4e8e channels_first \u6570\u636e\u683c\u5f0f\uff09\u3002\u5b83\u5fc5\u987b\u62e5\u6709 3 \u4e2a\u8f93\u5165\u901a\u9053\uff0c\u4e14\u5bbd\u9ad8\u5fc5\u987b\u4e0d\u5c0f\u4e8e 32\u3002\u4f8b\u5982 (200, 200, 3) \u662f\u4e00\u4e2a\u5408\u6cd5\u7684\u8f93\u5165\u5c3a\u5bf8\u3002 pooling : \u53ef\u9009\uff0c\u5f53 include_top \u4e3a False \u65f6\uff0c\u8be5\u53c2\u6570\u6307\u5b9a\u4e86\u7279\u5f81\u63d0\u53d6\u65f6\u7684\u6c60\u5316\u65b9\u5f0f\u3002 None \u4ee3\u8868\u4e0d\u6c60\u5316\uff0c\u76f4\u63a5\u8f93\u51fa\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u7684\u8f93\u51fa\uff0c\u8be5\u8f93\u51fa\u662f\u4e00\u4e2a\u56db\u7ef4\u5f20\u91cf\u3002 'avg' \u4ee3\u8868\u5168\u5c40\u5e73\u5747\u6c60\u5316\uff08GlobalAveragePooling2D\uff09\uff0c\u76f8\u5f53\u4e8e\u5728\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u540e\u9762\u518d\u52a0\u4e00\u5c42\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5c42\uff0c\u8f93\u51fa\u662f\u4e00\u4e2a\u4e8c\u7ef4\u5f20\u91cf\u3002 'max' \u4ee3\u8868\u5168\u5c40\u6700\u5927\u6c60\u5316 classes : \u53ef\u9009\uff0c\u56fe\u7247\u5206\u7c7b\u7684\u7c7b\u522b\u6570\uff0c\u4ec5\u5f53 include_top \u4e3a True \u5e76\u4e14\u4e0d\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u503c\u65f6\u53ef\u7528\u3002 \u8fd4\u56de\u503c \u4e00\u4e2a Keras Model \u5bf9\u8c61\u3002 \u53c2\u8003\u6587\u732e Very Deep Convolutional Networks for Large-Scale Image Recognition \uff1a\u5982\u679c\u5728\u7814\u7a76\u4e2d\u4f7f\u7528\u4e86VGG\uff0c\u8bf7\u5f15\u7528\u8be5\u8bba\u6587\u3002 License \u9884\u8bad\u7ec3\u6743\u503c\u7531 VGG at Oxford \u53d1\u5e03\u7684\u9884\u8bad\u7ec3\u6743\u503c\u79fb\u690d\u800c\u6765\uff0c\u57fa\u4e8e Creative Commons Attribution License \u3002 ResNet keras.applications.resnet.ResNet50(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) keras.applications.resnet.ResNet101(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) keras.applications.resnet.ResNet152(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) keras.applications.resnet_v2.ResNet50V2(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) keras.applications.resnet_v2.ResNet101V2(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) keras.applications.resnet_v2.ResNet152V2(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) keras.applications.resnext.ResNeXt50(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) keras.applications.resnext.ResNeXt101(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) ResNet, ResNetV2, ResNeXt \u6a21\u578b\uff0c\u6743\u503c\u7531 ImageNet \u8bad\u7ec3\u800c\u6765\u3002 \u8be5\u6a21\u578b\u53ef\u540c\u65f6\u6784\u5efa\u4e8e channels_first (\u901a\u9053\uff0c\u9ad8\u5ea6\uff0c\u5bbd\u5ea6) \u548c channels_last \uff08\u9ad8\u5ea6\uff0c\u5bbd\u5ea6\uff0c\u901a\u9053\uff09\u4e24\u79cd\u8f93\u5165\u7ef4\u5ea6\u987a\u5e8f\u3002 \u6a21\u578b\u9ed8\u8ba4\u8f93\u5165\u5c3a\u5bf8\u662f 224x224\u3002 \u53c2\u6570 include_top : \u662f\u5426\u5305\u62ec\u9876\u5c42\u7684\u5168\u8fde\u63a5\u5c42\u3002 weights : None \u4ee3\u8868\u968f\u673a\u521d\u59cb\u5316\uff0c 'imagenet' \u4ee3\u8868\u52a0\u8f7d\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6743\u503c\u3002 input_tensor : \u53ef\u9009\uff0cKeras tensor \u4f5c\u4e3a\u6a21\u578b\u7684\u8f93\u5165\uff08\u5373 layers.Input() \u8f93\u51fa\u7684 tensor\uff09\u3002 input_shape : \u53ef\u9009\uff0c\u8f93\u5165\u5c3a\u5bf8\u5143\u7ec4\uff0c\u4ec5\u5f53 include_top=False \u65f6\u6709\u6548\uff0c\u5426\u5219\u8f93\u5165\u5f62\u72b6\u5fc5\u987b\u662f (244, 244, 3) \uff08\u5bf9\u4e8e channels_last \u6570\u636e\u683c\u5f0f\uff09\uff0c\u6216\u8005 (3, 244, 244) \uff08\u5bf9\u4e8e channels_first \u6570\u636e\u683c\u5f0f\uff09\u3002\u5b83\u5fc5\u987b\u62e5\u6709 3 \u4e2a\u8f93\u5165\u901a\u9053\uff0c\u4e14\u5bbd\u9ad8\u5fc5\u987b\u4e0d\u5c0f\u4e8e 32\u3002\u4f8b\u5982 (200, 200, 3) \u662f\u4e00\u4e2a\u5408\u6cd5\u7684\u8f93\u5165\u5c3a\u5bf8\u3002 pooling : \u53ef\u9009\uff0c\u5f53 include_top \u4e3a False \u65f6\uff0c\u8be5\u53c2\u6570\u6307\u5b9a\u4e86\u7279\u5f81\u63d0\u53d6\u65f6\u7684\u6c60\u5316\u65b9\u5f0f\u3002 None \u4ee3\u8868\u4e0d\u6c60\u5316\uff0c\u76f4\u63a5\u8f93\u51fa\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u7684\u8f93\u51fa\uff0c\u8be5\u8f93\u51fa\u662f\u4e00\u4e2a\u56db\u7ef4\u5f20\u91cf\u3002 'avg' \u4ee3\u8868\u5168\u5c40\u5e73\u5747\u6c60\u5316\uff08GlobalAveragePooling2D\uff09\uff0c\u76f8\u5f53\u4e8e\u5728\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u540e\u9762\u518d\u52a0\u4e00\u5c42\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5c42\uff0c\u8f93\u51fa\u662f\u4e00\u4e2a\u4e8c\u7ef4\u5f20\u91cf\u3002 'max' \u4ee3\u8868\u5168\u5c40\u6700\u5927\u6c60\u5316 classes : \u53ef\u9009\uff0c\u56fe\u7247\u5206\u7c7b\u7684\u7c7b\u522b\u6570\uff0c\u4ec5\u5f53 include_top \u4e3a True \u5e76\u4e14\u4e0d\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u503c\u65f6\u53ef\u7528\u3002 \u8fd4\u56de\u503c \u4e00\u4e2a Keras Model \u5bf9\u8c61\u3002 \u53c2\u8003\u6587\u732e ResNet : Deep Residual Learning for Image Recognition ResNetV2 : Identity Mappings in Deep Residual Networks ResNeXt : Aggregated Residual Transformations for Deep Neural Networks License \u9884\u8bad\u7ec3\u6743\u503c\u7531\u4ee5\u4e0b\u63d0\u4f9b\uff1a ResNet : The original repository of Kaiming He under the MIT license . ResNetV2 : Facebook under the BSD license . ResNeXt : Facebook AI Research under the BSD license . InceptionV3 keras.applications.inception_v3.InceptionV3(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) Inception V3 \u6a21\u578b\uff0c\u6743\u503c\u7531 ImageNet \u8bad\u7ec3\u800c\u6765\u3002 \u8be5\u6a21\u578b\u53ef\u540c\u65f6\u6784\u5efa\u4e8e channels_first (\u901a\u9053\uff0c\u9ad8\u5ea6\uff0c\u5bbd\u5ea6) \u548c channels_last \uff08\u9ad8\u5ea6\uff0c\u5bbd\u5ea6\uff0c\u901a\u9053\uff09\u4e24\u79cd\u8f93\u5165\u7ef4\u5ea6\u987a\u5e8f\u3002 \u6a21\u578b\u9ed8\u8ba4\u8f93\u5165\u5c3a\u5bf8\u662f 299x299\u3002 \u53c2\u6570 include_top : \u662f\u5426\u5305\u62ec\u9876\u5c42\u7684\u5168\u8fde\u63a5\u5c42\u3002 weights : None \u4ee3\u8868\u968f\u673a\u521d\u59cb\u5316\uff0c 'imagenet' \u4ee3\u8868\u52a0\u8f7d\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6743\u503c\u3002 input_tensor : \u53ef\u9009\uff0cKeras tensor \u4f5c\u4e3a\u6a21\u578b\u7684\u8f93\u5165\uff08\u5373 layers.Input() \u8f93\u51fa\u7684 tensor\uff09\u3002 input_shape : \u53ef\u9009\uff0c\u8f93\u5165\u5c3a\u5bf8\u5143\u7ec4\uff0c\u4ec5\u5f53 include_top=False \u65f6\u6709\u6548\uff0c\u5426\u5219\u8f93\u5165\u5f62\u72b6\u5fc5\u987b\u662f (299, 299, 3) \uff08\u5bf9\u4e8e channels_last \u6570\u636e\u683c\u5f0f\uff09\uff0c\u6216\u8005 (3, 299, 299) \uff08\u5bf9\u4e8e channels_first \u6570\u636e\u683c\u5f0f\uff09\u3002\u5b83\u5fc5\u987b\u62e5\u6709 3 \u4e2a\u8f93\u5165\u901a\u9053\uff0c\u4e14\u5bbd\u9ad8\u5fc5\u987b\u4e0d\u5c0f\u4e8e 139\u3002\u4f8b\u5982 (150, 150, 3) \u662f\u4e00\u4e2a\u5408\u6cd5\u7684\u8f93\u5165\u5c3a\u5bf8\u3002 pooling : \u53ef\u9009\uff0c\u5f53 include_top \u4e3a False \u65f6\uff0c\u8be5\u53c2\u6570\u6307\u5b9a\u4e86\u7279\u5f81\u63d0\u53d6\u65f6\u7684\u6c60\u5316\u65b9\u5f0f\u3002 None \u4ee3\u8868\u4e0d\u6c60\u5316\uff0c\u76f4\u63a5\u8f93\u51fa\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u7684\u8f93\u51fa\uff0c\u8be5\u8f93\u51fa\u662f\u4e00\u4e2a\u56db\u7ef4\u5f20\u91cf\u3002 'avg' \u4ee3\u8868\u5168\u5c40\u5e73\u5747\u6c60\u5316\uff08GlobalAveragePooling2D\uff09\uff0c\u76f8\u5f53\u4e8e\u5728\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u540e\u9762\u518d\u52a0\u4e00\u5c42\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5c42\uff0c\u8f93\u51fa\u662f\u4e00\u4e2a\u4e8c\u7ef4\u5f20\u91cf\u3002 'max' \u4ee3\u8868\u5168\u5c40\u6700\u5927\u6c60\u5316 classes : \u53ef\u9009\uff0c\u56fe\u7247\u5206\u7c7b\u7684\u7c7b\u522b\u6570\uff0c\u4ec5\u5f53 include_top \u4e3a True \u5e76\u4e14\u4e0d\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u503c\u65f6\u53ef\u7528\u3002 \u8fd4\u56de\u503c \u4e00\u4e2a Keras Model \u5bf9\u8c61\u3002 \u53c2\u8003\u6587\u732e Rethinking the Inception Architecture for Computer Vision License \u9884\u8bad\u7ec3\u6743\u503c\u57fa\u4e8e Apache License \u3002 InceptionResNetV2 keras.applications.inception_resnet_v2.InceptionResNetV2(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) Inception-ResNet V2 \u6a21\u578b\uff0c\u6743\u503c\u7531 ImageNet \u8bad\u7ec3\u800c\u6765\u3002 \u8be5\u6a21\u578b\u53ef\u540c\u65f6\u6784\u5efa\u4e8e channels_first (\u901a\u9053\uff0c\u9ad8\u5ea6\uff0c\u5bbd\u5ea6) \u548c channels_last \uff08\u9ad8\u5ea6\uff0c\u5bbd\u5ea6\uff0c\u901a\u9053\uff09\u4e24\u79cd\u8f93\u5165\u7ef4\u5ea6\u987a\u5e8f\u3002 \u6a21\u578b\u9ed8\u8ba4\u8f93\u5165\u5c3a\u5bf8\u662f 299x299\u3002 \u53c2\u6570 include_top : \u662f\u5426\u5305\u62ec\u9876\u5c42\u7684\u5168\u8fde\u63a5\u5c42\u3002 weights : None \u4ee3\u8868\u968f\u673a\u521d\u59cb\u5316\uff0c 'imagenet' \u4ee3\u8868\u52a0\u8f7d\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6743\u503c\u3002 input_tensor : \u53ef\u9009\uff0cKeras tensor \u4f5c\u4e3a\u6a21\u578b\u7684\u8f93\u5165\uff08\u5373 layers.Input() \u8f93\u51fa\u7684 tensor\uff09\u3002 input_shape : \u53ef\u9009\uff0c\u8f93\u5165\u5c3a\u5bf8\u5143\u7ec4\uff0c\u4ec5\u5f53 include_top=False \u65f6\u6709\u6548\uff0c\u5426\u5219\u8f93\u5165\u5f62\u72b6\u5fc5\u987b\u662f (299, 299, 3) \uff08\u5bf9\u4e8e channels_last \u6570\u636e\u683c\u5f0f\uff09\uff0c\u6216\u8005 (3, 299, 299) \uff08\u5bf9\u4e8e channels_first \u6570\u636e\u683c\u5f0f\uff09\u3002\u5b83\u5fc5\u987b\u62e5\u6709 3 \u4e2a\u8f93\u5165\u901a\u9053\uff0c\u4e14\u5bbd\u9ad8\u5fc5\u987b\u4e0d\u5c0f\u4e8e 139\u3002\u4f8b\u5982 (150, 150, 3) \u662f\u4e00\u4e2a\u5408\u6cd5\u7684\u8f93\u5165\u5c3a\u5bf8\u3002 pooling : \u53ef\u9009\uff0c\u5f53 include_top \u4e3a False \u65f6\uff0c\u8be5\u53c2\u6570\u6307\u5b9a\u4e86\u7279\u5f81\u63d0\u53d6\u65f6\u7684\u6c60\u5316\u65b9\u5f0f\u3002 None \u4ee3\u8868\u4e0d\u6c60\u5316\uff0c\u76f4\u63a5\u8f93\u51fa\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u7684\u8f93\u51fa\uff0c\u8be5\u8f93\u51fa\u662f\u4e00\u4e2a\u56db\u7ef4\u5f20\u91cf\u3002 'avg' \u4ee3\u8868\u5168\u5c40\u5e73\u5747\u6c60\u5316\uff08GlobalAveragePooling2D\uff09\uff0c\u76f8\u5f53\u4e8e\u5728\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u540e\u9762\u518d\u52a0\u4e00\u5c42\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5c42\uff0c\u8f93\u51fa\u662f\u4e00\u4e2a\u4e8c\u7ef4\u5f20\u91cf\u3002 'max' \u4ee3\u8868\u5168\u5c40\u6700\u5927\u6c60\u5316 classes : \u53ef\u9009\uff0c\u56fe\u7247\u5206\u7c7b\u7684\u7c7b\u522b\u6570\uff0c\u4ec5\u5f53 include_top \u4e3a True \u5e76\u4e14\u4e0d\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u503c\u65f6\u53ef\u7528\u3002 \u8fd4\u56de\u503c \u4e00\u4e2a Keras Model \u5bf9\u8c61\u3002 \u53c2\u8003\u6587\u732e Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning License \u9884\u8bad\u7ec3\u6743\u503c\u57fa\u4e8e Apache License \u3002 MobileNet keras.applications.mobilenet.MobileNet(input_shape=None, alpha=1.0, depth_multiplier=1, dropout=1e-3, include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000) \u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684 MobileNet \u6a21\u578b\u3002 \u6ce8\u610f\uff0c\u8be5\u6a21\u578b\u76ee\u524d\u53ea\u652f\u6301 channels_last \u7684\u7ef4\u5ea6\u987a\u5e8f\uff08\u9ad8\u5ea6\u3001\u5bbd\u5ea6\u3001\u901a\u9053\uff09\u3002 \u6a21\u578b\u9ed8\u8ba4\u8f93\u5165\u5c3a\u5bf8\u662f 224x224\u3002 \u53c2\u6570 input_shape : \u53ef\u9009\uff0c\u8f93\u5165\u5c3a\u5bf8\u5143\u7ec4\uff0c\u4ec5\u5f53 include_top=False \u65f6\u6709\u6548\uff0c\u5426\u5219\u8f93\u5165\u5f62\u72b6\u5fc5\u987b\u662f (224, 224, 3) \uff08 channels_last \u683c\u5f0f\uff09\u6216 (3, 224, 224) \uff08 channels_first \u683c\u5f0f\uff09\u3002\u5b83\u5fc5\u987b\u4e3a 3 \u4e2a\u8f93\u5165\u901a\u9053\uff0c\u4e14\u5bbd\u9ad8\u5fc5\u987b\u4e0d\u5c0f\u4e8e 32\uff0c\u6bd4\u5982 (200, 200, 3) \u662f\u4e00\u4e2a\u5408\u6cd5\u7684\u8f93\u5165\u5c3a\u5bf8\u3002 alpha : \u63a7\u5236\u7f51\u7edc\u7684\u5bbd\u5ea6\uff1a \u5982\u679c alpha < 1.0\uff0c\u5219\u540c\u6bd4\u4f8b\u51cf\u5c11\u6bcf\u5c42\u7684\u6ee4\u6ce2\u5668\u4e2a\u6570\u3002 \u5982\u679c alpha > 1.0\uff0c\u5219\u540c\u6bd4\u4f8b\u589e\u52a0\u6bcf\u5c42\u7684\u6ee4\u6ce2\u5668\u4e2a\u6570\u3002 \u5982\u679c alpha = 1\uff0c\u4f7f\u7528\u8bba\u6587\u9ed8\u8ba4\u7684\u6ee4\u6ce2\u5668\u4e2a\u6570 depth_multiplier : depthwise\u5377\u79ef\u7684\u6df1\u5ea6\u4e58\u5b50\uff0c\u4e5f\u79f0\u4e3a\uff08\u5206\u8fa8\u7387\u4e58\u5b50\uff09 dropout : dropout \u6982\u7387 include_top : \u662f\u5426\u5305\u62ec\u9876\u5c42\u7684\u5168\u8fde\u63a5\u5c42\u3002 weights : None \u4ee3\u8868\u968f\u673a\u521d\u59cb\u5316\uff0c 'imagenet' \u4ee3\u8868\u52a0\u8f7d\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6743\u503c\u3002 input_tensor : \u53ef\u9009\uff0cKeras tensor \u4f5c\u4e3a\u6a21\u578b\u7684\u8f93\u5165\uff08\u6bd4\u5982 layers.Input() \u8f93\u51fa\u7684 tensor\uff09\u3002 pooling : \u53ef\u9009\uff0c\u5f53 include_top \u4e3a False \u65f6\uff0c\u8be5\u53c2\u6570\u6307\u5b9a\u4e86\u7279\u5f81\u63d0\u53d6\u65f6\u7684\u6c60\u5316\u65b9\u5f0f\u3002 None \u4ee3\u8868\u4e0d\u6c60\u5316\uff0c\u76f4\u63a5\u8f93\u51fa\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u7684\u8f93\u51fa\uff0c\u8be5\u8f93\u51fa\u662f\u4e00\u4e2a\u56db\u7ef4\u5f20\u91cf\u3002 'avg' \u4ee3\u8868\u5168\u5c40\u5e73\u5747\u6c60\u5316\uff08GlobalAveragePooling2D\uff09\uff0c\u76f8\u5f53\u4e8e\u5728\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u540e\u9762\u518d\u52a0\u4e00\u5c42\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5c42\uff0c\u8f93\u51fa\u662f\u4e00\u4e2a\u4e8c\u7ef4\u5f20\u91cf\u3002 'max' \u4ee3\u8868\u5168\u5c40\u6700\u5927\u6c60\u5316 classes : \u53ef\u9009\uff0c\u56fe\u7247\u5206\u7c7b\u7684\u7c7b\u522b\u6570\uff0c\u4ec5\u5f53 include_top \u4e3a True \u5e76\u4e14\u4e0d\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u503c\u65f6\u53ef\u7528\u3002 \u8fd4\u56de \u4e00\u4e2a Keras Model \u5bf9\u8c61\u3002 \u53c2\u8003\u6587\u732e MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications License \u9884\u8bad\u7ec3\u6743\u503c\u57fa\u4e8e Apache License \u3002 DenseNet keras.applications.densenet.DenseNet121(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) keras.applications.densenet.DenseNet169(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) keras.applications.densenet.DenseNet201(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) \u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684 DenseNet \u6a21\u578b\u3002 \u8be5\u6a21\u578b\u53ef\u540c\u65f6\u6784\u5efa\u4e8e channels_first (\u901a\u9053\uff0c\u9ad8\u5ea6\uff0c\u5bbd\u5ea6) \u548c channels_last \uff08\u9ad8\u5ea6\uff0c\u5bbd\u5ea6\uff0c\u901a\u9053\uff09\u4e24\u79cd\u8f93\u5165\u7ef4\u5ea6\u987a\u5e8f\u3002 \u6a21\u578b\u9ed8\u8ba4\u8f93\u5165\u5c3a\u5bf8\u662f 224x224\u3002 \u53c2\u6570 blocks : \u56db\u4e2a Dense Layers \u7684 block \u6570\u91cf\u3002 include_top : \u662f\u5426\u5305\u62ec\u9876\u5c42\u7684\u5168\u8fde\u63a5\u5c42\u3002 weights : None \u4ee3\u8868\u968f\u673a\u521d\u59cb\u5316\uff0c 'imagenet' \u4ee3\u8868\u52a0\u8f7d\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6743\u503c\u3002 input_tensor : \u53ef\u9009\uff0cKeras tensor \u4f5c\u4e3a\u6a21\u578b\u7684\u8f93\u5165\uff08\u6bd4\u5982 layers.Input() \u8f93\u51fa\u7684 tensor\uff09\u3002 input_shape: \u53ef\u9009\uff0c\u8f93\u5165\u5c3a\u5bf8\u5143\u7ec4\uff0c\u4ec5\u5f53 include_top=False \u65f6\u6709\u6548\uff08\u4e0d\u7136\u8f93\u5165\u5f62\u72b6\u5fc5\u987b\u662f (224, 224, 3) \uff08 channels_last \u683c\u5f0f\uff09\u6216 (3, 224, 224) \uff08 channels_first \u683c\u5f0f\uff09\uff0c\u56e0\u4e3a\u9884\u8bad\u7ec3\u6a21\u578b\u662f\u4ee5\u8fd9\u4e2a\u5927\u5c0f\u8bad\u7ec3\u7684\uff09\u3002\u5b83\u5fc5\u987b\u4e3a 3 \u4e2a\u8f93\u5165\u901a\u9053\uff0c\u4e14\u5bbd\u9ad8\u5fc5\u987b\u4e0d\u5c0f\u4e8e 32\uff0c\u6bd4\u5982 (200, 200, 3) \u662f\u4e00\u4e2a\u5408\u6cd5\u7684\u8f93\u5165\u5c3a\u5bf8\u3002 pooling : \u53ef\u9009\uff0c\u5f53 include_top \u4e3a False \u65f6\uff0c\u8be5\u53c2\u6570\u6307\u5b9a\u4e86\u7279\u5f81\u63d0\u53d6\u65f6\u7684\u6c60\u5316\u65b9\u5f0f\u3002 None \u4ee3\u8868\u4e0d\u6c60\u5316\uff0c\u76f4\u63a5\u8f93\u51fa\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u7684\u8f93\u51fa\uff0c\u8be5\u8f93\u51fa\u662f\u4e00\u4e2a\u56db\u7ef4\u5f20\u91cf\u3002 'avg' \u4ee3\u8868\u5168\u5c40\u5e73\u5747\u6c60\u5316\uff08GlobalAveragePooling2D\uff09\uff0c\u76f8\u5f53\u4e8e\u5728\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u540e\u9762\u518d\u52a0\u4e00\u5c42\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5c42\uff0c\u8f93\u51fa\u662f\u4e00\u4e2a\u4e8c\u7ef4\u5f20\u91cf\u3002 'max' \u4ee3\u8868\u5168\u5c40\u6700\u5927\u6c60\u5316. classes : \u53ef\u9009\uff0c\u56fe\u7247\u5206\u7c7b\u7684\u7c7b\u522b\u6570\uff0c\u4ec5\u5f53 include_top \u4e3a True \u5e76\u4e14\u4e0d\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u503c\u65f6\u53ef\u7528\u3002 \u8fd4\u56de \u4e00\u4e2a Keras Model \u5bf9\u8c61\u3002 \u53c2\u8003\u6587\u732e Densely Connected Convolutional Networks (CVPR 2017 Best Paper Award) Licence \u9884\u8bad\u7ec3\u6743\u503c\u57fa\u4e8e BSD 3-clause License \u3002 NASNet keras.applications.nasnet.NASNetLarge(input_shape=None, include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000) keras.applications.nasnet.NASNetMobile(input_shape=None, include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000) \u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u795e\u7ecf\u7ed3\u6784\u641c\u7d22\u7f51\u7edc\u6a21\u578b\uff08NASNet\uff09\u3002 NASNetLarge \u6a21\u578b\u9ed8\u8ba4\u7684\u8f93\u5165\u5c3a\u5bf8\u662f 331x331\uff0cNASNetMobile \u6a21\u578b\u9ed8\u8ba4\u7684\u8f93\u5165\u5c3a\u5bf8\u662f 224x224\u3002 \u53c2\u6570 input_shape : \u53ef\u9009\uff0c\u8f93\u5165\u5c3a\u5bf8\u5143\u7ec4\uff0c\u4ec5\u5f53 include_top=False \u65f6\u6709\u6548\uff0c\u5426\u5219\u5bf9\u4e8e NASNetMobile \u6a21\u578b\u6765\u8bf4\uff0c\u8f93\u5165\u5f62\u72b6\u5fc5\u987b\u662f (224, 224, 3) \uff08 channels_last \u683c\u5f0f\uff09\u6216 (3, 224, 224) \uff08 channels_first \u683c\u5f0f\uff09\uff0c\u5bf9\u4e8e NASNetLarge \u6765\u8bf4\uff0c\u8f93\u5165\u5f62\u72b6\u5fc5\u987b\u662f (331, 331, 3) \uff08 channels_last \u683c\u5f0f\uff09\u6216 (3, 331, 331) \uff08 channels_first \u683c\u5f0f\uff09\u3002\u5b83\u5fc5\u987b\u4e3a 3 \u4e2a\u8f93\u5165\u901a\u9053\uff0c\u4e14\u5bbd\u9ad8\u5fc5\u987b\u4e0d\u5c0f\u4e8e 32\uff0c\u6bd4\u5982 (200, 200, 3) \u662f\u4e00\u4e2a\u5408\u6cd5\u7684\u8f93\u5165\u5c3a\u5bf8\u3002 include_top : \u662f\u5426\u5305\u62ec\u9876\u5c42\u7684\u5168\u8fde\u63a5\u5c42\u3002 weights : None \u4ee3\u8868\u968f\u673a\u521d\u59cb\u5316\uff0c 'imagenet' \u4ee3\u8868\u52a0\u8f7d\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6743\u503c\u3002 input_tensor : \u53ef\u9009\uff0cKeras tensor \u4f5c\u4e3a\u6a21\u578b\u7684\u8f93\u5165\uff08\u6bd4\u5982 layers.Input() \u8f93\u51fa\u7684 tensor\uff09\u3002 pooling : \u53ef\u9009\uff0c\u5f53 include_top \u4e3a False \u65f6\uff0c\u8be5\u53c2\u6570\u6307\u5b9a\u4e86\u7279\u5f81\u63d0\u53d6\u65f6\u7684\u6c60\u5316\u65b9\u5f0f\u3002 None \u4ee3\u8868\u4e0d\u6c60\u5316\uff0c\u76f4\u63a5\u8f93\u51fa\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u7684\u8f93\u51fa\uff0c\u8be5\u8f93\u51fa\u662f\u4e00\u4e2a\u56db\u7ef4\u5f20\u91cf\u3002 'avg' \u4ee3\u8868\u5168\u5c40\u5e73\u5747\u6c60\u5316\uff08GlobalAveragePooling2D\uff09\uff0c\u76f8\u5f53\u4e8e\u5728\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u540e\u9762\u518d\u52a0\u4e00\u5c42\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5c42\uff0c\u8f93\u51fa\u662f\u4e00\u4e2a\u4e8c\u7ef4\u5f20\u91cf\u3002 'max' \u4ee3\u8868\u5168\u5c40\u6700\u5927\u6c60\u5316 classes : \u53ef\u9009\uff0c\u56fe\u7247\u5206\u7c7b\u7684\u7c7b\u522b\u6570\uff0c\u4ec5\u5f53 include_top \u4e3a True \u5e76\u4e14\u4e0d\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u503c\u65f6\u53ef\u7528\u3002 \u8fd4\u56de \u4e00\u4e2a Keras Model \u5b9e\u4f8b\u3002 \u53c2\u8003\u6587\u732e Learning Transferable Architectures for Scalable Image Recognition License \u9884\u8bad\u7ec3\u6743\u503c\u57fa\u4e8e Apache License \u3002 MobileNetV2 keras.applications.mobilenet_v2.MobileNetV2(input_shape=None, alpha=1.0, depth_multiplier=1, include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000) \u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684 MobileNetV2 \u6a21\u578b\u3002 \u8bf7\u6ce8\u610f\uff0c\u8be5\u6a21\u578b\u4ec5\u652f\u6301 'channels_last' \u6570\u636e\u683c\u5f0f\uff08\u9ad8\u5ea6\uff0c\u5bbd\u5ea6\uff0c\u901a\u9053)\u3002 \u6a21\u578b\u9ed8\u8ba4\u8f93\u51fa\u5c3a\u5bf8\u4e3a 224x224\u3002 \u53c2\u6570 input_shape : optional shape tuple, to be specified if you would like to use a model with an input img resolution that is not (224, 224, 3). It should have exactly 3 inputs channels (224, 224, 3). You can also omit this option if you would like to infer input_shape from an input_tensor. If you choose to include both input_tensor and input_shape then input_shape will be used if they match, if the shapes do not match then we will throw an error. E.g. (160, 160, 3) would be one valid value. alpha : \u63a7\u5236\u7f51\u7edc\u7684\u5bbd\u5ea6\u3002\u8fd9\u5728 MobileNetV2 \u8bba\u6587\u4e2d\u88ab\u79f0\u4f5c\u5bbd\u5ea6\u4e58\u5b50\u3002 \u5982\u679c alpha < 1.0\uff0c\u5219\u540c\u6bd4\u4f8b\u51cf\u5c11\u6bcf\u5c42\u7684\u6ee4\u6ce2\u5668\u4e2a\u6570\u3002 \u5982\u679c alpha > 1.0\uff0c\u5219\u540c\u6bd4\u4f8b\u589e\u52a0\u6bcf\u5c42\u7684\u6ee4\u6ce2\u5668\u4e2a\u6570\u3002 \u5982\u679c alpha = 1\uff0c\u4f7f\u7528\u8bba\u6587\u9ed8\u8ba4\u7684\u6ee4\u6ce2\u5668\u4e2a\u6570\u3002 depth_multiplier : depthwise \u5377\u79ef\u7684\u6df1\u5ea6\u4e58\u5b50\uff0c\u4e5f\u79f0\u4e3a\uff08\u5206\u8fa8\u7387\u4e58\u5b50\uff09 include_top : \u662f\u5426\u5305\u62ec\u9876\u5c42\u7684\u5168\u8fde\u63a5\u5c42\u3002 weights : None \u4ee3\u8868\u968f\u673a\u521d\u59cb\u5316\uff0c 'imagenet' \u4ee3\u8868\u52a0\u8f7d\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6743\u503c\u3002 input_tensor : \u53ef\u9009\uff0cKeras tensor \u4f5c\u4e3a\u6a21\u578b\u7684\u8f93\u5165\uff08\u5373 layers.Input() \u8f93\u51fa\u7684 tensor\uff09\u3002 pooling : \u53ef\u9009\uff0c\u5f53 include_top \u4e3a False \u65f6\uff0c\u8be5\u53c2\u6570\u6307\u5b9a\u4e86\u7279\u5f81\u63d0\u53d6\u65f6\u7684\u6c60\u5316\u65b9\u5f0f\u3002 None \u4ee3\u8868\u4e0d\u6c60\u5316\uff0c\u76f4\u63a5\u8f93\u51fa\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u7684\u8f93\u51fa\uff0c\u8be5\u8f93\u51fa\u662f\u4e00\u4e2a\u56db\u7ef4\u5f20\u91cf\u3002 'avg' \u4ee3\u8868\u5168\u5c40\u5e73\u5747\u6c60\u5316\uff08GlobalAveragePooling2D\uff09\uff0c\u76f8\u5f53\u4e8e\u5728\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u540e\u9762\u518d\u52a0\u4e00\u5c42\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5c42\uff0c\u8f93\u51fa\u662f\u4e00\u4e2a\u4e8c\u7ef4\u5f20\u91cf\u3002 'max' \u4ee3\u8868\u5168\u5c40\u6700\u5927\u6c60\u5316 classes : \u53ef\u9009\uff0c\u56fe\u7247\u5206\u7c7b\u7684\u7c7b\u522b\u6570\uff0c\u4ec5\u5f53 include_top \u4e3a True \u5e76\u4e14\u4e0d\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u503c\u65f6\u53ef\u7528\u3002 \u8fd4\u56de \u4e00\u4e2a Keras model \u5b9e\u4f8b\u3002 \u5f02\u5e38 ValueError : \u5982\u679c weights \u53c2\u6570\u975e\u6cd5\uff0c\u6216\u975e\u6cd5\u7684\u8f93\u5165\u5c3a\u5bf8\uff0c\u6216\u8005\u5f53 weights='imagenet' \u65f6\uff0c\u975e\u6cd5\u7684 depth_multiplier, alpha, rows\u3002 \u53c2\u8003\u6587\u732e MobileNetV2: Inverted Residuals and Linear Bottlenecks License \u9884\u8bad\u7ec3\u6743\u503c\u57fa\u4e8e Apache License .","title":"\u5e94\u7528 Applications"},{"location":"8.applications/#applications","text":"Keras \u7684\u5e94\u7528\u6a21\u5757\uff08keras.applications\uff09\u63d0\u4f9b\u4e86\u5e26\u6709\u9884\u8bad\u7ec3\u6743\u503c\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u53ef\u4ee5\u7528\u6765\u8fdb\u884c\u9884\u6d4b\u3001\u7279\u5f81\u63d0\u53d6\u548c\u5fae\u8c03\uff08fine-tuning\uff09\u3002 \u5f53\u4f60\u521d\u59cb\u5316\u4e00\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\u65f6\uff0c\u4f1a\u81ea\u52a8\u4e0b\u8f7d\u6743\u91cd\u5230 ~/.keras/models/ \u76ee\u5f55\u4e0b\u3002","title":"\u5e94\u7528 Applications"},{"location":"8.applications/#_1","text":"","title":"\u53ef\u7528\u7684\u6a21\u578b"},{"location":"8.applications/#imagenet","text":"Xception VGG16 VGG19 ResNet, ResNetV2, ResNeXt InceptionV3 InceptionResNetV2 MobileNet MobileNetV2 DenseNet NASNet \u6240\u6709\u7684\u8fd9\u4e9b\u67b6\u6784\u90fd\u517c\u5bb9\u6240\u6709\u7684\u540e\u7aef (TensorFlow, Theano \u548c CNTK)\uff0c\u5e76\u4e14\u4f1a\u5728\u5b9e\u4f8b\u5316\u65f6\uff0c\u6839\u636e Keras \u914d\u7f6e\u6587\u4ef6 \u301c/.keras/keras.json \u4e2d\u8bbe\u7f6e\u7684\u56fe\u50cf\u6570\u636e\u683c\u5f0f\u6784\u5efa\u6a21\u578b\u3002\u4e3e\u4e2a\u4f8b\u5b50\uff0c\u5982\u679c\u4f60\u8bbe\u7f6e image_data_format=channels_last \uff0c\u5219\u52a0\u8f7d\u7684\u6a21\u578b\u5c06\u6309\u7167 TensorFlow \u7684\u7ef4\u5ea6\u987a\u5e8f\u6765\u6784\u9020\uff0c\u5373\u300c\u9ad8\u5ea6-\u5bbd\u5ea6-\u6df1\u5ea6\u300d\uff08Height-Width-Depth\uff09\u7684\u987a\u5e8f\u3002 \u6ce8\u610f\uff1a \u5bf9\u4e8e Keras < 2.2.0 \uff0cXception \u6a21\u578b\u4ec5\u9002\u7528\u4e8e TensorFlow\uff0c\u56e0\u4e3a\u5b83\u4f9d\u8d56\u4e8e SeparableConvolution \u5c42\u3002 \u5bf9\u4e8e Keras < 2.1.5 \uff0cMobileNet \u6a21\u578b\u4ec5\u9002\u7528\u4e8e TensorFlow\uff0c\u56e0\u4e3a\u5b83\u4f9d\u8d56\u4e8e DepthwiseConvolution \u5c42\u3002","title":"\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u8fc7\u7684\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b\u7684\u6a21\u578b\uff1a"},{"location":"8.applications/#_2","text":"","title":"\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u7684\u4f7f\u7528\u793a\u4f8b"},{"location":"8.applications/#resnet50-imagenet","text":"from keras.applications.resnet50 import ResNet50 from keras.preprocessing import image from keras.applications.resnet50 import preprocess_input, decode_predictions import numpy as np model = ResNet50(weights='imagenet') img_path = 'elephant.jpg' img = image.load_img(img_path, target_size=(224, 224)) x = image.img_to_array(img) x = np.expand_dims(x, axis=0) x = preprocess_input(x) preds = model.predict(x) # \u5c06\u7ed3\u679c\u89e3\u7801\u4e3a\u5143\u7ec4\u5217\u8868 (class, description, probability) # (\u4e00\u4e2a\u5217\u8868\u4ee3\u8868\u6279\u6b21\u4e2d\u7684\u4e00\u4e2a\u6837\u672c\uff09 print('Predicted:', decode_predictions(preds, top=3)[0]) # Predicted: [(u'n02504013', u'Indian_elephant', 0.82658225), (u'n01871265', u'tusker', 0.1122357), (u'n02504458', u'African_elephant', 0.061040461)]","title":"\u4f7f\u7528 ResNet50 \u8fdb\u884c ImageNet \u5206\u7c7b"},{"location":"8.applications/#vgg16","text":"from keras.applications.vgg16 import VGG16 from keras.preprocessing import image from keras.applications.vgg16 import preprocess_input import numpy as np model = VGG16(weights='imagenet', include_top=False) img_path = 'elephant.jpg' img = image.load_img(img_path, target_size=(224, 224)) x = image.img_to_array(img) x = np.expand_dims(x, axis=0) x = preprocess_input(x) features = model.predict(x)","title":"\u4f7f\u7528 VGG16 \u63d0\u53d6\u7279\u5f81"},{"location":"8.applications/#vgg19","text":"from keras.applications.vgg19 import VGG19 from keras.preprocessing import image from keras.applications.vgg19 import preprocess_input from keras.models import Model import numpy as np base_model = VGG19(weights='imagenet') model = Model(inputs=base_model.input, outputs=base_model.get_layer('block4_pool').output) img_path = 'elephant.jpg' img = image.load_img(img_path, target_size=(224, 224)) x = image.img_to_array(img) x = np.expand_dims(x, axis=0) x = preprocess_input(x) block4_pool_features = model.predict(x)","title":"\u4eceVGG19 \u7684\u4efb\u610f\u4e2d\u95f4\u5c42\u4e2d\u62bd\u53d6\u7279\u5f81"},{"location":"8.applications/#inceptionv3","text":"from keras.applications.inception_v3 import InceptionV3 from keras.preprocessing import image from keras.models import Model from keras.layers import Dense, GlobalAveragePooling2D from keras import backend as K # \u6784\u5efa\u4e0d\u5e26\u5206\u7c7b\u5668\u7684\u9884\u8bad\u7ec3\u6a21\u578b base_model = InceptionV3(weights='imagenet', include_top=False) # \u6dfb\u52a0\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5c42 x = base_model.output x = GlobalAveragePooling2D()(x) # \u6dfb\u52a0\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42 x = Dense(1024, activation='relu')(x) # \u6dfb\u52a0\u4e00\u4e2a\u5206\u7c7b\u5668\uff0c\u5047\u8bbe\u6211\u4eec\u6709200\u4e2a\u7c7b predictions = Dense(200, activation='softmax')(x) # \u6784\u5efa\u6211\u4eec\u9700\u8981\u8bad\u7ec3\u7684\u5b8c\u6574\u6a21\u578b model = Model(inputs=base_model.input, outputs=predictions) # \u9996\u5148\uff0c\u6211\u4eec\u53ea\u8bad\u7ec3\u9876\u90e8\u7684\u51e0\u5c42\uff08\u968f\u673a\u521d\u59cb\u5316\u7684\u5c42\uff09 # \u9501\u4f4f\u6240\u6709 InceptionV3 \u7684\u5377\u79ef\u5c42 for layer in base_model.layers: layer.trainable = False # \u7f16\u8bd1\u6a21\u578b\uff08\u4e00\u5b9a\u8981\u5728\u9501\u5c42\u4ee5\u540e\u64cd\u4f5c\uff09 model.compile(optimizer='rmsprop', loss='categorical_crossentropy') # \u5728\u65b0\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u51e0\u4ee3 model.fit_generator(...) # \u73b0\u5728\u9876\u5c42\u5e94\u8be5\u8bad\u7ec3\u597d\u4e86\uff0c\u8ba9\u6211\u4eec\u5f00\u59cb\u5fae\u8c03 Inception V3 \u7684\u5377\u79ef\u5c42\u3002 # \u6211\u4eec\u4f1a\u9501\u4f4f\u5e95\u4e0b\u7684\u51e0\u5c42\uff0c\u7136\u540e\u8bad\u7ec3\u5176\u4f59\u7684\u9876\u5c42\u3002 # \u8ba9\u6211\u4eec\u770b\u770b\u6bcf\u4e00\u5c42\u7684\u540d\u5b57\u548c\u5c42\u53f7\uff0c\u770b\u770b\u6211\u4eec\u5e94\u8be5\u9501\u591a\u5c11\u5c42\u5462\uff1a for i, layer in enumerate(base_model.layers): print(i, layer.name) # \u6211\u4eec\u9009\u62e9\u8bad\u7ec3\u6700\u4e0a\u9762\u7684\u4e24\u4e2a Inception block # \u4e5f\u5c31\u662f\u8bf4\u9501\u4f4f\u524d\u9762249\u5c42\uff0c\u7136\u540e\u653e\u5f00\u4e4b\u540e\u7684\u5c42\u3002 for layer in model.layers[:249]: layer.trainable = False for layer in model.layers[249:]: layer.trainable = True # \u6211\u4eec\u9700\u8981\u91cd\u65b0\u7f16\u8bd1\u6a21\u578b\uff0c\u624d\u80fd\u4f7f\u4e0a\u9762\u7684\u4fee\u6539\u751f\u6548 # \u8ba9\u6211\u4eec\u8bbe\u7f6e\u4e00\u4e2a\u5f88\u4f4e\u7684\u5b66\u4e60\u7387\uff0c\u4f7f\u7528 SGD \u6765\u5fae\u8c03 from keras.optimizers import SGD model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy') # \u6211\u4eec\u7ee7\u7eed\u8bad\u7ec3\u6a21\u578b\uff0c\u8fd9\u6b21\u6211\u4eec\u8bad\u7ec3\u6700\u540e\u4e24\u4e2a Inception block # \u548c\u4e24\u4e2a\u5168\u8fde\u63a5\u5c42 model.fit_generator(...)","title":"\u5728\u65b0\u7c7b\u4e0a\u5fae\u8c03 InceptionV3"},{"location":"8.applications/#inceptionv3_1","text":"from keras.applications.inception_v3 import InceptionV3 from keras.layers import Input # \u8fd9\u4e5f\u53ef\u80fd\u662f\u4e0d\u540c\u7684 Keras \u6a21\u578b\u6216\u5c42\u7684\u8f93\u51fa input_tensor = Input(shape=(224, 224, 3)) # \u5047\u5b9a K.image_data_format() == 'channels_last' model = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=True)","title":"\u901a\u8fc7\u81ea\u5b9a\u4e49\u8f93\u5165\u5f20\u91cf\u6784\u5efa InceptionV3"},{"location":"8.applications/#_3","text":"\u6a21\u578b \u5927\u5c0f Top-1 \u51c6\u786e\u7387 Top-5 \u51c6\u786e\u7387 \u53c2\u6570\u6570\u91cf \u6df1\u5ea6 Xception 88 MB 0.790 0.945 22,910,480 126 VGG16 528 MB 0.713 0.901 138,357,544 23 VGG19 549 MB 0.713 0.900 143,667,240 26 ResNet50 98 MB 0.749 0.921 25,636,712 - ResNet101 171 MB 0.764 0.928 44,707,176 - ResNet152 232 MB 0.766 0.931 60,419,944 - ResNet50V2 98 MB 0.760 0.930 25,613,800 - ResNet101V2 171 MB 0.772 0.938 44,675,560 - ResNet152V2 232 MB 0.780 0.942 60,380,648 - ResNeXt50 96 MB 0.777 0.938 25,097,128 - ResNeXt101 170 MB 0.787 0.943 44,315,560 - InceptionV3 92 MB 0.779 0.937 23,851,784 159 InceptionResNetV2 215 MB 0.803 0.953 55,873,736 572 MobileNet 16 MB 0.704 0.895 4,253,864 88 MobileNetV2 14 MB 0.713 0.901 3,538,984 88 DenseNet121 33 MB 0.750 0.923 8,062,504 121 DenseNet169 57 MB 0.762 0.932 14,307,880 169 DenseNet201 80 MB 0.773 0.936 20,242,984 201 NASNetMobile 23 MB 0.744 0.919 5,326,716 - NASNetLarge 343 MB 0.825 0.960 88,949,818 - Top-1 \u51c6\u786e\u7387\u548c Top-5 \u51c6\u786e\u7387\u90fd\u662f\u5728 ImageNet \u9a8c\u8bc1\u96c6\u4e0a\u7684\u7ed3\u679c\u3002 Depth \u8868\u793a\u7f51\u7edc\u7684\u62d3\u6251\u6df1\u5ea6\u3002\u8fd9\u5305\u62ec\u6fc0\u6d3b\u5c42\uff0c\u6279\u6807\u51c6\u5316\u5c42\u7b49\u3002","title":"\u6a21\u578b\u6982\u89c8"},{"location":"8.applications/#xception","text":"keras.applications.xception.Xception(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) \u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684 Xception V1 \u6a21\u578b\u3002 \u5728 ImageNet \u4e0a\uff0c\u8be5\u6a21\u578b\u53d6\u5f97\u4e86\u9a8c\u8bc1\u96c6 top1 0.790 \u548c top5 0.945 \u7684\u51c6\u786e\u7387\u3002 \u6ce8\u610f\u8be5\u6a21\u578b\u53ea\u652f\u6301 channels_last \u7684\u7ef4\u5ea6\u987a\u5e8f\uff08\u9ad8\u5ea6\u3001\u5bbd\u5ea6\u3001\u901a\u9053\uff09\u3002 \u6a21\u578b\u9ed8\u8ba4\u8f93\u5165\u5c3a\u5bf8\u662f 299x299\u3002","title":"Xception"},{"location":"8.applications/#_4","text":"include_top : \u662f\u5426\u5305\u62ec\u9876\u5c42\u7684\u5168\u8fde\u63a5\u5c42\u3002 weights : None \u4ee3\u8868\u968f\u673a\u521d\u59cb\u5316\uff0c 'imagenet' \u4ee3\u8868\u52a0\u8f7d\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6743\u503c\u3002 input_tensor : \u53ef\u9009\uff0cKeras tensor \u4f5c\u4e3a\u6a21\u578b\u7684\u8f93\u5165\uff08\u5373 layers.Input() \u8f93\u51fa\u7684 tensor\uff09\u3002 input_shape : \u53ef\u9009\uff0c\u8f93\u5165\u5c3a\u5bf8\u5143\u7ec4\uff0c\u4ec5\u5f53 include_top=False \u65f6\u6709\u6548\uff08\u5426\u5219\u8f93\u5165\u5f62\u72b6\u5fc5\u987b\u662f (299, 299, 3) \uff0c\u56e0\u4e3a\u9884\u8bad\u7ec3\u6a21\u578b\u662f\u4ee5\u8fd9\u4e2a\u5927\u5c0f\u8bad\u7ec3\u7684\uff09\u3002\u5b83\u5fc5\u987b\u62e5\u6709 3 \u4e2a\u8f93\u5165\u901a\u9053\uff0c\u4e14\u5bbd\u9ad8\u5fc5\u987b\u4e0d\u5c0f\u4e8e 71\u3002\u4f8b\u5982 (150, 150, 3) \u662f\u4e00\u4e2a\u5408\u6cd5\u7684\u8f93\u5165\u5c3a\u5bf8\u3002 pooling : \u53ef\u9009\uff0c\u5f53 include_top \u4e3a False \u65f6\uff0c\u8be5\u53c2\u6570\u6307\u5b9a\u4e86\u7279\u5f81\u63d0\u53d6\u65f6\u7684\u6c60\u5316\u65b9\u5f0f\u3002 None \u4ee3\u8868\u4e0d\u6c60\u5316\uff0c\u76f4\u63a5\u8f93\u51fa\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u7684\u8f93\u51fa\uff0c\u8be5\u8f93\u51fa\u662f\u4e00\u4e2a 4D \u5f20\u91cf\u3002 'avg' \u4ee3\u8868\u5168\u5c40\u5e73\u5747\u6c60\u5316\uff08GlobalAveragePooling2D\uff09\uff0c\u76f8\u5f53\u4e8e\u5728\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u540e\u9762\u518d\u52a0\u4e00\u5c42\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5c42\uff0c\u8f93\u51fa\u662f\u4e00\u4e2a 2D \u5f20\u91cf\u3002 'max' \u4ee3\u8868\u5168\u5c40\u6700\u5927\u6c60\u5316\u3002 classes : \u53ef\u9009\uff0c\u56fe\u7247\u5206\u7c7b\u7684\u7c7b\u522b\u6570\uff0c\u4ec5\u5f53 include_top \u4e3a True \u5e76\u4e14\u4e0d\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u503c\u65f6\u53ef\u7528\u3002","title":"\u53c2\u6570"},{"location":"8.applications/#_5","text":"\u4e00\u4e2a Keras Model \u5bf9\u8c61.","title":"\u8fd4\u56de\u503c"},{"location":"8.applications/#_6","text":"Xception: Deep Learning with Depthwise Separable Convolutions","title":"\u53c2\u8003\u6587\u732e"},{"location":"8.applications/#license","text":"\u9884\u8bad\u7ec3\u6743\u503c\u7531\u6211\u4eec\u81ea\u5df1\u8bad\u7ec3\u800c\u6765\uff0c\u57fa\u4e8e MIT license \u53d1\u5e03\u3002","title":"License"},{"location":"8.applications/#vgg16_1","text":"keras.applications.vgg16.VGG16(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) VGG16 \u6a21\u578b\uff0c\u6743\u503c\u7531 ImageNet \u8bad\u7ec3\u800c\u6765\u3002 \u8be5\u6a21\u578b\u53ef\u540c\u65f6\u6784\u5efa\u4e8e channels_first (\u901a\u9053\uff0c\u9ad8\u5ea6\uff0c\u5bbd\u5ea6) \u548c channels_last \uff08\u9ad8\u5ea6\uff0c\u5bbd\u5ea6\uff0c\u901a\u9053\uff09\u4e24\u79cd\u8f93\u5165\u7ef4\u5ea6\u987a\u5e8f\u3002 \u6a21\u578b\u9ed8\u8ba4\u8f93\u5165\u5c3a\u5bf8\u662f 224x224\u3002","title":"VGG16"},{"location":"8.applications/#_7","text":"include_top : \u662f\u5426\u5305\u62ec\u9876\u5c42\u7684\u5168\u8fde\u63a5\u5c42\u3002 weights : None \u4ee3\u8868\u968f\u673a\u521d\u59cb\u5316\uff0c 'imagenet' \u4ee3\u8868\u52a0\u8f7d\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6743\u503c\u3002 input_tensor : \u53ef\u9009\uff0cKeras tensor \u4f5c\u4e3a\u6a21\u578b\u7684\u8f93\u5165\uff08\u5373 layers.Input() \u8f93\u51fa\u7684 tensor\uff09\u3002 input_shape : \u53ef\u9009\uff0c\u8f93\u5165\u5c3a\u5bf8\u5143\u7ec4\uff0c\u4ec5\u5f53 include_top=False \u65f6\u6709\u6548\uff0c\u5426\u5219\u8f93\u5165\u5f62\u72b6\u5fc5\u987b\u662f (244, 244, 3) \uff08\u5bf9\u4e8e channels_last \u6570\u636e\u683c\u5f0f\uff09\uff0c\u6216\u8005 (3, 244, 244) \uff08\u5bf9\u4e8e channels_first \u6570\u636e\u683c\u5f0f\uff09\u3002\u5b83\u5fc5\u987b\u62e5\u6709 3 \u4e2a\u8f93\u5165\u901a\u9053\uff0c\u4e14\u5bbd\u9ad8\u5fc5\u987b\u4e0d\u5c0f\u4e8e 32\u3002\u4f8b\u5982 (200, 200, 3) \u662f\u4e00\u4e2a\u5408\u6cd5\u7684\u8f93\u5165\u5c3a\u5bf8\u3002 pooling : \u53ef\u9009\uff0c\u5f53 include_top \u4e3a False \u65f6\uff0c\u8be5\u53c2\u6570\u6307\u5b9a\u4e86\u7279\u5f81\u63d0\u53d6\u65f6\u7684\u6c60\u5316\u65b9\u5f0f\u3002 None \u4ee3\u8868\u4e0d\u6c60\u5316\uff0c\u76f4\u63a5\u8f93\u51fa\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u7684\u8f93\u51fa\uff0c\u8be5\u8f93\u51fa\u662f\u4e00\u4e2a\u56db\u7ef4\u5f20\u91cf\u3002 'avg' \u4ee3\u8868\u5168\u5c40\u5e73\u5747\u6c60\u5316\uff08GlobalAveragePooling2D\uff09\uff0c\u76f8\u5f53\u4e8e\u5728\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u540e\u9762\u518d\u52a0\u4e00\u5c42\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5c42\uff0c\u8f93\u51fa\u662f\u4e00\u4e2a\u4e8c\u7ef4\u5f20\u91cf\u3002 'max' \u4ee3\u8868\u5168\u5c40\u6700\u5927\u6c60\u5316 classes : \u53ef\u9009\uff0c\u56fe\u7247\u5206\u7c7b\u7684\u7c7b\u522b\u6570\uff0c\u4ec5\u5f53 include_top \u4e3a True \u5e76\u4e14\u4e0d\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u503c\u65f6\u53ef\u7528\u3002","title":"\u53c2\u6570"},{"location":"8.applications/#_8","text":"\u4e00\u4e2a Keras Model \u5bf9\u8c61\u3002","title":"\u8fd4\u56de\u503c"},{"location":"8.applications/#_9","text":"Very Deep Convolutional Networks for Large-Scale Image Recognition \uff1a\u5982\u679c\u5728\u7814\u7a76\u4e2d\u4f7f\u7528\u4e86VGG\uff0c\u8bf7\u5f15\u7528\u8be5\u8bba\u6587\u3002","title":"\u53c2\u8003\u6587\u732e"},{"location":"8.applications/#license_1","text":"\u9884\u8bad\u7ec3\u6743\u503c\u7531 VGG at Oxford \u53d1\u5e03\u7684\u9884\u8bad\u7ec3\u6743\u503c\u79fb\u690d\u800c\u6765\uff0c\u57fa\u4e8e Creative Commons Attribution License \u3002","title":"License"},{"location":"8.applications/#vgg19_1","text":"keras.applications.vgg19.VGG19(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) VGG19 \u6a21\u578b\uff0c\u6743\u503c\u7531 ImageNet \u8bad\u7ec3\u800c\u6765\u3002 \u8be5\u6a21\u578b\u53ef\u540c\u65f6\u6784\u5efa\u4e8e channels_first (\u901a\u9053\uff0c\u9ad8\u5ea6\uff0c\u5bbd\u5ea6) \u548c channels_last \uff08\u9ad8\u5ea6\uff0c\u5bbd\u5ea6\uff0c\u901a\u9053\uff09\u4e24\u79cd\u8f93\u5165\u7ef4\u5ea6\u987a\u5e8f\u3002 \u6a21\u578b\u9ed8\u8ba4\u8f93\u5165\u5c3a\u5bf8\u662f 224x224\u3002","title":"VGG19"},{"location":"8.applications/#_10","text":"include_top : \u662f\u5426\u5305\u62ec\u9876\u5c42\u7684\u5168\u8fde\u63a5\u5c42\u3002 weights : None \u4ee3\u8868\u968f\u673a\u521d\u59cb\u5316\uff0c 'imagenet' \u4ee3\u8868\u52a0\u8f7d\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6743\u503c\u3002 input_tensor : \u53ef\u9009\uff0cKeras tensor \u4f5c\u4e3a\u6a21\u578b\u7684\u8f93\u5165\uff08\u5373 layers.Input() \u8f93\u51fa\u7684 tensor\uff09\u3002 input_shape : \u53ef\u9009\uff0c\u8f93\u5165\u5c3a\u5bf8\u5143\u7ec4\uff0c\u4ec5\u5f53 include_top=False \u65f6\u6709\u6548\uff0c\u5426\u5219\u8f93\u5165\u5f62\u72b6\u5fc5\u987b\u662f (244, 244, 3) \uff08\u5bf9\u4e8e channels_last \u6570\u636e\u683c\u5f0f\uff09\uff0c\u6216\u8005 (3, 244, 244) \uff08\u5bf9\u4e8e channels_first \u6570\u636e\u683c\u5f0f\uff09\u3002\u5b83\u5fc5\u987b\u62e5\u6709 3 \u4e2a\u8f93\u5165\u901a\u9053\uff0c\u4e14\u5bbd\u9ad8\u5fc5\u987b\u4e0d\u5c0f\u4e8e 32\u3002\u4f8b\u5982 (200, 200, 3) \u662f\u4e00\u4e2a\u5408\u6cd5\u7684\u8f93\u5165\u5c3a\u5bf8\u3002 pooling : \u53ef\u9009\uff0c\u5f53 include_top \u4e3a False \u65f6\uff0c\u8be5\u53c2\u6570\u6307\u5b9a\u4e86\u7279\u5f81\u63d0\u53d6\u65f6\u7684\u6c60\u5316\u65b9\u5f0f\u3002 None \u4ee3\u8868\u4e0d\u6c60\u5316\uff0c\u76f4\u63a5\u8f93\u51fa\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u7684\u8f93\u51fa\uff0c\u8be5\u8f93\u51fa\u662f\u4e00\u4e2a\u56db\u7ef4\u5f20\u91cf\u3002 'avg' \u4ee3\u8868\u5168\u5c40\u5e73\u5747\u6c60\u5316\uff08GlobalAveragePooling2D\uff09\uff0c\u76f8\u5f53\u4e8e\u5728\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u540e\u9762\u518d\u52a0\u4e00\u5c42\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5c42\uff0c\u8f93\u51fa\u662f\u4e00\u4e2a\u4e8c\u7ef4\u5f20\u91cf\u3002 'max' \u4ee3\u8868\u5168\u5c40\u6700\u5927\u6c60\u5316 classes : \u53ef\u9009\uff0c\u56fe\u7247\u5206\u7c7b\u7684\u7c7b\u522b\u6570\uff0c\u4ec5\u5f53 include_top \u4e3a True \u5e76\u4e14\u4e0d\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u503c\u65f6\u53ef\u7528\u3002","title":"\u53c2\u6570"},{"location":"8.applications/#_11","text":"\u4e00\u4e2a Keras Model \u5bf9\u8c61\u3002","title":"\u8fd4\u56de\u503c"},{"location":"8.applications/#_12","text":"Very Deep Convolutional Networks for Large-Scale Image Recognition \uff1a\u5982\u679c\u5728\u7814\u7a76\u4e2d\u4f7f\u7528\u4e86VGG\uff0c\u8bf7\u5f15\u7528\u8be5\u8bba\u6587\u3002","title":"\u53c2\u8003\u6587\u732e"},{"location":"8.applications/#license_2","text":"\u9884\u8bad\u7ec3\u6743\u503c\u7531 VGG at Oxford \u53d1\u5e03\u7684\u9884\u8bad\u7ec3\u6743\u503c\u79fb\u690d\u800c\u6765\uff0c\u57fa\u4e8e Creative Commons Attribution License \u3002","title":"License"},{"location":"8.applications/#resnet","text":"keras.applications.resnet.ResNet50(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) keras.applications.resnet.ResNet101(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) keras.applications.resnet.ResNet152(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) keras.applications.resnet_v2.ResNet50V2(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) keras.applications.resnet_v2.ResNet101V2(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) keras.applications.resnet_v2.ResNet152V2(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) keras.applications.resnext.ResNeXt50(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) keras.applications.resnext.ResNeXt101(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) ResNet, ResNetV2, ResNeXt \u6a21\u578b\uff0c\u6743\u503c\u7531 ImageNet \u8bad\u7ec3\u800c\u6765\u3002 \u8be5\u6a21\u578b\u53ef\u540c\u65f6\u6784\u5efa\u4e8e channels_first (\u901a\u9053\uff0c\u9ad8\u5ea6\uff0c\u5bbd\u5ea6) \u548c channels_last \uff08\u9ad8\u5ea6\uff0c\u5bbd\u5ea6\uff0c\u901a\u9053\uff09\u4e24\u79cd\u8f93\u5165\u7ef4\u5ea6\u987a\u5e8f\u3002 \u6a21\u578b\u9ed8\u8ba4\u8f93\u5165\u5c3a\u5bf8\u662f 224x224\u3002","title":"ResNet"},{"location":"8.applications/#_13","text":"include_top : \u662f\u5426\u5305\u62ec\u9876\u5c42\u7684\u5168\u8fde\u63a5\u5c42\u3002 weights : None \u4ee3\u8868\u968f\u673a\u521d\u59cb\u5316\uff0c 'imagenet' \u4ee3\u8868\u52a0\u8f7d\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6743\u503c\u3002 input_tensor : \u53ef\u9009\uff0cKeras tensor \u4f5c\u4e3a\u6a21\u578b\u7684\u8f93\u5165\uff08\u5373 layers.Input() \u8f93\u51fa\u7684 tensor\uff09\u3002 input_shape : \u53ef\u9009\uff0c\u8f93\u5165\u5c3a\u5bf8\u5143\u7ec4\uff0c\u4ec5\u5f53 include_top=False \u65f6\u6709\u6548\uff0c\u5426\u5219\u8f93\u5165\u5f62\u72b6\u5fc5\u987b\u662f (244, 244, 3) \uff08\u5bf9\u4e8e channels_last \u6570\u636e\u683c\u5f0f\uff09\uff0c\u6216\u8005 (3, 244, 244) \uff08\u5bf9\u4e8e channels_first \u6570\u636e\u683c\u5f0f\uff09\u3002\u5b83\u5fc5\u987b\u62e5\u6709 3 \u4e2a\u8f93\u5165\u901a\u9053\uff0c\u4e14\u5bbd\u9ad8\u5fc5\u987b\u4e0d\u5c0f\u4e8e 32\u3002\u4f8b\u5982 (200, 200, 3) \u662f\u4e00\u4e2a\u5408\u6cd5\u7684\u8f93\u5165\u5c3a\u5bf8\u3002 pooling : \u53ef\u9009\uff0c\u5f53 include_top \u4e3a False \u65f6\uff0c\u8be5\u53c2\u6570\u6307\u5b9a\u4e86\u7279\u5f81\u63d0\u53d6\u65f6\u7684\u6c60\u5316\u65b9\u5f0f\u3002 None \u4ee3\u8868\u4e0d\u6c60\u5316\uff0c\u76f4\u63a5\u8f93\u51fa\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u7684\u8f93\u51fa\uff0c\u8be5\u8f93\u51fa\u662f\u4e00\u4e2a\u56db\u7ef4\u5f20\u91cf\u3002 'avg' \u4ee3\u8868\u5168\u5c40\u5e73\u5747\u6c60\u5316\uff08GlobalAveragePooling2D\uff09\uff0c\u76f8\u5f53\u4e8e\u5728\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u540e\u9762\u518d\u52a0\u4e00\u5c42\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5c42\uff0c\u8f93\u51fa\u662f\u4e00\u4e2a\u4e8c\u7ef4\u5f20\u91cf\u3002 'max' \u4ee3\u8868\u5168\u5c40\u6700\u5927\u6c60\u5316 classes : \u53ef\u9009\uff0c\u56fe\u7247\u5206\u7c7b\u7684\u7c7b\u522b\u6570\uff0c\u4ec5\u5f53 include_top \u4e3a True \u5e76\u4e14\u4e0d\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u503c\u65f6\u53ef\u7528\u3002","title":"\u53c2\u6570"},{"location":"8.applications/#_14","text":"\u4e00\u4e2a Keras Model \u5bf9\u8c61\u3002","title":"\u8fd4\u56de\u503c"},{"location":"8.applications/#_15","text":"ResNet : Deep Residual Learning for Image Recognition ResNetV2 : Identity Mappings in Deep Residual Networks ResNeXt : Aggregated Residual Transformations for Deep Neural Networks","title":"\u53c2\u8003\u6587\u732e"},{"location":"8.applications/#license_3","text":"\u9884\u8bad\u7ec3\u6743\u503c\u7531\u4ee5\u4e0b\u63d0\u4f9b\uff1a ResNet : The original repository of Kaiming He under the MIT license . ResNetV2 : Facebook under the BSD license . ResNeXt : Facebook AI Research under the BSD license .","title":"License"},{"location":"8.applications/#inceptionv3_2","text":"keras.applications.inception_v3.InceptionV3(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) Inception V3 \u6a21\u578b\uff0c\u6743\u503c\u7531 ImageNet \u8bad\u7ec3\u800c\u6765\u3002 \u8be5\u6a21\u578b\u53ef\u540c\u65f6\u6784\u5efa\u4e8e channels_first (\u901a\u9053\uff0c\u9ad8\u5ea6\uff0c\u5bbd\u5ea6) \u548c channels_last \uff08\u9ad8\u5ea6\uff0c\u5bbd\u5ea6\uff0c\u901a\u9053\uff09\u4e24\u79cd\u8f93\u5165\u7ef4\u5ea6\u987a\u5e8f\u3002 \u6a21\u578b\u9ed8\u8ba4\u8f93\u5165\u5c3a\u5bf8\u662f 299x299\u3002","title":"InceptionV3"},{"location":"8.applications/#_16","text":"include_top : \u662f\u5426\u5305\u62ec\u9876\u5c42\u7684\u5168\u8fde\u63a5\u5c42\u3002 weights : None \u4ee3\u8868\u968f\u673a\u521d\u59cb\u5316\uff0c 'imagenet' \u4ee3\u8868\u52a0\u8f7d\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6743\u503c\u3002 input_tensor : \u53ef\u9009\uff0cKeras tensor \u4f5c\u4e3a\u6a21\u578b\u7684\u8f93\u5165\uff08\u5373 layers.Input() \u8f93\u51fa\u7684 tensor\uff09\u3002 input_shape : \u53ef\u9009\uff0c\u8f93\u5165\u5c3a\u5bf8\u5143\u7ec4\uff0c\u4ec5\u5f53 include_top=False \u65f6\u6709\u6548\uff0c\u5426\u5219\u8f93\u5165\u5f62\u72b6\u5fc5\u987b\u662f (299, 299, 3) \uff08\u5bf9\u4e8e channels_last \u6570\u636e\u683c\u5f0f\uff09\uff0c\u6216\u8005 (3, 299, 299) \uff08\u5bf9\u4e8e channels_first \u6570\u636e\u683c\u5f0f\uff09\u3002\u5b83\u5fc5\u987b\u62e5\u6709 3 \u4e2a\u8f93\u5165\u901a\u9053\uff0c\u4e14\u5bbd\u9ad8\u5fc5\u987b\u4e0d\u5c0f\u4e8e 139\u3002\u4f8b\u5982 (150, 150, 3) \u662f\u4e00\u4e2a\u5408\u6cd5\u7684\u8f93\u5165\u5c3a\u5bf8\u3002 pooling : \u53ef\u9009\uff0c\u5f53 include_top \u4e3a False \u65f6\uff0c\u8be5\u53c2\u6570\u6307\u5b9a\u4e86\u7279\u5f81\u63d0\u53d6\u65f6\u7684\u6c60\u5316\u65b9\u5f0f\u3002 None \u4ee3\u8868\u4e0d\u6c60\u5316\uff0c\u76f4\u63a5\u8f93\u51fa\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u7684\u8f93\u51fa\uff0c\u8be5\u8f93\u51fa\u662f\u4e00\u4e2a\u56db\u7ef4\u5f20\u91cf\u3002 'avg' \u4ee3\u8868\u5168\u5c40\u5e73\u5747\u6c60\u5316\uff08GlobalAveragePooling2D\uff09\uff0c\u76f8\u5f53\u4e8e\u5728\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u540e\u9762\u518d\u52a0\u4e00\u5c42\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5c42\uff0c\u8f93\u51fa\u662f\u4e00\u4e2a\u4e8c\u7ef4\u5f20\u91cf\u3002 'max' \u4ee3\u8868\u5168\u5c40\u6700\u5927\u6c60\u5316 classes : \u53ef\u9009\uff0c\u56fe\u7247\u5206\u7c7b\u7684\u7c7b\u522b\u6570\uff0c\u4ec5\u5f53 include_top \u4e3a True \u5e76\u4e14\u4e0d\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u503c\u65f6\u53ef\u7528\u3002","title":"\u53c2\u6570"},{"location":"8.applications/#_17","text":"\u4e00\u4e2a Keras Model \u5bf9\u8c61\u3002","title":"\u8fd4\u56de\u503c"},{"location":"8.applications/#_18","text":"Rethinking the Inception Architecture for Computer Vision","title":"\u53c2\u8003\u6587\u732e"},{"location":"8.applications/#license_4","text":"\u9884\u8bad\u7ec3\u6743\u503c\u57fa\u4e8e Apache License \u3002","title":"License"},{"location":"8.applications/#inceptionresnetv2","text":"keras.applications.inception_resnet_v2.InceptionResNetV2(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) Inception-ResNet V2 \u6a21\u578b\uff0c\u6743\u503c\u7531 ImageNet \u8bad\u7ec3\u800c\u6765\u3002 \u8be5\u6a21\u578b\u53ef\u540c\u65f6\u6784\u5efa\u4e8e channels_first (\u901a\u9053\uff0c\u9ad8\u5ea6\uff0c\u5bbd\u5ea6) \u548c channels_last \uff08\u9ad8\u5ea6\uff0c\u5bbd\u5ea6\uff0c\u901a\u9053\uff09\u4e24\u79cd\u8f93\u5165\u7ef4\u5ea6\u987a\u5e8f\u3002 \u6a21\u578b\u9ed8\u8ba4\u8f93\u5165\u5c3a\u5bf8\u662f 299x299\u3002","title":"InceptionResNetV2"},{"location":"8.applications/#_19","text":"include_top : \u662f\u5426\u5305\u62ec\u9876\u5c42\u7684\u5168\u8fde\u63a5\u5c42\u3002 weights : None \u4ee3\u8868\u968f\u673a\u521d\u59cb\u5316\uff0c 'imagenet' \u4ee3\u8868\u52a0\u8f7d\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6743\u503c\u3002 input_tensor : \u53ef\u9009\uff0cKeras tensor \u4f5c\u4e3a\u6a21\u578b\u7684\u8f93\u5165\uff08\u5373 layers.Input() \u8f93\u51fa\u7684 tensor\uff09\u3002 input_shape : \u53ef\u9009\uff0c\u8f93\u5165\u5c3a\u5bf8\u5143\u7ec4\uff0c\u4ec5\u5f53 include_top=False \u65f6\u6709\u6548\uff0c\u5426\u5219\u8f93\u5165\u5f62\u72b6\u5fc5\u987b\u662f (299, 299, 3) \uff08\u5bf9\u4e8e channels_last \u6570\u636e\u683c\u5f0f\uff09\uff0c\u6216\u8005 (3, 299, 299) \uff08\u5bf9\u4e8e channels_first \u6570\u636e\u683c\u5f0f\uff09\u3002\u5b83\u5fc5\u987b\u62e5\u6709 3 \u4e2a\u8f93\u5165\u901a\u9053\uff0c\u4e14\u5bbd\u9ad8\u5fc5\u987b\u4e0d\u5c0f\u4e8e 139\u3002\u4f8b\u5982 (150, 150, 3) \u662f\u4e00\u4e2a\u5408\u6cd5\u7684\u8f93\u5165\u5c3a\u5bf8\u3002 pooling : \u53ef\u9009\uff0c\u5f53 include_top \u4e3a False \u65f6\uff0c\u8be5\u53c2\u6570\u6307\u5b9a\u4e86\u7279\u5f81\u63d0\u53d6\u65f6\u7684\u6c60\u5316\u65b9\u5f0f\u3002 None \u4ee3\u8868\u4e0d\u6c60\u5316\uff0c\u76f4\u63a5\u8f93\u51fa\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u7684\u8f93\u51fa\uff0c\u8be5\u8f93\u51fa\u662f\u4e00\u4e2a\u56db\u7ef4\u5f20\u91cf\u3002 'avg' \u4ee3\u8868\u5168\u5c40\u5e73\u5747\u6c60\u5316\uff08GlobalAveragePooling2D\uff09\uff0c\u76f8\u5f53\u4e8e\u5728\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u540e\u9762\u518d\u52a0\u4e00\u5c42\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5c42\uff0c\u8f93\u51fa\u662f\u4e00\u4e2a\u4e8c\u7ef4\u5f20\u91cf\u3002 'max' \u4ee3\u8868\u5168\u5c40\u6700\u5927\u6c60\u5316 classes : \u53ef\u9009\uff0c\u56fe\u7247\u5206\u7c7b\u7684\u7c7b\u522b\u6570\uff0c\u4ec5\u5f53 include_top \u4e3a True \u5e76\u4e14\u4e0d\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u503c\u65f6\u53ef\u7528\u3002","title":"\u53c2\u6570"},{"location":"8.applications/#_20","text":"\u4e00\u4e2a Keras Model \u5bf9\u8c61\u3002","title":"\u8fd4\u56de\u503c"},{"location":"8.applications/#_21","text":"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning","title":"\u53c2\u8003\u6587\u732e"},{"location":"8.applications/#license_5","text":"\u9884\u8bad\u7ec3\u6743\u503c\u57fa\u4e8e Apache License \u3002","title":"License"},{"location":"8.applications/#mobilenet","text":"keras.applications.mobilenet.MobileNet(input_shape=None, alpha=1.0, depth_multiplier=1, dropout=1e-3, include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000) \u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684 MobileNet \u6a21\u578b\u3002 \u6ce8\u610f\uff0c\u8be5\u6a21\u578b\u76ee\u524d\u53ea\u652f\u6301 channels_last \u7684\u7ef4\u5ea6\u987a\u5e8f\uff08\u9ad8\u5ea6\u3001\u5bbd\u5ea6\u3001\u901a\u9053\uff09\u3002 \u6a21\u578b\u9ed8\u8ba4\u8f93\u5165\u5c3a\u5bf8\u662f 224x224\u3002","title":"MobileNet"},{"location":"8.applications/#_22","text":"input_shape : \u53ef\u9009\uff0c\u8f93\u5165\u5c3a\u5bf8\u5143\u7ec4\uff0c\u4ec5\u5f53 include_top=False \u65f6\u6709\u6548\uff0c\u5426\u5219\u8f93\u5165\u5f62\u72b6\u5fc5\u987b\u662f (224, 224, 3) \uff08 channels_last \u683c\u5f0f\uff09\u6216 (3, 224, 224) \uff08 channels_first \u683c\u5f0f\uff09\u3002\u5b83\u5fc5\u987b\u4e3a 3 \u4e2a\u8f93\u5165\u901a\u9053\uff0c\u4e14\u5bbd\u9ad8\u5fc5\u987b\u4e0d\u5c0f\u4e8e 32\uff0c\u6bd4\u5982 (200, 200, 3) \u662f\u4e00\u4e2a\u5408\u6cd5\u7684\u8f93\u5165\u5c3a\u5bf8\u3002 alpha : \u63a7\u5236\u7f51\u7edc\u7684\u5bbd\u5ea6\uff1a \u5982\u679c alpha < 1.0\uff0c\u5219\u540c\u6bd4\u4f8b\u51cf\u5c11\u6bcf\u5c42\u7684\u6ee4\u6ce2\u5668\u4e2a\u6570\u3002 \u5982\u679c alpha > 1.0\uff0c\u5219\u540c\u6bd4\u4f8b\u589e\u52a0\u6bcf\u5c42\u7684\u6ee4\u6ce2\u5668\u4e2a\u6570\u3002 \u5982\u679c alpha = 1\uff0c\u4f7f\u7528\u8bba\u6587\u9ed8\u8ba4\u7684\u6ee4\u6ce2\u5668\u4e2a\u6570 depth_multiplier : depthwise\u5377\u79ef\u7684\u6df1\u5ea6\u4e58\u5b50\uff0c\u4e5f\u79f0\u4e3a\uff08\u5206\u8fa8\u7387\u4e58\u5b50\uff09 dropout : dropout \u6982\u7387 include_top : \u662f\u5426\u5305\u62ec\u9876\u5c42\u7684\u5168\u8fde\u63a5\u5c42\u3002 weights : None \u4ee3\u8868\u968f\u673a\u521d\u59cb\u5316\uff0c 'imagenet' \u4ee3\u8868\u52a0\u8f7d\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6743\u503c\u3002 input_tensor : \u53ef\u9009\uff0cKeras tensor \u4f5c\u4e3a\u6a21\u578b\u7684\u8f93\u5165\uff08\u6bd4\u5982 layers.Input() \u8f93\u51fa\u7684 tensor\uff09\u3002 pooling : \u53ef\u9009\uff0c\u5f53 include_top \u4e3a False \u65f6\uff0c\u8be5\u53c2\u6570\u6307\u5b9a\u4e86\u7279\u5f81\u63d0\u53d6\u65f6\u7684\u6c60\u5316\u65b9\u5f0f\u3002 None \u4ee3\u8868\u4e0d\u6c60\u5316\uff0c\u76f4\u63a5\u8f93\u51fa\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u7684\u8f93\u51fa\uff0c\u8be5\u8f93\u51fa\u662f\u4e00\u4e2a\u56db\u7ef4\u5f20\u91cf\u3002 'avg' \u4ee3\u8868\u5168\u5c40\u5e73\u5747\u6c60\u5316\uff08GlobalAveragePooling2D\uff09\uff0c\u76f8\u5f53\u4e8e\u5728\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u540e\u9762\u518d\u52a0\u4e00\u5c42\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5c42\uff0c\u8f93\u51fa\u662f\u4e00\u4e2a\u4e8c\u7ef4\u5f20\u91cf\u3002 'max' \u4ee3\u8868\u5168\u5c40\u6700\u5927\u6c60\u5316 classes : \u53ef\u9009\uff0c\u56fe\u7247\u5206\u7c7b\u7684\u7c7b\u522b\u6570\uff0c\u4ec5\u5f53 include_top \u4e3a True \u5e76\u4e14\u4e0d\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u503c\u65f6\u53ef\u7528\u3002","title":"\u53c2\u6570"},{"location":"8.applications/#_23","text":"\u4e00\u4e2a Keras Model \u5bf9\u8c61\u3002","title":"\u8fd4\u56de"},{"location":"8.applications/#_24","text":"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications","title":"\u53c2\u8003\u6587\u732e"},{"location":"8.applications/#license_6","text":"\u9884\u8bad\u7ec3\u6743\u503c\u57fa\u4e8e Apache License \u3002","title":"License"},{"location":"8.applications/#densenet","text":"keras.applications.densenet.DenseNet121(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) keras.applications.densenet.DenseNet169(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) keras.applications.densenet.DenseNet201(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) \u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684 DenseNet \u6a21\u578b\u3002 \u8be5\u6a21\u578b\u53ef\u540c\u65f6\u6784\u5efa\u4e8e channels_first (\u901a\u9053\uff0c\u9ad8\u5ea6\uff0c\u5bbd\u5ea6) \u548c channels_last \uff08\u9ad8\u5ea6\uff0c\u5bbd\u5ea6\uff0c\u901a\u9053\uff09\u4e24\u79cd\u8f93\u5165\u7ef4\u5ea6\u987a\u5e8f\u3002 \u6a21\u578b\u9ed8\u8ba4\u8f93\u5165\u5c3a\u5bf8\u662f 224x224\u3002","title":"DenseNet"},{"location":"8.applications/#_25","text":"blocks : \u56db\u4e2a Dense Layers \u7684 block \u6570\u91cf\u3002 include_top : \u662f\u5426\u5305\u62ec\u9876\u5c42\u7684\u5168\u8fde\u63a5\u5c42\u3002 weights : None \u4ee3\u8868\u968f\u673a\u521d\u59cb\u5316\uff0c 'imagenet' \u4ee3\u8868\u52a0\u8f7d\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6743\u503c\u3002 input_tensor : \u53ef\u9009\uff0cKeras tensor \u4f5c\u4e3a\u6a21\u578b\u7684\u8f93\u5165\uff08\u6bd4\u5982 layers.Input() \u8f93\u51fa\u7684 tensor\uff09\u3002 input_shape: \u53ef\u9009\uff0c\u8f93\u5165\u5c3a\u5bf8\u5143\u7ec4\uff0c\u4ec5\u5f53 include_top=False \u65f6\u6709\u6548\uff08\u4e0d\u7136\u8f93\u5165\u5f62\u72b6\u5fc5\u987b\u662f (224, 224, 3) \uff08 channels_last \u683c\u5f0f\uff09\u6216 (3, 224, 224) \uff08 channels_first \u683c\u5f0f\uff09\uff0c\u56e0\u4e3a\u9884\u8bad\u7ec3\u6a21\u578b\u662f\u4ee5\u8fd9\u4e2a\u5927\u5c0f\u8bad\u7ec3\u7684\uff09\u3002\u5b83\u5fc5\u987b\u4e3a 3 \u4e2a\u8f93\u5165\u901a\u9053\uff0c\u4e14\u5bbd\u9ad8\u5fc5\u987b\u4e0d\u5c0f\u4e8e 32\uff0c\u6bd4\u5982 (200, 200, 3) \u662f\u4e00\u4e2a\u5408\u6cd5\u7684\u8f93\u5165\u5c3a\u5bf8\u3002 pooling : \u53ef\u9009\uff0c\u5f53 include_top \u4e3a False \u65f6\uff0c\u8be5\u53c2\u6570\u6307\u5b9a\u4e86\u7279\u5f81\u63d0\u53d6\u65f6\u7684\u6c60\u5316\u65b9\u5f0f\u3002 None \u4ee3\u8868\u4e0d\u6c60\u5316\uff0c\u76f4\u63a5\u8f93\u51fa\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u7684\u8f93\u51fa\uff0c\u8be5\u8f93\u51fa\u662f\u4e00\u4e2a\u56db\u7ef4\u5f20\u91cf\u3002 'avg' \u4ee3\u8868\u5168\u5c40\u5e73\u5747\u6c60\u5316\uff08GlobalAveragePooling2D\uff09\uff0c\u76f8\u5f53\u4e8e\u5728\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u540e\u9762\u518d\u52a0\u4e00\u5c42\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5c42\uff0c\u8f93\u51fa\u662f\u4e00\u4e2a\u4e8c\u7ef4\u5f20\u91cf\u3002 'max' \u4ee3\u8868\u5168\u5c40\u6700\u5927\u6c60\u5316. classes : \u53ef\u9009\uff0c\u56fe\u7247\u5206\u7c7b\u7684\u7c7b\u522b\u6570\uff0c\u4ec5\u5f53 include_top \u4e3a True \u5e76\u4e14\u4e0d\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u503c\u65f6\u53ef\u7528\u3002","title":"\u53c2\u6570"},{"location":"8.applications/#_26","text":"\u4e00\u4e2a Keras Model \u5bf9\u8c61\u3002","title":"\u8fd4\u56de"},{"location":"8.applications/#_27","text":"Densely Connected Convolutional Networks (CVPR 2017 Best Paper Award)","title":"\u53c2\u8003\u6587\u732e"},{"location":"8.applications/#licence","text":"\u9884\u8bad\u7ec3\u6743\u503c\u57fa\u4e8e BSD 3-clause License \u3002","title":"Licence"},{"location":"8.applications/#nasnet","text":"keras.applications.nasnet.NASNetLarge(input_shape=None, include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000) keras.applications.nasnet.NASNetMobile(input_shape=None, include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000) \u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u795e\u7ecf\u7ed3\u6784\u641c\u7d22\u7f51\u7edc\u6a21\u578b\uff08NASNet\uff09\u3002 NASNetLarge \u6a21\u578b\u9ed8\u8ba4\u7684\u8f93\u5165\u5c3a\u5bf8\u662f 331x331\uff0cNASNetMobile \u6a21\u578b\u9ed8\u8ba4\u7684\u8f93\u5165\u5c3a\u5bf8\u662f 224x224\u3002","title":"NASNet"},{"location":"8.applications/#_28","text":"input_shape : \u53ef\u9009\uff0c\u8f93\u5165\u5c3a\u5bf8\u5143\u7ec4\uff0c\u4ec5\u5f53 include_top=False \u65f6\u6709\u6548\uff0c\u5426\u5219\u5bf9\u4e8e NASNetMobile \u6a21\u578b\u6765\u8bf4\uff0c\u8f93\u5165\u5f62\u72b6\u5fc5\u987b\u662f (224, 224, 3) \uff08 channels_last \u683c\u5f0f\uff09\u6216 (3, 224, 224) \uff08 channels_first \u683c\u5f0f\uff09\uff0c\u5bf9\u4e8e NASNetLarge \u6765\u8bf4\uff0c\u8f93\u5165\u5f62\u72b6\u5fc5\u987b\u662f (331, 331, 3) \uff08 channels_last \u683c\u5f0f\uff09\u6216 (3, 331, 331) \uff08 channels_first \u683c\u5f0f\uff09\u3002\u5b83\u5fc5\u987b\u4e3a 3 \u4e2a\u8f93\u5165\u901a\u9053\uff0c\u4e14\u5bbd\u9ad8\u5fc5\u987b\u4e0d\u5c0f\u4e8e 32\uff0c\u6bd4\u5982 (200, 200, 3) \u662f\u4e00\u4e2a\u5408\u6cd5\u7684\u8f93\u5165\u5c3a\u5bf8\u3002 include_top : \u662f\u5426\u5305\u62ec\u9876\u5c42\u7684\u5168\u8fde\u63a5\u5c42\u3002 weights : None \u4ee3\u8868\u968f\u673a\u521d\u59cb\u5316\uff0c 'imagenet' \u4ee3\u8868\u52a0\u8f7d\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6743\u503c\u3002 input_tensor : \u53ef\u9009\uff0cKeras tensor \u4f5c\u4e3a\u6a21\u578b\u7684\u8f93\u5165\uff08\u6bd4\u5982 layers.Input() \u8f93\u51fa\u7684 tensor\uff09\u3002 pooling : \u53ef\u9009\uff0c\u5f53 include_top \u4e3a False \u65f6\uff0c\u8be5\u53c2\u6570\u6307\u5b9a\u4e86\u7279\u5f81\u63d0\u53d6\u65f6\u7684\u6c60\u5316\u65b9\u5f0f\u3002 None \u4ee3\u8868\u4e0d\u6c60\u5316\uff0c\u76f4\u63a5\u8f93\u51fa\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u7684\u8f93\u51fa\uff0c\u8be5\u8f93\u51fa\u662f\u4e00\u4e2a\u56db\u7ef4\u5f20\u91cf\u3002 'avg' \u4ee3\u8868\u5168\u5c40\u5e73\u5747\u6c60\u5316\uff08GlobalAveragePooling2D\uff09\uff0c\u76f8\u5f53\u4e8e\u5728\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u540e\u9762\u518d\u52a0\u4e00\u5c42\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5c42\uff0c\u8f93\u51fa\u662f\u4e00\u4e2a\u4e8c\u7ef4\u5f20\u91cf\u3002 'max' \u4ee3\u8868\u5168\u5c40\u6700\u5927\u6c60\u5316 classes : \u53ef\u9009\uff0c\u56fe\u7247\u5206\u7c7b\u7684\u7c7b\u522b\u6570\uff0c\u4ec5\u5f53 include_top \u4e3a True \u5e76\u4e14\u4e0d\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u503c\u65f6\u53ef\u7528\u3002","title":"\u53c2\u6570"},{"location":"8.applications/#_29","text":"\u4e00\u4e2a Keras Model \u5b9e\u4f8b\u3002","title":"\u8fd4\u56de"},{"location":"8.applications/#_30","text":"Learning Transferable Architectures for Scalable Image Recognition","title":"\u53c2\u8003\u6587\u732e"},{"location":"8.applications/#license_7","text":"\u9884\u8bad\u7ec3\u6743\u503c\u57fa\u4e8e Apache License \u3002","title":"License"},{"location":"8.applications/#mobilenetv2","text":"keras.applications.mobilenet_v2.MobileNetV2(input_shape=None, alpha=1.0, depth_multiplier=1, include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000) \u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684 MobileNetV2 \u6a21\u578b\u3002 \u8bf7\u6ce8\u610f\uff0c\u8be5\u6a21\u578b\u4ec5\u652f\u6301 'channels_last' \u6570\u636e\u683c\u5f0f\uff08\u9ad8\u5ea6\uff0c\u5bbd\u5ea6\uff0c\u901a\u9053)\u3002 \u6a21\u578b\u9ed8\u8ba4\u8f93\u51fa\u5c3a\u5bf8\u4e3a 224x224\u3002","title":"MobileNetV2"},{"location":"8.applications/#_31","text":"input_shape : optional shape tuple, to be specified if you would like to use a model with an input img resolution that is not (224, 224, 3). It should have exactly 3 inputs channels (224, 224, 3). You can also omit this option if you would like to infer input_shape from an input_tensor. If you choose to include both input_tensor and input_shape then input_shape will be used if they match, if the shapes do not match then we will throw an error. E.g. (160, 160, 3) would be one valid value. alpha : \u63a7\u5236\u7f51\u7edc\u7684\u5bbd\u5ea6\u3002\u8fd9\u5728 MobileNetV2 \u8bba\u6587\u4e2d\u88ab\u79f0\u4f5c\u5bbd\u5ea6\u4e58\u5b50\u3002 \u5982\u679c alpha < 1.0\uff0c\u5219\u540c\u6bd4\u4f8b\u51cf\u5c11\u6bcf\u5c42\u7684\u6ee4\u6ce2\u5668\u4e2a\u6570\u3002 \u5982\u679c alpha > 1.0\uff0c\u5219\u540c\u6bd4\u4f8b\u589e\u52a0\u6bcf\u5c42\u7684\u6ee4\u6ce2\u5668\u4e2a\u6570\u3002 \u5982\u679c alpha = 1\uff0c\u4f7f\u7528\u8bba\u6587\u9ed8\u8ba4\u7684\u6ee4\u6ce2\u5668\u4e2a\u6570\u3002 depth_multiplier : depthwise \u5377\u79ef\u7684\u6df1\u5ea6\u4e58\u5b50\uff0c\u4e5f\u79f0\u4e3a\uff08\u5206\u8fa8\u7387\u4e58\u5b50\uff09 include_top : \u662f\u5426\u5305\u62ec\u9876\u5c42\u7684\u5168\u8fde\u63a5\u5c42\u3002 weights : None \u4ee3\u8868\u968f\u673a\u521d\u59cb\u5316\uff0c 'imagenet' \u4ee3\u8868\u52a0\u8f7d\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6743\u503c\u3002 input_tensor : \u53ef\u9009\uff0cKeras tensor \u4f5c\u4e3a\u6a21\u578b\u7684\u8f93\u5165\uff08\u5373 layers.Input() \u8f93\u51fa\u7684 tensor\uff09\u3002 pooling : \u53ef\u9009\uff0c\u5f53 include_top \u4e3a False \u65f6\uff0c\u8be5\u53c2\u6570\u6307\u5b9a\u4e86\u7279\u5f81\u63d0\u53d6\u65f6\u7684\u6c60\u5316\u65b9\u5f0f\u3002 None \u4ee3\u8868\u4e0d\u6c60\u5316\uff0c\u76f4\u63a5\u8f93\u51fa\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u7684\u8f93\u51fa\uff0c\u8be5\u8f93\u51fa\u662f\u4e00\u4e2a\u56db\u7ef4\u5f20\u91cf\u3002 'avg' \u4ee3\u8868\u5168\u5c40\u5e73\u5747\u6c60\u5316\uff08GlobalAveragePooling2D\uff09\uff0c\u76f8\u5f53\u4e8e\u5728\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u540e\u9762\u518d\u52a0\u4e00\u5c42\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5c42\uff0c\u8f93\u51fa\u662f\u4e00\u4e2a\u4e8c\u7ef4\u5f20\u91cf\u3002 'max' \u4ee3\u8868\u5168\u5c40\u6700\u5927\u6c60\u5316 classes : \u53ef\u9009\uff0c\u56fe\u7247\u5206\u7c7b\u7684\u7c7b\u522b\u6570\uff0c\u4ec5\u5f53 include_top \u4e3a True \u5e76\u4e14\u4e0d\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u503c\u65f6\u53ef\u7528\u3002","title":"\u53c2\u6570"},{"location":"8.applications/#_32","text":"\u4e00\u4e2a Keras model \u5b9e\u4f8b\u3002","title":"\u8fd4\u56de"},{"location":"8.applications/#_33","text":"ValueError : \u5982\u679c weights \u53c2\u6570\u975e\u6cd5\uff0c\u6216\u975e\u6cd5\u7684\u8f93\u5165\u5c3a\u5bf8\uff0c\u6216\u8005\u5f53 weights='imagenet' \u65f6\uff0c\u975e\u6cd5\u7684 depth_multiplier, alpha, rows\u3002","title":"\u5f02\u5e38"},{"location":"8.applications/#_34","text":"MobileNetV2: Inverted Residuals and Linear Bottlenecks","title":"\u53c2\u8003\u6587\u732e"},{"location":"8.applications/#license_8","text":"\u9884\u8bad\u7ec3\u6743\u503c\u57fa\u4e8e Apache License .","title":"License"},{"location":"9.backend/","text":"Keras \u540e\u7aef \u4ec0\u4e48\u662f \u300c\u540e\u7aef\u300d\uff1f Keras \u662f\u4e00\u4e2a\u6a21\u578b\u7ea7\u5e93\uff0c\u4e3a\u5f00\u53d1\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u5c42\u6b21\u7684\u6784\u5efa\u6a21\u5757\u3002\u5b83\u4e0d\u5904\u7406\u8bf8\u5982\u5f20\u91cf\u4e58\u79ef\u548c\u5377\u79ef\u7b49\u4f4e\u7ea7\u64cd\u4f5c\u3002\u76f8\u53cd\uff0c\u5b83\u4f9d\u8d56\u4e8e\u4e00\u4e2a\u4e13\u95e8\u7684\u3001\u4f18\u5316\u7684\u5f20\u91cf\u64cd\u4f5c\u5e93\u6765\u5b8c\u6210\u8fd9\u4e2a\u64cd\u4f5c\uff0c\u5b83\u53ef\u4ee5\u4f5c\u4e3a Keras \u7684\u300c\u540e\u7aef\u5f15\u64ce\u300d\u3002\u76f8\u6bd4\u5355\u72ec\u5730\u9009\u62e9\u4e00\u4e2a\u5f20\u91cf\u5e93\uff0c\u800c\u5c06 Keras \u7684\u5b9e\u73b0\u4e0e\u8be5\u5e93\u76f8\u5173\u8054\uff0cKeras \u4ee5\u6a21\u5757\u65b9\u5f0f\u5904\u7406\u8fd9\u4e2a\u95ee\u9898\uff0c\u5e76\u4e14\u53ef\u4ee5\u5c06\u51e0\u4e2a\u4e0d\u540c\u7684\u540e\u7aef\u5f15\u64ce\u65e0\u7f1d\u5d4c\u5165\u5230 Keras \u4e2d\u3002 \u76ee\u524d\uff0cKeras \u6709\u4e09\u4e2a\u540e\u7aef\u5b9e\u73b0\u53ef\u7528: TensorFlow \u540e\u7aef\uff0c Theano \u540e\u7aef\uff0c CNTK \u540e\u7aef\u3002 TensorFlow \u662f\u7531 Google \u5f00\u53d1\u7684\u4e00\u4e2a\u5f00\u6e90\u7b26\u53f7\u7ea7\u5f20\u91cf\u64cd\u4f5c\u6846\u67b6\u3002 Theano \u662f\u7531\u8499\u7279\u5229\u5c14\u5927\u5b66\u7684 LISA Lab \u5f00\u53d1\u7684\u4e00\u4e2a\u5f00\u6e90\u7b26\u53f7\u7ea7\u5f20\u91cf\u64cd\u4f5c\u6846\u67b6\u3002 CNTK \u662f\u7531\u5fae\u8f6f\u5f00\u53d1\u7684\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u5f00\u6e90\u5de5\u5177\u5305\u3002 \u5c06\u6765\uff0c\u6211\u4eec\u53ef\u80fd\u4f1a\u6dfb\u52a0\u66f4\u591a\u540e\u7aef\u9009\u9879\u3002 \u4ece\u4e00\u4e2a\u540e\u7aef\u5207\u6362\u5230\u53e6\u4e00\u4e2a\u540e\u7aef \u5982\u679c\u60a8\u81f3\u5c11\u8fd0\u884c\u8fc7\u4e00\u6b21 Keras\uff0c\u60a8\u5c06\u5728\u4ee5\u4e0b\u4f4d\u7f6e\u627e\u5230 Keras \u914d\u7f6e\u6587\u4ef6\uff1a $HOME/.keras/keras.json \u5982\u679c\u5b83\u4e0d\u5728\u90a3\u91cc\uff0c\u4f60\u53ef\u4ee5\u521b\u5efa\u5b83\u3002 Windows\u7528\u6237\u6ce8\u610f\u4e8b\u9879\uff1a \u8bf7\u5c06 $HOME \u4fee\u6539\u4e3a %USERPROFILE% \u3002 \u9ed8\u8ba4\u7684\u914d\u7f6e\u6587\u4ef6\u5982\u4e0b\u6240\u793a\uff1a { \"image_data_format\": \"channels_last\", \"epsilon\": 1e-07, \"floatx\": \"float32\", \"backend\": \"tensorflow\" } \u53ea\u9700\u5c06\u5b57\u6bb5 backend \u66f4\u6539\u4e3a theano \uff0c tensorflow \u6216 cntk \uff0cKeras \u5c06\u5728\u4e0b\u6b21\u8fd0\u884c Keras \u4ee3\u7801\u65f6\u4f7f\u7528\u65b0\u7684\u914d\u7f6e\u3002 \u4f60\u4e5f\u53ef\u4ee5\u5b9a\u4e49\u73af\u5883\u53d8\u91cf KERAS_BACKEND \uff0c\u8fd9\u4f1a\u8986\u76d6\u914d\u7f6e\u6587\u4ef6\u4e2d\u5b9a\u4e49\u7684\u5185\u5bb9\uff1a KERAS_BACKEND=tensorflow python -c \"from keras import backend\" Using TensorFlow backend. \u5728 Keras \u4e2d\uff0c\u53ef\u4ee5\u52a0\u8f7d\u6bd4 \"tensorflow\" , \"theano\" \u548c \"cntk\" \u66f4\u591a\u7684\u540e\u7aef\u3002 Keras \u4e5f\u53ef\u4ee5\u4f7f\u7528\u5916\u90e8\u540e\u7aef\uff0c\u8fd9\u53ef\u4ee5\u901a\u8fc7\u66f4\u6539 keras.json \u914d\u7f6e\u6587\u4ef6\u548c \"backend\" \u8bbe\u7f6e\u6765\u6267\u884c\u3002 \u5047\u8bbe\u60a8\u6709\u4e00\u4e2a\u540d\u4e3a my_module \u7684 Python \u6a21\u5757\uff0c\u60a8\u5e0c\u671b\u5c06\u5176\u7528\u4f5c\u5916\u90e8\u540e\u7aef\u3002 keras.json \u914d\u7f6e\u6587\u4ef6\u5c06\u66f4\u6539\u5982\u4e0b\uff1a { \"image_data_format\": \"channels_last\", \"epsilon\": 1e-07, \"floatx\": \"float32\", \"backend\": \"my_package.my_module\" } \u5fc5\u987b\u9a8c\u8bc1\u5916\u90e8\u540e\u7aef\u624d\u80fd\u4f7f\u7528\uff0c\u6709\u6548\u7684\u540e\u7aef\u5fc5\u987b\u5177\u6709\u4ee5\u4e0b\u51fd\u6570\uff1a placeholder , variable and function . \u5982\u679c\u7531\u4e8e\u7f3a\u5c11\u5fc5\u9700\u7684\u6761\u76ee\u800c\u5bfc\u81f4\u5916\u90e8\u540e\u7aef\u65e0\u6548\uff0c\u5219\u4f1a\u8bb0\u5f55\u9519\u8bef\uff0c\u901a\u77e5\u7f3a\u5c11\u54ea\u4e9b\u6761\u76ee\u3002 keras.json \u8be6\u7ec6\u914d\u7f6e The keras.json \u914d\u7f6e\u6587\u4ef6\u5305\u542b\u4ee5\u4e0b\u8bbe\u7f6e\uff1a { \"image_data_format\": \"channels_last\", \"epsilon\": 1e-07, \"floatx\": \"float32\", \"backend\": \"tensorflow\" } \u60a8\u53ef\u4ee5\u901a\u8fc7\u7f16\u8f91 $ HOME/.keras/keras.json \u6765\u66f4\u6539\u8fd9\u4e9b\u8bbe\u7f6e\u3002 image_data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216\u8005 \"channels_first\" \u3002\u5b83\u6307\u5b9a\u4e86 Keras \u5c06\u9075\u5faa\u7684\u6570\u636e\u683c\u5f0f\u7ea6\u5b9a\u3002( keras.backend.image_data_format() \u8fd4\u56de\u5b83\u3002) \u5bf9\u4e8e 2D \u6570\u636e (\u4f8b\u5982\u56fe\u50cf)\uff0c \"channels_last\" \u5047\u5b9a\u4e3a (rows, cols, channels) \uff0c\u800c \"channels_first\" \u5047\u5b9a\u4e3a (channels, rows, cols) \u3002 \u5bf9\u4e8e 3D \u6570\u636e\uff0c \"channels_last\" \u5047\u5b9a\u4e3a (conv_dim1, conv_dim2, conv_dim3, channels) \uff0c\u800c \"channels_first\" \u5047\u5b9a\u4e3a (channels, conv_dim1, conv_dim2, conv_dim3) \u3002 epsilon : \u6d6e\u70b9\u6570\uff0c\u7528\u4e8e\u907f\u514d\u5728\u67d0\u4e9b\u64cd\u4f5c\u4e2d\u88ab\u96f6\u9664\u7684\u6570\u5b57\u6a21\u7cca\u5e38\u91cf\u3002 floatx : \u5b57\u7b26\u4e32\uff0c \"float16\" , \"float32\" , \u6216 \"float64\" \u3002\u9ed8\u8ba4\u6d6e\u70b9\u7cbe\u5ea6\u3002 backend : \u5b57\u7b26\u4e32\uff0c \"tensorflow\" , \"theano\" , \u6216 \"cntk\" \u3002 \u4f7f\u7528\u62bd\u8c61 Keras \u540e\u7aef\u7f16\u5199\u65b0\u4ee3\u7801 \u5982\u679c\u4f60\u5e0c\u671b\u4f60\u7f16\u5199\u7684 Keras \u6a21\u5757\u4e0e Theano ( th ) \u548c TensorFlow ( tf ) \u517c\u5bb9\uff0c\u5219\u5fc5\u987b\u901a\u8fc7\u62bd\u8c61 Keras \u540e\u7aef API \u6765\u7f16\u5199\u5b83\u4eec\u3002\u4ee5\u4e0b\u662f\u4e00\u4e2a\u4ecb\u7ecd\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u5bfc\u5165\u540e\u7aef\u6a21\u5757\uff1a from keras import backend as K \u4e0b\u9762\u7684\u4ee3\u7801\u5b9e\u4f8b\u5316\u4e00\u4e2a\u8f93\u5165\u5360\u4f4d\u7b26\u3002\u5b83\u7b49\u4ef7\u4e8e tf.placeholder() \u6216 th.tensor.matrix() , th.tensor.tensor3() , \u7b49\u7b49\u3002 inputs = K.placeholder(shape=(2, 4, 5)) # \u540c\u6837\u53ef\u4ee5\uff1a inputs = K.placeholder(shape=(None, 4, 5)) # \u540c\u6837\u53ef\u4ee5\uff1a inputs = K.placeholder(ndim=3) \u4e0b\u9762\u7684\u4ee3\u7801\u5b9e\u4f8b\u5316\u4e00\u4e2a\u53d8\u91cf\u3002\u5b83\u7b49\u4ef7\u4e8e tf.Variable() \u6216 th.shared() \u3002 import numpy as np val = np.random.random((3, 4, 5)) var = K.variable(value=val) # \u5168 0 \u53d8\u91cf\uff1a var = K.zeros(shape=(3, 4, 5)) # \u5168 1 \u53d8\u91cf\uff1a var = K.ones(shape=(3, 4, 5)) \u4f60\u9700\u8981\u7684\u5927\u591a\u6570\u5f20\u91cf\u64cd\u4f5c\u90fd\u53ef\u4ee5\u50cf\u5728 TensorFlow \u6216 Theano \u4e2d\u90a3\u6837\u5b8c\u6210\uff1a # \u4f7f\u7528\u968f\u673a\u6570\u521d\u59cb\u5316\u5f20\u91cf b = K.random_uniform_variable(shape=(3, 4), low=0, high=1) # \u5747\u5300\u5206\u5e03 c = K.random_normal_variable(shape=(3, 4), mean=0, scale=1) # \u9ad8\u65af\u5206\u5e03 d = K.random_normal_variable(shape=(3, 4), mean=0, scale=1) # \u5f20\u91cf\u8fd0\u7b97 a = b + c * K.abs(d) c = K.dot(a, K.transpose(b)) a = K.sum(b, axis=1) a = K.softmax(b) a = K.concatenate([b, c], axis=-1) # \u7b49\u7b49 \u540e\u7aef\u51fd\u6570 epsilon keras.backend.epsilon() \u8fd4\u56de\u6570\u5b57\u8868\u8fbe\u5f0f\u4e2d\u4f7f\u7528\u7684\u6a21\u7cca\u56e0\u5b50\u7684\u503c\u3002 \u8fd4\u56de \u4e00\u4e2a\u6d6e\u70b9\u6570\u3002 \u4f8b\u5b50 >>> keras.backend.epsilon() 1e-07 set_epsilon keras.backend.set_epsilon(e) \u8bbe\u7f6e\u6570\u5b57\u8868\u8fbe\u5f0f\u4e2d\u4f7f\u7528\u7684\u6a21\u7cca\u56e0\u5b50\u7684\u503c\u3002 \u53c2\u6570 e : \u6d6e\u70b9\u6570\u3002\u65b0\u7684 epsilon \u503c\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> K.epsilon() 1e-07 >>> K.set_epsilon(1e-05) >>> K.epsilon() 1e-05 floatx keras.backend.floatx() \u4ee5\u5b57\u7b26\u4e32\u5f62\u5f0f\u8fd4\u56de\u9ed8\u8ba4\u7684\u6d6e\u70b9\u7c7b\u578b\u3002 (\u4f8b\u5982\uff0c'float16', 'float32', 'float64')\u3002 \u8fd4\u56de \u5b57\u7b26\u4e32\uff0c\u5f53\u524d\u9ed8\u8ba4\u7684\u6d6e\u70b9\u7c7b\u578b\u3002 \u4f8b\u5b50 >>> keras.backend.floatx() 'float32' set_floatx keras.backend.set_floatx(floatx) \u8bbe\u7f6e\u9ed8\u8ba4\u7684\u6d6e\u70b9\u7c7b\u578b\u3002 \u53c2\u6570 floatx : \u5b57\u7b26\u4e32\uff0c'float16', 'float32', \u6216 'float64'\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> K.floatx() 'float32' >>> K.set_floatx('float16') >>> K.floatx() 'float16' cast_to_floatx keras.backend.cast_to_floatx(x) \u5c06 Numpy \u6570\u7ec4\u8f6c\u6362\u4e3a\u9ed8\u8ba4\u7684 Keras \u6d6e\u70b9\u7c7b\u578b\u3002 \u53c2\u6570 x : Numpy \u6570\u7ec4\u3002 \u8fd4\u56de \u76f8\u540c\u7684 Numpy \u6570\u7ec4\uff0c\u8f6c\u6362\u4e3a\u5b83\u7684\u65b0\u7c7b\u578b\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> K.floatx() 'float32' >>> arr = numpy.array([1.0, 2.0], dtype='float64') >>> arr.dtype dtype('float64') >>> new_arr = K.cast_to_floatx(arr) >>> new_arr array([ 1., 2.], dtype=float32) >>> new_arr.dtype dtype('float32') image_data_format keras.backend.image_data_format() \u8fd4\u56de\u9ed8\u8ba4\u56fe\u50cf\u6570\u636e\u683c\u5f0f\u7ea6\u5b9a ('channels_first' \u6216 'channels_last')\u3002 \u8fd4\u56de \u4e00\u4e2a\u5b57\u7b26\u4e32\uff0c 'channels_first' \u6216 'channels_last' \u4f8b\u5b50 >>> keras.backend.image_data_format() 'channels_first' set_image_data_format keras.backend.set_image_data_format(data_format) \u8bbe\u7f6e\u6570\u636e\u683c\u5f0f\u7ea6\u5b9a\u7684\u503c\u3002 \u53c2\u6570 data_format : \u5b57\u7b26\u4e32\u3002 'channels_first' \u6216 'channels_last' \u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> K.image_data_format() 'channels_first' >>> K.set_image_data_format('channels_last') >>> K.image_data_format() 'channels_last' reset_uids keras.backend.reset_uids() \u91cd\u7f6e\u56fe\u7684\u6807\u8bc6\u7b26\u3002 get_uid keras.backend.get_uid(prefix='') \u83b7\u53d6\u9ed8\u8ba4\u8ba1\u7b97\u56fe\u7684 uid\u3002 \u53c2\u6570 prefix : \u56fe\u7684\u53ef\u9009\u524d\u7f00\u3002 \u8fd4\u56de \u56fe\u7684\u552f\u4e00\u6807\u8bc6\u7b26\u3002 clear_session keras.backend.clear_session() \u9500\u6bc1\u5f53\u524d\u7684 TF \u56fe\u5e76\u521b\u5efa\u4e00\u4e2a\u65b0\u56fe\u3002 \u6709\u7528\u4e8e\u907f\u514d\u65e7\u6a21\u578b/\u7f51\u7edc\u5c42\u6df7\u4e71\u3002 manual_variable_initialization keras.backend.manual_variable_initialization(value) \u8bbe\u7f6e\u53d8\u91cf\u624b\u52a8\u521d\u59cb\u5316\u7684\u6807\u5fd7\u3002 \u8fd9\u4e2a\u5e03\u5c14\u6807\u5fd7\u51b3\u5b9a\u4e86\u53d8\u91cf\u662f\u5426\u5e94\u8be5\u5728\u5b9e\u4f8b\u5316\u65f6\u521d\u59cb\u5316\uff08\u9ed8\u8ba4\uff09\uff0c \u6216\u8005\u7528\u6237\u662f\u5426\u5e94\u8be5\u81ea\u5df1\u5904\u7406\u521d\u59cb\u5316 \uff08\u4f8b\u5982\u901a\u8fc7 tf.initialize_all_variables() \uff09\u3002 \u53c2\u6570 value : Python \u5e03\u5c14\u503c\u3002 learning_phase keras.backend.learning_phase() \u8fd4\u56de\u5b66\u4e60\u9636\u6bb5\u7684\u6807\u5fd7\u3002 \u5b66\u4e60\u9636\u6bb5\u6807\u5fd7\u662f\u4e00\u4e2a\u5e03\u5c14\u5f20\u91cf\uff080 = test\uff0c1 = train\uff09\uff0c \u5b83\u4f5c\u4e3a\u8f93\u5165\u4f20\u9012\u7ed9\u4efb\u4f55\u7684 Keras \u51fd\u6570\uff0c\u4ee5\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5 \u65f6\u6267\u884c\u4e0d\u540c\u7684\u884c\u4e3a\u64cd\u4f5c\u3002 \u8fd4\u56de \u5b66\u4e60\u9636\u6bb5 (\u6807\u91cf\u6574\u6570\u5f20\u91cf\u6216 python \u6574\u6570)\u3002 set_learning_phase keras.backend.set_learning_phase(value) \u5c06\u5b66\u4e60\u9636\u6bb5\u8bbe\u7f6e\u4e3a\u56fa\u5b9a\u503c\u3002 \u53c2\u6570 value : \u5b66\u4e60\u9636\u6bb5\u7684\u503c\uff0c0 \u6216 1\uff08\u6574\u6570\uff09\u3002 \u5f02\u5e38 ValueError : \u5982\u679c value \u65e2\u4e0d\u662f 0 \u4e5f\u4e0d\u662f 1 \u3002 is_sparse keras.backend.is_sparse(tensor) \u5224\u65ad\u5f20\u91cf\u662f\u5426\u662f\u7a00\u758f\u5f20\u91cf\u3002 \u53c2\u6570 tensor : \u4e00\u4e2a\u5f20\u91cf\u5b9e\u4f8b\u3002 \u8fd4\u56de \u5e03\u5c14\u503c\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> a = K.placeholder((2, 2), sparse=False) >>> print(K.is_sparse(a)) False >>> b = K.placeholder((2, 2), sparse=True) >>> print(K.is_sparse(b)) True to_dense keras.backend.to_dense(tensor) \u5c06\u7a00\u758f\u5f20\u91cf\u8f6c\u6362\u4e3a\u7a20\u5bc6\u5f20\u91cf\u5e76\u8fd4\u56de\u3002 \u53c2\u6570 tensor : \u5f20\u91cf\u5b9e\u4f8b\uff08\u53ef\u80fd\u7a00\u758f\uff09\u3002 \u8fd4\u56de \u4e00\u4e2a\u7a20\u5bc6\u5f20\u91cf\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> b = K.placeholder((2, 2), sparse=True) >>> print(K.is_sparse(b)) True >>> c = K.to_dense(b) >>> print(K.is_sparse(c)) False variable keras.backend.variable(value, dtype=None, name=None, constraint=None) \u5b9e\u4f8b\u5316\u4e00\u4e2a\u53d8\u91cf\u5e76\u8fd4\u56de\u5b83\u3002 \u53c2\u6570 value : Numpy \u6570\u7ec4\uff0c\u5f20\u91cf\u7684\u521d\u59cb\u503c\u3002 dtype : \u5f20\u91cf\u7c7b\u578b\u3002 name : \u5f20\u91cf\u7684\u53ef\u9009\u540d\u79f0\u5b57\u7b26\u4e32\u3002 constraint : \u5728\u4f18\u5316\u5668\u66f4\u65b0\u540e\u5e94\u7528\u4e8e\u53d8\u91cf\u7684\u53ef\u9009\u6295\u5f71\u51fd\u6570\u3002 \u8fd4\u56de \u53d8\u91cf\u5b9e\u4f8b\uff08\u5305\u542b Keras \u5143\u6570\u636e\uff09 \u4f8b\u5b50 >>> from keras import backend as K >>> val = np.array([[1, 2], [3, 4]]) >>> kvar = K.variable(value=val, dtype='float64', name='example_var') >>> K.dtype(kvar) 'float64' >>> print(kvar) example_var >>> K.eval(kvar) array([[ 1., 2.], [ 3., 4.]]) constant keras.backend.constant(value, dtype=None, shape=None, name=None) \u521b\u5efa\u4e00\u4e2a\u5e38\u6570\u5f20\u91cf\u3002 \u53c2\u6570 value : \u4e00\u4e2a\u5e38\u6570\u503c\uff08\u6216\u5217\u8868\uff09 dtype : \u7ed3\u679c\u5f20\u91cf\u7684\u5143\u7d20\u7c7b\u578b\u3002 shape : \u53ef\u9009\u7684\u7ed3\u679c\u5f20\u91cf\u7684\u5c3a\u5bf8\u3002 name : \u53ef\u9009\u7684\u5f20\u91cf\u7684\u540d\u79f0\u3002 \u8fd4\u56de \u4e00\u4e2a\u5e38\u6570\u5f20\u91cf\u3002 is_keras_tensor keras.backend.is_keras_tensor(x) \u5224\u65ad x \u662f\u5426\u662f Keras \u5f20\u91cf \u300cKeras\u5f20\u91cf\u300d\u662f\u7531 Keras \u5c42\uff08 Layer \u7c7b\uff09\u6216 Input \u8fd4\u56de\u7684\u5f20\u91cf\u3002 \u53c2\u6570 x : \u5019\u9009\u5f20\u91cf\u3002 \u8fd4\u56de \u5e03\u5c14\u503c\uff1a\u53c2\u6570\u662f\u5426\u662f Keras \u5f20\u91cf\u3002 \u5f02\u5e38 ValueError : \u5982\u679c x \u4e0d\u662f\u4e00\u4e2a\u7b26\u53f7\u5f20\u91cf\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> from keras.layers import Input, Dense >>> np_var = numpy.array([1, 2]) >>> K.is_keras_tensor(np_var) # \u4e00\u4e2a Numpy \u6570\u7ec4\u4e0d\u662f\u4e00\u4e2a\u7b26\u53f7\u5f20\u91cf\u3002 ValueError >>> k_var = tf.placeholder('float32', shape=(1,1)) >>> K.is_keras_tensor(k_var) # \u5728 Keras \u4e4b\u5916\u95f4\u63a5\u521b\u5efa\u7684\u53d8\u91cf\u4e0d\u662f Keras \u5f20\u91cf\u3002 False >>> keras_var = K.variable(np_var) >>> K.is_keras_tensor(keras_var) # Keras \u540e\u7aef\u521b\u5efa\u7684\u53d8\u91cf\u4e0d\u662f Keras \u5f20\u91cf\u3002 False >>> keras_placeholder = K.placeholder(shape=(2, 4, 5)) >>> K.is_keras_tensor(keras_placeholder) # \u5360\u4f4d\u7b26\u4e0d\u662f Keras \u5f20\u91cf\u3002 False >>> keras_input = Input([10]) >>> K.is_keras_tensor(keras_input) # \u8f93\u5165 Input \u662f Keras \u5f20\u91cf\u3002 True >>> keras_layer_output = Dense(10)(keras_input) >>> K.is_keras_tensor(keras_layer_output) # \u4efb\u4f55 Keras \u5c42\u8f93\u51fa\u90fd\u662f Keras \u5f20\u91cf\u3002 True is_tensor keras.backend.is_tensor(x) placeholder keras.backend.placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None) \u5b9e\u4f8b\u5316\u4e00\u4e2a\u5360\u4f4d\u7b26\u5f20\u91cf\u5e76\u8fd4\u56de\u5b83\u3002 \u53c2\u6570 shape : \u5360\u4f4d\u7b26\u5c3a\u5bf8 (\u6574\u6570\u5143\u7ec4\uff0c\u53ef\u80fd\u5305\u542b None \u9879)\u3002 ndim : \u5f20\u91cf\u7684\u8f74\u6570\u3002 { shape , ndim } \u81f3\u5c11\u4e00\u4e2a\u9700\u8981\u88ab\u6307\u5b9a\u3002 \u5982\u679c\u4e24\u4e2a\u90fd\u88ab\u6307\u5b9a\uff0c\u90a3\u4e48\u4f7f\u7528 shape \u3002 dtype : \u5360\u4f4d\u7b26\u7c7b\u578b\u3002 sparse : \u5e03\u5c14\u503c\uff0c\u5360\u4f4d\u7b26\u662f\u5426\u5e94\u8be5\u6709\u4e00\u4e2a\u7a00\u758f\u7c7b\u578b\u3002 name : \u53ef\u9009\u7684\u5360\u4f4d\u7b26\u7684\u540d\u79f0\u5b57\u7b26\u4e32\u3002 \u8fd4\u56de \u5f20\u91cf\u5b9e\u4f8b\uff08\u5305\u62ec Keras \u5143\u6570\u636e\uff09\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> input_ph = K.placeholder(shape=(2, 4, 5)) >>> input_ph._keras_shape (2, 4, 5) >>> input_ph <tf.Tensor 'Placeholder_4:0' shape=(2, 4, 5) dtype=float32> is_placeholder keras.backend.is_placeholder(x) \u5224\u65ad x \u662f\u5426\u662f\u5360\u4f4d\u7b26\u3002 \u53c2\u6570 x : \u5019\u9009\u5360\u4f4d\u7b26\u3002 \u8fd4\u56de \u5e03\u5c14\u503c\u3002 shape keras.backend.shape(x) \u8fd4\u56de\u5f20\u91cf\u6216\u53d8\u91cf\u7684\u7b26\u53f7\u5c3a\u5bf8\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u7b26\u53f7\u5c3a\u5bf8\uff08\u5b83\u672c\u8eab\u5c31\u662f\u5f20\u91cf\uff09\u3002 \u4f8b\u5b50 # TensorFlow \u4f8b\u5b50 >>> from keras import backend as K >>> tf_session = K.get_session() >>> val = np.array([[1, 2], [3, 4]]) >>> kvar = K.variable(value=val) >>> inputs = keras.backend.placeholder(shape=(2, 4, 5)) >>> K.shape(kvar) <tf.Tensor 'Shape_8:0' shape=(2,) dtype=int32> >>> K.shape(inputs) <tf.Tensor 'Shape_9:0' shape=(3,) dtype=int32> # \u8981\u5f97\u5230\u6574\u6570\u5c3a\u5bf8 (\u76f8\u53cd\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528 K.int_shape(x)) >>> K.shape(kvar).eval(session=tf_session) array([2, 2], dtype=int32) >>> K.shape(inputs).eval(session=tf_session) array([2, 4, 5], dtype=int32) int_shape keras.backend.int_shape(x) \u8fd4\u56de\u5f20\u91cf\u6216\u53d8\u91cf\u7684\u5c3a\u5bf8\uff0c\u4f5c\u4e3a int \u6216 None \u9879\u7684\u5143\u7ec4\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u6574\u6570\u5143\u7ec4\uff08\u6216 None \u9879\uff09\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> inputs = K.placeholder(shape=(2, 4, 5)) >>> K.int_shape(inputs) (2, 4, 5) >>> val = np.array([[1, 2], [3, 4]]) >>> kvar = K.variable(value=val) >>> K.int_shape(kvar) (2, 2) Numpy \u5b9e\u73b0 def int_shape(x): return x.shape ndim keras.backend.ndim(x) \u4ee5\u6574\u6570\u5f62\u5f0f\u8fd4\u56de\u5f20\u91cf\u4e2d\u7684\u8f74\u6570\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u6574\u6570 (\u6807\u91cf), \u8f74\u7684\u6570\u91cf\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> inputs = K.placeholder(shape=(2, 4, 5)) >>> val = np.array([[1, 2], [3, 4]]) >>> kvar = K.variable(value=val) >>> K.ndim(inputs) 3 >>> K.ndim(kvar) 2 Numpy \u5b9e\u73b0 def ndim(x): return x.ndim dtype keras.backend.dtype(x) \u4ee5\u5b57\u7b26\u4e32\u5f62\u5f0f\u8fd4\u56de Keras \u5f20\u91cf\u6216\u53d8\u91cf\u7684 dtype\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u5b57\u7b26\u4e32\uff0c x \u7684 dtype\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> K.dtype(K.placeholder(shape=(2,4,5))) 'float32' >>> K.dtype(K.placeholder(shape=(2,4,5), dtype='float32')) 'float32' >>> K.dtype(K.placeholder(shape=(2,4,5), dtype='float64')) 'float64' # Keras \u53d8\u91cf >>> kvar = K.variable(np.array([[1, 2], [3, 4]])) >>> K.dtype(kvar) 'float32_ref' >>> kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32') >>> K.dtype(kvar) 'float32_ref' eval keras.backend.eval(x) \u4f30\u8ba1\u4e00\u4e2a\u53d8\u91cf\u7684\u503c\u3002 \u53c2\u6570 x : \u53d8\u91cf\u3002 \u8fd4\u56de Numpy \u6570\u7ec4\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32') >>> K.eval(kvar) array([[ 1., 2.], [ 3., 4.]], dtype=float32) zeros keras.backend.zeros(shape, dtype=None, name=None) \u5b9e\u4f8b\u5316\u4e00\u4e2a\u5168\u96f6\u53d8\u91cf\u5e76\u8fd4\u56de\u5b83\u3002 \u53c2\u6570 shape : \u6574\u6570\u5143\u7ec4\uff0c\u8fd4\u56de\u7684Keras\u53d8\u91cf\u7684\u5c3a\u5bf8\u3002 dtype : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684 Keras \u53d8\u91cf\u7684\u6570\u636e\u7c7b\u578b\u3002 name : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684 Keras \u53d8\u91cf\u7684\u540d\u79f0\u3002 \u8fd4\u56de \u4e00\u4e2a\u53d8\u91cf\uff08\u5305\u62ec Keras \u5143\u6570\u636e\uff09\uff0c\u7528 0.0 \u586b\u5145\u3002 \u8bf7\u6ce8\u610f\uff0c\u5982\u679c shape \u662f\u7b26\u53f7\u5316\u7684\uff0c\u6211\u4eec\u4e0d\u80fd\u8fd4\u56de\u4e00\u4e2a\u53d8\u91cf\uff0c \u800c\u4f1a\u8fd4\u56de\u4e00\u4e2a\u52a8\u6001\u5c3a\u5bf8\u7684\u5f20\u91cf\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> kvar = K.zeros((3,4)) >>> K.eval(kvar) array([[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]], dtype=float32) ones keras.backend.ones(shape, dtype=None, name=None) \u5b9e\u4f8b\u5316\u4e00\u4e2a\u5168\u4e00\u53d8\u91cf\u5e76\u8fd4\u56de\u5b83\u3002 \u53c2\u6570 shape : \u6574\u6570\u5143\u7ec4\uff0c\u8fd4\u56de\u7684Keras\u53d8\u91cf\u7684\u5c3a\u5bf8\u3002 dtype : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684 Keras \u53d8\u91cf\u7684\u6570\u636e\u7c7b\u578b\u3002 name : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684 Keras \u53d8\u91cf\u7684\u540d\u79f0\u3002 \u8fd4\u56de \u4e00\u4e2a Keras \u53d8\u91cf\uff0c\u7528 1.0 \u586b\u5145\u3002 \u8bf7\u6ce8\u610f\uff0c\u5982\u679c shape \u662f\u7b26\u53f7\u5316\u7684\uff0c\u6211\u4eec\u4e0d\u80fd\u8fd4\u56de\u4e00\u4e2a\u53d8\u91cf\uff0c \u800c\u4f1a\u8fd4\u56de\u4e00\u4e2a\u52a8\u6001\u5c3a\u5bf8\u7684\u5f20\u91cf\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> kvar = K.ones((3,4)) >>> K.eval(kvar) array([[ 1., 1., 1., 1.], [ 1., 1., 1., 1.], [ 1., 1., 1., 1.]], dtype=float32) eye keras.backend.eye(size, dtype=None, name=None) \u5b9e\u4f8b\u5316\u4e00\u4e2a\u5355\u4f4d\u77e9\u9635\u5e76\u8fd4\u56de\u5b83\u3002 \u53c2\u6570 size : \u6574\u6570\uff0c\u884c/\u5217\u7684\u6570\u76ee\u3002 dtype : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684 Keras \u53d8\u91cf\u7684\u6570\u636e\u7c7b\u578b\u3002 name : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684 Keras \u53d8\u91cf\u7684\u540d\u79f0\u3002 \u8fd4\u56de Keras \u53d8\u91cf\uff0c\u4e00\u4e2a\u5355\u4f4d\u77e9\u9635\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> kvar = K.eye(3) >>> K.eval(kvar) array([[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]], dtype=float32) zeros_like keras.backend.zeros_like(x, dtype=None, name=None) \u5b9e\u4f8b\u5316\u4e0e\u53e6\u4e00\u4e2a\u5f20\u91cf\u76f8\u540c\u5c3a\u5bf8\u7684\u5168\u96f6\u53d8\u91cf\u3002 \u53c2\u6570 x : Keras \u53d8\u91cf\u6216 Keras \u5f20\u91cf\u3002 dtype : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684 Keras \u53d8\u91cf\u7684\u7c7b\u578b\u3002 \u5982\u679c\u4e3a None\uff0c\u5219\u4f7f\u7528 x \u7684\u7c7b\u578b\u3002 name : \u5b57\u7b26\u4e32\uff0c\u6240\u521b\u5efa\u7684\u53d8\u91cf\u7684\u540d\u79f0\u3002 \u8fd4\u56de \u4e00\u4e2a Keras \u53d8\u91cf\uff0c\u5176\u5f62\u72b6\u4e3a x\uff0c\u7528\u96f6\u586b\u5145\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> kvar = K.variable(np.random.random((2,3))) >>> kvar_zeros = K.zeros_like(kvar) >>> K.eval(kvar_zeros) array([[ 0., 0., 0.], [ 0., 0., 0.]], dtype=float32) ones_like keras.backend.ones_like(x, dtype=None, name=None) \u5b9e\u4f8b\u5316\u4e0e\u53e6\u4e00\u4e2a\u5f20\u91cf\u76f8\u540c\u5f62\u72b6\u7684\u5168\u4e00\u53d8\u91cf\u3002 \u53c2\u6570 x : Keras \u53d8\u91cf\u6216\u5f20\u91cf\u3002 dtype : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684 Keras \u53d8\u91cf\u7684\u7c7b\u578b\u3002 \u5982\u679c\u4e3a None\uff0c\u5219\u4f7f\u7528 x \u7684\u7c7b\u578b\u3002 name : \u5b57\u7b26\u4e32\uff0c\u6240\u521b\u5efa\u7684\u53d8\u91cf\u7684\u540d\u79f0\u3002 \u8fd4\u56de \u4e00\u4e2a Keras \u53d8\u91cf\uff0c\u5176\u5f62\u72b6\u4e3a x\uff0c\u7528\u4e00\u586b\u5145\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> kvar = K.variable(np.random.random((2,3))) >>> kvar_ones = K.ones_like(kvar) >>> K.eval(kvar_ones) array([[ 1., 1., 1.], [ 1., 1., 1.]], dtype=float32) identity keras.backend.identity(x, name=None) \u8fd4\u56de\u4e0e\u8f93\u5165\u5f20\u91cf\u76f8\u540c\u5185\u5bb9\u7684\u5f20\u91cf\u3002 \u53c2\u6570 x : \u8f93\u5165\u5f20\u91cf\u3002 name : \u5b57\u7b26\u4e32\uff0c\u6240\u521b\u5efa\u7684\u53d8\u91cf\u7684\u540d\u79f0\u3002 \u8fd4\u56de \u4e00\u4e2a\u76f8\u540c\u5c3a\u5bf8\u3001\u7c7b\u578b\u548c\u5185\u5bb9\u7684\u5f20\u91cf\u3002 random_uniform_variable keras.backend.random_uniform_variable(shape, low, high, dtype=None, name=None, seed=None) \u4f7f\u7528\u4ece\u5747\u5300\u5206\u5e03\u4e2d\u62bd\u6837\u51fa\u6765\u7684\u503c\u6765\u5b9e\u4f8b\u5316\u53d8\u91cf\u3002 \u53c2\u6570 shape : \u6574\u6570\u5143\u7ec4\uff0c\u8fd4\u56de\u7684 Keras \u53d8\u91cf\u7684\u5c3a\u5bf8\u3002 low : \u6d6e\u70b9\u6570\uff0c\u8f93\u51fa\u95f4\u9694\u7684\u4e0b\u754c\u3002 high : \u6d6e\u70b9\u6570\uff0c\u8f93\u51fa\u95f4\u9694\u7684\u4e0a\u754c\u3002 dtype : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684 Keras \u53d8\u91cf\u7684\u6570\u636e\u7c7b\u578b\u3002 name : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684 Keras \u53d8\u91cf\u7684\u540d\u79f0\u3002 seed : \u6574\u6570\uff0c\u968f\u673a\u79cd\u5b50\u3002 \u8fd4\u56de \u4e00\u4e2a Keras \u53d8\u91cf\uff0c\u4ee5\u62bd\u53d6\u7684\u6837\u672c\u586b\u5145\u3002 \u4f8b\u5b50 # TensorFlow \u793a\u4f8b >>> kvar = K.random_uniform_variable((2,3), 0, 1) >>> kvar <tensorflow.python.ops.variables.Variable object at 0x10ab40b10> >>> K.eval(kvar) array([[ 0.10940075, 0.10047495, 0.476143 ], [ 0.66137183, 0.00869417, 0.89220798]], dtype=float32) random_normal_variable keras.backend.random_normal_variable(shape, mean, scale, dtype=None, name=None, seed=None) \u4f7f\u7528\u4ece\u6b63\u6001\u5206\u5e03\u4e2d\u62bd\u53d6\u7684\u503c\u5b9e\u4f8b\u5316\u4e00\u4e2a\u53d8\u91cf\u3002 \u53c2\u6570 shape : \u6574\u6570\u5143\u7ec4\uff0c\u8fd4\u56de\u7684Keras\u53d8\u91cf\u7684\u5c3a\u5bf8\u3002 mean : \u6d6e\u70b9\u578b\uff0c\u6b63\u6001\u5206\u5e03\u5e73\u5747\u503c\u3002 scale : \u6d6e\u70b9\u578b\uff0c\u6b63\u6001\u5206\u5e03\u6807\u51c6\u5dee\u3002 dtype : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684Keras\u53d8\u91cf\u7684 dtype\u3002 name : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684Keras\u53d8\u91cf\u7684\u540d\u79f0\u3002 seed : \u6574\u6570\uff0c\u968f\u673a\u79cd\u5b50\u3002 \u8fd4\u56de \u4e00\u4e2a Keras \u53d8\u91cf\uff0c\u4ee5\u62bd\u53d6\u7684\u6837\u672c\u586b\u5145\u3002 \u4f8b\u5b50 # TensorFlow \u793a\u4f8b >>> kvar = K.random_normal_variable((2,3), 0, 1) >>> kvar <tensorflow.python.ops.variables.Variable object at 0x10ab12dd0> >>> K.eval(kvar) array([[ 1.19591331, 0.68685907, -0.63814116], [ 0.92629528, 0.28055015, 1.70484698]], dtype=float32) count_params keras.backend.count_params(x) \u8fd4\u56de Keras \u53d8\u91cf\u6216\u5f20\u91cf\u4e2d\u7684\u9759\u6001\u5143\u7d20\u6570\u3002 \u53c2\u6570 x : Keras \u53d8\u91cf\u6216\u5f20\u91cf\u3002 \u8fd4\u56de \u6574\u6570\uff0c x \u4e2d\u7684\u5143\u7d20\u6570\u91cf\uff0c\u5373\uff0c\u6570\u7ec4\u4e2d\u9759\u6001\u7ef4\u5ea6\u7684\u4e58\u79ef\u3002 \u4f8b\u5b50 >>> kvar = K.zeros((2,3)) >>> K.count_params(kvar) 6 >>> K.eval(kvar) array([[ 0., 0., 0.], [ 0., 0., 0.]], dtype=float32) cast keras.backend.cast(x, dtype) \u5c06\u5f20\u91cf\u8f6c\u6362\u5230\u4e0d\u540c\u7684 dtype \u5e76\u8fd4\u56de\u3002 \u4f60\u53ef\u4ee5\u8f6c\u6362\u4e00\u4e2a Keras \u53d8\u91cf\uff0c\u4f46\u5b83\u4ecd\u7136\u8fd4\u56de\u4e00\u4e2a Keras \u5f20\u91cf\u3002 \u53c2\u6570 x : Keras \u5f20\u91cf\uff08\u6216\u53d8\u91cf\uff09\u3002 dtype : \u5b57\u7b26\u4e32\uff0c ( 'float16' , 'float32' \u6216 'float64' )\u3002 \u8fd4\u56de Keras \u5f20\u91cf\uff0c\u7c7b\u578b\u4e3a dtype \u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> input = K.placeholder((2, 3), dtype='float32') >>> input <tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32> # It doesn't work in-place as below. >>> K.cast(input, dtype='float16') <tf.Tensor 'Cast_1:0' shape=(2, 3) dtype=float16> >>> input <tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32> # you need to assign it. >>> input = K.cast(input, dtype='float16') >>> input <tf.Tensor 'Cast_2:0' shape=(2, 3) dtype=float16> update keras.backend.update(x, new_x) \u5c06 x \u7684\u503c\u66f4\u65b0\u4e3a new_x \u3002 \u53c2\u6570 x : \u4e00\u4e2a Variable \u3002 new_x : \u4e00\u4e2a\u4e0e x \u5c3a\u5bf8\u76f8\u540c\u7684\u5f20\u91cf\u3002 \u8fd4\u56de \u66f4\u65b0\u540e\u7684\u53d8\u91cf x \u3002 update_add keras.backend.update_add(x, increment) \u901a\u8fc7\u589e\u52a0 increment \u6765\u66f4\u65b0 x \u7684\u503c\u3002 \u53c2\u6570 x : \u4e00\u4e2a Variable \u3002 increment : \u4e0e x \u5f62\u72b6\u76f8\u540c\u7684\u5f20\u91cf\u3002 \u8fd4\u56de \u66f4\u65b0\u540e\u7684\u53d8\u91cf x \u3002 update_sub keras.backend.update_sub(x, decrement) \u901a\u8fc7\u51cf decrement \u6765\u66f4\u65b0 x \u7684\u503c\u3002 \u53c2\u6570 x : \u4e00\u4e2a Variable \u3002 decrement : \u4e0e x \u5f62\u72b6\u76f8\u540c\u7684\u5f20\u91cf\u3002 \u8fd4\u56de \u66f4\u65b0\u540e\u7684\u53d8\u91cf x \u3002 moving_average_update keras.backend.moving_average_update(x, value, momentum) \u8ba1\u7b97\u53d8\u91cf\u7684\u79fb\u52a8\u5e73\u5747\u503c\u3002 \u53c2\u6570 x : \u4e00\u4e2a Variable \u3002 value : \u4e0e x \u5f62\u72b6\u76f8\u540c\u7684\u5f20\u91cf\u3002 momentum : \u79fb\u52a8\u5e73\u5747\u52a8\u91cf\u3002 \u8fd4\u56de \u66f4\u65b0\u53d8\u91cf\u7684\u64cd\u4f5c\u3002 dot keras.backend.dot(x, y) \u5c06 2 \u4e2a\u5f20\u91cf\uff08\u548c/\u6216\u53d8\u91cf\uff09\u76f8\u4e58\u5e76\u8fd4\u56de\u4e00\u4e2a \u5f20\u91cf \u3002 \u5f53\u8bd5\u56fe\u5c06 nD \u5f20\u91cf\u4e0e nD \u5f20\u91cf\u76f8\u4e58\u65f6\uff0c \u5b83\u4f1a\u91cd\u73b0 Theano \u884c\u4e3a\u3002 (\u4f8b\u5982 (2, 3) * (4, 3, 5) -> (2, 4, 5) ) \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 y : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\uff0c x \u548c y \u7684\u70b9\u79ef\u3002 \u4f8b\u5b50 # \u5f20\u91cf\u4e4b\u95f4\u7684\u70b9\u79ef >>> x = K.placeholder(shape=(2, 3)) >>> y = K.placeholder(shape=(3, 4)) >>> xy = K.dot(x, y) >>> xy <tf.Tensor 'MatMul_9:0' shape=(2, 4) dtype=float32> # \u5f20\u91cf\u4e4b\u95f4\u7684\u70b9\u79ef >>> x = K.placeholder(shape=(32, 28, 3)) >>> y = K.placeholder(shape=(3, 4)) >>> xy = K.dot(x, y) >>> xy <tf.Tensor 'MatMul_9:0' shape=(32, 28, 4) dtype=float32> # \u7c7b Theano \u884c\u4e3a\u7684\u4f8b\u5b50 >>> x = K.random_uniform_variable(shape=(2, 3), low=0, high=1) >>> y = K.ones((4, 3, 5)) >>> xy = K.dot(x, y) >>> K.int_shape(xy) (2, 4, 5) batch_dot keras.backend.batch_dot(x, y, axes=None) \u6279\u91cf\u5316\u7684\u70b9\u79ef\u3002 \u5f53 x \u548c y \u662f\u6279\u91cf\u6570\u636e\u65f6\uff0c batch_dot \u7528\u4e8e\u8ba1\u7b97 x \u548c y \u7684\u70b9\u79ef\uff0c \u5373\u5c3a\u5bf8\u4e3a (batch_size, :) \u3002 batch_dot \u4ea7\u751f\u4e00\u4e2a\u6bd4\u8f93\u5165\u5c3a\u5bf8\u66f4\u5c0f\u7684\u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u5982\u679c\u7ef4\u6570\u51cf\u5c11\u5230 1\uff0c\u6211\u4eec\u4f7f\u7528 expand_dims \u6765\u786e\u4fdd ndim \u81f3\u5c11\u4e3a 2\u3002 \u53c2\u6570 x : ndim >= 2 \u7684 Keras \u5f20\u91cf\u6216\u53d8\u91cf\u3002 y : ndim >= 2 \u7684 Keras \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axes : \u8868\u793a\u76ee\u6807\u7ef4\u5ea6\u7684\u6574\u6570\u6216\u5217\u8868\u3002 axes[0] \u548c axes[1] \u7684\u957f\u5ea6\u5fc5\u987b\u76f8\u540c\u3002 \u8fd4\u56de \u4e00\u4e2a\u5c3a\u5bf8\u7b49\u4e8e x \u7684\u5c3a\u5bf8\uff08\u51cf\u53bb\u603b\u548c\u7684\u7ef4\u5ea6\uff09\u548c y \u7684\u5c3a\u5bf8\uff08\u51cf\u53bb\u6279\u6b21\u7ef4\u5ea6\u548c\u603b\u548c\u7684\u7ef4\u5ea6\uff09\u7684\u8fde\u63a5\u7684\u5f20\u91cf\u3002 \u5982\u679c\u6700\u540e\u7684\u79e9\u4e3a 1\uff0c\u6211\u4eec\u5c06\u5b83\u91cd\u65b0\u8f6c\u6362\u4e3a (batch_size, 1) \u3002 \u4f8b\u5b50 \u5047\u8bbe x = [[1, 2], [3, 4]] \u548c y = [[5, 6], [7, 8]] \uff0c batch_dot(x, y, axes=1) = [[17], [53]] \u662f x.dot(y.T) \u7684\u4e3b\u5bf9\u89d2\u7ebf\uff0c \u5c3d\u7ba1\u6211\u4eec\u4e0d\u9700\u8981\u8ba1\u7b97\u975e\u5bf9\u89d2\u5143\u7d20\u3002 \u4f2a\u4ee3\u7801\uff1a inner_products = [] for xi, yi in zip(x, y): inner_products.append(xi.dot(yi)) result = stack(inner_products) \u5c3a\u5bf8\u63a8\u65ad\uff1a \u8ba9 x \u7684\u5c3a\u5bf8\u4e3a (100, 20) \uff0c\u4ee5\u53ca y \u7684\u5c3a\u5bf8\u4e3a (100, 30, 20) \u3002 \u5982\u679c axes \u662f (1, 2)\uff0c\u8981\u627e\u51fa\u7ed3\u679c\u5f20\u91cf\u7684\u5c3a\u5bf8\uff0c \u5faa\u73af x \u548c y \u7684\u5c3a\u5bf8\u7684\u6bcf\u4e00\u4e2a\u7ef4\u5ea6\u3002 x.shape[0] : 100 : \u9644\u52a0\u5230\u8f93\u51fa\u5f62\u72b6\uff0c x.shape[1] : 20 : \u4e0d\u9644\u52a0\u5230\u8f93\u51fa\u5f62\u72b6\uff0c x \u7684\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\u5df2\u7ecf\u88ab\u52a0\u548c\u4e86 ( dot_axes[0] = 1)\u3002 y.shape[0] : 100 : \u4e0d\u9644\u52a0\u5230\u8f93\u51fa\u5f62\u72b6\uff0c\u603b\u662f\u5ffd\u7565 y \u7684\u7b2c\u4e00\u7ef4 y.shape[1] : 30 : \u9644\u52a0\u5230\u8f93\u51fa\u5f62\u72b6\uff0c y.shape[2] : 20 : \u4e0d\u9644\u52a0\u5230\u8f93\u51fa\u5f62\u72b6\uff0c y \u7684\u7b2c\u4e8c\u4e2a\u7ef4\u5ea6\u5df2\u7ecf\u88ab\u52a0\u548c\u4e86 ( dot_axes[0] = 2)\u3002 output_shape = (100, 30) >>> x_batch = K.ones(shape=(32, 20, 1)) >>> y_batch = K.ones(shape=(32, 30, 20)) >>> xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=[1, 2]) >>> K.int_shape(xy_batch_dot) (32, 1, 30) transpose keras.backend.transpose(x) \u5c06\u5f20\u91cf\u8f6c\u7f6e\u5e76\u8fd4\u56de\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 \u4f8b\u5b50 >>> var = K.variable([[1, 2, 3], [4, 5, 6]]) >>> K.eval(var) array([[ 1., 2., 3.], [ 4., 5., 6.]], dtype=float32) >>> var_transposed = K.transpose(var) >>> K.eval(var_transposed) array([[ 1., 4.], [ 2., 5.], [ 3., 6.]], dtype=float32) >>> inputs = K.placeholder((2, 3)) >>> inputs <tf.Tensor 'Placeholder_11:0' shape=(2, 3) dtype=float32> >>> input_transposed = K.transpose(inputs) >>> input_transposed <tf.Tensor 'transpose_4:0' shape=(3, 2) dtype=float32> gather keras.backend.gather(reference, indices) \u5728\u5f20\u91cf reference \u4e2d\u68c0\u7d22\u7d22\u5f15 indices \u7684\u5143\u7d20\u3002 \u53c2\u6570 reference : \u4e00\u4e2a\u5f20\u91cf\u3002 indices : \u7d22\u5f15\u7684\u6574\u6570\u5f20\u91cf\u3002 \u8fd4\u56de \u4e0e reference \u7c7b\u578b\u76f8\u540c\u7684\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def gather(reference, indices): return reference[indices] max keras.backend.max(x, axis=None, keepdims=False) \u5f20\u91cf\u4e2d\u7684\u6700\u5927\u503c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u4e00\u4e2a\u6574\u6570\uff0c\u9700\u8981\u5728\u54ea\u4e2a\u8f74\u5bfb\u627e\u6700\u5927\u503c\u3002 keepdims : \u5e03\u5c14\u503c\uff0c\u662f\u5426\u4fdd\u7559\u539f\u5c3a\u5bf8\u3002 \u5982\u679c keepdims \u4e3a False \uff0c\u5219\u5f20\u91cf\u7684\u79e9\u51cf 1\u3002 \u5982\u679c keepdims \u4e3a True \uff0c\u7f29\u5c0f\u7684\u7ef4\u5ea6\u4fdd\u7559\u4e3a\u957f\u5ea6 1\u3002 \u8fd4\u56de x \u4e2d\u6700\u5927\u503c\u7684\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def max(x, axis=None, keepdims=False): if isinstance(axis, list): axis = tuple(axis) return np.max(x, axis=axis, keepdims=keepdims) min keras.backend.min(x, axis=None, keepdims=False) \u5f20\u91cf\u4e2d\u7684\u6700\u5c0f\u503c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u4e00\u4e2a\u6574\u6570\uff0c\u9700\u8981\u5728\u54ea\u4e2a\u8f74\u5bfb\u627e\u6700\u5927\u503c\u3002 keepdims : \u5e03\u5c14\u503c\uff0c\u662f\u5426\u4fdd\u7559\u539f\u5c3a\u5bf8\u3002 \u5982\u679c keepdims \u4e3a False \uff0c\u5219\u5f20\u91cf\u7684\u79e9\u51cf 1\u3002 \u5982\u679c keepdims \u4e3a True \uff0c\u7f29\u5c0f\u7684\u7ef4\u5ea6\u4fdd\u7559\u4e3a\u957f\u5ea6 1\u3002 \u8fd4\u56de x \u4e2d\u6700\u5c0f\u503c\u7684\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def min(x, axis=None, keepdims=False): if isinstance(axis, list): axis = tuple(axis) return np.min(x, axis=axis, keepdims=keepdims) sum keras.backend.sum(x, axis=None, keepdims=False) \u8ba1\u7b97\u5f20\u91cf\u5728\u67d0\u4e00\u6307\u5b9a\u8f74\u7684\u548c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u4e00\u4e2a\u6574\u6570\uff0c\u9700\u8981\u52a0\u548c\u7684\u8f74\u3002 keepdims : \u5e03\u5c14\u503c\uff0c\u662f\u5426\u4fdd\u7559\u539f\u5c3a\u5bf8\u3002 \u5982\u679c keepdims \u4e3a False \uff0c\u5219\u5f20\u91cf\u7684\u79e9\u51cf 1\u3002 \u5982\u679c keepdims \u4e3a True \uff0c\u7f29\u5c0f\u7684\u7ef4\u5ea6\u4fdd\u7559\u4e3a\u957f\u5ea6 1\u3002 \u8fd4\u56de x \u7684\u548c\u7684\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def sum(x, axis=None, keepdims=False): if isinstance(axis, list): axis = tuple(axis) return np.sum(x, axis=axis, keepdims=keepdims) prod keras.backend.prod(x, axis=None, keepdims=False) \u5728\u67d0\u4e00\u6307\u5b9a\u8f74\uff0c\u8ba1\u7b97\u5f20\u91cf\u4e2d\u7684\u503c\u7684\u4e58\u79ef\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u4e00\u4e2a\u6574\u6570\u9700\u8981\u8ba1\u7b97\u4e58\u79ef\u7684\u8f74\u3002 keepdims : \u5e03\u5c14\u503c\uff0c\u662f\u5426\u4fdd\u7559\u539f\u5c3a\u5bf8\u3002 \u5982\u679c keepdims \u4e3a False \uff0c\u5219\u5f20\u91cf\u7684\u79e9\u51cf 1\u3002 \u5982\u679c keepdims \u4e3a True \uff0c\u7f29\u5c0f\u7684\u7ef4\u5ea6\u4fdd\u7559\u4e3a\u957f\u5ea6 1\u3002 \u8fd4\u56de x \u7684\u5143\u7d20\u7684\u4e58\u79ef\u7684\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def prod(x, axis=None, keepdims=False): if isinstance(axis, list): axis = tuple(axis) return np.prod(x, axis=axis, keepdims=keepdims) cumsum keras.backend.cumsum(x, axis=0) \u5728\u67d0\u4e00\u6307\u5b9a\u8f74\uff0c\u8ba1\u7b97\u5f20\u91cf\u4e2d\u7684\u503c\u7684\u7d2f\u52a0\u548c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u4e00\u4e2a\u6574\u6570\uff0c\u9700\u8981\u52a0\u548c\u7684\u8f74\u3002 \u8fd4\u56de x \u5728 axis \u8f74\u7684\u7d2f\u52a0\u548c\u7684\u5f20\u91cf\u3002 cumprod keras.backend.cumprod(x, axis=0) \u5728\u67d0\u4e00\u6307\u5b9a\u8f74\uff0c\u8ba1\u7b97\u5f20\u91cf\u4e2d\u7684\u503c\u7684\u7d2f\u79ef\u4e58\u79ef\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u4e00\u4e2a\u6574\u6570\uff0c\u9700\u8981\u8ba1\u7b97\u4e58\u79ef\u7684\u8f74\u3002 \u8fd4\u56de x \u5728 axis \u8f74\u7684\u7d2f\u4e58\u7684\u5f20\u91cf\u3002 var keras.backend.var(x, axis=None, keepdims=False) \u5f20\u91cf\u5728\u67d0\u4e00\u6307\u5b9a\u8f74\u7684\u65b9\u5dee\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u4e00\u4e2a\u6574\u6570\uff0c\u8981\u8ba1\u7b97\u65b9\u5dee\u7684\u8f74\u3002 keepdims : \u5e03\u5c14\u503c\uff0c\u662f\u5426\u4fdd\u7559\u539f\u5c3a\u5bf8\u3002 \u5982\u679c keepdims \u4e3a False \uff0c\u5219\u5f20\u91cf\u7684\u79e9\u51cf 1\u3002 \u5982\u679c keepdims \u4e3a True \uff0c\u7f29\u5c0f\u7684\u7ef4\u5ea6\u4fdd\u7559\u4e3a\u957f\u5ea6 1\u3002 \u8fd4\u56de x \u5143\u7d20\u7684\u65b9\u5dee\u7684\u5f20\u91cf\u3002 std keras.backend.std(x, axis=None, keepdims=False) \u5f20\u91cf\u5728\u67d0\u4e00\u6307\u5b9a\u8f74\u7684\u6807\u51c6\u5dee\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u4e00\u4e2a\u6574\u6570\uff0c\u8981\u8ba1\u7b97\u6807\u51c6\u5dee\u7684\u8f74\u3002 keepdims : \u5e03\u5c14\u503c\uff0c\u662f\u5426\u4fdd\u7559\u539f\u5c3a\u5bf8\u3002 \u5982\u679c keepdims \u4e3a False \uff0c\u5219\u5f20\u91cf\u7684\u79e9\u51cf 1\u3002 \u5982\u679c keepdims \u4e3a True \uff0c\u7f29\u5c0f\u7684\u7ef4\u5ea6\u4fdd\u7559\u4e3a\u957f\u5ea6 1\u3002 \u8fd4\u56de x \u5143\u7d20\u7684\u6807\u51c6\u5dee\u7684\u5f20\u91cf\u3002 mean keras.backend.mean(x, axis=None, keepdims=False) \u5f20\u91cf\u5728\u67d0\u4e00\u6307\u5b9a\u8f74\u7684\u5747\u503c\u3002 \u53c2\u6570 x : A tensor or variable. axis : \u6574\u6570\u6216\u5217\u8868\u3002\u9700\u8981\u8ba1\u7b97\u5747\u503c\u7684\u8f74\u3002 keepdims : \u5e03\u5c14\u503c\uff0c\u662f\u5426\u4fdd\u7559\u539f\u5c3a\u5bf8\u3002 \u5982\u679c keepdims \u4e3a False \uff0c\u5219 axis \u4e2d\u6bcf\u4e00\u9879\u7684\u5f20\u91cf\u79e9\u51cf 1\u3002 \u5982\u679c keepdims \u4e3a True \uff0c\u5219\u7f29\u5c0f\u7684\u7ef4\u5ea6\u4fdd\u7559\u4e3a\u957f\u5ea6 1\u3002 \u8fd4\u56de x \u5143\u7d20\u7684\u5747\u503c\u7684\u5f20\u91cf\u3002 any keras.backend.any(x, axis=None, keepdims=False) reduction \u6309\u4f4d\u5f52\u7ea6\uff08\u903b\u8f91 OR\uff09\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u6267\u884c\u5f52\u7ea6\u64cd\u4f5c\u7684\u8f74\u3002 keepdims : \u662f\u5426\u653e\u5f03\u6216\u5e7f\u64ad\u5f52\u7ea6\u7684\u8f74\u3002 \u8fd4\u56de \u4e00\u4e2a uint8 \u5f20\u91cf (0s \u548c 1s)\u3002 all keras.backend.all(x, axis=None, keepdims=False) \u6309\u4f4d\u5f52\u7ea6\uff08\u903b\u8f91 AND\uff09\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u6267\u884c\u5f52\u7ea6\u64cd\u4f5c\u7684\u8f74\u3002 keepdims : \u662f\u5426\u653e\u5f03\u6216\u5e7f\u64ad\u5f52\u7ea6\u7684\u8f74\u3002 \u8fd4\u56de \u4e00\u4e2a uint8 \u5f20\u91cf (0s \u548c 1s)\u3002 argmax keras.backend.argmax(x, axis=-1) \u8fd4\u56de\u6307\u5b9a\u8f74\u7684\u6700\u5927\u503c\u7684\u7d22\u5f15\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u6267\u884c\u5f52\u7ea6\u64cd\u4f5c\u7684\u8f74\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 argmin keras.backend.argmin(x, axis=-1) \u8fd4\u56de\u6307\u5b9a\u8f74\u7684\u6700\u5c0f\u503c\u7684\u7d22\u5f15\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u6267\u884c\u5f52\u7ea6\u64cd\u4f5c\u7684\u8f74\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 square keras.backend.square(x) \u5143\u7d20\u7ea7\u7684\u5e73\u65b9\u64cd\u4f5c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 abs keras.backend.abs(x) \u5143\u7d20\u7ea7\u7684\u7edd\u5bf9\u503c\u64cd\u4f5c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 sqrt keras.backend.sqrt(x) \u5143\u7d20\u7ea7\u7684\u5e73\u65b9\u6839\u64cd\u4f5c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 exp keras.backend.exp(x) \u5143\u7d20\u7ea7\u7684\u6307\u6570\u8fd0\u7b97\u64cd\u4f5c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 log keras.backend.log(x) \u5143\u7d20\u7ea7\u7684\u5bf9\u6570\u8fd0\u7b97\u64cd\u4f5c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 logsumexp keras.backend.logsumexp(x, axis=None, keepdims=False) \u8ba1\u7b97 log(sum(exp(\u5f20\u91cf\u5728\u67d0\u4e00\u8f74\u7684\u5143\u7d20)))\u3002 \u8fd9\u4e2a\u51fd\u6570\u5728\u6570\u503c\u4e0a\u6bd4 log(sum(exp(x))) \u66f4\u7a33\u5b9a\u3002 \u5b83\u907f\u514d\u4e86\u6c42\u5927\u8f93\u5165\u7684\u6307\u6570\u9020\u6210\u7684\u4e0a\u6ea2\uff0c\u4ee5\u53ca\u6c42\u5c0f\u8f93\u5165\u7684\u5bf9\u6570\u9020\u6210\u7684\u4e0b\u6ea2\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u4e00\u4e2a\u6574\u6570\uff0c\u9700\u8981\u5f52\u7ea6\u7684\u8f74\u3002 keepdims : \u5e03\u5c14\u503c\uff0c\u662f\u5426\u4fdd\u7559\u539f\u5c3a\u5bf8\u3002 \u5982\u679c keepdims \u4e3a False \uff0c\u5219\u5f20\u91cf\u7684\u79e9\u51cf 1\u3002 \u5982\u679c keepdims \u4e3a True \uff0c\u7f29\u5c0f\u7684\u7ef4\u5ea6\u4fdd\u7559\u4e3a\u957f\u5ea6 1\u3002 \u8fd4\u56de \u5f52\u7ea6\u540e\u7684\u5f20\u91cf\u3002 round keras.backend.round(x) \u5143\u7d20\u7ea7\u5730\u56db\u820d\u4e94\u5165\u5230\u6700\u63a5\u8fd1\u7684\u6574\u6570\u3002 \u5728\u5e73\u5c40\u7684\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528\u7684\u820d\u5165\u6a21\u5f0f\u662f\u300c\u5076\u6570\u7684\u4e00\u534a\u300d\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 sign keras.backend.sign(x) \u5143\u7d20\u7ea7\u7684\u7b26\u53f7\u8fd0\u7b97\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 pow keras.backend.pow(x, a) \u5143\u7d20\u7ea7\u7684\u6307\u6570\u8fd0\u7b97\u64cd\u4f5c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 a : Python \u6574\u6570\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 clip keras.backend.clip(x, min_value, max_value) \u5143\u7d20\u7ea7\u88c1\u526a\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 min_value : Python \u6d6e\u70b9\u6216\u6574\u6570\u3002 max_value : Python \u6d6e\u70b9\u6216\u6574\u6570\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 equal keras.backend.equal(x, y) \u9010\u4e2a\u5143\u7d20\u5bf9\u6bd4\u4e24\u4e2a\u5f20\u91cf\u7684\u76f8\u7b49\u60c5\u51b5\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 y : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5e03\u5c14\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def equal(x, y): return x == y not_equal keras.backend.not_equal(x, y) \u9010\u4e2a\u5143\u7d20\u5bf9\u6bd4\u4e24\u4e2a\u5f20\u91cf\u7684\u4e0d\u76f8\u7b49\u60c5\u51b5\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 y : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5e03\u5c14\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def not_equal(x, y): return x != y greater keras.backend.greater(x, y) \u9010\u4e2a\u5143\u7d20\u6bd4\u5bf9 (x > y) \u7684\u771f\u503c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 y : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5e03\u5c14\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def greater(x, y): return x > y greater_equal keras.backend.greater_equal(x, y) \u9010\u4e2a\u5143\u7d20\u6bd4\u5bf9 (x >= y) \u7684\u771f\u503c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 y : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5e03\u5c14\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def greater_equal(x, y): return x >= y less keras.backend.less(x, y) \u9010\u4e2a\u5143\u7d20\u6bd4\u5bf9 (x < y) \u7684\u771f\u503c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 y : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5e03\u5c14\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def less(x, y): return x < y less_equal keras.backend.less_equal(x, y) \u9010\u4e2a\u5143\u7d20\u6bd4\u5bf9 (x <= y) \u7684\u771f\u503c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 y : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5e03\u5c14\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def less_equal(x, y): return x <= y maximum keras.backend.maximum(x, y) \u9010\u4e2a\u5143\u7d20\u6bd4\u5bf9\u4e24\u4e2a\u5f20\u91cf\u7684\u6700\u5927\u503c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 y : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def maximum(x, y): return np.maximum(x, y) minimum keras.backend.minimum(x, y) \u9010\u4e2a\u5143\u7d20\u6bd4\u5bf9\u4e24\u4e2a\u5f20\u91cf\u7684\u6700\u5c0f\u503c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 y : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def minimum(x, y): return np.minimum(x, y) sin keras.backend.sin(x) \u9010\u4e2a\u5143\u7d20\u8ba1\u7b97 x \u7684 sin \u503c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 cos keras.backend.cos(x) \u9010\u4e2a\u5143\u7d20\u8ba1\u7b97 x \u7684 cos \u503c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 normalize_batch_in_training keras.backend.normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon=0.001) \u8ba1\u7b97\u6279\u6b21\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee\uff0c\u7136\u540e\u5728\u6279\u6b21\u4e0a\u5e94\u7528\u6279\u6b21\u6807\u51c6\u5316\u3002 \u53c2\u6570 x : \u8f93\u5165\u5f20\u91cf\u6216\u53d8\u91cf\u3002 gamma : \u7528\u4e8e\u7f29\u653e\u8f93\u5165\u7684\u5f20\u91cf\u3002 beta : \u7528\u4e8e\u4e2d\u5fc3\u5316\u8f93\u5165\u7684\u5f20\u91cf\u3002 reduction_axes : \u6574\u6570\u8fed\u4ee3\uff0c\u9700\u8981\u6807\u51c6\u5316\u7684\u8f74\u3002 epsilon : \u6a21\u7cca\u56e0\u5b50\u3002 \u8fd4\u56de \u957f\u5ea6\u4e3a 3 \u4e2a\u5143\u7ec4\uff0c (normalized_tensor, mean, variance) \u3002 batch_normalization keras.backend.batch_normalization(x, mean, var, beta, gamma, epsilon=0.001) \u5728\u7ed9\u5b9a\u7684 mean\uff0cvar\uff0cbeta \u548c gamma \u4e0a\u5e94\u7528\u6279\u91cf\u6807\u51c6\u5316\u3002 \u5373\uff0c\u8fd4\u56de\uff1a output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta \u53c2\u6570 x : \u8f93\u5165\u5f20\u91cf\u6216\u53d8\u91cf\u3002 mean : \u6279\u6b21\u7684\u5747\u503c\u3002 var : \u6279\u6b21\u7684\u65b9\u5dee\u3002 beta : \u7528\u4e8e\u4e2d\u5fc3\u5316\u8f93\u5165\u7684\u5f20\u91cf\u3002 gamma : \u7528\u4e8e\u7f29\u653e\u8f93\u5165\u7684\u5f20\u91cf\u3002 epsilon : \u6a21\u7cca\u56e0\u5b50\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 concatenate keras.backend.concatenate(tensors, axis=-1) \u57fa\u4e8e\u6307\u5b9a\u7684\u8f74\uff0c\u8fde\u63a5\u5f20\u91cf\u7684\u5217\u8868\u3002 \u53c2\u6570 tensors : \u9700\u8981\u8fde\u63a5\u7684\u5f20\u91cf\u5217\u8868\u3002 axis : \u8fde\u63a5\u7684\u8f74\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 reshape keras.backend.reshape(x, shape) \u5c06\u5f20\u91cf\u91cd\u5851\u4e3a\u6307\u5b9a\u7684\u5c3a\u5bf8\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 shape : \u76ee\u6807\u5c3a\u5bf8\u5143\u7ec4\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 permute_dimensions keras.backend.permute_dimensions(x, pattern) \u91cd\u65b0\u6392\u5217\u5f20\u91cf\u7684\u8f74\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 pattern : \u7ef4\u5ea6\u7d22\u5f15\u7684\u5143\u7ec4\uff0c\u4f8b\u5982 (0, 2, 1) \u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 resize_images keras.backend.resize_images(x, height_factor, width_factor, data_format) \u8c03\u6574 4D \u5f20\u91cf\u4e2d\u5305\u542b\u7684\u56fe\u50cf\u7684\u5927\u5c0f\u3002 \u53c2\u6570 x : \u9700\u8981\u8c03\u6574\u7684\u5f20\u91cf\u6216\u53d8\u91cf\u3002 height_factor : \u6b63\u6574\u6570\u3002 width_factor : \u6b63\u6574\u6570\u3002 data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216 \"channels_first\" \u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f \"channels_last\" \u4e5f\u4e0d\u662f \"channels_first\" \u3002 resize_volumes keras.backend.resize_volumes(x, depth_factor, height_factor, width_factor, data_format) \u8c03\u6574 5D \u5f20\u91cf\u4e2d\u5305\u542b\u7684\u4f53\u79ef\u3002 \u53c2\u6570 x : \u9700\u8981\u8c03\u6574\u7684\u5f20\u91cf\u6216\u53d8\u91cf\u3002 depth_factor : \u6b63\u6574\u6570\u3002 height_factor : \u6b63\u6574\u6570\u3002 width_factor : \u6b63\u6574\u6570\u3002 data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216 \"channels_first\" \u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f \"channels_last\" \u4e5f\u4e0d\u662f \"channels_first\" \u3002 repeat_elements keras.backend.repeat_elements(x, rep, axis) \u6cbf\u67d0\u4e00\u8f74\u91cd\u590d\u5f20\u91cf\u7684\u5143\u7d20\uff0c\u5982 np.repeat \u3002 \u5982\u679c x \u7684\u5c3a\u5bf8\u4e3a (s1\uff0cs2\uff0cs3) \u800c axis \u4e3a 1 \uff0c \u5219\u8f93\u51fa\u5c3a\u5bf8\u4e3a (s1\uff0cs2 * rep\uff0cs3\uff09 \u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 rep : Python \u6574\u6570\uff0c\u91cd\u590d\u6b21\u6570\u3002 axis : \u9700\u8981\u91cd\u590d\u7684\u8f74\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 repeat keras.backend.repeat(x, n) \u91cd\u590d\u4e00\u4e2a 2D \u5f20\u91cf\u3002 \u5982\u679c x \u7684\u5c3a\u5bf8\u4e3a (samples, dim) \u5e76\u4e14 n \u4e3a 2 \uff0c \u5219\u8f93\u51fa\u7684\u5c3a\u5bf8\u4e3a (samples, 2, dim) \u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 n : Python \u6574\u6570\uff0c\u91cd\u590d\u6b21\u6570\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 arange keras.backend.arange(start, stop=None, step=1, dtype='int32') \u521b\u5efa\u4e00\u4e2a\u5305\u542b\u6574\u6570\u5e8f\u5217\u7684 1D \u5f20\u91cf\u3002 \u8be5\u51fd\u6570\u53c2\u6570\u4e0e Theano \u7684 arange \u51fd\u6570\u7684\u7ea6\u5b9a\u76f8\u540c\uff1a \u5982\u679c\u53ea\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53c2\u6570\uff0c\u90a3\u5b83\u5c31\u662f stop \u53c2\u6570\u3002 \u8fd4\u56de\u7684\u5f20\u91cf\u7684\u9ed8\u8ba4\u7c7b\u578b\u662f int32 \uff0c\u4ee5\u5339\u914d TensorFlow \u7684\u9ed8\u8ba4\u503c\u3002 \u53c2\u6570 start : \u8d77\u59cb\u503c\u3002 stop : \u7ed3\u675f\u503c\u3002 step : \u4e24\u4e2a\u8fde\u7eed\u503c\u4e4b\u95f4\u7684\u5dee\u3002 dtype : \u8981\u4f7f\u7528\u7684\u6574\u6570\u7c7b\u578b\u3002 \u8fd4\u56de \u4e00\u4e2a\u6574\u6570\u5f20\u91cf\u3002 tile keras.backend.tile(x, n) \u521b\u5efa\u4e00\u4e2a\u7528 n \u5e73\u94fa \u7684 x \u5f20\u91cf\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 n : \u6574\u6570\u5217\u8868\u3002\u957f\u5ea6\u5fc5\u987b\u4e0e x \u4e2d\u7684\u7ef4\u6570\u76f8\u540c\u3002 \u8fd4\u56de \u4e00\u4e2a\u5e73\u94fa\u7684\u5f20\u91cf\u3002 flatten keras.backend.flatten(x) \u5c55\u5e73\u4e00\u4e2a\u5f20\u91cf\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u91cd\u65b0\u8c03\u6574\u4e3a 1D \u7684\u5f20\u91cf\u3002 batch_flatten keras.backend.batch_flatten(x) \u5c06\u4e00\u4e2a nD \u5f20\u91cf\u53d8\u6210\u4e00\u4e2a \u7b2c 0 \u7ef4\u76f8\u540c\u7684 2D \u5f20\u91cf\u3002 \u6362\u53e5\u8bdd\u8bf4\uff0c\u5b83\u5c06\u6279\u6b21\u4e2d\u7684\u6bcf\u4e00\u4e2a\u6837\u672c\u5c55\u5e73\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 expand_dims keras.backend.expand_dims(x, axis=-1) \u5728\u7d22\u5f15 axis \u8f74\uff0c\u6dfb\u52a0 1 \u4e2a\u5c3a\u5bf8\u7684\u7ef4\u5ea6\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u9700\u8981\u6dfb\u52a0\u65b0\u7684\u8f74\u7684\u4f4d\u7f6e\u3002 \u8fd4\u56de \u4e00\u4e2a\u6269\u5c55\u7ef4\u5ea6\u7684\u8f74\u3002 squeeze keras.backend.squeeze(x, axis) \u5728\u7d22\u5f15 axis \u8f74\uff0c\u79fb\u9664 1 \u4e2a\u5c3a\u5bf8\u7684\u7ef4\u5ea6\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u9700\u8981\u4e22\u5f03\u7684\u8f74\u3002 \u8fd4\u56de \u4e00\u4e2a\u4e0e x \u6570\u636e\u76f8\u540c\u4f46\u7ef4\u5ea6\u964d\u4f4e\u7684\u5f20\u91cf\u3002 temporal_padding keras.backend.temporal_padding(x, padding=(1, 1)) \u586b\u5145 3D \u5f20\u91cf\u7684\u4e2d\u95f4\u7ef4\u5ea6\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 padding : 2 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\uff0c\u5728\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\u7684\u5f00\u59cb\u548c\u7ed3\u675f\u5904\u6dfb\u52a0\u591a\u5c11\u4e2a\u96f6\u3002 \u8fd4\u56de \u4e00\u4e2a\u586b\u5145\u7684 3D \u5f20\u91cf\u3002 spatial_2d_padding keras.backend.spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None) \u586b\u5145 4D \u5f20\u91cf\u7684\u7b2c\u4e8c\u7ef4\u548c\u7b2c\u4e09\u7ef4\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 padding : 2 \u5143\u7ec4\u7684\u5143\u7ec4\uff0c\u586b\u5145\u6a21\u5f0f\u3002 data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216 \"channels_first\" \u3002 \u8fd4\u56de \u4e00\u4e2a\u586b\u5145\u7684 4D \u5f20\u91cf\u3002 \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f \"channels_last\" \u4e5f\u4e0d\u662f \"channels_first\" \u3002 spatial_3d_padding keras.backend.spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None) \u6cbf\u7740\u6df1\u5ea6\u3001\u9ad8\u5ea6\u5bbd\u5ea6\u4e09\u4e2a\u7ef4\u5ea6\u586b\u5145 5D \u5f20\u91cf\u3002 \u5206\u522b\u4f7f\u7528 \"padding[0]\", \"padding[1]\" \u548c \"padding[2]\" \u6765\u5de6\u53f3\u586b\u5145\u8fd9\u4e9b\u7ef4\u5ea6\u3002 \u5bf9\u4e8e 'channels_last' \u6570\u636e\u683c\u5f0f\uff0c \u7b2c 2\u30013\u30014 \u7ef4\u5c06\u88ab\u586b\u5145\u3002 \u5bf9\u4e8e 'channels_first' \u6570\u636e\u683c\u5f0f\uff0c \u7b2c 3\u30014\u30015 \u7ef4\u5c06\u88ab\u586b\u5145\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 padding : 3 \u5143\u7ec4\u7684\u5143\u7ec4\uff0c\u586b\u5145\u6a21\u5f0f\u3002 data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216 \"channels_first\" \u3002 \u8fd4\u56de \u4e00\u4e2a\u586b\u5145\u7684 5D \u5f20\u91cf\u3002 \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f \"channels_last\" \u4e5f\u4e0d\u662f \"channels_first\" \u3002 stack keras.backend.stack(x, axis=0) \u5c06\u79e9 \u4e3a R \u7684\u5f20\u91cf\u5217\u8868\u5806\u53e0\u6210\u79e9\u4e3a R + 1 \u7684\u5f20\u91cf\u3002 \u53c2\u6570 x : \u5f20\u91cf\u5217\u8868\u3002 axis : \u9700\u8981\u6267\u884c\u5806\u53e0\u7684\u8f74\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 one_hot keras.backend.one_hot(indices, num_classes) \u8ba1\u7b97\u4e00\u4e2a\u6574\u6570\u5f20\u91cf\u7684 one-hot \u8868\u793a\u3002 \u53c2\u6570 indices : nD \u6574\u6570\uff0c\u5c3a\u5bf8\u4e3a (batch_size, dim1, dim2, ... dim(n-1)) num_classes : \u6574\u6570\uff0c\u9700\u8981\u8003\u8651\u7684\u7c7b\u522b\u6570\u3002 \u8fd4\u56de \u8f93\u5165\u7684 (n + 1)D one-hot \u8868\u793a\uff0c \u5c3a\u5bf8\u4e3a (batch_size, dim1, dim2, ... dim(n-1), num_classes) \u3002 reverse keras.backend.reverse(x, axes) \u6cbf\u6307\u5b9a\u7684\u8f74\u53cd\u8f6c\u5f20\u91cf\u3002 \u53c2\u6570 x : \u9700\u8981\u53cd\u8f6c\u7684\u5f20\u91cf\u3002 axes : \u6574\u6570\u6216\u6574\u6570\u8fed\u4ee3\u3002\u9700\u8981\u53cd\u8f6c\u7684\u8f74\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def reverse(x, axes): if isinstance(axes, list): axes = tuple(axes) return np.flip(x, axes) slice keras.backend.slice(x, start, size) \u4ece\u5f20\u91cf\u4e2d\u63d0\u53d6\u4e00\u4e2a\u5207\u7247\u3002 \u53c2\u6570 x : \u8f93\u5165\u5f20\u91cf\u3002 start : \u6574\u6570\u5217\u8868/\u5143\u7ec4\uff0c\u8868\u660e\u6bcf\u4e2a\u8f74\u7684\u8d77\u59cb\u5207\u7247\u7d22\u5f15\u4f4d\u7f6e\u3002 size : \u6574\u6570\u5217\u8868/\u5143\u7ec4\uff0c\u8868\u660e\u6bcf\u4e2a\u8f74\u4e0a\u5207\u7247\u591a\u5c11\u7ef4\u5ea6\u3002 \u8fd4\u56de \u4e00\u4e2a\u5207\u7247\u5f20\u91cf\uff1a new_x = x[start[0]: start[0] + size[0], ..., start[-1]: start[-1] + size[-1]] get_value keras.backend.get_value(x) \u8fd4\u56de\u4e00\u4e2a\u53d8\u91cf\u7684\u503c\u3002 \u53c2\u6570 x : \u8f93\u5165\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a Numpy \u6570\u7ec4\u3002 batch_get_value keras.backend.batch_get_value(ops) \u8fd4\u56de\u591a\u4e2a\u5f20\u91cf\u53d8\u91cf\u7684\u503c\u3002 \u53c2\u6570 ops : \u8981\u8fd0\u884c\u7684\u64cd\u4f5c\u5217\u8868\u3002 \u8fd4\u56de \u4e00\u4e2a Numpy \u6570\u7ec4\u7684\u5217\u8868\u3002 set_value keras.backend.set_value(x, value) \u4f7f\u7528 Numpy \u6570\u7ec4\u8bbe\u7f6e\u53d8\u91cf\u7684\u503c\u3002 \u53c2\u6570 x : \u9700\u8981\u8bbe\u7f6e\u65b0\u503c\u7684\u5f20\u91cf\u3002 value : \u9700\u8981\u8bbe\u7f6e\u7684\u503c\uff0c \u4e00\u4e2a\u5c3a\u5bf8\u76f8\u540c\u7684 Numpy \u6570\u7ec4\u3002 batch_set_value keras.backend.batch_set_value(tuples) \u4e00\u6b21\u8bbe\u7f6e\u591a\u4e2a\u5f20\u91cf\u53d8\u91cf\u7684\u503c\u3002 \u53c2\u6570 tuples : \u5143\u7ec4 (tensor, value) \u7684\u5217\u8868\u3002 value \u5e94\u8be5\u662f\u4e00\u4e2a Numpy \u6570\u7ec4\u3002 print_tensor keras.backend.print_tensor(x, message='') \u5728\u8bc4\u4f30\u65f6\u6253\u5370 message \u548c\u5f20\u91cf\u7684\u503c\u3002 \u8bf7\u6ce8\u610f\uff0c print_tensor \u8fd4\u56de\u4e00\u4e2a\u4e0e x \u76f8\u540c\u7684\u65b0\u5f20\u91cf\uff0c\u5e94\u8be5\u5728\u540e\u9762\u7684\u4ee3\u7801\u4e2d\u4f7f\u7528\u5b83\u3002\u5426\u5219\u5728\u8bc4\u4f30\u8fc7\u7a0b\u4e2d\u4e0d\u4f1a\u8003\u8651\u6253\u5370\u64cd\u4f5c\u3002 \u4f8b\u5b50 >>> x = K.print_tensor(x, message=\"x is: \") \u53c2\u6570 x : \u9700\u8981\u6253\u5370\u7684\u5f20\u91cf\u3002 message : \u9700\u8981\u4e0e\u5f20\u91cf\u4e00\u8d77\u6253\u5370\u7684\u6d88\u606f\u3002 \u8fd4\u56de \u540c\u4e00\u4e2a\u4e0d\u53d8\u7684\u5f20\u91cf x \u3002 function keras.backend.function(inputs, outputs, updates=None) \u5b9e\u4f8b\u5316 Keras \u51fd\u6570\u3002 \u53c2\u6570 inputs : \u5360\u4f4d\u7b26\u5f20\u91cf\u5217\u8868\u3002 outputs : \u8f93\u51fa\u5f20\u91cf\u5217\u8868\u3002 updates : \u66f4\u65b0\u64cd\u4f5c\u5217\u8868\u3002 **kwargs : \u9700\u8981\u4f20\u9012\u7ed9 tf.Session.run \u7684\u53c2\u6570\u3002 \u8fd4\u56de \u8f93\u51fa\u503c\u4e3a Numpy \u6570\u7ec4\u3002 \u5f02\u5e38 ValueError : \u5982\u679c\u65e0\u6548\u7684 kwargs \u88ab\u4f20\u5165\u3002 gradients keras.backend.gradients(loss, variables) \u8fd4\u56de variables \u5728 loss \u4e0a\u7684\u68af\u5ea6\u3002 \u53c2\u6570 loss : \u9700\u8981\u6700\u5c0f\u5316\u7684\u6807\u91cf\u5f20\u91cf\u3002 variables : \u53d8\u91cf\u5217\u8868\u3002 \u8fd4\u56de \u4e00\u4e2a\u68af\u5ea6\u5f20\u91cf\u3002 stop_gradient keras.backend.stop_gradient(variables) \u8fd4\u56de variables \uff0c\u4f46\u662f\u5bf9\u4e8e\u5176\u4ed6\u53d8\u91cf\uff0c\u5176\u68af\u5ea6\u4e3a\u96f6\u3002 \u53c2\u6570 variables : \u9700\u8981\u8003\u8651\u7684\u5f20\u91cf\u6216\u5f20\u91cf\u5217\u8868\uff0c\u4efb\u4f55\u7684\u5176\u4ed6\u53d8\u91cf\u4fdd\u6301\u4e0d\u53d8\u3002 \u8fd4\u56de \u5355\u4e2a\u5f20\u91cf\u6216\u5f20\u91cf\u5217\u8868\uff08\u53d6\u51b3\u4e8e\u4f20\u9012\u7684\u53c2\u6570\uff09\uff0c \u4e0e\u4efb\u4f55\u5176\u4ed6\u53d8\u91cf\u5177\u6709\u6052\u5b9a\u7684\u68af\u5ea6\u3002 rnn keras.backend.rnn(step_function, inputs, initial_states, go_backwards=False, mask=None, constants=None, unroll=False, input_length=None) \u5728\u5f20\u91cf\u7684\u65f6\u95f4\u7ef4\u5ea6\u8fed\u4ee3\u3002 \u53c2\u6570 step_function : RNN \u6b65\u9aa4\u51fd\u6570\uff0c inputs : \u5c3a\u5bf8\u4e3a (samples, ...) \u7684\u5f20\u91cf (\u4e0d\u542b\u65f6\u95f4\u7ef4\u5ea6), \u8868\u793a\u6279\u6b21\u6837\u54c1\u5728\u67d0\u4e2a\u65f6\u95f4\u6b65\u7684\u8f93\u5165\u3002 states : \u5f20\u91cf\u5217\u8868\u3002 outputs : \u5c3a\u5bf8\u4e3a (samples, output_dim) \u7684\u5f20\u91cf (\u4e0d\u542b\u65f6\u95f4\u7ef4\u5ea6) new_states : \u5f20\u91cf\u5217\u8868\uff0c\u4e0e states \u957f\u5ea6\u548c\u5c3a\u5bf8\u76f8\u540c\u3002 \u5217\u8868\u4e2d\u7684\u7b2c\u4e00\u4e2a\u72b6\u6001\u5fc5\u987b\u662f\u524d\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u8f93\u51fa\u5f20\u91cf\u3002 inputs : \u65f6\u5e8f\u6570\u636e\u5f20\u91cf (samples, time, ...) (\u6700\u5c11 3D)\u3002 initial_states : \u5c3a\u5bf8\u4e3a (samples, output_dim) \u7684\u5f20\u91cf (\u4e0d\u542b\u65f6\u95f4\u7ef4\u5ea6)\uff0c\u5305\u542b\u6b65\u9aa4\u51fd\u6570\u4e2d\u4f7f\u7528\u7684\u72b6\u6001\u7684\u521d\u59cb\u503c\u3002 go_backwards : \u5e03\u5c14\u503c\u3002\u5982\u679c\u4e3a True\uff0c\u4ee5\u76f8\u53cd\u7684\u987a\u5e8f\u5728\u65f6\u95f4\u7ef4\u4e0a\u8fdb\u884c\u8fed\u4ee3\u5e76\u8fd4\u56de\u76f8\u53cd\u7684\u5e8f\u5217\u3002 mask : \u5c3a\u5bf8\u4e3a (samples, time, 1) \u7684\u4e8c\u8fdb\u5236\u5f20\u91cf\uff0c\u5bf9\u4e8e\u88ab\u5c4f\u853d\u7684\u6bcf\u4e2a\u5143\u7d20\u90fd\u4e3a\u96f6\u3002 constants : \u6bcf\u4e2a\u6b65\u9aa4\u4f20\u9012\u7684\u5e38\u91cf\u503c\u5217\u8868\u3002 unroll : \u662f\u5426\u5c55\u5f00 RNN \u6216\u4f7f\u7528\u7b26\u53f7\u5faa\u73af\uff08\u4f9d\u8d56\u4e8e\u540e\u7aef\u7684 while_loop \u6216 scan \uff09\u3002 input_length : \u4e0e TensorFlow \u5b9e\u73b0\u4e0d\u76f8\u5173\u3002\u5982\u679c\u4f7f\u7528 Theano \u5c55\u5f00\uff0c\u5219\u5fc5\u987b\u6307\u5b9a\u3002 \u8fd4\u56de \u4e00\u4e2a\u5143\u7ec4\uff0c (last_output, outputs, new_states) \u3002 last_output : rnn \u7684\u6700\u540e\u8f93\u51fa\uff0c\u5c3a\u5bf8\u4e3a (samples, ...) \u3002 outputs : \u5c3a\u5bf8\u4e3a (samples, time, ...) \u7684\u5f20\u91cf\uff0c\u5176\u4e2d \u6bcf\u4e00\u9879 outputs[s, t] \u662f\u6837\u672c s \u5728\u65f6\u95f4 t \u7684\u6b65\u9aa4\u51fd\u6570\u8f93\u51fa\u503c\u3002 new_states : \u5f20\u91cf\u5217\u8868\uff0c\u6709\u6b65\u9aa4\u51fd\u6570\u8fd4\u56de\u7684\u6700\u540e\u72b6\u6001\uff0c \u5c3a\u5bf8\u4e3a (samples, ...) \u3002 \u5f02\u5e38 ValueError : \u5982\u679c\u8f93\u5165\u7684\u7ef4\u5ea6\u5c0f\u4e8e 3\u3002 ValueError : \u5982\u679c unroll \u4e3a True \u4f46\u8f93\u5165\u65f6\u95f4\u6b65\u5e76\u4e0d\u662f\u56fa\u5b9a\u7684\u6570\u5b57\u3002 ValueError : \u5982\u679c\u63d0\u4f9b\u4e86 mask (\u975e None ) \u4f46\u672a\u63d0\u4f9b states ( len(states) == 0)\u3002 switch keras.backend.switch(condition, then_expression, else_expression) \u6839\u636e\u4e00\u4e2a\u6807\u91cf\u503c\u5728\u4e24\u4e2a\u64cd\u4f5c\u4e4b\u95f4\u5207\u6362\u3002 \u8bf7\u6ce8\u610f\uff0c then_expression \u548c else_expression \u90fd\u5e94\u8be5\u662f \u76f8\u540c\u5c3a\u5bf8 \u7684\u7b26\u53f7\u5f20\u91cf\u3002 \u53c2\u6570 condition : \u5f20\u91cf ( int \u6216 bool )\u3002 then_expression : \u5f20\u91cf\u6216\u8fd4\u56de\u5f20\u91cf\u7684\u53ef\u8c03\u7528\u51fd\u6570\u3002 else_expression : \u5f20\u91cf\u6216\u8fd4\u56de\u5f20\u91cf\u7684\u53ef\u8c03\u7528\u51fd\u6570\u3002 \u8fd4\u56de \u9009\u62e9\u7684\u5f20\u91cf\u3002 \u5f02\u5e38 ValueError : \u5982\u679c condition \u7684\u79e9\u5927\u4e8e\u4e24\u4e2a\u8868\u8fbe\u5f0f\u7684\u79e9\u5e8f\u3002 in_train_phase keras.backend.in_train_phase(x, alt, training=None) \u5728\u8bad\u7ec3\u9636\u6bb5\u9009\u62e9 x \uff0c\u5176\u4ed6\u9636\u6bb5\u9009\u62e9 alt \u3002 \u8bf7\u6ce8\u610f alt \u5e94\u8be5\u4e0e x \u5c3a\u5bf8\u76f8\u540c\u3002 \u53c2\u6570 x : \u5728\u8bad\u7ec3\u9636\u6bb5\u9700\u8981\u8fd4\u56de\u7684 x (\u5f20\u91cf\u6216\u8fd4\u56de\u5f20\u91cf\u7684\u53ef\u8c03\u7528\u51fd\u6570)\u3002 alt : \u5728\u5176\u4ed6\u9636\u6bb5\u9700\u8981\u8fd4\u56de\u7684 alt (\u5f20\u91cf\u6216\u8fd4\u56de\u5f20\u91cf\u7684\u53ef\u8c03\u7528\u51fd\u6570)\u3002 training : \u53ef\u9009\u7684\u6807\u91cf\u5f20\u91cf (\u6216 Python \u5e03\u5c14\u503c\uff0c\u6216\u8005 Python \u6574\u6570)\uff0c \u4ee5\u6307\u5b9a\u5b66\u4e60\u9636\u6bb5\u3002 \u8fd4\u56de \u57fa\u4e8e training \u6807\u5fd7\uff0c\u8981\u4e48\u8fd4\u56de x \uff0c\u8981\u4e48\u8fd4\u56de alt \u3002 training \u6807\u5fd7\u9ed8\u8ba4\u4e3a K.learning_phase() \u3002 in_test_phase keras.backend.in_test_phase(x, alt, training=None) \u5728\u6d4b\u8bd5\u9636\u6bb5\u9009\u62e9 x \uff0c\u5176\u4ed6\u9636\u6bb5\u9009\u62e9 alt \u3002 \u8bf7\u6ce8\u610f alt \u5e94\u8be5\u4e0e x \u5c3a\u5bf8\u76f8\u540c\u3002 \u53c2\u6570 x : \u5728\u8bad\u7ec3\u9636\u6bb5\u9700\u8981\u8fd4\u56de\u7684 x (\u5f20\u91cf\u6216\u8fd4\u56de\u5f20\u91cf\u7684\u53ef\u8c03\u7528\u51fd\u6570)\u3002 alt : \u5728\u5176\u4ed6\u9636\u6bb5\u9700\u8981\u8fd4\u56de\u7684 alt (\u5f20\u91cf\u6216\u8fd4\u56de\u5f20\u91cf\u7684\u53ef\u8c03\u7528\u51fd\u6570)\u3002 training : \u53ef\u9009\u7684\u6807\u91cf\u5f20\u91cf (\u6216 Python \u5e03\u5c14\u503c\uff0c\u6216\u8005 Python \u6574\u6570)\uff0c \u4ee5\u6307\u5b9a\u5b66\u4e60\u9636\u6bb5\u3002 \u8fd4\u56de \u57fa\u4e8e K.learning_phase \uff0c\u8981\u4e48\u8fd4\u56de x \uff0c\u8981\u4e48\u8fd4\u56de alt \u3002 relu keras.backend.relu(x, alpha=0.0, max_value=None) ReLU \u6574\u6d41\u7ebf\u6027\u5355\u4f4d\u3002 \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u5b83\u8fd4\u56de\u9010\u4e2a\u5143\u7d20\u7684 max(x, 0) \u503c\u3002 \u53c2\u6570 x : \u4e00\u4e2a\u5f20\u91cf\u6216\u53d8\u91cf\u3002 alpha : \u4e00\u4e2a\u6807\u91cf\uff0c\u8d1f\u6570\u90e8\u5206\u7684\u659c\u7387\uff08\u9ed8\u8ba4\u4e3a 0. \uff09\u3002 max_value : \u9971\u548c\u5ea6\u9608\u503c\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def relu(x, alpha=0., max_value=None, threshold=0.): if max_value is None: max_value = np.inf above_threshold = x * (x >= threshold) above_threshold = np.clip(above_threshold, 0.0, max_value) below_threshold = alpha * (x - threshold) * (x < threshold) return below_threshold + above_threshold elu keras.backend.elu(x, alpha=1.0) \u6307\u6570\u7ebf\u6027\u5355\u5143\u3002 \u53c2\u6570 x : \u7528\u4e8e\u8ba1\u7b97\u6fc0\u6d3b\u51fd\u6570\u7684\u5f20\u91cf\u6216\u53d8\u91cf\u3002 alpha : \u4e00\u4e2a\u6807\u91cf\uff0c\u8d1f\u6570\u90e8\u5206\u7684\u659c\u7387\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def elu(x, alpha=1.): return x * (x > 0) + alpha * (np.exp(x) - 1.) * (x < 0) softmax keras.backend.softmax(x) \u5f20\u91cf\u7684 Softmax \u503c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def softmax(x, axis=-1): y = np.exp(x - np.max(x, axis, keepdims=True)) return y / np.sum(y, axis, keepdims=True) softplus keras.backend.softplus(x) \u5f20\u91cf\u7684 Softplus \u503c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def softplus(x): return np.log(1. + np.exp(x)) softsign keras.backend.softsign(x) \u5f20\u91cf\u7684 Softsign \u503c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 categorical_crossentropy keras.backend.categorical_crossentropy(target, output, from_logits=False) \u8f93\u51fa\u5f20\u91cf\u4e0e\u76ee\u6807\u5f20\u91cf\u4e4b\u95f4\u7684\u5206\u7c7b\u4ea4\u53c9\u71b5\u3002 \u53c2\u6570 target : \u4e0e output \u5c3a\u5bf8\u76f8\u540c\u7684\u5f20\u91cf\u3002 output : \u7531 softmax \u4ea7\u751f\u7684\u5f20\u91cf (\u9664\u975e from_logits \u4e3a True\uff0c \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b output \u5e94\u8be5\u662f\u5bf9\u6570\u5f62\u5f0f)\u3002 from_logits : \u5e03\u5c14\u503c\uff0c output \u662f softmax \u7684\u7ed3\u679c\uff0c \u8fd8\u662f\u5bf9\u6570\u5f62\u5f0f\u7684\u5f20\u91cf\u3002 \u8fd4\u56de \u8f93\u51fa\u5f20\u91cf\u3002 sparse_categorical_crossentropy keras.backend.sparse_categorical_crossentropy(target, output, from_logits=False) \u7a00\u758f\u8868\u793a\u7684\u6574\u6570\u503c\u76ee\u6807\u7684\u5206\u7c7b\u4ea4\u53c9\u71b5\u3002 \u53c2\u6570 target : \u4e00\u4e2a\u6574\u6570\u5f20\u91cf\u3002 output : \u7531 softmax \u4ea7\u751f\u7684\u5f20\u91cf (\u9664\u975e from_logits \u4e3a True\uff0c \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b output \u5e94\u8be5\u662f\u5bf9\u6570\u5f62\u5f0f)\u3002 from_logits : \u5e03\u5c14\u503c\uff0c output \u662f softmax \u7684\u7ed3\u679c\uff0c \u8fd8\u662f\u5bf9\u6570\u5f62\u5f0f\u7684\u5f20\u91cf\u3002 \u8fd4\u56de \u8f93\u51fa\u5f20\u91cf\u3002 binary_crossentropy keras.backend.binary_crossentropy(target, output, from_logits=False) \u8f93\u51fa\u5f20\u91cf\u4e0e\u76ee\u6807\u5f20\u91cf\u4e4b\u95f4\u7684\u4e8c\u8fdb\u5236\u4ea4\u53c9\u71b5\u3002 \u53c2\u6570 target : \u4e0e output \u5c3a\u5bf8\u76f8\u540c\u7684\u5f20\u91cf\u3002 output : \u4e00\u4e2a\u5f20\u91cf\u3002 from_logits : output \u662f\u5426\u662f\u5bf9\u6570\u5f20\u91cf\u3002 \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u8ba4\u4e3a output \u7f16\u7801\u4e86\u6982\u7387\u5206\u5e03\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 sigmoid keras.backend.sigmoid(x) \u9010\u4e2a\u5143\u7d20\u6c42 sigmoid \u503c\u3002 \u53c2\u6570 x : \u4e00\u4e2a\u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def sigmoid(x): return 1. / (1. + np.exp(-x)) hard_sigmoid keras.backend.hard_sigmoid(x) \u5206\u6bb5\u7684 sigmoid \u7ebf\u6027\u8fd1\u4f3c\u3002\u901f\u5ea6\u6bd4 sigmoid \u66f4\u5feb\u3002 \u5982\u679c x < -2.5 \uff0c\u8fd4\u56de 0 \u3002 \u5982\u679c x > 2.5 \uff0c\u8fd4\u56de 1 \u3002 \u5982\u679c -2.5 <= x <= 2.5 \uff0c\u8fd4\u56de 0.2 * x + 0.5 \u3002 \u53c2\u6570 x : \u4e00\u4e2a\u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def hard_sigmoid(x): y = 0.2 * x + 0.5 return np.clip(y, 0, 1) tanh keras.backend.tanh(x) \u9010\u4e2a\u5143\u7d20\u6c42 tanh \u503c\u3002 \u53c2\u6570 x : \u4e00\u4e2a\u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def tanh(x): return np.tanh(x) dropout keras.backend.dropout(x, level, noise_shape=None, seed=None) \u5c06 x \u4e2d\u7684\u67d0\u4e9b\u9879\u968f\u673a\u8bbe\u7f6e\u4e3a\u96f6\uff0c\u540c\u65f6\u7f29\u653e\u6574\u4e2a\u5f20\u91cf\u3002 \u53c2\u6570 x : \u5f20\u91cf level : \u5f20\u91cf\u4e2d\u5c06\u88ab\u8bbe\u7f6e\u4e3a 0 \u7684\u9879\u7684\u6bd4\u4f8b\u3002 noise_shape : \u968f\u673a\u751f\u6210\u7684 \u4fdd\u7559/\u4e22\u5f03 \u6807\u5fd7\u7684\u5c3a\u5bf8\uff0c \u5fc5\u987b\u53ef\u4ee5\u5e7f\u64ad\u5230 x \u7684\u5c3a\u5bf8\u3002 seed : \u4fdd\u8bc1\u786e\u5b9a\u6027\u7684\u968f\u673a\u79cd\u5b50\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 l2_normalize keras.backend.l2_normalize(x, axis=None) \u5728\u6307\u5b9a\u7684\u8f74\u4f7f\u7528 L2 \u8303\u5f0f \u6807\u51c6\u5316\u4e00\u4e2a\u5f20\u91cf\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u9700\u8981\u6267\u884c\u6807\u51c6\u5316\u7684\u8f74\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def l2_normalize(x, axis=-1): y = np.max(np.sum(x ** 2, axis, keepdims=True), axis, keepdims=True) return x / np.sqrt(y) in_top_k keras.backend.in_top_k(predictions, targets, k) \u5224\u65ad targets \u662f\u5426\u5728 predictions \u7684\u524d k \u4e2a\u4e2d\u3002 \u53c2\u6570 predictions : \u4e00\u4e2a\u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch_size, classes) \uff0c\u7c7b\u578b\u4e3a float32 \u3002 targets : \u4e00\u4e2a 1D \u5f20\u91cf\uff0c\u957f\u5ea6\u4e3a batch_size \uff0c\u7c7b\u578b\u4e3a int32 \u6216 int64 \u3002 k : \u4e00\u4e2a int \uff0c\u8981\u8003\u8651\u7684\u9876\u90e8\u5143\u7d20\u7684\u6570\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a 1D \u5f20\u91cf\uff0c\u957f\u5ea6\u4e3a batch_size \uff0c\u7c7b\u578b\u4e3a bool \u3002 \u5982\u679c predictions[i, targets[i]] \u5728 predictions[i] \u7684 top- k \u503c\u4e2d\uff0c \u5219 output[i] \u4e3a True \u3002 conv1d keras.backend.conv1d(x, kernel, strides=1, padding='valid', data_format=None, dilation_rate=1) 1D \u5377\u79ef\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 kernel : \u6838\u5f20\u91cf\u3002 strides : \u6b65\u957f\u6574\u578b\u3002 padding : \u5b57\u7b26\u4e32\uff0c \"same\" , \"causal\" \u6216 \"valid\" \u3002 data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216 \"channels_first\" \u3002 dilation_rate : \u6574\u6570\u81a8\u80c0\u7387\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\uff0c1D \u5377\u79ef\u7ed3\u679c\u3002 \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f channels_last \u4e5f\u4e0d\u662f channels_first \u3002 conv2d keras.backend.conv2d(x, kernel, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1)) 2D \u5377\u79ef\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 kernel : \u6838\u5f20\u91cf\u3002 strides : \u6b65\u957f\u5143\u7ec4\u3002 padding : \u5b57\u7b26\u4e32\uff0c \"same\" \u6216 \"valid\" \u3002 data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216 \"channels_first\" \u3002 \u5bf9\u4e8e\u8f93\u5165/\u5377\u79ef\u6838/\u8f93\u51fa\uff0c\u662f\u5426\u4f7f\u7528 Theano \u6216 TensorFlow/CNTK\u6570\u636e\u683c\u5f0f\u3002 dilation_rate : 2 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\uff0c2D \u5377\u79ef\u7ed3\u679c\u3002 \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f channels_last \u4e5f\u4e0d\u662f channels_first \u3002 conv2d_transpose keras.backend.conv2d_transpose(x, kernel, output_shape, strides=(1, 1), padding='valid', data_format=None) 2D \u53cd\u5377\u79ef (\u5373\u8f6c\u7f6e\u5377\u79ef)\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 kernel : \u6838\u5f20\u91cf\u3002 output_shape : \u8868\u793a\u8f93\u51fa\u5c3a\u5bf8\u7684 1D \u6574\u578b\u5f20\u91cf\u3002 strides : \u6b65\u957f\u5143\u7ec4\u3002 padding : \u5b57\u7b26\u4e32\uff0c \"same\" \u6216 \"valid\" \u3002 data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216 \"channels_first\" \u3002 \u5bf9\u4e8e\u8f93\u5165/\u5377\u79ef\u6838/\u8f93\u51fa\uff0c\u662f\u5426\u4f7f\u7528 Theano \u6216 TensorFlow/CNTK\u6570\u636e\u683c\u5f0f\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\uff0c\u8f6c\u7f6e\u7684 2D \u5377\u79ef\u7684\u7ed3\u679c\u3002 \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f channels_last \u4e5f\u4e0d\u662f channels_first \u3002 separable_conv1d keras.backend.separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1, padding='valid', data_format=None, dilation_rate=1) \u5e26\u53ef\u5206\u79bb\u6ee4\u6ce2\u5668\u7684 1D \u5377\u79ef\u3002 \u53c2\u6570 x : \u8f93\u5165\u5f20\u91cf\u3002 depthwise_kernel : \u7528\u4e8e\u6df1\u5ea6\u5377\u79ef\u7684\u5377\u79ef\u6838\u3002 pointwise_kernel : 1x1 \u5377\u79ef\u6838\u3002 strides : \u6b65\u957f\u6574\u6570\u3002 padding : \u5b57\u7b26\u4e32\uff0c \"same\" \u6216 \"valid\" \u3002 data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216 \"channels_first\" \u3002 dilation_rate : \u6574\u6570\u81a8\u80c0\u7387\u3002 \u8fd4\u56de \u8f93\u51fa\u5f20\u91cf\u3002 \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f channels_last \u4e5f\u4e0d\u662f channels_first \u3002 separable_conv2d keras.backend.separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1)) \u5e26\u53ef\u5206\u79bb\u6ee4\u6ce2\u5668\u7684 2D \u5377\u79ef\u3002 \u53c2\u6570 x : \u8f93\u5165\u5f20\u91cf\u3002 depthwise_kernel : \u7528\u4e8e\u6df1\u5ea6\u5377\u79ef\u7684\u5377\u79ef\u6838\u3002 pointwise_kernel : 1x1 \u5377\u79ef\u6838\u3002 strides : \u6b65\u957f\u5143\u7ec4 (\u957f\u5ea6\u4e3a 2)\u3002 padding : \u5b57\u7b26\u4e32\uff0c \"same\" \u6216 \"valid\" \u3002 data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216 \"channels_first\" \u3002 dilation_rate : \u6574\u6570\u5143\u7ec4\uff0c\u53ef\u5206\u79bb\u5377\u79ef\u7684\u81a8\u80c0\u7387\u3002 \u8fd4\u56de \u8f93\u51fa\u5f20\u91cf\u3002 \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f channels_last \u4e5f\u4e0d\u662f channels_first \u3002 depthwise_conv2d keras.backend.depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1)) \u5e26\u53ef\u5206\u79bb\u6ee4\u6ce2\u5668\u7684 2D \u5377\u79ef\u3002 \u53c2\u6570 x : \u8f93\u5165\u5f20\u91cf\u3002 depthwise_kernel : \u7528\u4e8e\u6df1\u5ea6\u5377\u79ef\u7684\u5377\u79ef\u6838\u3002 strides : \u6b65\u957f\u5143\u7ec4 (\u957f\u5ea6\u4e3a 2)\u3002 padding : \u5b57\u7b26\u4e32\uff0c \"same\" \u6216 \"valid\" \u3002 data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216 \"channels_first\" \u3002 dilation_rate : \u6574\u6570\u5143\u7ec4\uff0c\u53ef\u5206\u79bb\u5377\u79ef\u7684\u81a8\u80c0\u7387\u3002 \u8fd4\u56de \u8f93\u51fa\u5f20\u91cf\u3002 \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f channels_last \u4e5f\u4e0d\u662f channels_first \u3002 conv3d keras.backend.conv3d(x, kernel, strides=(1, 1, 1), padding='valid', data_format=None, dilation_rate=(1, 1, 1)) 3D \u5377\u79ef\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 kernel : \u6838\u5f20\u91cf\u3002 strides : \u6b65\u957f\u5143\u7ec4\u3002 padding : \u5b57\u7b26\u4e32\uff0c \"same\" \u6216 \"valid\" \u3002 data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216 \"channels_first\" \u3002 dilation_rate : 3 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\uff0c3D \u5377\u79ef\u7684\u7ed3\u679c\u3002 \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f channels_last \u4e5f\u4e0d\u662f channels_first \u3002 conv3d_transpose keras.backend.conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1), padding='valid', data_format=None) 3D \u53cd\u5377\u79ef (\u5373\u8f6c\u7f6e\u5377\u79ef)\u3002 \u53c2\u6570 x : \u8f93\u5165\u5f20\u91cf\u3002 kernel : \u6838\u5f20\u91cf\u3002 output_shape : \u8868\u793a\u8f93\u51fa\u5c3a\u5bf8\u7684 1D \u6574\u6570\u5f20\u91cf\u3002 strides : \u6b65\u957f\u5143\u7ec4\u3002 padding : \u5b57\u7b26\u4e32\uff0c \"same\" \u6216 \"valid\" \u3002 data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216 \"channels_first\" \u3002 \u5bf9\u4e8e\u8f93\u5165/\u5377\u79ef\u6838/\u8f93\u51fa\uff0c\u662f\u5426\u4f7f\u7528 Theano \u6216 TensorFlow/CNTK\u6570\u636e\u683c\u5f0f\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\uff0c3D \u8f6c\u7f6e\u5377\u79ef\u7684\u7ed3\u679c\u3002 \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f channels_last \u4e5f\u4e0d\u662f channels_first \u3002 pool2d keras.backend.pool2d(x, pool_size, strides=(1, 1), padding='valid', data_format=None, pool_mode='max') 2D \u6c60\u5316\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 pool_size : 2 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\u3002 strides : 2 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\u3002 padding : \u5b57\u7b26\u4e32\uff0c \"same\" \u6216 \"valid\" \u3002 data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216 \"channels_first\" \u3002 pool_mode : \u5b57\u7b26\u4e32\uff0c \"max\" \u6216 \"avg\" \u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\uff0c2D \u6c60\u5316\u7684\u7ed3\u679c\u3002 \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f channels_last \u4e5f\u4e0d\u662f channels_first \u3002 ValueError : if pool_mode \u65e2\u4e0d\u662f \"max\" \u4e5f\u4e0d\u662f \"avg\" \u3002 pool3d keras.backend.pool3d(x, pool_size, strides=(1, 1, 1), padding='valid', data_format=None, pool_mode='max') 3D \u6c60\u5316\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 pool_size : 3 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\u3002 strides : 3 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\u3002 padding : \u5b57\u7b26\u4e32\uff0c \"same\" \u6216 \"valid\" \u3002 data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216 \"channels_first\" \u3002 pool_mode : \u5b57\u7b26\u4e32\uff0c \"max\" \u6216 \"avg\" \u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\uff0c3D \u6c60\u5316\u7684\u7ed3\u679c\u3002 \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f channels_last \u4e5f\u4e0d\u662f channels_first \u3002 ValueError : if pool_mode \u65e2\u4e0d\u662f \"max\" \u4e5f\u4e0d\u662f \"avg\" \u3002 bias_add keras.backend.bias_add(x, bias, data_format=None) \u7ed9\u5f20\u91cf\u6dfb\u52a0\u4e00\u4e2a\u504f\u7f6e\u5411\u91cf\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 bias : \u9700\u8981\u6dfb\u52a0\u7684\u504f\u7f6e\u5411\u91cf\u3002 data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216 \"channels_first\" \u3002 \u8fd4\u56de \u8f93\u51fa\u5f20\u91cf\u3002 \u5f02\u5e38 ValueError : \u4ee5\u4e0b\u4e24\u79cd\u60c5\u51b5\u4e4b\u4e00\uff1a \u65e0\u6548\u7684 data_format \u53c2\u6570\u3002 \u65e0\u6548\u7684\u504f\u7f6e\u5411\u91cf\u5c3a\u5bf8\u3002 \u504f\u7f6e\u5e94\u8be5\u662f\u4e00\u4e2a ndim(x)-1 \u7ef4\u7684\u5411\u91cf\u6216\u5f20\u91cf\u3002 random_normal keras.backend.random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None) \u8fd4\u56de\u6b63\u6001\u5206\u5e03\u503c\u7684\u5f20\u91cf\u3002 \u53c2\u6570 shape : \u4e00\u4e2a\u6574\u6570\u5143\u7ec4\uff0c\u9700\u8981\u521b\u5efa\u7684\u5f20\u91cf\u7684\u5c3a\u5bf8\u3002 mean : \u4e00\u4e2a\u6d6e\u70b9\u6570\uff0c\u62bd\u6837\u7684\u6b63\u6001\u5206\u5e03\u5e73\u5747\u503c\u3002 stddev : \u4e00\u4e2a\u6d6e\u70b9\u6570\uff0c\u62bd\u6837\u7684\u6b63\u6001\u5206\u5e03\u6807\u51c6\u5dee\u3002 dtype : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684\u5f20\u91cf\u7684\u6570\u636e\u7c7b\u578b\u3002 seed : \u6574\u6570\uff0c\u968f\u673a\u79cd\u5b50\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 random_uniform keras.backend.random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None) \u8fd4\u56de\u5747\u5300\u5206\u5e03\u503c\u7684\u5f20\u91cf\u3002 \u53c2\u6570 shape : \u4e00\u4e2a\u6574\u6570\u5143\u7ec4\uff0c\u9700\u8981\u521b\u5efa\u7684\u5f20\u91cf\u7684\u5c3a\u5bf8\u3002 minval : \u4e00\u4e2a\u6d6e\u70b9\u6570\uff0c\u62bd\u6837\u7684\u5747\u5300\u5206\u5e03\u4e0b\u754c\u3002 maxval : \u4e00\u4e2a\u6d6e\u70b9\u6570\uff0c\u62bd\u6837\u7684\u5747\u5300\u5206\u5e03\u4e0a\u754c\u3002 dtype : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684\u5f20\u91cf\u7684\u6570\u636e\u7c7b\u578b\u3002 seed : \u6574\u6570\uff0c\u968f\u673a\u79cd\u5b50\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 random_binomial keras.backend.random_binomial(shape, p=0.0, dtype=None, seed=None) \u8fd4\u56de\u968f\u673a\u4e8c\u9879\u5206\u5e03\u503c\u7684\u5f20\u91cf\u3002 \u53c2\u6570 shape : \u4e00\u4e2a\u6574\u6570\u5143\u7ec4\uff0c\u9700\u8981\u521b\u5efa\u7684\u5f20\u91cf\u7684\u5c3a\u5bf8\u3002 p : \u4e00\u4e2a\u6d6e\u70b9\u6570\uff0c 0. <= p <= 1 \uff0c\u4e8c\u9879\u5206\u5e03\u7684\u6982\u7387\u3002 dtype : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684\u5f20\u91cf\u7684\u6570\u636e\u7c7b\u578b\u3002 seed : \u6574\u6570\uff0c\u968f\u673a\u79cd\u5b50\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 truncated_normal keras.backend.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None) \u8fd4\u56de\u622a\u65ad\u7684\u968f\u673a\u6b63\u6001\u5206\u5e03\u503c\u7684\u5f20\u91cf\u3002 \u751f\u6210\u7684\u503c\u9075\u5faa\u5177\u6709\u6307\u5b9a\u5e73\u5747\u503c\u548c\u6807\u51c6\u5dee\u7684\u6b63\u6001\u5206\u5e03\uff0c \u6b64\u5916\uff0c\u5176\u4e2d\u6570\u503c\u5927\u4e8e\u5e73\u5747\u503c\u4e24\u4e2a\u6807\u51c6\u5dee\u7684\u5c06\u88ab\u4e22\u5f03\u548c\u91cd\u65b0\u6311\u9009\u3002 \u53c2\u6570 shape : \u4e00\u4e2a\u6574\u6570\u5143\u7ec4\uff0c\u9700\u8981\u521b\u5efa\u7684\u5f20\u91cf\u7684\u5c3a\u5bf8\u3002 mean : \u5e73\u5747\u503c\u3002 stddev : \u6807\u51c6\u5dee\u3002 dtype : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684\u5f20\u91cf\u7684\u6570\u636e\u7c7b\u578b\u3002 seed : \u6574\u6570\uff0c\u968f\u673a\u79cd\u5b50\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 ctc_label_dense_to_sparse keras.backend.ctc_label_dense_to_sparse(labels, label_lengths) \u5c06 CTC \u6807\u7b7e\u4ece\u5bc6\u96c6\u8f6c\u6362\u4e3a\u7a00\u758f\u8868\u793a\u3002 \u53c2\u6570 labels : \u5bc6\u96c6 CTC \u6807\u7b7e\u3002 label_lengths : \u6807\u7b7e\u957f\u5ea6\u3002 \u8fd4\u56de \u4e00\u4e2a\u8868\u793a\u6807\u7b7e\u7684\u7a00\u758f\u5f20\u91cf\u3002 ctc_batch_cost keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length) \u5728\u6bcf\u4e2a\u6279\u6b21\u5143\u7d20\u4e0a\u8fd0\u884c CTC \u635f\u5931\u7b97\u6cd5\u3002 \u53c2\u6570 y_true : \u5f20\u91cf (samples, max_string_length) \uff0c \u5305\u542b\u771f\u5b9e\u6807\u7b7e\u3002 y_pred : \u5f20\u91cf (samples, time_steps, num_categories) \uff0c \u5305\u542b\u9884\u6d4b\u503c\uff0c\u6216 softmax \u8f93\u51fa\u3002 input_length : \u5f20\u91cf (samples, 1) \uff0c \u5305\u542b y_pred \u4e2d\u6bcf\u4e2a\u6279\u6b21\u6837\u672c\u7684\u5e8f\u5217\u957f\u5ea6\u3002 label_length : \u5f20\u91cf (samples, 1) \uff0c \u5305\u542b y_true \u4e2d\u6bcf\u4e2a\u6279\u6b21\u6837\u672c\u7684\u5e8f\u5217\u957f\u5ea6\u3002 \u8fd4\u56de \u5c3a\u5bf8\u4e3a (samples,1) \u7684\u5f20\u91cf\uff0c\u5305\u542b\u6bcf\u4e00\u4e2a\u5143\u7d20\u7684 CTC \u635f\u5931\u3002 ctc_decode keras.backend.ctc_decode(y_pred, input_length, greedy=True, beam_width=100, top_paths=1) \u89e3\u7801 softmax \u7684\u8f93\u51fa\u3002 \u53ef\u4ee5\u4f7f\u7528\u8d2a\u5fc3\u641c\u7d22\uff08\u4e5f\u79f0\u4e3a\u6700\u4f18\u8def\u5f84\uff09\u6216\u53d7\u9650\u5b57\u5178\u641c\u7d22\u3002 \u53c2\u6570 y_pred : \u5f20\u91cf (samples, time_steps, num_categories) \uff0c \u5305\u542b\u9884\u6d4b\u503c\uff0c\u6216 softmax \u8f93\u51fa\u3002 input_length : \u5f20\u91cf (samples,) \uff0c \u5305\u542b y_pred \u4e2d\u6bcf\u4e2a\u6279\u6b21\u6837\u672c\u7684\u5e8f\u5217\u957f\u5ea6\u3002 greedy : \u5982\u679c\u4e3a True \uff0c\u5219\u6267\u884c\u66f4\u5feb\u901f\u7684\u6700\u4f18\u8def\u5f84\u641c\u7d22\uff0c\u800c\u4e0d\u4f7f\u7528\u5b57\u5178\u3002 beam_width : \u5982\u679c greedy \u4e3a false \uff0c\u5c06\u4f7f\u7528\u8be5\u5bbd\u5ea6\u7684 beam \u641c\u7d22\u89e3\u7801\u5668\u641c\u7d22\u3002 top_paths : \u5982\u679c greedy \u4e3a false \uff0c \u5c06\u8fd4\u56de\u591a\u5c11\u6761\u6700\u53ef\u80fd\u7684\u8def\u5f84\u3002 \u8fd4\u56de Tuple : List : \u5982\u679c greedy \u4e3a true \uff0c\u8fd4\u56de\u5305\u542b\u89e3\u7801\u5e8f\u5217\u7684\u4e00\u4e2a\u5143\u7d20\u7684\u5217\u8868\u3002 \u5982\u679c\u4e3a false \uff0c\u8fd4\u56de\u6700\u53ef\u80fd\u89e3\u7801\u5e8f\u5217\u7684 top_paths \u3002 Important : \u7a7a\u767d\u6807\u7b7e\u8fd4\u56de\u4e3a -1 \u3002\u5305\u542b\u6bcf\u4e2a\u89e3\u7801\u5e8f\u5217\u7684\u5bf9\u6570\u6982\u7387\u7684\u5f20\u91cf (top_paths,) \u3002 map_fn keras.backend.map_fn(fn, elems, name=None, dtype=None) \u5c06\u51fd\u6570fn\u6620\u5c04\u5230\u5143\u7d20 elems \u4e0a\u5e76\u8fd4\u56de\u8f93\u51fa\u3002 \u53c2\u6570 fn : \u5c06\u5728\u6bcf\u4e2a\u5143\u7d20\u4e0a\u8c03\u7528\u7684\u53ef\u8c03\u7528\u51fd\u6570\u3002 elems : \u5f20\u91cf\u3002 name : \u6620\u5c04\u8282\u70b9\u5728\u56fe\u4e2d\u7684\u5b57\u7b26\u4e32\u540d\u79f0\u3002 dtype : \u8f93\u51fa\u6570\u636e\u683c\u5f0f\u3002 \u8fd4\u56de \u6570\u636e\u7c7b\u578b\u4e3a dtype \u7684\u5f20\u91cf\u3002 foldl keras.backend.foldl(fn, elems, initializer=None, name=None) \u4f7f\u7528 fn \u5f52\u7ea6 elems\uff0c\u4ee5\u4ece\u5de6\u5230\u53f3\u7ec4\u5408\u5b83\u4eec\u3002 \u53c2\u6570 fn : \u5c06\u5728\u6bcf\u4e2a\u5143\u7d20\u548c\u4e00\u4e2a\u7d2f\u52a0\u5668\u4e0a\u8c03\u7528\u7684\u53ef\u8c03\u7528\u51fd\u6570\uff0c\u4f8b\u5982 lambda acc, x: acc + x \u3002 elems : \u5f20\u91cf\u3002 initializer : \u7b2c\u4e00\u4e2a\u4f7f\u7528\u7684\u503c (\u5982\u679c\u4e3a None\uff0c\u4f7f\u7528 elems[0] )\u3002 name : foldl \u8282\u70b9\u5728\u56fe\u4e2d\u7684\u5b57\u7b26\u4e32\u540d\u79f0\u3002 \u8fd4\u56de \u4e0e initializer \u7c7b\u578b\u548c\u5c3a\u5bf8\u76f8\u540c\u7684\u5f20\u91cf\u3002 foldr keras.backend.foldr(fn, elems, initializer=None, name=None) \u4f7f\u7528 fn \u5f52\u7ea6 elems\uff0c\u4ee5\u4ece\u53f3\u5230\u5de6\u7ec4\u5408\u5b83\u4eec\u3002 \u53c2\u6570 fn : \u5c06\u5728\u6bcf\u4e2a\u5143\u7d20\u548c\u4e00\u4e2a\u7d2f\u52a0\u5668\u4e0a\u8c03\u7528\u7684\u53ef\u8c03\u7528\u51fd\u6570\uff0c\u4f8b\u5982 lambda acc, x: acc + x \u3002 elems : \u5f20\u91cf\u3002 initializer : \u7b2c\u4e00\u4e2a\u4f7f\u7528\u7684\u503c (\u5982\u679c\u4e3a None\uff0c\u4f7f\u7528 elems[-1] )\u3002 name : foldr \u8282\u70b9\u5728\u56fe\u4e2d\u7684\u5b57\u7b26\u4e32\u540d\u79f0\u3002 \u8fd4\u56de \u4e0e initializer \u7c7b\u578b\u548c\u5c3a\u5bf8\u76f8\u540c\u7684\u5f20\u91cf\u3002 local_conv1d keras.backend.local_conv1d(inputs, kernel, kernel_size, strides, data_format=None) \u5728\u4e0d\u5171\u4eab\u6743\u503c\u7684\u60c5\u51b5\u4e0b\uff0c\u8fd0\u7528 1D \u5377\u79ef\u3002 \u53c2\u6570 inputs : 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch_size, steps, input_dim) kernel : \u5377\u79ef\u7684\u975e\u5171\u4eab\u6743\u91cd, \u5c3a\u5bf8\u4e3a (output_items, feature_dim, filters) kernel_size : \u4e00\u4e2a\u6574\u6570\u7684\u5143\u7ec4\uff0c \u6307\u5b9a 1D \u5377\u79ef\u7a97\u53e3\u7684\u957f\u5ea6\u3002 strides : \u4e00\u4e2a\u6574\u6570\u7684\u5143\u7ec4\uff0c \u6307\u5b9a\u5377\u79ef\u6b65\u957f\u3002 data_format : \u6570\u636e\u683c\u5f0f\uff0cchannels_first \u6216 channels_last\u3002 \u8fd4\u56de \u8fd0\u7528\u4e0d\u5171\u4eab\u6743\u91cd\u7684 1D \u5377\u79ef\u4e4b\u540e\u7684\u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch_size, output_length, filters)\u3002 \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f channels_last \u4e5f\u4e0d\u662f channels_first \u3002 local_conv2d keras.backend.local_conv2d(inputs, kernel, kernel_size, strides, output_shape, data_format=None) \u5728\u4e0d\u5171\u4eab\u6743\u503c\u7684\u60c5\u51b5\u4e0b\uff0c\u8fd0\u7528 2D \u5377\u79ef\u3002 \u53c2\u6570 inputs : \u5982\u679c data_format='channels_first' \uff0c \u5219\u4e3a\u5c3a\u5bf8\u4e3a (batch_size, filters, new_rows, new_cols) \u7684 4D \u5f20\u91cf\u3002 \u5982\u679c data_format='channels_last' \uff0c \u5219\u4e3a\u5c3a\u5bf8\u4e3a (batch_size, new_rows, new_cols, filters) \u7684 4D \u5f20\u91cf\u3002 kernel : \u5377\u79ef\u7684\u975e\u5171\u4eab\u6743\u91cd, \u5c3a\u5bf8\u4e3a (output_items, feature_dim, filters) kernel_size : 2 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\uff0c \u6307\u5b9a 2D \u5377\u79ef\u7a97\u53e3\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u3002 strides : 2 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\uff0c \u6307\u5b9a 2D \u5377\u79ef\u6cbf\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u65b9\u5411\u7684\u6b65\u957f\u3002 output_shape : \u5143\u7ec4 (output_row, output_col) \u3002 data_format : \u6570\u636e\u683c\u5f0f\uff0cchannels_first \u6216 channels_last\u3002 \u8fd4\u56de \u4e00\u4e2a 4D \u5f20\u91cf\u3002 \u5982\u679c data_format='channels_first' \uff0c\u5c3a\u5bf8\u4e3a (batch_size, filters, new_rows, new_cols)\u3002 \u5982\u679c data_format='channels_last' \uff0c\u5c3a\u5bf8\u4e3a (batch_size, new_rows, new_cols, filters) \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f channels_last \u4e5f\u4e0d\u662f channels_first \u3002 backend backend.backend() \u516c\u5f00\u53ef\u7528\u7684\u65b9\u6cd5\uff0c\u4ee5\u786e\u5b9a\u5f53\u524d\u540e\u7aef\u3002 \u8fd4\u56de \u5b57\u7b26\u4e32\uff0cKeras \u76ee\u524d\u6b63\u5728\u4f7f\u7528\u7684\u540e\u7aef\u540d\u3002 \u4f8b\u5b50 >>> keras.backend.backend() 'tensorflow'","title":"Keras \u540e\u7aef"},{"location":"9.backend/#keras","text":"","title":"Keras \u540e\u7aef"},{"location":"9.backend/#_1","text":"Keras \u662f\u4e00\u4e2a\u6a21\u578b\u7ea7\u5e93\uff0c\u4e3a\u5f00\u53d1\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u5c42\u6b21\u7684\u6784\u5efa\u6a21\u5757\u3002\u5b83\u4e0d\u5904\u7406\u8bf8\u5982\u5f20\u91cf\u4e58\u79ef\u548c\u5377\u79ef\u7b49\u4f4e\u7ea7\u64cd\u4f5c\u3002\u76f8\u53cd\uff0c\u5b83\u4f9d\u8d56\u4e8e\u4e00\u4e2a\u4e13\u95e8\u7684\u3001\u4f18\u5316\u7684\u5f20\u91cf\u64cd\u4f5c\u5e93\u6765\u5b8c\u6210\u8fd9\u4e2a\u64cd\u4f5c\uff0c\u5b83\u53ef\u4ee5\u4f5c\u4e3a Keras \u7684\u300c\u540e\u7aef\u5f15\u64ce\u300d\u3002\u76f8\u6bd4\u5355\u72ec\u5730\u9009\u62e9\u4e00\u4e2a\u5f20\u91cf\u5e93\uff0c\u800c\u5c06 Keras \u7684\u5b9e\u73b0\u4e0e\u8be5\u5e93\u76f8\u5173\u8054\uff0cKeras \u4ee5\u6a21\u5757\u65b9\u5f0f\u5904\u7406\u8fd9\u4e2a\u95ee\u9898\uff0c\u5e76\u4e14\u53ef\u4ee5\u5c06\u51e0\u4e2a\u4e0d\u540c\u7684\u540e\u7aef\u5f15\u64ce\u65e0\u7f1d\u5d4c\u5165\u5230 Keras \u4e2d\u3002 \u76ee\u524d\uff0cKeras \u6709\u4e09\u4e2a\u540e\u7aef\u5b9e\u73b0\u53ef\u7528: TensorFlow \u540e\u7aef\uff0c Theano \u540e\u7aef\uff0c CNTK \u540e\u7aef\u3002 TensorFlow \u662f\u7531 Google \u5f00\u53d1\u7684\u4e00\u4e2a\u5f00\u6e90\u7b26\u53f7\u7ea7\u5f20\u91cf\u64cd\u4f5c\u6846\u67b6\u3002 Theano \u662f\u7531\u8499\u7279\u5229\u5c14\u5927\u5b66\u7684 LISA Lab \u5f00\u53d1\u7684\u4e00\u4e2a\u5f00\u6e90\u7b26\u53f7\u7ea7\u5f20\u91cf\u64cd\u4f5c\u6846\u67b6\u3002 CNTK \u662f\u7531\u5fae\u8f6f\u5f00\u53d1\u7684\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u5f00\u6e90\u5de5\u5177\u5305\u3002 \u5c06\u6765\uff0c\u6211\u4eec\u53ef\u80fd\u4f1a\u6dfb\u52a0\u66f4\u591a\u540e\u7aef\u9009\u9879\u3002","title":"\u4ec0\u4e48\u662f \u300c\u540e\u7aef\u300d\uff1f"},{"location":"9.backend/#_2","text":"\u5982\u679c\u60a8\u81f3\u5c11\u8fd0\u884c\u8fc7\u4e00\u6b21 Keras\uff0c\u60a8\u5c06\u5728\u4ee5\u4e0b\u4f4d\u7f6e\u627e\u5230 Keras \u914d\u7f6e\u6587\u4ef6\uff1a $HOME/.keras/keras.json \u5982\u679c\u5b83\u4e0d\u5728\u90a3\u91cc\uff0c\u4f60\u53ef\u4ee5\u521b\u5efa\u5b83\u3002 Windows\u7528\u6237\u6ce8\u610f\u4e8b\u9879\uff1a \u8bf7\u5c06 $HOME \u4fee\u6539\u4e3a %USERPROFILE% \u3002 \u9ed8\u8ba4\u7684\u914d\u7f6e\u6587\u4ef6\u5982\u4e0b\u6240\u793a\uff1a { \"image_data_format\": \"channels_last\", \"epsilon\": 1e-07, \"floatx\": \"float32\", \"backend\": \"tensorflow\" } \u53ea\u9700\u5c06\u5b57\u6bb5 backend \u66f4\u6539\u4e3a theano \uff0c tensorflow \u6216 cntk \uff0cKeras \u5c06\u5728\u4e0b\u6b21\u8fd0\u884c Keras \u4ee3\u7801\u65f6\u4f7f\u7528\u65b0\u7684\u914d\u7f6e\u3002 \u4f60\u4e5f\u53ef\u4ee5\u5b9a\u4e49\u73af\u5883\u53d8\u91cf KERAS_BACKEND \uff0c\u8fd9\u4f1a\u8986\u76d6\u914d\u7f6e\u6587\u4ef6\u4e2d\u5b9a\u4e49\u7684\u5185\u5bb9\uff1a KERAS_BACKEND=tensorflow python -c \"from keras import backend\" Using TensorFlow backend. \u5728 Keras \u4e2d\uff0c\u53ef\u4ee5\u52a0\u8f7d\u6bd4 \"tensorflow\" , \"theano\" \u548c \"cntk\" \u66f4\u591a\u7684\u540e\u7aef\u3002 Keras \u4e5f\u53ef\u4ee5\u4f7f\u7528\u5916\u90e8\u540e\u7aef\uff0c\u8fd9\u53ef\u4ee5\u901a\u8fc7\u66f4\u6539 keras.json \u914d\u7f6e\u6587\u4ef6\u548c \"backend\" \u8bbe\u7f6e\u6765\u6267\u884c\u3002 \u5047\u8bbe\u60a8\u6709\u4e00\u4e2a\u540d\u4e3a my_module \u7684 Python \u6a21\u5757\uff0c\u60a8\u5e0c\u671b\u5c06\u5176\u7528\u4f5c\u5916\u90e8\u540e\u7aef\u3002 keras.json \u914d\u7f6e\u6587\u4ef6\u5c06\u66f4\u6539\u5982\u4e0b\uff1a { \"image_data_format\": \"channels_last\", \"epsilon\": 1e-07, \"floatx\": \"float32\", \"backend\": \"my_package.my_module\" } \u5fc5\u987b\u9a8c\u8bc1\u5916\u90e8\u540e\u7aef\u624d\u80fd\u4f7f\u7528\uff0c\u6709\u6548\u7684\u540e\u7aef\u5fc5\u987b\u5177\u6709\u4ee5\u4e0b\u51fd\u6570\uff1a placeholder , variable and function . \u5982\u679c\u7531\u4e8e\u7f3a\u5c11\u5fc5\u9700\u7684\u6761\u76ee\u800c\u5bfc\u81f4\u5916\u90e8\u540e\u7aef\u65e0\u6548\uff0c\u5219\u4f1a\u8bb0\u5f55\u9519\u8bef\uff0c\u901a\u77e5\u7f3a\u5c11\u54ea\u4e9b\u6761\u76ee\u3002","title":"\u4ece\u4e00\u4e2a\u540e\u7aef\u5207\u6362\u5230\u53e6\u4e00\u4e2a\u540e\u7aef"},{"location":"9.backend/#kerasjson","text":"The keras.json \u914d\u7f6e\u6587\u4ef6\u5305\u542b\u4ee5\u4e0b\u8bbe\u7f6e\uff1a { \"image_data_format\": \"channels_last\", \"epsilon\": 1e-07, \"floatx\": \"float32\", \"backend\": \"tensorflow\" } \u60a8\u53ef\u4ee5\u901a\u8fc7\u7f16\u8f91 $ HOME/.keras/keras.json \u6765\u66f4\u6539\u8fd9\u4e9b\u8bbe\u7f6e\u3002 image_data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216\u8005 \"channels_first\" \u3002\u5b83\u6307\u5b9a\u4e86 Keras \u5c06\u9075\u5faa\u7684\u6570\u636e\u683c\u5f0f\u7ea6\u5b9a\u3002( keras.backend.image_data_format() \u8fd4\u56de\u5b83\u3002) \u5bf9\u4e8e 2D \u6570\u636e (\u4f8b\u5982\u56fe\u50cf)\uff0c \"channels_last\" \u5047\u5b9a\u4e3a (rows, cols, channels) \uff0c\u800c \"channels_first\" \u5047\u5b9a\u4e3a (channels, rows, cols) \u3002 \u5bf9\u4e8e 3D \u6570\u636e\uff0c \"channels_last\" \u5047\u5b9a\u4e3a (conv_dim1, conv_dim2, conv_dim3, channels) \uff0c\u800c \"channels_first\" \u5047\u5b9a\u4e3a (channels, conv_dim1, conv_dim2, conv_dim3) \u3002 epsilon : \u6d6e\u70b9\u6570\uff0c\u7528\u4e8e\u907f\u514d\u5728\u67d0\u4e9b\u64cd\u4f5c\u4e2d\u88ab\u96f6\u9664\u7684\u6570\u5b57\u6a21\u7cca\u5e38\u91cf\u3002 floatx : \u5b57\u7b26\u4e32\uff0c \"float16\" , \"float32\" , \u6216 \"float64\" \u3002\u9ed8\u8ba4\u6d6e\u70b9\u7cbe\u5ea6\u3002 backend : \u5b57\u7b26\u4e32\uff0c \"tensorflow\" , \"theano\" , \u6216 \"cntk\" \u3002","title":"keras.json \u8be6\u7ec6\u914d\u7f6e"},{"location":"9.backend/#keras_1","text":"\u5982\u679c\u4f60\u5e0c\u671b\u4f60\u7f16\u5199\u7684 Keras \u6a21\u5757\u4e0e Theano ( th ) \u548c TensorFlow ( tf ) \u517c\u5bb9\uff0c\u5219\u5fc5\u987b\u901a\u8fc7\u62bd\u8c61 Keras \u540e\u7aef API \u6765\u7f16\u5199\u5b83\u4eec\u3002\u4ee5\u4e0b\u662f\u4e00\u4e2a\u4ecb\u7ecd\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u5bfc\u5165\u540e\u7aef\u6a21\u5757\uff1a from keras import backend as K \u4e0b\u9762\u7684\u4ee3\u7801\u5b9e\u4f8b\u5316\u4e00\u4e2a\u8f93\u5165\u5360\u4f4d\u7b26\u3002\u5b83\u7b49\u4ef7\u4e8e tf.placeholder() \u6216 th.tensor.matrix() , th.tensor.tensor3() , \u7b49\u7b49\u3002 inputs = K.placeholder(shape=(2, 4, 5)) # \u540c\u6837\u53ef\u4ee5\uff1a inputs = K.placeholder(shape=(None, 4, 5)) # \u540c\u6837\u53ef\u4ee5\uff1a inputs = K.placeholder(ndim=3) \u4e0b\u9762\u7684\u4ee3\u7801\u5b9e\u4f8b\u5316\u4e00\u4e2a\u53d8\u91cf\u3002\u5b83\u7b49\u4ef7\u4e8e tf.Variable() \u6216 th.shared() \u3002 import numpy as np val = np.random.random((3, 4, 5)) var = K.variable(value=val) # \u5168 0 \u53d8\u91cf\uff1a var = K.zeros(shape=(3, 4, 5)) # \u5168 1 \u53d8\u91cf\uff1a var = K.ones(shape=(3, 4, 5)) \u4f60\u9700\u8981\u7684\u5927\u591a\u6570\u5f20\u91cf\u64cd\u4f5c\u90fd\u53ef\u4ee5\u50cf\u5728 TensorFlow \u6216 Theano \u4e2d\u90a3\u6837\u5b8c\u6210\uff1a # \u4f7f\u7528\u968f\u673a\u6570\u521d\u59cb\u5316\u5f20\u91cf b = K.random_uniform_variable(shape=(3, 4), low=0, high=1) # \u5747\u5300\u5206\u5e03 c = K.random_normal_variable(shape=(3, 4), mean=0, scale=1) # \u9ad8\u65af\u5206\u5e03 d = K.random_normal_variable(shape=(3, 4), mean=0, scale=1) # \u5f20\u91cf\u8fd0\u7b97 a = b + c * K.abs(d) c = K.dot(a, K.transpose(b)) a = K.sum(b, axis=1) a = K.softmax(b) a = K.concatenate([b, c], axis=-1) # \u7b49\u7b49","title":"\u4f7f\u7528\u62bd\u8c61 Keras \u540e\u7aef\u7f16\u5199\u65b0\u4ee3\u7801"},{"location":"9.backend/#_3","text":"","title":"\u540e\u7aef\u51fd\u6570"},{"location":"9.backend/#epsilon","text":"keras.backend.epsilon() \u8fd4\u56de\u6570\u5b57\u8868\u8fbe\u5f0f\u4e2d\u4f7f\u7528\u7684\u6a21\u7cca\u56e0\u5b50\u7684\u503c\u3002 \u8fd4\u56de \u4e00\u4e2a\u6d6e\u70b9\u6570\u3002 \u4f8b\u5b50 >>> keras.backend.epsilon() 1e-07","title":"epsilon"},{"location":"9.backend/#set_epsilon","text":"keras.backend.set_epsilon(e) \u8bbe\u7f6e\u6570\u5b57\u8868\u8fbe\u5f0f\u4e2d\u4f7f\u7528\u7684\u6a21\u7cca\u56e0\u5b50\u7684\u503c\u3002 \u53c2\u6570 e : \u6d6e\u70b9\u6570\u3002\u65b0\u7684 epsilon \u503c\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> K.epsilon() 1e-07 >>> K.set_epsilon(1e-05) >>> K.epsilon() 1e-05","title":"set_epsilon"},{"location":"9.backend/#floatx","text":"keras.backend.floatx() \u4ee5\u5b57\u7b26\u4e32\u5f62\u5f0f\u8fd4\u56de\u9ed8\u8ba4\u7684\u6d6e\u70b9\u7c7b\u578b\u3002 (\u4f8b\u5982\uff0c'float16', 'float32', 'float64')\u3002 \u8fd4\u56de \u5b57\u7b26\u4e32\uff0c\u5f53\u524d\u9ed8\u8ba4\u7684\u6d6e\u70b9\u7c7b\u578b\u3002 \u4f8b\u5b50 >>> keras.backend.floatx() 'float32'","title":"floatx"},{"location":"9.backend/#set_floatx","text":"keras.backend.set_floatx(floatx) \u8bbe\u7f6e\u9ed8\u8ba4\u7684\u6d6e\u70b9\u7c7b\u578b\u3002 \u53c2\u6570 floatx : \u5b57\u7b26\u4e32\uff0c'float16', 'float32', \u6216 'float64'\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> K.floatx() 'float32' >>> K.set_floatx('float16') >>> K.floatx() 'float16'","title":"set_floatx"},{"location":"9.backend/#cast_to_floatx","text":"keras.backend.cast_to_floatx(x) \u5c06 Numpy \u6570\u7ec4\u8f6c\u6362\u4e3a\u9ed8\u8ba4\u7684 Keras \u6d6e\u70b9\u7c7b\u578b\u3002 \u53c2\u6570 x : Numpy \u6570\u7ec4\u3002 \u8fd4\u56de \u76f8\u540c\u7684 Numpy \u6570\u7ec4\uff0c\u8f6c\u6362\u4e3a\u5b83\u7684\u65b0\u7c7b\u578b\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> K.floatx() 'float32' >>> arr = numpy.array([1.0, 2.0], dtype='float64') >>> arr.dtype dtype('float64') >>> new_arr = K.cast_to_floatx(arr) >>> new_arr array([ 1., 2.], dtype=float32) >>> new_arr.dtype dtype('float32')","title":"cast_to_floatx"},{"location":"9.backend/#image_data_format","text":"keras.backend.image_data_format() \u8fd4\u56de\u9ed8\u8ba4\u56fe\u50cf\u6570\u636e\u683c\u5f0f\u7ea6\u5b9a ('channels_first' \u6216 'channels_last')\u3002 \u8fd4\u56de \u4e00\u4e2a\u5b57\u7b26\u4e32\uff0c 'channels_first' \u6216 'channels_last' \u4f8b\u5b50 >>> keras.backend.image_data_format() 'channels_first'","title":"image_data_format"},{"location":"9.backend/#set_image_data_format","text":"keras.backend.set_image_data_format(data_format) \u8bbe\u7f6e\u6570\u636e\u683c\u5f0f\u7ea6\u5b9a\u7684\u503c\u3002 \u53c2\u6570 data_format : \u5b57\u7b26\u4e32\u3002 'channels_first' \u6216 'channels_last' \u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> K.image_data_format() 'channels_first' >>> K.set_image_data_format('channels_last') >>> K.image_data_format() 'channels_last'","title":"set_image_data_format"},{"location":"9.backend/#reset_uids","text":"keras.backend.reset_uids() \u91cd\u7f6e\u56fe\u7684\u6807\u8bc6\u7b26\u3002","title":"reset_uids"},{"location":"9.backend/#get_uid","text":"keras.backend.get_uid(prefix='') \u83b7\u53d6\u9ed8\u8ba4\u8ba1\u7b97\u56fe\u7684 uid\u3002 \u53c2\u6570 prefix : \u56fe\u7684\u53ef\u9009\u524d\u7f00\u3002 \u8fd4\u56de \u56fe\u7684\u552f\u4e00\u6807\u8bc6\u7b26\u3002","title":"get_uid"},{"location":"9.backend/#clear_session","text":"keras.backend.clear_session() \u9500\u6bc1\u5f53\u524d\u7684 TF \u56fe\u5e76\u521b\u5efa\u4e00\u4e2a\u65b0\u56fe\u3002 \u6709\u7528\u4e8e\u907f\u514d\u65e7\u6a21\u578b/\u7f51\u7edc\u5c42\u6df7\u4e71\u3002","title":"clear_session"},{"location":"9.backend/#manual_variable_initialization","text":"keras.backend.manual_variable_initialization(value) \u8bbe\u7f6e\u53d8\u91cf\u624b\u52a8\u521d\u59cb\u5316\u7684\u6807\u5fd7\u3002 \u8fd9\u4e2a\u5e03\u5c14\u6807\u5fd7\u51b3\u5b9a\u4e86\u53d8\u91cf\u662f\u5426\u5e94\u8be5\u5728\u5b9e\u4f8b\u5316\u65f6\u521d\u59cb\u5316\uff08\u9ed8\u8ba4\uff09\uff0c \u6216\u8005\u7528\u6237\u662f\u5426\u5e94\u8be5\u81ea\u5df1\u5904\u7406\u521d\u59cb\u5316 \uff08\u4f8b\u5982\u901a\u8fc7 tf.initialize_all_variables() \uff09\u3002 \u53c2\u6570 value : Python \u5e03\u5c14\u503c\u3002","title":"manual_variable_initialization"},{"location":"9.backend/#learning_phase","text":"keras.backend.learning_phase() \u8fd4\u56de\u5b66\u4e60\u9636\u6bb5\u7684\u6807\u5fd7\u3002 \u5b66\u4e60\u9636\u6bb5\u6807\u5fd7\u662f\u4e00\u4e2a\u5e03\u5c14\u5f20\u91cf\uff080 = test\uff0c1 = train\uff09\uff0c \u5b83\u4f5c\u4e3a\u8f93\u5165\u4f20\u9012\u7ed9\u4efb\u4f55\u7684 Keras \u51fd\u6570\uff0c\u4ee5\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5 \u65f6\u6267\u884c\u4e0d\u540c\u7684\u884c\u4e3a\u64cd\u4f5c\u3002 \u8fd4\u56de \u5b66\u4e60\u9636\u6bb5 (\u6807\u91cf\u6574\u6570\u5f20\u91cf\u6216 python \u6574\u6570)\u3002","title":"learning_phase"},{"location":"9.backend/#set_learning_phase","text":"keras.backend.set_learning_phase(value) \u5c06\u5b66\u4e60\u9636\u6bb5\u8bbe\u7f6e\u4e3a\u56fa\u5b9a\u503c\u3002 \u53c2\u6570 value : \u5b66\u4e60\u9636\u6bb5\u7684\u503c\uff0c0 \u6216 1\uff08\u6574\u6570\uff09\u3002 \u5f02\u5e38 ValueError : \u5982\u679c value \u65e2\u4e0d\u662f 0 \u4e5f\u4e0d\u662f 1 \u3002","title":"set_learning_phase"},{"location":"9.backend/#is_sparse","text":"keras.backend.is_sparse(tensor) \u5224\u65ad\u5f20\u91cf\u662f\u5426\u662f\u7a00\u758f\u5f20\u91cf\u3002 \u53c2\u6570 tensor : \u4e00\u4e2a\u5f20\u91cf\u5b9e\u4f8b\u3002 \u8fd4\u56de \u5e03\u5c14\u503c\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> a = K.placeholder((2, 2), sparse=False) >>> print(K.is_sparse(a)) False >>> b = K.placeholder((2, 2), sparse=True) >>> print(K.is_sparse(b)) True","title":"is_sparse"},{"location":"9.backend/#to_dense","text":"keras.backend.to_dense(tensor) \u5c06\u7a00\u758f\u5f20\u91cf\u8f6c\u6362\u4e3a\u7a20\u5bc6\u5f20\u91cf\u5e76\u8fd4\u56de\u3002 \u53c2\u6570 tensor : \u5f20\u91cf\u5b9e\u4f8b\uff08\u53ef\u80fd\u7a00\u758f\uff09\u3002 \u8fd4\u56de \u4e00\u4e2a\u7a20\u5bc6\u5f20\u91cf\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> b = K.placeholder((2, 2), sparse=True) >>> print(K.is_sparse(b)) True >>> c = K.to_dense(b) >>> print(K.is_sparse(c)) False","title":"to_dense"},{"location":"9.backend/#variable","text":"keras.backend.variable(value, dtype=None, name=None, constraint=None) \u5b9e\u4f8b\u5316\u4e00\u4e2a\u53d8\u91cf\u5e76\u8fd4\u56de\u5b83\u3002 \u53c2\u6570 value : Numpy \u6570\u7ec4\uff0c\u5f20\u91cf\u7684\u521d\u59cb\u503c\u3002 dtype : \u5f20\u91cf\u7c7b\u578b\u3002 name : \u5f20\u91cf\u7684\u53ef\u9009\u540d\u79f0\u5b57\u7b26\u4e32\u3002 constraint : \u5728\u4f18\u5316\u5668\u66f4\u65b0\u540e\u5e94\u7528\u4e8e\u53d8\u91cf\u7684\u53ef\u9009\u6295\u5f71\u51fd\u6570\u3002 \u8fd4\u56de \u53d8\u91cf\u5b9e\u4f8b\uff08\u5305\u542b Keras \u5143\u6570\u636e\uff09 \u4f8b\u5b50 >>> from keras import backend as K >>> val = np.array([[1, 2], [3, 4]]) >>> kvar = K.variable(value=val, dtype='float64', name='example_var') >>> K.dtype(kvar) 'float64' >>> print(kvar) example_var >>> K.eval(kvar) array([[ 1., 2.], [ 3., 4.]])","title":"variable"},{"location":"9.backend/#constant","text":"keras.backend.constant(value, dtype=None, shape=None, name=None) \u521b\u5efa\u4e00\u4e2a\u5e38\u6570\u5f20\u91cf\u3002 \u53c2\u6570 value : \u4e00\u4e2a\u5e38\u6570\u503c\uff08\u6216\u5217\u8868\uff09 dtype : \u7ed3\u679c\u5f20\u91cf\u7684\u5143\u7d20\u7c7b\u578b\u3002 shape : \u53ef\u9009\u7684\u7ed3\u679c\u5f20\u91cf\u7684\u5c3a\u5bf8\u3002 name : \u53ef\u9009\u7684\u5f20\u91cf\u7684\u540d\u79f0\u3002 \u8fd4\u56de \u4e00\u4e2a\u5e38\u6570\u5f20\u91cf\u3002","title":"constant"},{"location":"9.backend/#is_keras_tensor","text":"keras.backend.is_keras_tensor(x) \u5224\u65ad x \u662f\u5426\u662f Keras \u5f20\u91cf \u300cKeras\u5f20\u91cf\u300d\u662f\u7531 Keras \u5c42\uff08 Layer \u7c7b\uff09\u6216 Input \u8fd4\u56de\u7684\u5f20\u91cf\u3002 \u53c2\u6570 x : \u5019\u9009\u5f20\u91cf\u3002 \u8fd4\u56de \u5e03\u5c14\u503c\uff1a\u53c2\u6570\u662f\u5426\u662f Keras \u5f20\u91cf\u3002 \u5f02\u5e38 ValueError : \u5982\u679c x \u4e0d\u662f\u4e00\u4e2a\u7b26\u53f7\u5f20\u91cf\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> from keras.layers import Input, Dense >>> np_var = numpy.array([1, 2]) >>> K.is_keras_tensor(np_var) # \u4e00\u4e2a Numpy \u6570\u7ec4\u4e0d\u662f\u4e00\u4e2a\u7b26\u53f7\u5f20\u91cf\u3002 ValueError >>> k_var = tf.placeholder('float32', shape=(1,1)) >>> K.is_keras_tensor(k_var) # \u5728 Keras \u4e4b\u5916\u95f4\u63a5\u521b\u5efa\u7684\u53d8\u91cf\u4e0d\u662f Keras \u5f20\u91cf\u3002 False >>> keras_var = K.variable(np_var) >>> K.is_keras_tensor(keras_var) # Keras \u540e\u7aef\u521b\u5efa\u7684\u53d8\u91cf\u4e0d\u662f Keras \u5f20\u91cf\u3002 False >>> keras_placeholder = K.placeholder(shape=(2, 4, 5)) >>> K.is_keras_tensor(keras_placeholder) # \u5360\u4f4d\u7b26\u4e0d\u662f Keras \u5f20\u91cf\u3002 False >>> keras_input = Input([10]) >>> K.is_keras_tensor(keras_input) # \u8f93\u5165 Input \u662f Keras \u5f20\u91cf\u3002 True >>> keras_layer_output = Dense(10)(keras_input) >>> K.is_keras_tensor(keras_layer_output) # \u4efb\u4f55 Keras \u5c42\u8f93\u51fa\u90fd\u662f Keras \u5f20\u91cf\u3002 True","title":"is_keras_tensor"},{"location":"9.backend/#is_tensor","text":"keras.backend.is_tensor(x)","title":"is_tensor"},{"location":"9.backend/#placeholder","text":"keras.backend.placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None) \u5b9e\u4f8b\u5316\u4e00\u4e2a\u5360\u4f4d\u7b26\u5f20\u91cf\u5e76\u8fd4\u56de\u5b83\u3002 \u53c2\u6570 shape : \u5360\u4f4d\u7b26\u5c3a\u5bf8 (\u6574\u6570\u5143\u7ec4\uff0c\u53ef\u80fd\u5305\u542b None \u9879)\u3002 ndim : \u5f20\u91cf\u7684\u8f74\u6570\u3002 { shape , ndim } \u81f3\u5c11\u4e00\u4e2a\u9700\u8981\u88ab\u6307\u5b9a\u3002 \u5982\u679c\u4e24\u4e2a\u90fd\u88ab\u6307\u5b9a\uff0c\u90a3\u4e48\u4f7f\u7528 shape \u3002 dtype : \u5360\u4f4d\u7b26\u7c7b\u578b\u3002 sparse : \u5e03\u5c14\u503c\uff0c\u5360\u4f4d\u7b26\u662f\u5426\u5e94\u8be5\u6709\u4e00\u4e2a\u7a00\u758f\u7c7b\u578b\u3002 name : \u53ef\u9009\u7684\u5360\u4f4d\u7b26\u7684\u540d\u79f0\u5b57\u7b26\u4e32\u3002 \u8fd4\u56de \u5f20\u91cf\u5b9e\u4f8b\uff08\u5305\u62ec Keras \u5143\u6570\u636e\uff09\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> input_ph = K.placeholder(shape=(2, 4, 5)) >>> input_ph._keras_shape (2, 4, 5) >>> input_ph <tf.Tensor 'Placeholder_4:0' shape=(2, 4, 5) dtype=float32>","title":"placeholder"},{"location":"9.backend/#is_placeholder","text":"keras.backend.is_placeholder(x) \u5224\u65ad x \u662f\u5426\u662f\u5360\u4f4d\u7b26\u3002 \u53c2\u6570 x : \u5019\u9009\u5360\u4f4d\u7b26\u3002 \u8fd4\u56de \u5e03\u5c14\u503c\u3002","title":"is_placeholder"},{"location":"9.backend/#shape","text":"keras.backend.shape(x) \u8fd4\u56de\u5f20\u91cf\u6216\u53d8\u91cf\u7684\u7b26\u53f7\u5c3a\u5bf8\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u7b26\u53f7\u5c3a\u5bf8\uff08\u5b83\u672c\u8eab\u5c31\u662f\u5f20\u91cf\uff09\u3002 \u4f8b\u5b50 # TensorFlow \u4f8b\u5b50 >>> from keras import backend as K >>> tf_session = K.get_session() >>> val = np.array([[1, 2], [3, 4]]) >>> kvar = K.variable(value=val) >>> inputs = keras.backend.placeholder(shape=(2, 4, 5)) >>> K.shape(kvar) <tf.Tensor 'Shape_8:0' shape=(2,) dtype=int32> >>> K.shape(inputs) <tf.Tensor 'Shape_9:0' shape=(3,) dtype=int32> # \u8981\u5f97\u5230\u6574\u6570\u5c3a\u5bf8 (\u76f8\u53cd\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528 K.int_shape(x)) >>> K.shape(kvar).eval(session=tf_session) array([2, 2], dtype=int32) >>> K.shape(inputs).eval(session=tf_session) array([2, 4, 5], dtype=int32)","title":"shape"},{"location":"9.backend/#int_shape","text":"keras.backend.int_shape(x) \u8fd4\u56de\u5f20\u91cf\u6216\u53d8\u91cf\u7684\u5c3a\u5bf8\uff0c\u4f5c\u4e3a int \u6216 None \u9879\u7684\u5143\u7ec4\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u6574\u6570\u5143\u7ec4\uff08\u6216 None \u9879\uff09\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> inputs = K.placeholder(shape=(2, 4, 5)) >>> K.int_shape(inputs) (2, 4, 5) >>> val = np.array([[1, 2], [3, 4]]) >>> kvar = K.variable(value=val) >>> K.int_shape(kvar) (2, 2) Numpy \u5b9e\u73b0 def int_shape(x): return x.shape","title":"int_shape"},{"location":"9.backend/#ndim","text":"keras.backend.ndim(x) \u4ee5\u6574\u6570\u5f62\u5f0f\u8fd4\u56de\u5f20\u91cf\u4e2d\u7684\u8f74\u6570\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u6574\u6570 (\u6807\u91cf), \u8f74\u7684\u6570\u91cf\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> inputs = K.placeholder(shape=(2, 4, 5)) >>> val = np.array([[1, 2], [3, 4]]) >>> kvar = K.variable(value=val) >>> K.ndim(inputs) 3 >>> K.ndim(kvar) 2 Numpy \u5b9e\u73b0 def ndim(x): return x.ndim","title":"ndim"},{"location":"9.backend/#dtype","text":"keras.backend.dtype(x) \u4ee5\u5b57\u7b26\u4e32\u5f62\u5f0f\u8fd4\u56de Keras \u5f20\u91cf\u6216\u53d8\u91cf\u7684 dtype\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u5b57\u7b26\u4e32\uff0c x \u7684 dtype\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> K.dtype(K.placeholder(shape=(2,4,5))) 'float32' >>> K.dtype(K.placeholder(shape=(2,4,5), dtype='float32')) 'float32' >>> K.dtype(K.placeholder(shape=(2,4,5), dtype='float64')) 'float64' # Keras \u53d8\u91cf >>> kvar = K.variable(np.array([[1, 2], [3, 4]])) >>> K.dtype(kvar) 'float32_ref' >>> kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32') >>> K.dtype(kvar) 'float32_ref'","title":"dtype"},{"location":"9.backend/#eval","text":"keras.backend.eval(x) \u4f30\u8ba1\u4e00\u4e2a\u53d8\u91cf\u7684\u503c\u3002 \u53c2\u6570 x : \u53d8\u91cf\u3002 \u8fd4\u56de Numpy \u6570\u7ec4\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32') >>> K.eval(kvar) array([[ 1., 2.], [ 3., 4.]], dtype=float32)","title":"eval"},{"location":"9.backend/#zeros","text":"keras.backend.zeros(shape, dtype=None, name=None) \u5b9e\u4f8b\u5316\u4e00\u4e2a\u5168\u96f6\u53d8\u91cf\u5e76\u8fd4\u56de\u5b83\u3002 \u53c2\u6570 shape : \u6574\u6570\u5143\u7ec4\uff0c\u8fd4\u56de\u7684Keras\u53d8\u91cf\u7684\u5c3a\u5bf8\u3002 dtype : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684 Keras \u53d8\u91cf\u7684\u6570\u636e\u7c7b\u578b\u3002 name : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684 Keras \u53d8\u91cf\u7684\u540d\u79f0\u3002 \u8fd4\u56de \u4e00\u4e2a\u53d8\u91cf\uff08\u5305\u62ec Keras \u5143\u6570\u636e\uff09\uff0c\u7528 0.0 \u586b\u5145\u3002 \u8bf7\u6ce8\u610f\uff0c\u5982\u679c shape \u662f\u7b26\u53f7\u5316\u7684\uff0c\u6211\u4eec\u4e0d\u80fd\u8fd4\u56de\u4e00\u4e2a\u53d8\u91cf\uff0c \u800c\u4f1a\u8fd4\u56de\u4e00\u4e2a\u52a8\u6001\u5c3a\u5bf8\u7684\u5f20\u91cf\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> kvar = K.zeros((3,4)) >>> K.eval(kvar) array([[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]], dtype=float32)","title":"zeros"},{"location":"9.backend/#ones","text":"keras.backend.ones(shape, dtype=None, name=None) \u5b9e\u4f8b\u5316\u4e00\u4e2a\u5168\u4e00\u53d8\u91cf\u5e76\u8fd4\u56de\u5b83\u3002 \u53c2\u6570 shape : \u6574\u6570\u5143\u7ec4\uff0c\u8fd4\u56de\u7684Keras\u53d8\u91cf\u7684\u5c3a\u5bf8\u3002 dtype : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684 Keras \u53d8\u91cf\u7684\u6570\u636e\u7c7b\u578b\u3002 name : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684 Keras \u53d8\u91cf\u7684\u540d\u79f0\u3002 \u8fd4\u56de \u4e00\u4e2a Keras \u53d8\u91cf\uff0c\u7528 1.0 \u586b\u5145\u3002 \u8bf7\u6ce8\u610f\uff0c\u5982\u679c shape \u662f\u7b26\u53f7\u5316\u7684\uff0c\u6211\u4eec\u4e0d\u80fd\u8fd4\u56de\u4e00\u4e2a\u53d8\u91cf\uff0c \u800c\u4f1a\u8fd4\u56de\u4e00\u4e2a\u52a8\u6001\u5c3a\u5bf8\u7684\u5f20\u91cf\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> kvar = K.ones((3,4)) >>> K.eval(kvar) array([[ 1., 1., 1., 1.], [ 1., 1., 1., 1.], [ 1., 1., 1., 1.]], dtype=float32)","title":"ones"},{"location":"9.backend/#eye","text":"keras.backend.eye(size, dtype=None, name=None) \u5b9e\u4f8b\u5316\u4e00\u4e2a\u5355\u4f4d\u77e9\u9635\u5e76\u8fd4\u56de\u5b83\u3002 \u53c2\u6570 size : \u6574\u6570\uff0c\u884c/\u5217\u7684\u6570\u76ee\u3002 dtype : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684 Keras \u53d8\u91cf\u7684\u6570\u636e\u7c7b\u578b\u3002 name : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684 Keras \u53d8\u91cf\u7684\u540d\u79f0\u3002 \u8fd4\u56de Keras \u53d8\u91cf\uff0c\u4e00\u4e2a\u5355\u4f4d\u77e9\u9635\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> kvar = K.eye(3) >>> K.eval(kvar) array([[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]], dtype=float32)","title":"eye"},{"location":"9.backend/#zeros_like","text":"keras.backend.zeros_like(x, dtype=None, name=None) \u5b9e\u4f8b\u5316\u4e0e\u53e6\u4e00\u4e2a\u5f20\u91cf\u76f8\u540c\u5c3a\u5bf8\u7684\u5168\u96f6\u53d8\u91cf\u3002 \u53c2\u6570 x : Keras \u53d8\u91cf\u6216 Keras \u5f20\u91cf\u3002 dtype : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684 Keras \u53d8\u91cf\u7684\u7c7b\u578b\u3002 \u5982\u679c\u4e3a None\uff0c\u5219\u4f7f\u7528 x \u7684\u7c7b\u578b\u3002 name : \u5b57\u7b26\u4e32\uff0c\u6240\u521b\u5efa\u7684\u53d8\u91cf\u7684\u540d\u79f0\u3002 \u8fd4\u56de \u4e00\u4e2a Keras \u53d8\u91cf\uff0c\u5176\u5f62\u72b6\u4e3a x\uff0c\u7528\u96f6\u586b\u5145\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> kvar = K.variable(np.random.random((2,3))) >>> kvar_zeros = K.zeros_like(kvar) >>> K.eval(kvar_zeros) array([[ 0., 0., 0.], [ 0., 0., 0.]], dtype=float32)","title":"zeros_like"},{"location":"9.backend/#ones_like","text":"keras.backend.ones_like(x, dtype=None, name=None) \u5b9e\u4f8b\u5316\u4e0e\u53e6\u4e00\u4e2a\u5f20\u91cf\u76f8\u540c\u5f62\u72b6\u7684\u5168\u4e00\u53d8\u91cf\u3002 \u53c2\u6570 x : Keras \u53d8\u91cf\u6216\u5f20\u91cf\u3002 dtype : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684 Keras \u53d8\u91cf\u7684\u7c7b\u578b\u3002 \u5982\u679c\u4e3a None\uff0c\u5219\u4f7f\u7528 x \u7684\u7c7b\u578b\u3002 name : \u5b57\u7b26\u4e32\uff0c\u6240\u521b\u5efa\u7684\u53d8\u91cf\u7684\u540d\u79f0\u3002 \u8fd4\u56de \u4e00\u4e2a Keras \u53d8\u91cf\uff0c\u5176\u5f62\u72b6\u4e3a x\uff0c\u7528\u4e00\u586b\u5145\u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> kvar = K.variable(np.random.random((2,3))) >>> kvar_ones = K.ones_like(kvar) >>> K.eval(kvar_ones) array([[ 1., 1., 1.], [ 1., 1., 1.]], dtype=float32)","title":"ones_like"},{"location":"9.backend/#identity","text":"keras.backend.identity(x, name=None) \u8fd4\u56de\u4e0e\u8f93\u5165\u5f20\u91cf\u76f8\u540c\u5185\u5bb9\u7684\u5f20\u91cf\u3002 \u53c2\u6570 x : \u8f93\u5165\u5f20\u91cf\u3002 name : \u5b57\u7b26\u4e32\uff0c\u6240\u521b\u5efa\u7684\u53d8\u91cf\u7684\u540d\u79f0\u3002 \u8fd4\u56de \u4e00\u4e2a\u76f8\u540c\u5c3a\u5bf8\u3001\u7c7b\u578b\u548c\u5185\u5bb9\u7684\u5f20\u91cf\u3002","title":"identity"},{"location":"9.backend/#random_uniform_variable","text":"keras.backend.random_uniform_variable(shape, low, high, dtype=None, name=None, seed=None) \u4f7f\u7528\u4ece\u5747\u5300\u5206\u5e03\u4e2d\u62bd\u6837\u51fa\u6765\u7684\u503c\u6765\u5b9e\u4f8b\u5316\u53d8\u91cf\u3002 \u53c2\u6570 shape : \u6574\u6570\u5143\u7ec4\uff0c\u8fd4\u56de\u7684 Keras \u53d8\u91cf\u7684\u5c3a\u5bf8\u3002 low : \u6d6e\u70b9\u6570\uff0c\u8f93\u51fa\u95f4\u9694\u7684\u4e0b\u754c\u3002 high : \u6d6e\u70b9\u6570\uff0c\u8f93\u51fa\u95f4\u9694\u7684\u4e0a\u754c\u3002 dtype : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684 Keras \u53d8\u91cf\u7684\u6570\u636e\u7c7b\u578b\u3002 name : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684 Keras \u53d8\u91cf\u7684\u540d\u79f0\u3002 seed : \u6574\u6570\uff0c\u968f\u673a\u79cd\u5b50\u3002 \u8fd4\u56de \u4e00\u4e2a Keras \u53d8\u91cf\uff0c\u4ee5\u62bd\u53d6\u7684\u6837\u672c\u586b\u5145\u3002 \u4f8b\u5b50 # TensorFlow \u793a\u4f8b >>> kvar = K.random_uniform_variable((2,3), 0, 1) >>> kvar <tensorflow.python.ops.variables.Variable object at 0x10ab40b10> >>> K.eval(kvar) array([[ 0.10940075, 0.10047495, 0.476143 ], [ 0.66137183, 0.00869417, 0.89220798]], dtype=float32)","title":"random_uniform_variable"},{"location":"9.backend/#random_normal_variable","text":"keras.backend.random_normal_variable(shape, mean, scale, dtype=None, name=None, seed=None) \u4f7f\u7528\u4ece\u6b63\u6001\u5206\u5e03\u4e2d\u62bd\u53d6\u7684\u503c\u5b9e\u4f8b\u5316\u4e00\u4e2a\u53d8\u91cf\u3002 \u53c2\u6570 shape : \u6574\u6570\u5143\u7ec4\uff0c\u8fd4\u56de\u7684Keras\u53d8\u91cf\u7684\u5c3a\u5bf8\u3002 mean : \u6d6e\u70b9\u578b\uff0c\u6b63\u6001\u5206\u5e03\u5e73\u5747\u503c\u3002 scale : \u6d6e\u70b9\u578b\uff0c\u6b63\u6001\u5206\u5e03\u6807\u51c6\u5dee\u3002 dtype : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684Keras\u53d8\u91cf\u7684 dtype\u3002 name : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684Keras\u53d8\u91cf\u7684\u540d\u79f0\u3002 seed : \u6574\u6570\uff0c\u968f\u673a\u79cd\u5b50\u3002 \u8fd4\u56de \u4e00\u4e2a Keras \u53d8\u91cf\uff0c\u4ee5\u62bd\u53d6\u7684\u6837\u672c\u586b\u5145\u3002 \u4f8b\u5b50 # TensorFlow \u793a\u4f8b >>> kvar = K.random_normal_variable((2,3), 0, 1) >>> kvar <tensorflow.python.ops.variables.Variable object at 0x10ab12dd0> >>> K.eval(kvar) array([[ 1.19591331, 0.68685907, -0.63814116], [ 0.92629528, 0.28055015, 1.70484698]], dtype=float32)","title":"random_normal_variable"},{"location":"9.backend/#count_params","text":"keras.backend.count_params(x) \u8fd4\u56de Keras \u53d8\u91cf\u6216\u5f20\u91cf\u4e2d\u7684\u9759\u6001\u5143\u7d20\u6570\u3002 \u53c2\u6570 x : Keras \u53d8\u91cf\u6216\u5f20\u91cf\u3002 \u8fd4\u56de \u6574\u6570\uff0c x \u4e2d\u7684\u5143\u7d20\u6570\u91cf\uff0c\u5373\uff0c\u6570\u7ec4\u4e2d\u9759\u6001\u7ef4\u5ea6\u7684\u4e58\u79ef\u3002 \u4f8b\u5b50 >>> kvar = K.zeros((2,3)) >>> K.count_params(kvar) 6 >>> K.eval(kvar) array([[ 0., 0., 0.], [ 0., 0., 0.]], dtype=float32)","title":"count_params"},{"location":"9.backend/#cast","text":"keras.backend.cast(x, dtype) \u5c06\u5f20\u91cf\u8f6c\u6362\u5230\u4e0d\u540c\u7684 dtype \u5e76\u8fd4\u56de\u3002 \u4f60\u53ef\u4ee5\u8f6c\u6362\u4e00\u4e2a Keras \u53d8\u91cf\uff0c\u4f46\u5b83\u4ecd\u7136\u8fd4\u56de\u4e00\u4e2a Keras \u5f20\u91cf\u3002 \u53c2\u6570 x : Keras \u5f20\u91cf\uff08\u6216\u53d8\u91cf\uff09\u3002 dtype : \u5b57\u7b26\u4e32\uff0c ( 'float16' , 'float32' \u6216 'float64' )\u3002 \u8fd4\u56de Keras \u5f20\u91cf\uff0c\u7c7b\u578b\u4e3a dtype \u3002 \u4f8b\u5b50 >>> from keras import backend as K >>> input = K.placeholder((2, 3), dtype='float32') >>> input <tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32> # It doesn't work in-place as below. >>> K.cast(input, dtype='float16') <tf.Tensor 'Cast_1:0' shape=(2, 3) dtype=float16> >>> input <tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32> # you need to assign it. >>> input = K.cast(input, dtype='float16') >>> input <tf.Tensor 'Cast_2:0' shape=(2, 3) dtype=float16>","title":"cast"},{"location":"9.backend/#update","text":"keras.backend.update(x, new_x) \u5c06 x \u7684\u503c\u66f4\u65b0\u4e3a new_x \u3002 \u53c2\u6570 x : \u4e00\u4e2a Variable \u3002 new_x : \u4e00\u4e2a\u4e0e x \u5c3a\u5bf8\u76f8\u540c\u7684\u5f20\u91cf\u3002 \u8fd4\u56de \u66f4\u65b0\u540e\u7684\u53d8\u91cf x \u3002","title":"update"},{"location":"9.backend/#update_add","text":"keras.backend.update_add(x, increment) \u901a\u8fc7\u589e\u52a0 increment \u6765\u66f4\u65b0 x \u7684\u503c\u3002 \u53c2\u6570 x : \u4e00\u4e2a Variable \u3002 increment : \u4e0e x \u5f62\u72b6\u76f8\u540c\u7684\u5f20\u91cf\u3002 \u8fd4\u56de \u66f4\u65b0\u540e\u7684\u53d8\u91cf x \u3002","title":"update_add"},{"location":"9.backend/#update_sub","text":"keras.backend.update_sub(x, decrement) \u901a\u8fc7\u51cf decrement \u6765\u66f4\u65b0 x \u7684\u503c\u3002 \u53c2\u6570 x : \u4e00\u4e2a Variable \u3002 decrement : \u4e0e x \u5f62\u72b6\u76f8\u540c\u7684\u5f20\u91cf\u3002 \u8fd4\u56de \u66f4\u65b0\u540e\u7684\u53d8\u91cf x \u3002","title":"update_sub"},{"location":"9.backend/#moving_average_update","text":"keras.backend.moving_average_update(x, value, momentum) \u8ba1\u7b97\u53d8\u91cf\u7684\u79fb\u52a8\u5e73\u5747\u503c\u3002 \u53c2\u6570 x : \u4e00\u4e2a Variable \u3002 value : \u4e0e x \u5f62\u72b6\u76f8\u540c\u7684\u5f20\u91cf\u3002 momentum : \u79fb\u52a8\u5e73\u5747\u52a8\u91cf\u3002 \u8fd4\u56de \u66f4\u65b0\u53d8\u91cf\u7684\u64cd\u4f5c\u3002","title":"moving_average_update"},{"location":"9.backend/#dot","text":"keras.backend.dot(x, y) \u5c06 2 \u4e2a\u5f20\u91cf\uff08\u548c/\u6216\u53d8\u91cf\uff09\u76f8\u4e58\u5e76\u8fd4\u56de\u4e00\u4e2a \u5f20\u91cf \u3002 \u5f53\u8bd5\u56fe\u5c06 nD \u5f20\u91cf\u4e0e nD \u5f20\u91cf\u76f8\u4e58\u65f6\uff0c \u5b83\u4f1a\u91cd\u73b0 Theano \u884c\u4e3a\u3002 (\u4f8b\u5982 (2, 3) * (4, 3, 5) -> (2, 4, 5) ) \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 y : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\uff0c x \u548c y \u7684\u70b9\u79ef\u3002 \u4f8b\u5b50 # \u5f20\u91cf\u4e4b\u95f4\u7684\u70b9\u79ef >>> x = K.placeholder(shape=(2, 3)) >>> y = K.placeholder(shape=(3, 4)) >>> xy = K.dot(x, y) >>> xy <tf.Tensor 'MatMul_9:0' shape=(2, 4) dtype=float32> # \u5f20\u91cf\u4e4b\u95f4\u7684\u70b9\u79ef >>> x = K.placeholder(shape=(32, 28, 3)) >>> y = K.placeholder(shape=(3, 4)) >>> xy = K.dot(x, y) >>> xy <tf.Tensor 'MatMul_9:0' shape=(32, 28, 4) dtype=float32> # \u7c7b Theano \u884c\u4e3a\u7684\u4f8b\u5b50 >>> x = K.random_uniform_variable(shape=(2, 3), low=0, high=1) >>> y = K.ones((4, 3, 5)) >>> xy = K.dot(x, y) >>> K.int_shape(xy) (2, 4, 5)","title":"dot"},{"location":"9.backend/#batch_dot","text":"keras.backend.batch_dot(x, y, axes=None) \u6279\u91cf\u5316\u7684\u70b9\u79ef\u3002 \u5f53 x \u548c y \u662f\u6279\u91cf\u6570\u636e\u65f6\uff0c batch_dot \u7528\u4e8e\u8ba1\u7b97 x \u548c y \u7684\u70b9\u79ef\uff0c \u5373\u5c3a\u5bf8\u4e3a (batch_size, :) \u3002 batch_dot \u4ea7\u751f\u4e00\u4e2a\u6bd4\u8f93\u5165\u5c3a\u5bf8\u66f4\u5c0f\u7684\u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u5982\u679c\u7ef4\u6570\u51cf\u5c11\u5230 1\uff0c\u6211\u4eec\u4f7f\u7528 expand_dims \u6765\u786e\u4fdd ndim \u81f3\u5c11\u4e3a 2\u3002 \u53c2\u6570 x : ndim >= 2 \u7684 Keras \u5f20\u91cf\u6216\u53d8\u91cf\u3002 y : ndim >= 2 \u7684 Keras \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axes : \u8868\u793a\u76ee\u6807\u7ef4\u5ea6\u7684\u6574\u6570\u6216\u5217\u8868\u3002 axes[0] \u548c axes[1] \u7684\u957f\u5ea6\u5fc5\u987b\u76f8\u540c\u3002 \u8fd4\u56de \u4e00\u4e2a\u5c3a\u5bf8\u7b49\u4e8e x \u7684\u5c3a\u5bf8\uff08\u51cf\u53bb\u603b\u548c\u7684\u7ef4\u5ea6\uff09\u548c y \u7684\u5c3a\u5bf8\uff08\u51cf\u53bb\u6279\u6b21\u7ef4\u5ea6\u548c\u603b\u548c\u7684\u7ef4\u5ea6\uff09\u7684\u8fde\u63a5\u7684\u5f20\u91cf\u3002 \u5982\u679c\u6700\u540e\u7684\u79e9\u4e3a 1\uff0c\u6211\u4eec\u5c06\u5b83\u91cd\u65b0\u8f6c\u6362\u4e3a (batch_size, 1) \u3002 \u4f8b\u5b50 \u5047\u8bbe x = [[1, 2], [3, 4]] \u548c y = [[5, 6], [7, 8]] \uff0c batch_dot(x, y, axes=1) = [[17], [53]] \u662f x.dot(y.T) \u7684\u4e3b\u5bf9\u89d2\u7ebf\uff0c \u5c3d\u7ba1\u6211\u4eec\u4e0d\u9700\u8981\u8ba1\u7b97\u975e\u5bf9\u89d2\u5143\u7d20\u3002 \u4f2a\u4ee3\u7801\uff1a inner_products = [] for xi, yi in zip(x, y): inner_products.append(xi.dot(yi)) result = stack(inner_products) \u5c3a\u5bf8\u63a8\u65ad\uff1a \u8ba9 x \u7684\u5c3a\u5bf8\u4e3a (100, 20) \uff0c\u4ee5\u53ca y \u7684\u5c3a\u5bf8\u4e3a (100, 30, 20) \u3002 \u5982\u679c axes \u662f (1, 2)\uff0c\u8981\u627e\u51fa\u7ed3\u679c\u5f20\u91cf\u7684\u5c3a\u5bf8\uff0c \u5faa\u73af x \u548c y \u7684\u5c3a\u5bf8\u7684\u6bcf\u4e00\u4e2a\u7ef4\u5ea6\u3002 x.shape[0] : 100 : \u9644\u52a0\u5230\u8f93\u51fa\u5f62\u72b6\uff0c x.shape[1] : 20 : \u4e0d\u9644\u52a0\u5230\u8f93\u51fa\u5f62\u72b6\uff0c x \u7684\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\u5df2\u7ecf\u88ab\u52a0\u548c\u4e86 ( dot_axes[0] = 1)\u3002 y.shape[0] : 100 : \u4e0d\u9644\u52a0\u5230\u8f93\u51fa\u5f62\u72b6\uff0c\u603b\u662f\u5ffd\u7565 y \u7684\u7b2c\u4e00\u7ef4 y.shape[1] : 30 : \u9644\u52a0\u5230\u8f93\u51fa\u5f62\u72b6\uff0c y.shape[2] : 20 : \u4e0d\u9644\u52a0\u5230\u8f93\u51fa\u5f62\u72b6\uff0c y \u7684\u7b2c\u4e8c\u4e2a\u7ef4\u5ea6\u5df2\u7ecf\u88ab\u52a0\u548c\u4e86 ( dot_axes[0] = 2)\u3002 output_shape = (100, 30) >>> x_batch = K.ones(shape=(32, 20, 1)) >>> y_batch = K.ones(shape=(32, 30, 20)) >>> xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=[1, 2]) >>> K.int_shape(xy_batch_dot) (32, 1, 30)","title":"batch_dot"},{"location":"9.backend/#transpose","text":"keras.backend.transpose(x) \u5c06\u5f20\u91cf\u8f6c\u7f6e\u5e76\u8fd4\u56de\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 \u4f8b\u5b50 >>> var = K.variable([[1, 2, 3], [4, 5, 6]]) >>> K.eval(var) array([[ 1., 2., 3.], [ 4., 5., 6.]], dtype=float32) >>> var_transposed = K.transpose(var) >>> K.eval(var_transposed) array([[ 1., 4.], [ 2., 5.], [ 3., 6.]], dtype=float32) >>> inputs = K.placeholder((2, 3)) >>> inputs <tf.Tensor 'Placeholder_11:0' shape=(2, 3) dtype=float32> >>> input_transposed = K.transpose(inputs) >>> input_transposed <tf.Tensor 'transpose_4:0' shape=(3, 2) dtype=float32>","title":"transpose"},{"location":"9.backend/#gather","text":"keras.backend.gather(reference, indices) \u5728\u5f20\u91cf reference \u4e2d\u68c0\u7d22\u7d22\u5f15 indices \u7684\u5143\u7d20\u3002 \u53c2\u6570 reference : \u4e00\u4e2a\u5f20\u91cf\u3002 indices : \u7d22\u5f15\u7684\u6574\u6570\u5f20\u91cf\u3002 \u8fd4\u56de \u4e0e reference \u7c7b\u578b\u76f8\u540c\u7684\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def gather(reference, indices): return reference[indices]","title":"gather"},{"location":"9.backend/#max","text":"keras.backend.max(x, axis=None, keepdims=False) \u5f20\u91cf\u4e2d\u7684\u6700\u5927\u503c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u4e00\u4e2a\u6574\u6570\uff0c\u9700\u8981\u5728\u54ea\u4e2a\u8f74\u5bfb\u627e\u6700\u5927\u503c\u3002 keepdims : \u5e03\u5c14\u503c\uff0c\u662f\u5426\u4fdd\u7559\u539f\u5c3a\u5bf8\u3002 \u5982\u679c keepdims \u4e3a False \uff0c\u5219\u5f20\u91cf\u7684\u79e9\u51cf 1\u3002 \u5982\u679c keepdims \u4e3a True \uff0c\u7f29\u5c0f\u7684\u7ef4\u5ea6\u4fdd\u7559\u4e3a\u957f\u5ea6 1\u3002 \u8fd4\u56de x \u4e2d\u6700\u5927\u503c\u7684\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def max(x, axis=None, keepdims=False): if isinstance(axis, list): axis = tuple(axis) return np.max(x, axis=axis, keepdims=keepdims)","title":"max"},{"location":"9.backend/#min","text":"keras.backend.min(x, axis=None, keepdims=False) \u5f20\u91cf\u4e2d\u7684\u6700\u5c0f\u503c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u4e00\u4e2a\u6574\u6570\uff0c\u9700\u8981\u5728\u54ea\u4e2a\u8f74\u5bfb\u627e\u6700\u5927\u503c\u3002 keepdims : \u5e03\u5c14\u503c\uff0c\u662f\u5426\u4fdd\u7559\u539f\u5c3a\u5bf8\u3002 \u5982\u679c keepdims \u4e3a False \uff0c\u5219\u5f20\u91cf\u7684\u79e9\u51cf 1\u3002 \u5982\u679c keepdims \u4e3a True \uff0c\u7f29\u5c0f\u7684\u7ef4\u5ea6\u4fdd\u7559\u4e3a\u957f\u5ea6 1\u3002 \u8fd4\u56de x \u4e2d\u6700\u5c0f\u503c\u7684\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def min(x, axis=None, keepdims=False): if isinstance(axis, list): axis = tuple(axis) return np.min(x, axis=axis, keepdims=keepdims)","title":"min"},{"location":"9.backend/#sum","text":"keras.backend.sum(x, axis=None, keepdims=False) \u8ba1\u7b97\u5f20\u91cf\u5728\u67d0\u4e00\u6307\u5b9a\u8f74\u7684\u548c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u4e00\u4e2a\u6574\u6570\uff0c\u9700\u8981\u52a0\u548c\u7684\u8f74\u3002 keepdims : \u5e03\u5c14\u503c\uff0c\u662f\u5426\u4fdd\u7559\u539f\u5c3a\u5bf8\u3002 \u5982\u679c keepdims \u4e3a False \uff0c\u5219\u5f20\u91cf\u7684\u79e9\u51cf 1\u3002 \u5982\u679c keepdims \u4e3a True \uff0c\u7f29\u5c0f\u7684\u7ef4\u5ea6\u4fdd\u7559\u4e3a\u957f\u5ea6 1\u3002 \u8fd4\u56de x \u7684\u548c\u7684\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def sum(x, axis=None, keepdims=False): if isinstance(axis, list): axis = tuple(axis) return np.sum(x, axis=axis, keepdims=keepdims)","title":"sum"},{"location":"9.backend/#prod","text":"keras.backend.prod(x, axis=None, keepdims=False) \u5728\u67d0\u4e00\u6307\u5b9a\u8f74\uff0c\u8ba1\u7b97\u5f20\u91cf\u4e2d\u7684\u503c\u7684\u4e58\u79ef\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u4e00\u4e2a\u6574\u6570\u9700\u8981\u8ba1\u7b97\u4e58\u79ef\u7684\u8f74\u3002 keepdims : \u5e03\u5c14\u503c\uff0c\u662f\u5426\u4fdd\u7559\u539f\u5c3a\u5bf8\u3002 \u5982\u679c keepdims \u4e3a False \uff0c\u5219\u5f20\u91cf\u7684\u79e9\u51cf 1\u3002 \u5982\u679c keepdims \u4e3a True \uff0c\u7f29\u5c0f\u7684\u7ef4\u5ea6\u4fdd\u7559\u4e3a\u957f\u5ea6 1\u3002 \u8fd4\u56de x \u7684\u5143\u7d20\u7684\u4e58\u79ef\u7684\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def prod(x, axis=None, keepdims=False): if isinstance(axis, list): axis = tuple(axis) return np.prod(x, axis=axis, keepdims=keepdims)","title":"prod"},{"location":"9.backend/#cumsum","text":"keras.backend.cumsum(x, axis=0) \u5728\u67d0\u4e00\u6307\u5b9a\u8f74\uff0c\u8ba1\u7b97\u5f20\u91cf\u4e2d\u7684\u503c\u7684\u7d2f\u52a0\u548c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u4e00\u4e2a\u6574\u6570\uff0c\u9700\u8981\u52a0\u548c\u7684\u8f74\u3002 \u8fd4\u56de x \u5728 axis \u8f74\u7684\u7d2f\u52a0\u548c\u7684\u5f20\u91cf\u3002","title":"cumsum"},{"location":"9.backend/#cumprod","text":"keras.backend.cumprod(x, axis=0) \u5728\u67d0\u4e00\u6307\u5b9a\u8f74\uff0c\u8ba1\u7b97\u5f20\u91cf\u4e2d\u7684\u503c\u7684\u7d2f\u79ef\u4e58\u79ef\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u4e00\u4e2a\u6574\u6570\uff0c\u9700\u8981\u8ba1\u7b97\u4e58\u79ef\u7684\u8f74\u3002 \u8fd4\u56de x \u5728 axis \u8f74\u7684\u7d2f\u4e58\u7684\u5f20\u91cf\u3002","title":"cumprod"},{"location":"9.backend/#var","text":"keras.backend.var(x, axis=None, keepdims=False) \u5f20\u91cf\u5728\u67d0\u4e00\u6307\u5b9a\u8f74\u7684\u65b9\u5dee\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u4e00\u4e2a\u6574\u6570\uff0c\u8981\u8ba1\u7b97\u65b9\u5dee\u7684\u8f74\u3002 keepdims : \u5e03\u5c14\u503c\uff0c\u662f\u5426\u4fdd\u7559\u539f\u5c3a\u5bf8\u3002 \u5982\u679c keepdims \u4e3a False \uff0c\u5219\u5f20\u91cf\u7684\u79e9\u51cf 1\u3002 \u5982\u679c keepdims \u4e3a True \uff0c\u7f29\u5c0f\u7684\u7ef4\u5ea6\u4fdd\u7559\u4e3a\u957f\u5ea6 1\u3002 \u8fd4\u56de x \u5143\u7d20\u7684\u65b9\u5dee\u7684\u5f20\u91cf\u3002","title":"var"},{"location":"9.backend/#std","text":"keras.backend.std(x, axis=None, keepdims=False) \u5f20\u91cf\u5728\u67d0\u4e00\u6307\u5b9a\u8f74\u7684\u6807\u51c6\u5dee\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u4e00\u4e2a\u6574\u6570\uff0c\u8981\u8ba1\u7b97\u6807\u51c6\u5dee\u7684\u8f74\u3002 keepdims : \u5e03\u5c14\u503c\uff0c\u662f\u5426\u4fdd\u7559\u539f\u5c3a\u5bf8\u3002 \u5982\u679c keepdims \u4e3a False \uff0c\u5219\u5f20\u91cf\u7684\u79e9\u51cf 1\u3002 \u5982\u679c keepdims \u4e3a True \uff0c\u7f29\u5c0f\u7684\u7ef4\u5ea6\u4fdd\u7559\u4e3a\u957f\u5ea6 1\u3002 \u8fd4\u56de x \u5143\u7d20\u7684\u6807\u51c6\u5dee\u7684\u5f20\u91cf\u3002","title":"std"},{"location":"9.backend/#mean","text":"keras.backend.mean(x, axis=None, keepdims=False) \u5f20\u91cf\u5728\u67d0\u4e00\u6307\u5b9a\u8f74\u7684\u5747\u503c\u3002 \u53c2\u6570 x : A tensor or variable. axis : \u6574\u6570\u6216\u5217\u8868\u3002\u9700\u8981\u8ba1\u7b97\u5747\u503c\u7684\u8f74\u3002 keepdims : \u5e03\u5c14\u503c\uff0c\u662f\u5426\u4fdd\u7559\u539f\u5c3a\u5bf8\u3002 \u5982\u679c keepdims \u4e3a False \uff0c\u5219 axis \u4e2d\u6bcf\u4e00\u9879\u7684\u5f20\u91cf\u79e9\u51cf 1\u3002 \u5982\u679c keepdims \u4e3a True \uff0c\u5219\u7f29\u5c0f\u7684\u7ef4\u5ea6\u4fdd\u7559\u4e3a\u957f\u5ea6 1\u3002 \u8fd4\u56de x \u5143\u7d20\u7684\u5747\u503c\u7684\u5f20\u91cf\u3002","title":"mean"},{"location":"9.backend/#any","text":"keras.backend.any(x, axis=None, keepdims=False) reduction \u6309\u4f4d\u5f52\u7ea6\uff08\u903b\u8f91 OR\uff09\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u6267\u884c\u5f52\u7ea6\u64cd\u4f5c\u7684\u8f74\u3002 keepdims : \u662f\u5426\u653e\u5f03\u6216\u5e7f\u64ad\u5f52\u7ea6\u7684\u8f74\u3002 \u8fd4\u56de \u4e00\u4e2a uint8 \u5f20\u91cf (0s \u548c 1s)\u3002","title":"any"},{"location":"9.backend/#all","text":"keras.backend.all(x, axis=None, keepdims=False) \u6309\u4f4d\u5f52\u7ea6\uff08\u903b\u8f91 AND\uff09\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u6267\u884c\u5f52\u7ea6\u64cd\u4f5c\u7684\u8f74\u3002 keepdims : \u662f\u5426\u653e\u5f03\u6216\u5e7f\u64ad\u5f52\u7ea6\u7684\u8f74\u3002 \u8fd4\u56de \u4e00\u4e2a uint8 \u5f20\u91cf (0s \u548c 1s)\u3002","title":"all"},{"location":"9.backend/#argmax","text":"keras.backend.argmax(x, axis=-1) \u8fd4\u56de\u6307\u5b9a\u8f74\u7684\u6700\u5927\u503c\u7684\u7d22\u5f15\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u6267\u884c\u5f52\u7ea6\u64cd\u4f5c\u7684\u8f74\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002","title":"argmax"},{"location":"9.backend/#argmin","text":"keras.backend.argmin(x, axis=-1) \u8fd4\u56de\u6307\u5b9a\u8f74\u7684\u6700\u5c0f\u503c\u7684\u7d22\u5f15\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u6267\u884c\u5f52\u7ea6\u64cd\u4f5c\u7684\u8f74\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002","title":"argmin"},{"location":"9.backend/#square","text":"keras.backend.square(x) \u5143\u7d20\u7ea7\u7684\u5e73\u65b9\u64cd\u4f5c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002","title":"square"},{"location":"9.backend/#abs","text":"keras.backend.abs(x) \u5143\u7d20\u7ea7\u7684\u7edd\u5bf9\u503c\u64cd\u4f5c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002","title":"abs"},{"location":"9.backend/#sqrt","text":"keras.backend.sqrt(x) \u5143\u7d20\u7ea7\u7684\u5e73\u65b9\u6839\u64cd\u4f5c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002","title":"sqrt"},{"location":"9.backend/#exp","text":"keras.backend.exp(x) \u5143\u7d20\u7ea7\u7684\u6307\u6570\u8fd0\u7b97\u64cd\u4f5c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002","title":"exp"},{"location":"9.backend/#log","text":"keras.backend.log(x) \u5143\u7d20\u7ea7\u7684\u5bf9\u6570\u8fd0\u7b97\u64cd\u4f5c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002","title":"log"},{"location":"9.backend/#logsumexp","text":"keras.backend.logsumexp(x, axis=None, keepdims=False) \u8ba1\u7b97 log(sum(exp(\u5f20\u91cf\u5728\u67d0\u4e00\u8f74\u7684\u5143\u7d20)))\u3002 \u8fd9\u4e2a\u51fd\u6570\u5728\u6570\u503c\u4e0a\u6bd4 log(sum(exp(x))) \u66f4\u7a33\u5b9a\u3002 \u5b83\u907f\u514d\u4e86\u6c42\u5927\u8f93\u5165\u7684\u6307\u6570\u9020\u6210\u7684\u4e0a\u6ea2\uff0c\u4ee5\u53ca\u6c42\u5c0f\u8f93\u5165\u7684\u5bf9\u6570\u9020\u6210\u7684\u4e0b\u6ea2\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u4e00\u4e2a\u6574\u6570\uff0c\u9700\u8981\u5f52\u7ea6\u7684\u8f74\u3002 keepdims : \u5e03\u5c14\u503c\uff0c\u662f\u5426\u4fdd\u7559\u539f\u5c3a\u5bf8\u3002 \u5982\u679c keepdims \u4e3a False \uff0c\u5219\u5f20\u91cf\u7684\u79e9\u51cf 1\u3002 \u5982\u679c keepdims \u4e3a True \uff0c\u7f29\u5c0f\u7684\u7ef4\u5ea6\u4fdd\u7559\u4e3a\u957f\u5ea6 1\u3002 \u8fd4\u56de \u5f52\u7ea6\u540e\u7684\u5f20\u91cf\u3002","title":"logsumexp"},{"location":"9.backend/#round","text":"keras.backend.round(x) \u5143\u7d20\u7ea7\u5730\u56db\u820d\u4e94\u5165\u5230\u6700\u63a5\u8fd1\u7684\u6574\u6570\u3002 \u5728\u5e73\u5c40\u7684\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528\u7684\u820d\u5165\u6a21\u5f0f\u662f\u300c\u5076\u6570\u7684\u4e00\u534a\u300d\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002","title":"round"},{"location":"9.backend/#sign","text":"keras.backend.sign(x) \u5143\u7d20\u7ea7\u7684\u7b26\u53f7\u8fd0\u7b97\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002","title":"sign"},{"location":"9.backend/#pow","text":"keras.backend.pow(x, a) \u5143\u7d20\u7ea7\u7684\u6307\u6570\u8fd0\u7b97\u64cd\u4f5c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 a : Python \u6574\u6570\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002","title":"pow"},{"location":"9.backend/#clip","text":"keras.backend.clip(x, min_value, max_value) \u5143\u7d20\u7ea7\u88c1\u526a\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 min_value : Python \u6d6e\u70b9\u6216\u6574\u6570\u3002 max_value : Python \u6d6e\u70b9\u6216\u6574\u6570\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002","title":"clip"},{"location":"9.backend/#equal","text":"keras.backend.equal(x, y) \u9010\u4e2a\u5143\u7d20\u5bf9\u6bd4\u4e24\u4e2a\u5f20\u91cf\u7684\u76f8\u7b49\u60c5\u51b5\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 y : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5e03\u5c14\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def equal(x, y): return x == y","title":"equal"},{"location":"9.backend/#not_equal","text":"keras.backend.not_equal(x, y) \u9010\u4e2a\u5143\u7d20\u5bf9\u6bd4\u4e24\u4e2a\u5f20\u91cf\u7684\u4e0d\u76f8\u7b49\u60c5\u51b5\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 y : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5e03\u5c14\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def not_equal(x, y): return x != y","title":"not_equal"},{"location":"9.backend/#greater","text":"keras.backend.greater(x, y) \u9010\u4e2a\u5143\u7d20\u6bd4\u5bf9 (x > y) \u7684\u771f\u503c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 y : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5e03\u5c14\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def greater(x, y): return x > y","title":"greater"},{"location":"9.backend/#greater_equal","text":"keras.backend.greater_equal(x, y) \u9010\u4e2a\u5143\u7d20\u6bd4\u5bf9 (x >= y) \u7684\u771f\u503c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 y : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5e03\u5c14\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def greater_equal(x, y): return x >= y","title":"greater_equal"},{"location":"9.backend/#less","text":"keras.backend.less(x, y) \u9010\u4e2a\u5143\u7d20\u6bd4\u5bf9 (x < y) \u7684\u771f\u503c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 y : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5e03\u5c14\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def less(x, y): return x < y","title":"less"},{"location":"9.backend/#less_equal","text":"keras.backend.less_equal(x, y) \u9010\u4e2a\u5143\u7d20\u6bd4\u5bf9 (x <= y) \u7684\u771f\u503c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 y : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5e03\u5c14\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def less_equal(x, y): return x <= y","title":"less_equal"},{"location":"9.backend/#maximum","text":"keras.backend.maximum(x, y) \u9010\u4e2a\u5143\u7d20\u6bd4\u5bf9\u4e24\u4e2a\u5f20\u91cf\u7684\u6700\u5927\u503c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 y : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def maximum(x, y): return np.maximum(x, y)","title":"maximum"},{"location":"9.backend/#minimum","text":"keras.backend.minimum(x, y) \u9010\u4e2a\u5143\u7d20\u6bd4\u5bf9\u4e24\u4e2a\u5f20\u91cf\u7684\u6700\u5c0f\u503c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 y : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def minimum(x, y): return np.minimum(x, y)","title":"minimum"},{"location":"9.backend/#sin","text":"keras.backend.sin(x) \u9010\u4e2a\u5143\u7d20\u8ba1\u7b97 x \u7684 sin \u503c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002","title":"sin"},{"location":"9.backend/#cos","text":"keras.backend.cos(x) \u9010\u4e2a\u5143\u7d20\u8ba1\u7b97 x \u7684 cos \u503c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002","title":"cos"},{"location":"9.backend/#normalize_batch_in_training","text":"keras.backend.normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon=0.001) \u8ba1\u7b97\u6279\u6b21\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee\uff0c\u7136\u540e\u5728\u6279\u6b21\u4e0a\u5e94\u7528\u6279\u6b21\u6807\u51c6\u5316\u3002 \u53c2\u6570 x : \u8f93\u5165\u5f20\u91cf\u6216\u53d8\u91cf\u3002 gamma : \u7528\u4e8e\u7f29\u653e\u8f93\u5165\u7684\u5f20\u91cf\u3002 beta : \u7528\u4e8e\u4e2d\u5fc3\u5316\u8f93\u5165\u7684\u5f20\u91cf\u3002 reduction_axes : \u6574\u6570\u8fed\u4ee3\uff0c\u9700\u8981\u6807\u51c6\u5316\u7684\u8f74\u3002 epsilon : \u6a21\u7cca\u56e0\u5b50\u3002 \u8fd4\u56de \u957f\u5ea6\u4e3a 3 \u4e2a\u5143\u7ec4\uff0c (normalized_tensor, mean, variance) \u3002","title":"normalize_batch_in_training"},{"location":"9.backend/#batch_normalization","text":"keras.backend.batch_normalization(x, mean, var, beta, gamma, epsilon=0.001) \u5728\u7ed9\u5b9a\u7684 mean\uff0cvar\uff0cbeta \u548c gamma \u4e0a\u5e94\u7528\u6279\u91cf\u6807\u51c6\u5316\u3002 \u5373\uff0c\u8fd4\u56de\uff1a output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta \u53c2\u6570 x : \u8f93\u5165\u5f20\u91cf\u6216\u53d8\u91cf\u3002 mean : \u6279\u6b21\u7684\u5747\u503c\u3002 var : \u6279\u6b21\u7684\u65b9\u5dee\u3002 beta : \u7528\u4e8e\u4e2d\u5fc3\u5316\u8f93\u5165\u7684\u5f20\u91cf\u3002 gamma : \u7528\u4e8e\u7f29\u653e\u8f93\u5165\u7684\u5f20\u91cf\u3002 epsilon : \u6a21\u7cca\u56e0\u5b50\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002","title":"batch_normalization"},{"location":"9.backend/#concatenate","text":"keras.backend.concatenate(tensors, axis=-1) \u57fa\u4e8e\u6307\u5b9a\u7684\u8f74\uff0c\u8fde\u63a5\u5f20\u91cf\u7684\u5217\u8868\u3002 \u53c2\u6570 tensors : \u9700\u8981\u8fde\u63a5\u7684\u5f20\u91cf\u5217\u8868\u3002 axis : \u8fde\u63a5\u7684\u8f74\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002","title":"concatenate"},{"location":"9.backend/#reshape","text":"keras.backend.reshape(x, shape) \u5c06\u5f20\u91cf\u91cd\u5851\u4e3a\u6307\u5b9a\u7684\u5c3a\u5bf8\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 shape : \u76ee\u6807\u5c3a\u5bf8\u5143\u7ec4\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002","title":"reshape"},{"location":"9.backend/#permute_dimensions","text":"keras.backend.permute_dimensions(x, pattern) \u91cd\u65b0\u6392\u5217\u5f20\u91cf\u7684\u8f74\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 pattern : \u7ef4\u5ea6\u7d22\u5f15\u7684\u5143\u7ec4\uff0c\u4f8b\u5982 (0, 2, 1) \u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002","title":"permute_dimensions"},{"location":"9.backend/#resize_images","text":"keras.backend.resize_images(x, height_factor, width_factor, data_format) \u8c03\u6574 4D \u5f20\u91cf\u4e2d\u5305\u542b\u7684\u56fe\u50cf\u7684\u5927\u5c0f\u3002 \u53c2\u6570 x : \u9700\u8981\u8c03\u6574\u7684\u5f20\u91cf\u6216\u53d8\u91cf\u3002 height_factor : \u6b63\u6574\u6570\u3002 width_factor : \u6b63\u6574\u6570\u3002 data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216 \"channels_first\" \u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f \"channels_last\" \u4e5f\u4e0d\u662f \"channels_first\" \u3002","title":"resize_images"},{"location":"9.backend/#resize_volumes","text":"keras.backend.resize_volumes(x, depth_factor, height_factor, width_factor, data_format) \u8c03\u6574 5D \u5f20\u91cf\u4e2d\u5305\u542b\u7684\u4f53\u79ef\u3002 \u53c2\u6570 x : \u9700\u8981\u8c03\u6574\u7684\u5f20\u91cf\u6216\u53d8\u91cf\u3002 depth_factor : \u6b63\u6574\u6570\u3002 height_factor : \u6b63\u6574\u6570\u3002 width_factor : \u6b63\u6574\u6570\u3002 data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216 \"channels_first\" \u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f \"channels_last\" \u4e5f\u4e0d\u662f \"channels_first\" \u3002","title":"resize_volumes"},{"location":"9.backend/#repeat_elements","text":"keras.backend.repeat_elements(x, rep, axis) \u6cbf\u67d0\u4e00\u8f74\u91cd\u590d\u5f20\u91cf\u7684\u5143\u7d20\uff0c\u5982 np.repeat \u3002 \u5982\u679c x \u7684\u5c3a\u5bf8\u4e3a (s1\uff0cs2\uff0cs3) \u800c axis \u4e3a 1 \uff0c \u5219\u8f93\u51fa\u5c3a\u5bf8\u4e3a (s1\uff0cs2 * rep\uff0cs3\uff09 \u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 rep : Python \u6574\u6570\uff0c\u91cd\u590d\u6b21\u6570\u3002 axis : \u9700\u8981\u91cd\u590d\u7684\u8f74\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002","title":"repeat_elements"},{"location":"9.backend/#repeat","text":"keras.backend.repeat(x, n) \u91cd\u590d\u4e00\u4e2a 2D \u5f20\u91cf\u3002 \u5982\u679c x \u7684\u5c3a\u5bf8\u4e3a (samples, dim) \u5e76\u4e14 n \u4e3a 2 \uff0c \u5219\u8f93\u51fa\u7684\u5c3a\u5bf8\u4e3a (samples, 2, dim) \u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 n : Python \u6574\u6570\uff0c\u91cd\u590d\u6b21\u6570\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002","title":"repeat"},{"location":"9.backend/#arange","text":"keras.backend.arange(start, stop=None, step=1, dtype='int32') \u521b\u5efa\u4e00\u4e2a\u5305\u542b\u6574\u6570\u5e8f\u5217\u7684 1D \u5f20\u91cf\u3002 \u8be5\u51fd\u6570\u53c2\u6570\u4e0e Theano \u7684 arange \u51fd\u6570\u7684\u7ea6\u5b9a\u76f8\u540c\uff1a \u5982\u679c\u53ea\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53c2\u6570\uff0c\u90a3\u5b83\u5c31\u662f stop \u53c2\u6570\u3002 \u8fd4\u56de\u7684\u5f20\u91cf\u7684\u9ed8\u8ba4\u7c7b\u578b\u662f int32 \uff0c\u4ee5\u5339\u914d TensorFlow \u7684\u9ed8\u8ba4\u503c\u3002 \u53c2\u6570 start : \u8d77\u59cb\u503c\u3002 stop : \u7ed3\u675f\u503c\u3002 step : \u4e24\u4e2a\u8fde\u7eed\u503c\u4e4b\u95f4\u7684\u5dee\u3002 dtype : \u8981\u4f7f\u7528\u7684\u6574\u6570\u7c7b\u578b\u3002 \u8fd4\u56de \u4e00\u4e2a\u6574\u6570\u5f20\u91cf\u3002","title":"arange"},{"location":"9.backend/#tile","text":"keras.backend.tile(x, n) \u521b\u5efa\u4e00\u4e2a\u7528 n \u5e73\u94fa \u7684 x \u5f20\u91cf\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 n : \u6574\u6570\u5217\u8868\u3002\u957f\u5ea6\u5fc5\u987b\u4e0e x \u4e2d\u7684\u7ef4\u6570\u76f8\u540c\u3002 \u8fd4\u56de \u4e00\u4e2a\u5e73\u94fa\u7684\u5f20\u91cf\u3002","title":"tile"},{"location":"9.backend/#flatten","text":"keras.backend.flatten(x) \u5c55\u5e73\u4e00\u4e2a\u5f20\u91cf\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u91cd\u65b0\u8c03\u6574\u4e3a 1D \u7684\u5f20\u91cf\u3002","title":"flatten"},{"location":"9.backend/#batch_flatten","text":"keras.backend.batch_flatten(x) \u5c06\u4e00\u4e2a nD \u5f20\u91cf\u53d8\u6210\u4e00\u4e2a \u7b2c 0 \u7ef4\u76f8\u540c\u7684 2D \u5f20\u91cf\u3002 \u6362\u53e5\u8bdd\u8bf4\uff0c\u5b83\u5c06\u6279\u6b21\u4e2d\u7684\u6bcf\u4e00\u4e2a\u6837\u672c\u5c55\u5e73\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002","title":"batch_flatten"},{"location":"9.backend/#expand_dims","text":"keras.backend.expand_dims(x, axis=-1) \u5728\u7d22\u5f15 axis \u8f74\uff0c\u6dfb\u52a0 1 \u4e2a\u5c3a\u5bf8\u7684\u7ef4\u5ea6\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u9700\u8981\u6dfb\u52a0\u65b0\u7684\u8f74\u7684\u4f4d\u7f6e\u3002 \u8fd4\u56de \u4e00\u4e2a\u6269\u5c55\u7ef4\u5ea6\u7684\u8f74\u3002","title":"expand_dims"},{"location":"9.backend/#squeeze","text":"keras.backend.squeeze(x, axis) \u5728\u7d22\u5f15 axis \u8f74\uff0c\u79fb\u9664 1 \u4e2a\u5c3a\u5bf8\u7684\u7ef4\u5ea6\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u9700\u8981\u4e22\u5f03\u7684\u8f74\u3002 \u8fd4\u56de \u4e00\u4e2a\u4e0e x \u6570\u636e\u76f8\u540c\u4f46\u7ef4\u5ea6\u964d\u4f4e\u7684\u5f20\u91cf\u3002","title":"squeeze"},{"location":"9.backend/#temporal_padding","text":"keras.backend.temporal_padding(x, padding=(1, 1)) \u586b\u5145 3D \u5f20\u91cf\u7684\u4e2d\u95f4\u7ef4\u5ea6\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 padding : 2 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\uff0c\u5728\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\u7684\u5f00\u59cb\u548c\u7ed3\u675f\u5904\u6dfb\u52a0\u591a\u5c11\u4e2a\u96f6\u3002 \u8fd4\u56de \u4e00\u4e2a\u586b\u5145\u7684 3D \u5f20\u91cf\u3002","title":"temporal_padding"},{"location":"9.backend/#spatial_2d_padding","text":"keras.backend.spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None) \u586b\u5145 4D \u5f20\u91cf\u7684\u7b2c\u4e8c\u7ef4\u548c\u7b2c\u4e09\u7ef4\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 padding : 2 \u5143\u7ec4\u7684\u5143\u7ec4\uff0c\u586b\u5145\u6a21\u5f0f\u3002 data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216 \"channels_first\" \u3002 \u8fd4\u56de \u4e00\u4e2a\u586b\u5145\u7684 4D \u5f20\u91cf\u3002 \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f \"channels_last\" \u4e5f\u4e0d\u662f \"channels_first\" \u3002","title":"spatial_2d_padding"},{"location":"9.backend/#spatial_3d_padding","text":"keras.backend.spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None) \u6cbf\u7740\u6df1\u5ea6\u3001\u9ad8\u5ea6\u5bbd\u5ea6\u4e09\u4e2a\u7ef4\u5ea6\u586b\u5145 5D \u5f20\u91cf\u3002 \u5206\u522b\u4f7f\u7528 \"padding[0]\", \"padding[1]\" \u548c \"padding[2]\" \u6765\u5de6\u53f3\u586b\u5145\u8fd9\u4e9b\u7ef4\u5ea6\u3002 \u5bf9\u4e8e 'channels_last' \u6570\u636e\u683c\u5f0f\uff0c \u7b2c 2\u30013\u30014 \u7ef4\u5c06\u88ab\u586b\u5145\u3002 \u5bf9\u4e8e 'channels_first' \u6570\u636e\u683c\u5f0f\uff0c \u7b2c 3\u30014\u30015 \u7ef4\u5c06\u88ab\u586b\u5145\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 padding : 3 \u5143\u7ec4\u7684\u5143\u7ec4\uff0c\u586b\u5145\u6a21\u5f0f\u3002 data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216 \"channels_first\" \u3002 \u8fd4\u56de \u4e00\u4e2a\u586b\u5145\u7684 5D \u5f20\u91cf\u3002 \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f \"channels_last\" \u4e5f\u4e0d\u662f \"channels_first\" \u3002","title":"spatial_3d_padding"},{"location":"9.backend/#stack","text":"keras.backend.stack(x, axis=0) \u5c06\u79e9 \u4e3a R \u7684\u5f20\u91cf\u5217\u8868\u5806\u53e0\u6210\u79e9\u4e3a R + 1 \u7684\u5f20\u91cf\u3002 \u53c2\u6570 x : \u5f20\u91cf\u5217\u8868\u3002 axis : \u9700\u8981\u6267\u884c\u5806\u53e0\u7684\u8f74\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002","title":"stack"},{"location":"9.backend/#one_hot","text":"keras.backend.one_hot(indices, num_classes) \u8ba1\u7b97\u4e00\u4e2a\u6574\u6570\u5f20\u91cf\u7684 one-hot \u8868\u793a\u3002 \u53c2\u6570 indices : nD \u6574\u6570\uff0c\u5c3a\u5bf8\u4e3a (batch_size, dim1, dim2, ... dim(n-1)) num_classes : \u6574\u6570\uff0c\u9700\u8981\u8003\u8651\u7684\u7c7b\u522b\u6570\u3002 \u8fd4\u56de \u8f93\u5165\u7684 (n + 1)D one-hot \u8868\u793a\uff0c \u5c3a\u5bf8\u4e3a (batch_size, dim1, dim2, ... dim(n-1), num_classes) \u3002","title":"one_hot"},{"location":"9.backend/#reverse","text":"keras.backend.reverse(x, axes) \u6cbf\u6307\u5b9a\u7684\u8f74\u53cd\u8f6c\u5f20\u91cf\u3002 \u53c2\u6570 x : \u9700\u8981\u53cd\u8f6c\u7684\u5f20\u91cf\u3002 axes : \u6574\u6570\u6216\u6574\u6570\u8fed\u4ee3\u3002\u9700\u8981\u53cd\u8f6c\u7684\u8f74\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def reverse(x, axes): if isinstance(axes, list): axes = tuple(axes) return np.flip(x, axes)","title":"reverse"},{"location":"9.backend/#slice","text":"keras.backend.slice(x, start, size) \u4ece\u5f20\u91cf\u4e2d\u63d0\u53d6\u4e00\u4e2a\u5207\u7247\u3002 \u53c2\u6570 x : \u8f93\u5165\u5f20\u91cf\u3002 start : \u6574\u6570\u5217\u8868/\u5143\u7ec4\uff0c\u8868\u660e\u6bcf\u4e2a\u8f74\u7684\u8d77\u59cb\u5207\u7247\u7d22\u5f15\u4f4d\u7f6e\u3002 size : \u6574\u6570\u5217\u8868/\u5143\u7ec4\uff0c\u8868\u660e\u6bcf\u4e2a\u8f74\u4e0a\u5207\u7247\u591a\u5c11\u7ef4\u5ea6\u3002 \u8fd4\u56de \u4e00\u4e2a\u5207\u7247\u5f20\u91cf\uff1a new_x = x[start[0]: start[0] + size[0], ..., start[-1]: start[-1] + size[-1]]","title":"slice"},{"location":"9.backend/#get_value","text":"keras.backend.get_value(x) \u8fd4\u56de\u4e00\u4e2a\u53d8\u91cf\u7684\u503c\u3002 \u53c2\u6570 x : \u8f93\u5165\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a Numpy \u6570\u7ec4\u3002","title":"get_value"},{"location":"9.backend/#batch_get_value","text":"keras.backend.batch_get_value(ops) \u8fd4\u56de\u591a\u4e2a\u5f20\u91cf\u53d8\u91cf\u7684\u503c\u3002 \u53c2\u6570 ops : \u8981\u8fd0\u884c\u7684\u64cd\u4f5c\u5217\u8868\u3002 \u8fd4\u56de \u4e00\u4e2a Numpy \u6570\u7ec4\u7684\u5217\u8868\u3002","title":"batch_get_value"},{"location":"9.backend/#set_value","text":"keras.backend.set_value(x, value) \u4f7f\u7528 Numpy \u6570\u7ec4\u8bbe\u7f6e\u53d8\u91cf\u7684\u503c\u3002 \u53c2\u6570 x : \u9700\u8981\u8bbe\u7f6e\u65b0\u503c\u7684\u5f20\u91cf\u3002 value : \u9700\u8981\u8bbe\u7f6e\u7684\u503c\uff0c \u4e00\u4e2a\u5c3a\u5bf8\u76f8\u540c\u7684 Numpy \u6570\u7ec4\u3002","title":"set_value"},{"location":"9.backend/#batch_set_value","text":"keras.backend.batch_set_value(tuples) \u4e00\u6b21\u8bbe\u7f6e\u591a\u4e2a\u5f20\u91cf\u53d8\u91cf\u7684\u503c\u3002 \u53c2\u6570 tuples : \u5143\u7ec4 (tensor, value) \u7684\u5217\u8868\u3002 value \u5e94\u8be5\u662f\u4e00\u4e2a Numpy \u6570\u7ec4\u3002","title":"batch_set_value"},{"location":"9.backend/#print_tensor","text":"keras.backend.print_tensor(x, message='') \u5728\u8bc4\u4f30\u65f6\u6253\u5370 message \u548c\u5f20\u91cf\u7684\u503c\u3002 \u8bf7\u6ce8\u610f\uff0c print_tensor \u8fd4\u56de\u4e00\u4e2a\u4e0e x \u76f8\u540c\u7684\u65b0\u5f20\u91cf\uff0c\u5e94\u8be5\u5728\u540e\u9762\u7684\u4ee3\u7801\u4e2d\u4f7f\u7528\u5b83\u3002\u5426\u5219\u5728\u8bc4\u4f30\u8fc7\u7a0b\u4e2d\u4e0d\u4f1a\u8003\u8651\u6253\u5370\u64cd\u4f5c\u3002 \u4f8b\u5b50 >>> x = K.print_tensor(x, message=\"x is: \") \u53c2\u6570 x : \u9700\u8981\u6253\u5370\u7684\u5f20\u91cf\u3002 message : \u9700\u8981\u4e0e\u5f20\u91cf\u4e00\u8d77\u6253\u5370\u7684\u6d88\u606f\u3002 \u8fd4\u56de \u540c\u4e00\u4e2a\u4e0d\u53d8\u7684\u5f20\u91cf x \u3002","title":"print_tensor"},{"location":"9.backend/#function","text":"keras.backend.function(inputs, outputs, updates=None) \u5b9e\u4f8b\u5316 Keras \u51fd\u6570\u3002 \u53c2\u6570 inputs : \u5360\u4f4d\u7b26\u5f20\u91cf\u5217\u8868\u3002 outputs : \u8f93\u51fa\u5f20\u91cf\u5217\u8868\u3002 updates : \u66f4\u65b0\u64cd\u4f5c\u5217\u8868\u3002 **kwargs : \u9700\u8981\u4f20\u9012\u7ed9 tf.Session.run \u7684\u53c2\u6570\u3002 \u8fd4\u56de \u8f93\u51fa\u503c\u4e3a Numpy \u6570\u7ec4\u3002 \u5f02\u5e38 ValueError : \u5982\u679c\u65e0\u6548\u7684 kwargs \u88ab\u4f20\u5165\u3002","title":"function"},{"location":"9.backend/#gradients","text":"keras.backend.gradients(loss, variables) \u8fd4\u56de variables \u5728 loss \u4e0a\u7684\u68af\u5ea6\u3002 \u53c2\u6570 loss : \u9700\u8981\u6700\u5c0f\u5316\u7684\u6807\u91cf\u5f20\u91cf\u3002 variables : \u53d8\u91cf\u5217\u8868\u3002 \u8fd4\u56de \u4e00\u4e2a\u68af\u5ea6\u5f20\u91cf\u3002","title":"gradients"},{"location":"9.backend/#stop_gradient","text":"keras.backend.stop_gradient(variables) \u8fd4\u56de variables \uff0c\u4f46\u662f\u5bf9\u4e8e\u5176\u4ed6\u53d8\u91cf\uff0c\u5176\u68af\u5ea6\u4e3a\u96f6\u3002 \u53c2\u6570 variables : \u9700\u8981\u8003\u8651\u7684\u5f20\u91cf\u6216\u5f20\u91cf\u5217\u8868\uff0c\u4efb\u4f55\u7684\u5176\u4ed6\u53d8\u91cf\u4fdd\u6301\u4e0d\u53d8\u3002 \u8fd4\u56de \u5355\u4e2a\u5f20\u91cf\u6216\u5f20\u91cf\u5217\u8868\uff08\u53d6\u51b3\u4e8e\u4f20\u9012\u7684\u53c2\u6570\uff09\uff0c \u4e0e\u4efb\u4f55\u5176\u4ed6\u53d8\u91cf\u5177\u6709\u6052\u5b9a\u7684\u68af\u5ea6\u3002","title":"stop_gradient"},{"location":"9.backend/#rnn","text":"keras.backend.rnn(step_function, inputs, initial_states, go_backwards=False, mask=None, constants=None, unroll=False, input_length=None) \u5728\u5f20\u91cf\u7684\u65f6\u95f4\u7ef4\u5ea6\u8fed\u4ee3\u3002 \u53c2\u6570 step_function : RNN \u6b65\u9aa4\u51fd\u6570\uff0c inputs : \u5c3a\u5bf8\u4e3a (samples, ...) \u7684\u5f20\u91cf (\u4e0d\u542b\u65f6\u95f4\u7ef4\u5ea6), \u8868\u793a\u6279\u6b21\u6837\u54c1\u5728\u67d0\u4e2a\u65f6\u95f4\u6b65\u7684\u8f93\u5165\u3002 states : \u5f20\u91cf\u5217\u8868\u3002 outputs : \u5c3a\u5bf8\u4e3a (samples, output_dim) \u7684\u5f20\u91cf (\u4e0d\u542b\u65f6\u95f4\u7ef4\u5ea6) new_states : \u5f20\u91cf\u5217\u8868\uff0c\u4e0e states \u957f\u5ea6\u548c\u5c3a\u5bf8\u76f8\u540c\u3002 \u5217\u8868\u4e2d\u7684\u7b2c\u4e00\u4e2a\u72b6\u6001\u5fc5\u987b\u662f\u524d\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u8f93\u51fa\u5f20\u91cf\u3002 inputs : \u65f6\u5e8f\u6570\u636e\u5f20\u91cf (samples, time, ...) (\u6700\u5c11 3D)\u3002 initial_states : \u5c3a\u5bf8\u4e3a (samples, output_dim) \u7684\u5f20\u91cf (\u4e0d\u542b\u65f6\u95f4\u7ef4\u5ea6)\uff0c\u5305\u542b\u6b65\u9aa4\u51fd\u6570\u4e2d\u4f7f\u7528\u7684\u72b6\u6001\u7684\u521d\u59cb\u503c\u3002 go_backwards : \u5e03\u5c14\u503c\u3002\u5982\u679c\u4e3a True\uff0c\u4ee5\u76f8\u53cd\u7684\u987a\u5e8f\u5728\u65f6\u95f4\u7ef4\u4e0a\u8fdb\u884c\u8fed\u4ee3\u5e76\u8fd4\u56de\u76f8\u53cd\u7684\u5e8f\u5217\u3002 mask : \u5c3a\u5bf8\u4e3a (samples, time, 1) \u7684\u4e8c\u8fdb\u5236\u5f20\u91cf\uff0c\u5bf9\u4e8e\u88ab\u5c4f\u853d\u7684\u6bcf\u4e2a\u5143\u7d20\u90fd\u4e3a\u96f6\u3002 constants : \u6bcf\u4e2a\u6b65\u9aa4\u4f20\u9012\u7684\u5e38\u91cf\u503c\u5217\u8868\u3002 unroll : \u662f\u5426\u5c55\u5f00 RNN \u6216\u4f7f\u7528\u7b26\u53f7\u5faa\u73af\uff08\u4f9d\u8d56\u4e8e\u540e\u7aef\u7684 while_loop \u6216 scan \uff09\u3002 input_length : \u4e0e TensorFlow \u5b9e\u73b0\u4e0d\u76f8\u5173\u3002\u5982\u679c\u4f7f\u7528 Theano \u5c55\u5f00\uff0c\u5219\u5fc5\u987b\u6307\u5b9a\u3002 \u8fd4\u56de \u4e00\u4e2a\u5143\u7ec4\uff0c (last_output, outputs, new_states) \u3002 last_output : rnn \u7684\u6700\u540e\u8f93\u51fa\uff0c\u5c3a\u5bf8\u4e3a (samples, ...) \u3002 outputs : \u5c3a\u5bf8\u4e3a (samples, time, ...) \u7684\u5f20\u91cf\uff0c\u5176\u4e2d \u6bcf\u4e00\u9879 outputs[s, t] \u662f\u6837\u672c s \u5728\u65f6\u95f4 t \u7684\u6b65\u9aa4\u51fd\u6570\u8f93\u51fa\u503c\u3002 new_states : \u5f20\u91cf\u5217\u8868\uff0c\u6709\u6b65\u9aa4\u51fd\u6570\u8fd4\u56de\u7684\u6700\u540e\u72b6\u6001\uff0c \u5c3a\u5bf8\u4e3a (samples, ...) \u3002 \u5f02\u5e38 ValueError : \u5982\u679c\u8f93\u5165\u7684\u7ef4\u5ea6\u5c0f\u4e8e 3\u3002 ValueError : \u5982\u679c unroll \u4e3a True \u4f46\u8f93\u5165\u65f6\u95f4\u6b65\u5e76\u4e0d\u662f\u56fa\u5b9a\u7684\u6570\u5b57\u3002 ValueError : \u5982\u679c\u63d0\u4f9b\u4e86 mask (\u975e None ) \u4f46\u672a\u63d0\u4f9b states ( len(states) == 0)\u3002","title":"rnn"},{"location":"9.backend/#switch","text":"keras.backend.switch(condition, then_expression, else_expression) \u6839\u636e\u4e00\u4e2a\u6807\u91cf\u503c\u5728\u4e24\u4e2a\u64cd\u4f5c\u4e4b\u95f4\u5207\u6362\u3002 \u8bf7\u6ce8\u610f\uff0c then_expression \u548c else_expression \u90fd\u5e94\u8be5\u662f \u76f8\u540c\u5c3a\u5bf8 \u7684\u7b26\u53f7\u5f20\u91cf\u3002 \u53c2\u6570 condition : \u5f20\u91cf ( int \u6216 bool )\u3002 then_expression : \u5f20\u91cf\u6216\u8fd4\u56de\u5f20\u91cf\u7684\u53ef\u8c03\u7528\u51fd\u6570\u3002 else_expression : \u5f20\u91cf\u6216\u8fd4\u56de\u5f20\u91cf\u7684\u53ef\u8c03\u7528\u51fd\u6570\u3002 \u8fd4\u56de \u9009\u62e9\u7684\u5f20\u91cf\u3002 \u5f02\u5e38 ValueError : \u5982\u679c condition \u7684\u79e9\u5927\u4e8e\u4e24\u4e2a\u8868\u8fbe\u5f0f\u7684\u79e9\u5e8f\u3002","title":"switch"},{"location":"9.backend/#in_train_phase","text":"keras.backend.in_train_phase(x, alt, training=None) \u5728\u8bad\u7ec3\u9636\u6bb5\u9009\u62e9 x \uff0c\u5176\u4ed6\u9636\u6bb5\u9009\u62e9 alt \u3002 \u8bf7\u6ce8\u610f alt \u5e94\u8be5\u4e0e x \u5c3a\u5bf8\u76f8\u540c\u3002 \u53c2\u6570 x : \u5728\u8bad\u7ec3\u9636\u6bb5\u9700\u8981\u8fd4\u56de\u7684 x (\u5f20\u91cf\u6216\u8fd4\u56de\u5f20\u91cf\u7684\u53ef\u8c03\u7528\u51fd\u6570)\u3002 alt : \u5728\u5176\u4ed6\u9636\u6bb5\u9700\u8981\u8fd4\u56de\u7684 alt (\u5f20\u91cf\u6216\u8fd4\u56de\u5f20\u91cf\u7684\u53ef\u8c03\u7528\u51fd\u6570)\u3002 training : \u53ef\u9009\u7684\u6807\u91cf\u5f20\u91cf (\u6216 Python \u5e03\u5c14\u503c\uff0c\u6216\u8005 Python \u6574\u6570)\uff0c \u4ee5\u6307\u5b9a\u5b66\u4e60\u9636\u6bb5\u3002 \u8fd4\u56de \u57fa\u4e8e training \u6807\u5fd7\uff0c\u8981\u4e48\u8fd4\u56de x \uff0c\u8981\u4e48\u8fd4\u56de alt \u3002 training \u6807\u5fd7\u9ed8\u8ba4\u4e3a K.learning_phase() \u3002","title":"in_train_phase"},{"location":"9.backend/#in_test_phase","text":"keras.backend.in_test_phase(x, alt, training=None) \u5728\u6d4b\u8bd5\u9636\u6bb5\u9009\u62e9 x \uff0c\u5176\u4ed6\u9636\u6bb5\u9009\u62e9 alt \u3002 \u8bf7\u6ce8\u610f alt \u5e94\u8be5\u4e0e x \u5c3a\u5bf8\u76f8\u540c\u3002 \u53c2\u6570 x : \u5728\u8bad\u7ec3\u9636\u6bb5\u9700\u8981\u8fd4\u56de\u7684 x (\u5f20\u91cf\u6216\u8fd4\u56de\u5f20\u91cf\u7684\u53ef\u8c03\u7528\u51fd\u6570)\u3002 alt : \u5728\u5176\u4ed6\u9636\u6bb5\u9700\u8981\u8fd4\u56de\u7684 alt (\u5f20\u91cf\u6216\u8fd4\u56de\u5f20\u91cf\u7684\u53ef\u8c03\u7528\u51fd\u6570)\u3002 training : \u53ef\u9009\u7684\u6807\u91cf\u5f20\u91cf (\u6216 Python \u5e03\u5c14\u503c\uff0c\u6216\u8005 Python \u6574\u6570)\uff0c \u4ee5\u6307\u5b9a\u5b66\u4e60\u9636\u6bb5\u3002 \u8fd4\u56de \u57fa\u4e8e K.learning_phase \uff0c\u8981\u4e48\u8fd4\u56de x \uff0c\u8981\u4e48\u8fd4\u56de alt \u3002","title":"in_test_phase"},{"location":"9.backend/#relu","text":"keras.backend.relu(x, alpha=0.0, max_value=None) ReLU \u6574\u6d41\u7ebf\u6027\u5355\u4f4d\u3002 \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u5b83\u8fd4\u56de\u9010\u4e2a\u5143\u7d20\u7684 max(x, 0) \u503c\u3002 \u53c2\u6570 x : \u4e00\u4e2a\u5f20\u91cf\u6216\u53d8\u91cf\u3002 alpha : \u4e00\u4e2a\u6807\u91cf\uff0c\u8d1f\u6570\u90e8\u5206\u7684\u659c\u7387\uff08\u9ed8\u8ba4\u4e3a 0. \uff09\u3002 max_value : \u9971\u548c\u5ea6\u9608\u503c\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def relu(x, alpha=0., max_value=None, threshold=0.): if max_value is None: max_value = np.inf above_threshold = x * (x >= threshold) above_threshold = np.clip(above_threshold, 0.0, max_value) below_threshold = alpha * (x - threshold) * (x < threshold) return below_threshold + above_threshold","title":"relu"},{"location":"9.backend/#elu","text":"keras.backend.elu(x, alpha=1.0) \u6307\u6570\u7ebf\u6027\u5355\u5143\u3002 \u53c2\u6570 x : \u7528\u4e8e\u8ba1\u7b97\u6fc0\u6d3b\u51fd\u6570\u7684\u5f20\u91cf\u6216\u53d8\u91cf\u3002 alpha : \u4e00\u4e2a\u6807\u91cf\uff0c\u8d1f\u6570\u90e8\u5206\u7684\u659c\u7387\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def elu(x, alpha=1.): return x * (x > 0) + alpha * (np.exp(x) - 1.) * (x < 0)","title":"elu"},{"location":"9.backend/#softmax","text":"keras.backend.softmax(x) \u5f20\u91cf\u7684 Softmax \u503c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def softmax(x, axis=-1): y = np.exp(x - np.max(x, axis, keepdims=True)) return y / np.sum(y, axis, keepdims=True)","title":"softmax"},{"location":"9.backend/#softplus","text":"keras.backend.softplus(x) \u5f20\u91cf\u7684 Softplus \u503c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def softplus(x): return np.log(1. + np.exp(x))","title":"softplus"},{"location":"9.backend/#softsign","text":"keras.backend.softsign(x) \u5f20\u91cf\u7684 Softsign \u503c\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002","title":"softsign"},{"location":"9.backend/#categorical_crossentropy","text":"keras.backend.categorical_crossentropy(target, output, from_logits=False) \u8f93\u51fa\u5f20\u91cf\u4e0e\u76ee\u6807\u5f20\u91cf\u4e4b\u95f4\u7684\u5206\u7c7b\u4ea4\u53c9\u71b5\u3002 \u53c2\u6570 target : \u4e0e output \u5c3a\u5bf8\u76f8\u540c\u7684\u5f20\u91cf\u3002 output : \u7531 softmax \u4ea7\u751f\u7684\u5f20\u91cf (\u9664\u975e from_logits \u4e3a True\uff0c \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b output \u5e94\u8be5\u662f\u5bf9\u6570\u5f62\u5f0f)\u3002 from_logits : \u5e03\u5c14\u503c\uff0c output \u662f softmax \u7684\u7ed3\u679c\uff0c \u8fd8\u662f\u5bf9\u6570\u5f62\u5f0f\u7684\u5f20\u91cf\u3002 \u8fd4\u56de \u8f93\u51fa\u5f20\u91cf\u3002","title":"categorical_crossentropy"},{"location":"9.backend/#sparse_categorical_crossentropy","text":"keras.backend.sparse_categorical_crossentropy(target, output, from_logits=False) \u7a00\u758f\u8868\u793a\u7684\u6574\u6570\u503c\u76ee\u6807\u7684\u5206\u7c7b\u4ea4\u53c9\u71b5\u3002 \u53c2\u6570 target : \u4e00\u4e2a\u6574\u6570\u5f20\u91cf\u3002 output : \u7531 softmax \u4ea7\u751f\u7684\u5f20\u91cf (\u9664\u975e from_logits \u4e3a True\uff0c \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b output \u5e94\u8be5\u662f\u5bf9\u6570\u5f62\u5f0f)\u3002 from_logits : \u5e03\u5c14\u503c\uff0c output \u662f softmax \u7684\u7ed3\u679c\uff0c \u8fd8\u662f\u5bf9\u6570\u5f62\u5f0f\u7684\u5f20\u91cf\u3002 \u8fd4\u56de \u8f93\u51fa\u5f20\u91cf\u3002","title":"sparse_categorical_crossentropy"},{"location":"9.backend/#binary_crossentropy","text":"keras.backend.binary_crossentropy(target, output, from_logits=False) \u8f93\u51fa\u5f20\u91cf\u4e0e\u76ee\u6807\u5f20\u91cf\u4e4b\u95f4\u7684\u4e8c\u8fdb\u5236\u4ea4\u53c9\u71b5\u3002 \u53c2\u6570 target : \u4e0e output \u5c3a\u5bf8\u76f8\u540c\u7684\u5f20\u91cf\u3002 output : \u4e00\u4e2a\u5f20\u91cf\u3002 from_logits : output \u662f\u5426\u662f\u5bf9\u6570\u5f20\u91cf\u3002 \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u8ba4\u4e3a output \u7f16\u7801\u4e86\u6982\u7387\u5206\u5e03\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002","title":"binary_crossentropy"},{"location":"9.backend/#sigmoid","text":"keras.backend.sigmoid(x) \u9010\u4e2a\u5143\u7d20\u6c42 sigmoid \u503c\u3002 \u53c2\u6570 x : \u4e00\u4e2a\u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def sigmoid(x): return 1. / (1. + np.exp(-x))","title":"sigmoid"},{"location":"9.backend/#hard_sigmoid","text":"keras.backend.hard_sigmoid(x) \u5206\u6bb5\u7684 sigmoid \u7ebf\u6027\u8fd1\u4f3c\u3002\u901f\u5ea6\u6bd4 sigmoid \u66f4\u5feb\u3002 \u5982\u679c x < -2.5 \uff0c\u8fd4\u56de 0 \u3002 \u5982\u679c x > 2.5 \uff0c\u8fd4\u56de 1 \u3002 \u5982\u679c -2.5 <= x <= 2.5 \uff0c\u8fd4\u56de 0.2 * x + 0.5 \u3002 \u53c2\u6570 x : \u4e00\u4e2a\u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def hard_sigmoid(x): y = 0.2 * x + 0.5 return np.clip(y, 0, 1)","title":"hard_sigmoid"},{"location":"9.backend/#tanh","text":"keras.backend.tanh(x) \u9010\u4e2a\u5143\u7d20\u6c42 tanh \u503c\u3002 \u53c2\u6570 x : \u4e00\u4e2a\u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def tanh(x): return np.tanh(x)","title":"tanh"},{"location":"9.backend/#dropout","text":"keras.backend.dropout(x, level, noise_shape=None, seed=None) \u5c06 x \u4e2d\u7684\u67d0\u4e9b\u9879\u968f\u673a\u8bbe\u7f6e\u4e3a\u96f6\uff0c\u540c\u65f6\u7f29\u653e\u6574\u4e2a\u5f20\u91cf\u3002 \u53c2\u6570 x : \u5f20\u91cf level : \u5f20\u91cf\u4e2d\u5c06\u88ab\u8bbe\u7f6e\u4e3a 0 \u7684\u9879\u7684\u6bd4\u4f8b\u3002 noise_shape : \u968f\u673a\u751f\u6210\u7684 \u4fdd\u7559/\u4e22\u5f03 \u6807\u5fd7\u7684\u5c3a\u5bf8\uff0c \u5fc5\u987b\u53ef\u4ee5\u5e7f\u64ad\u5230 x \u7684\u5c3a\u5bf8\u3002 seed : \u4fdd\u8bc1\u786e\u5b9a\u6027\u7684\u968f\u673a\u79cd\u5b50\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002","title":"dropout"},{"location":"9.backend/#l2_normalize","text":"keras.backend.l2_normalize(x, axis=None) \u5728\u6307\u5b9a\u7684\u8f74\u4f7f\u7528 L2 \u8303\u5f0f \u6807\u51c6\u5316\u4e00\u4e2a\u5f20\u91cf\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 axis : \u9700\u8981\u6267\u884c\u6807\u51c6\u5316\u7684\u8f74\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 Numpy \u5b9e\u73b0 def l2_normalize(x, axis=-1): y = np.max(np.sum(x ** 2, axis, keepdims=True), axis, keepdims=True) return x / np.sqrt(y)","title":"l2_normalize"},{"location":"9.backend/#in_top_k","text":"keras.backend.in_top_k(predictions, targets, k) \u5224\u65ad targets \u662f\u5426\u5728 predictions \u7684\u524d k \u4e2a\u4e2d\u3002 \u53c2\u6570 predictions : \u4e00\u4e2a\u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch_size, classes) \uff0c\u7c7b\u578b\u4e3a float32 \u3002 targets : \u4e00\u4e2a 1D \u5f20\u91cf\uff0c\u957f\u5ea6\u4e3a batch_size \uff0c\u7c7b\u578b\u4e3a int32 \u6216 int64 \u3002 k : \u4e00\u4e2a int \uff0c\u8981\u8003\u8651\u7684\u9876\u90e8\u5143\u7d20\u7684\u6570\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a 1D \u5f20\u91cf\uff0c\u957f\u5ea6\u4e3a batch_size \uff0c\u7c7b\u578b\u4e3a bool \u3002 \u5982\u679c predictions[i, targets[i]] \u5728 predictions[i] \u7684 top- k \u503c\u4e2d\uff0c \u5219 output[i] \u4e3a True \u3002","title":"in_top_k"},{"location":"9.backend/#conv1d","text":"keras.backend.conv1d(x, kernel, strides=1, padding='valid', data_format=None, dilation_rate=1) 1D \u5377\u79ef\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 kernel : \u6838\u5f20\u91cf\u3002 strides : \u6b65\u957f\u6574\u578b\u3002 padding : \u5b57\u7b26\u4e32\uff0c \"same\" , \"causal\" \u6216 \"valid\" \u3002 data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216 \"channels_first\" \u3002 dilation_rate : \u6574\u6570\u81a8\u80c0\u7387\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\uff0c1D \u5377\u79ef\u7ed3\u679c\u3002 \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f channels_last \u4e5f\u4e0d\u662f channels_first \u3002","title":"conv1d"},{"location":"9.backend/#conv2d","text":"keras.backend.conv2d(x, kernel, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1)) 2D \u5377\u79ef\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 kernel : \u6838\u5f20\u91cf\u3002 strides : \u6b65\u957f\u5143\u7ec4\u3002 padding : \u5b57\u7b26\u4e32\uff0c \"same\" \u6216 \"valid\" \u3002 data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216 \"channels_first\" \u3002 \u5bf9\u4e8e\u8f93\u5165/\u5377\u79ef\u6838/\u8f93\u51fa\uff0c\u662f\u5426\u4f7f\u7528 Theano \u6216 TensorFlow/CNTK\u6570\u636e\u683c\u5f0f\u3002 dilation_rate : 2 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\uff0c2D \u5377\u79ef\u7ed3\u679c\u3002 \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f channels_last \u4e5f\u4e0d\u662f channels_first \u3002","title":"conv2d"},{"location":"9.backend/#conv2d_transpose","text":"keras.backend.conv2d_transpose(x, kernel, output_shape, strides=(1, 1), padding='valid', data_format=None) 2D \u53cd\u5377\u79ef (\u5373\u8f6c\u7f6e\u5377\u79ef)\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 kernel : \u6838\u5f20\u91cf\u3002 output_shape : \u8868\u793a\u8f93\u51fa\u5c3a\u5bf8\u7684 1D \u6574\u578b\u5f20\u91cf\u3002 strides : \u6b65\u957f\u5143\u7ec4\u3002 padding : \u5b57\u7b26\u4e32\uff0c \"same\" \u6216 \"valid\" \u3002 data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216 \"channels_first\" \u3002 \u5bf9\u4e8e\u8f93\u5165/\u5377\u79ef\u6838/\u8f93\u51fa\uff0c\u662f\u5426\u4f7f\u7528 Theano \u6216 TensorFlow/CNTK\u6570\u636e\u683c\u5f0f\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\uff0c\u8f6c\u7f6e\u7684 2D \u5377\u79ef\u7684\u7ed3\u679c\u3002 \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f channels_last \u4e5f\u4e0d\u662f channels_first \u3002","title":"conv2d_transpose"},{"location":"9.backend/#separable_conv1d","text":"keras.backend.separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1, padding='valid', data_format=None, dilation_rate=1) \u5e26\u53ef\u5206\u79bb\u6ee4\u6ce2\u5668\u7684 1D \u5377\u79ef\u3002 \u53c2\u6570 x : \u8f93\u5165\u5f20\u91cf\u3002 depthwise_kernel : \u7528\u4e8e\u6df1\u5ea6\u5377\u79ef\u7684\u5377\u79ef\u6838\u3002 pointwise_kernel : 1x1 \u5377\u79ef\u6838\u3002 strides : \u6b65\u957f\u6574\u6570\u3002 padding : \u5b57\u7b26\u4e32\uff0c \"same\" \u6216 \"valid\" \u3002 data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216 \"channels_first\" \u3002 dilation_rate : \u6574\u6570\u81a8\u80c0\u7387\u3002 \u8fd4\u56de \u8f93\u51fa\u5f20\u91cf\u3002 \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f channels_last \u4e5f\u4e0d\u662f channels_first \u3002","title":"separable_conv1d"},{"location":"9.backend/#separable_conv2d","text":"keras.backend.separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1)) \u5e26\u53ef\u5206\u79bb\u6ee4\u6ce2\u5668\u7684 2D \u5377\u79ef\u3002 \u53c2\u6570 x : \u8f93\u5165\u5f20\u91cf\u3002 depthwise_kernel : \u7528\u4e8e\u6df1\u5ea6\u5377\u79ef\u7684\u5377\u79ef\u6838\u3002 pointwise_kernel : 1x1 \u5377\u79ef\u6838\u3002 strides : \u6b65\u957f\u5143\u7ec4 (\u957f\u5ea6\u4e3a 2)\u3002 padding : \u5b57\u7b26\u4e32\uff0c \"same\" \u6216 \"valid\" \u3002 data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216 \"channels_first\" \u3002 dilation_rate : \u6574\u6570\u5143\u7ec4\uff0c\u53ef\u5206\u79bb\u5377\u79ef\u7684\u81a8\u80c0\u7387\u3002 \u8fd4\u56de \u8f93\u51fa\u5f20\u91cf\u3002 \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f channels_last \u4e5f\u4e0d\u662f channels_first \u3002","title":"separable_conv2d"},{"location":"9.backend/#depthwise_conv2d","text":"keras.backend.depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1)) \u5e26\u53ef\u5206\u79bb\u6ee4\u6ce2\u5668\u7684 2D \u5377\u79ef\u3002 \u53c2\u6570 x : \u8f93\u5165\u5f20\u91cf\u3002 depthwise_kernel : \u7528\u4e8e\u6df1\u5ea6\u5377\u79ef\u7684\u5377\u79ef\u6838\u3002 strides : \u6b65\u957f\u5143\u7ec4 (\u957f\u5ea6\u4e3a 2)\u3002 padding : \u5b57\u7b26\u4e32\uff0c \"same\" \u6216 \"valid\" \u3002 data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216 \"channels_first\" \u3002 dilation_rate : \u6574\u6570\u5143\u7ec4\uff0c\u53ef\u5206\u79bb\u5377\u79ef\u7684\u81a8\u80c0\u7387\u3002 \u8fd4\u56de \u8f93\u51fa\u5f20\u91cf\u3002 \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f channels_last \u4e5f\u4e0d\u662f channels_first \u3002","title":"depthwise_conv2d"},{"location":"9.backend/#conv3d","text":"keras.backend.conv3d(x, kernel, strides=(1, 1, 1), padding='valid', data_format=None, dilation_rate=(1, 1, 1)) 3D \u5377\u79ef\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 kernel : \u6838\u5f20\u91cf\u3002 strides : \u6b65\u957f\u5143\u7ec4\u3002 padding : \u5b57\u7b26\u4e32\uff0c \"same\" \u6216 \"valid\" \u3002 data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216 \"channels_first\" \u3002 dilation_rate : 3 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\uff0c3D \u5377\u79ef\u7684\u7ed3\u679c\u3002 \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f channels_last \u4e5f\u4e0d\u662f channels_first \u3002","title":"conv3d"},{"location":"9.backend/#conv3d_transpose","text":"keras.backend.conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1), padding='valid', data_format=None) 3D \u53cd\u5377\u79ef (\u5373\u8f6c\u7f6e\u5377\u79ef)\u3002 \u53c2\u6570 x : \u8f93\u5165\u5f20\u91cf\u3002 kernel : \u6838\u5f20\u91cf\u3002 output_shape : \u8868\u793a\u8f93\u51fa\u5c3a\u5bf8\u7684 1D \u6574\u6570\u5f20\u91cf\u3002 strides : \u6b65\u957f\u5143\u7ec4\u3002 padding : \u5b57\u7b26\u4e32\uff0c \"same\" \u6216 \"valid\" \u3002 data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216 \"channels_first\" \u3002 \u5bf9\u4e8e\u8f93\u5165/\u5377\u79ef\u6838/\u8f93\u51fa\uff0c\u662f\u5426\u4f7f\u7528 Theano \u6216 TensorFlow/CNTK\u6570\u636e\u683c\u5f0f\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\uff0c3D \u8f6c\u7f6e\u5377\u79ef\u7684\u7ed3\u679c\u3002 \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f channels_last \u4e5f\u4e0d\u662f channels_first \u3002","title":"conv3d_transpose"},{"location":"9.backend/#pool2d","text":"keras.backend.pool2d(x, pool_size, strides=(1, 1), padding='valid', data_format=None, pool_mode='max') 2D \u6c60\u5316\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 pool_size : 2 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\u3002 strides : 2 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\u3002 padding : \u5b57\u7b26\u4e32\uff0c \"same\" \u6216 \"valid\" \u3002 data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216 \"channels_first\" \u3002 pool_mode : \u5b57\u7b26\u4e32\uff0c \"max\" \u6216 \"avg\" \u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\uff0c2D \u6c60\u5316\u7684\u7ed3\u679c\u3002 \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f channels_last \u4e5f\u4e0d\u662f channels_first \u3002 ValueError : if pool_mode \u65e2\u4e0d\u662f \"max\" \u4e5f\u4e0d\u662f \"avg\" \u3002","title":"pool2d"},{"location":"9.backend/#pool3d","text":"keras.backend.pool3d(x, pool_size, strides=(1, 1, 1), padding='valid', data_format=None, pool_mode='max') 3D \u6c60\u5316\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 pool_size : 3 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\u3002 strides : 3 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\u3002 padding : \u5b57\u7b26\u4e32\uff0c \"same\" \u6216 \"valid\" \u3002 data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216 \"channels_first\" \u3002 pool_mode : \u5b57\u7b26\u4e32\uff0c \"max\" \u6216 \"avg\" \u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\uff0c3D \u6c60\u5316\u7684\u7ed3\u679c\u3002 \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f channels_last \u4e5f\u4e0d\u662f channels_first \u3002 ValueError : if pool_mode \u65e2\u4e0d\u662f \"max\" \u4e5f\u4e0d\u662f \"avg\" \u3002","title":"pool3d"},{"location":"9.backend/#bias_add","text":"keras.backend.bias_add(x, bias, data_format=None) \u7ed9\u5f20\u91cf\u6dfb\u52a0\u4e00\u4e2a\u504f\u7f6e\u5411\u91cf\u3002 \u53c2\u6570 x : \u5f20\u91cf\u6216\u53d8\u91cf\u3002 bias : \u9700\u8981\u6dfb\u52a0\u7684\u504f\u7f6e\u5411\u91cf\u3002 data_format : \u5b57\u7b26\u4e32\uff0c \"channels_last\" \u6216 \"channels_first\" \u3002 \u8fd4\u56de \u8f93\u51fa\u5f20\u91cf\u3002 \u5f02\u5e38 ValueError : \u4ee5\u4e0b\u4e24\u79cd\u60c5\u51b5\u4e4b\u4e00\uff1a \u65e0\u6548\u7684 data_format \u53c2\u6570\u3002 \u65e0\u6548\u7684\u504f\u7f6e\u5411\u91cf\u5c3a\u5bf8\u3002 \u504f\u7f6e\u5e94\u8be5\u662f\u4e00\u4e2a ndim(x)-1 \u7ef4\u7684\u5411\u91cf\u6216\u5f20\u91cf\u3002","title":"bias_add"},{"location":"9.backend/#random_normal","text":"keras.backend.random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None) \u8fd4\u56de\u6b63\u6001\u5206\u5e03\u503c\u7684\u5f20\u91cf\u3002 \u53c2\u6570 shape : \u4e00\u4e2a\u6574\u6570\u5143\u7ec4\uff0c\u9700\u8981\u521b\u5efa\u7684\u5f20\u91cf\u7684\u5c3a\u5bf8\u3002 mean : \u4e00\u4e2a\u6d6e\u70b9\u6570\uff0c\u62bd\u6837\u7684\u6b63\u6001\u5206\u5e03\u5e73\u5747\u503c\u3002 stddev : \u4e00\u4e2a\u6d6e\u70b9\u6570\uff0c\u62bd\u6837\u7684\u6b63\u6001\u5206\u5e03\u6807\u51c6\u5dee\u3002 dtype : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684\u5f20\u91cf\u7684\u6570\u636e\u7c7b\u578b\u3002 seed : \u6574\u6570\uff0c\u968f\u673a\u79cd\u5b50\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002","title":"random_normal"},{"location":"9.backend/#random_uniform","text":"keras.backend.random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None) \u8fd4\u56de\u5747\u5300\u5206\u5e03\u503c\u7684\u5f20\u91cf\u3002 \u53c2\u6570 shape : \u4e00\u4e2a\u6574\u6570\u5143\u7ec4\uff0c\u9700\u8981\u521b\u5efa\u7684\u5f20\u91cf\u7684\u5c3a\u5bf8\u3002 minval : \u4e00\u4e2a\u6d6e\u70b9\u6570\uff0c\u62bd\u6837\u7684\u5747\u5300\u5206\u5e03\u4e0b\u754c\u3002 maxval : \u4e00\u4e2a\u6d6e\u70b9\u6570\uff0c\u62bd\u6837\u7684\u5747\u5300\u5206\u5e03\u4e0a\u754c\u3002 dtype : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684\u5f20\u91cf\u7684\u6570\u636e\u7c7b\u578b\u3002 seed : \u6574\u6570\uff0c\u968f\u673a\u79cd\u5b50\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002","title":"random_uniform"},{"location":"9.backend/#random_binomial","text":"keras.backend.random_binomial(shape, p=0.0, dtype=None, seed=None) \u8fd4\u56de\u968f\u673a\u4e8c\u9879\u5206\u5e03\u503c\u7684\u5f20\u91cf\u3002 \u53c2\u6570 shape : \u4e00\u4e2a\u6574\u6570\u5143\u7ec4\uff0c\u9700\u8981\u521b\u5efa\u7684\u5f20\u91cf\u7684\u5c3a\u5bf8\u3002 p : \u4e00\u4e2a\u6d6e\u70b9\u6570\uff0c 0. <= p <= 1 \uff0c\u4e8c\u9879\u5206\u5e03\u7684\u6982\u7387\u3002 dtype : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684\u5f20\u91cf\u7684\u6570\u636e\u7c7b\u578b\u3002 seed : \u6574\u6570\uff0c\u968f\u673a\u79cd\u5b50\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002","title":"random_binomial"},{"location":"9.backend/#truncated_normal","text":"keras.backend.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None) \u8fd4\u56de\u622a\u65ad\u7684\u968f\u673a\u6b63\u6001\u5206\u5e03\u503c\u7684\u5f20\u91cf\u3002 \u751f\u6210\u7684\u503c\u9075\u5faa\u5177\u6709\u6307\u5b9a\u5e73\u5747\u503c\u548c\u6807\u51c6\u5dee\u7684\u6b63\u6001\u5206\u5e03\uff0c \u6b64\u5916\uff0c\u5176\u4e2d\u6570\u503c\u5927\u4e8e\u5e73\u5747\u503c\u4e24\u4e2a\u6807\u51c6\u5dee\u7684\u5c06\u88ab\u4e22\u5f03\u548c\u91cd\u65b0\u6311\u9009\u3002 \u53c2\u6570 shape : \u4e00\u4e2a\u6574\u6570\u5143\u7ec4\uff0c\u9700\u8981\u521b\u5efa\u7684\u5f20\u91cf\u7684\u5c3a\u5bf8\u3002 mean : \u5e73\u5747\u503c\u3002 stddev : \u6807\u51c6\u5dee\u3002 dtype : \u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u7684\u5f20\u91cf\u7684\u6570\u636e\u7c7b\u578b\u3002 seed : \u6574\u6570\uff0c\u968f\u673a\u79cd\u5b50\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002","title":"truncated_normal"},{"location":"9.backend/#ctc_label_dense_to_sparse","text":"keras.backend.ctc_label_dense_to_sparse(labels, label_lengths) \u5c06 CTC \u6807\u7b7e\u4ece\u5bc6\u96c6\u8f6c\u6362\u4e3a\u7a00\u758f\u8868\u793a\u3002 \u53c2\u6570 labels : \u5bc6\u96c6 CTC \u6807\u7b7e\u3002 label_lengths : \u6807\u7b7e\u957f\u5ea6\u3002 \u8fd4\u56de \u4e00\u4e2a\u8868\u793a\u6807\u7b7e\u7684\u7a00\u758f\u5f20\u91cf\u3002","title":"ctc_label_dense_to_sparse"},{"location":"9.backend/#ctc_batch_cost","text":"keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length) \u5728\u6bcf\u4e2a\u6279\u6b21\u5143\u7d20\u4e0a\u8fd0\u884c CTC \u635f\u5931\u7b97\u6cd5\u3002 \u53c2\u6570 y_true : \u5f20\u91cf (samples, max_string_length) \uff0c \u5305\u542b\u771f\u5b9e\u6807\u7b7e\u3002 y_pred : \u5f20\u91cf (samples, time_steps, num_categories) \uff0c \u5305\u542b\u9884\u6d4b\u503c\uff0c\u6216 softmax \u8f93\u51fa\u3002 input_length : \u5f20\u91cf (samples, 1) \uff0c \u5305\u542b y_pred \u4e2d\u6bcf\u4e2a\u6279\u6b21\u6837\u672c\u7684\u5e8f\u5217\u957f\u5ea6\u3002 label_length : \u5f20\u91cf (samples, 1) \uff0c \u5305\u542b y_true \u4e2d\u6bcf\u4e2a\u6279\u6b21\u6837\u672c\u7684\u5e8f\u5217\u957f\u5ea6\u3002 \u8fd4\u56de \u5c3a\u5bf8\u4e3a (samples,1) \u7684\u5f20\u91cf\uff0c\u5305\u542b\u6bcf\u4e00\u4e2a\u5143\u7d20\u7684 CTC \u635f\u5931\u3002","title":"ctc_batch_cost"},{"location":"9.backend/#ctc_decode","text":"keras.backend.ctc_decode(y_pred, input_length, greedy=True, beam_width=100, top_paths=1) \u89e3\u7801 softmax \u7684\u8f93\u51fa\u3002 \u53ef\u4ee5\u4f7f\u7528\u8d2a\u5fc3\u641c\u7d22\uff08\u4e5f\u79f0\u4e3a\u6700\u4f18\u8def\u5f84\uff09\u6216\u53d7\u9650\u5b57\u5178\u641c\u7d22\u3002 \u53c2\u6570 y_pred : \u5f20\u91cf (samples, time_steps, num_categories) \uff0c \u5305\u542b\u9884\u6d4b\u503c\uff0c\u6216 softmax \u8f93\u51fa\u3002 input_length : \u5f20\u91cf (samples,) \uff0c \u5305\u542b y_pred \u4e2d\u6bcf\u4e2a\u6279\u6b21\u6837\u672c\u7684\u5e8f\u5217\u957f\u5ea6\u3002 greedy : \u5982\u679c\u4e3a True \uff0c\u5219\u6267\u884c\u66f4\u5feb\u901f\u7684\u6700\u4f18\u8def\u5f84\u641c\u7d22\uff0c\u800c\u4e0d\u4f7f\u7528\u5b57\u5178\u3002 beam_width : \u5982\u679c greedy \u4e3a false \uff0c\u5c06\u4f7f\u7528\u8be5\u5bbd\u5ea6\u7684 beam \u641c\u7d22\u89e3\u7801\u5668\u641c\u7d22\u3002 top_paths : \u5982\u679c greedy \u4e3a false \uff0c \u5c06\u8fd4\u56de\u591a\u5c11\u6761\u6700\u53ef\u80fd\u7684\u8def\u5f84\u3002 \u8fd4\u56de Tuple : List : \u5982\u679c greedy \u4e3a true \uff0c\u8fd4\u56de\u5305\u542b\u89e3\u7801\u5e8f\u5217\u7684\u4e00\u4e2a\u5143\u7d20\u7684\u5217\u8868\u3002 \u5982\u679c\u4e3a false \uff0c\u8fd4\u56de\u6700\u53ef\u80fd\u89e3\u7801\u5e8f\u5217\u7684 top_paths \u3002 Important : \u7a7a\u767d\u6807\u7b7e\u8fd4\u56de\u4e3a -1 \u3002\u5305\u542b\u6bcf\u4e2a\u89e3\u7801\u5e8f\u5217\u7684\u5bf9\u6570\u6982\u7387\u7684\u5f20\u91cf (top_paths,) \u3002","title":"ctc_decode"},{"location":"9.backend/#map_fn","text":"keras.backend.map_fn(fn, elems, name=None, dtype=None) \u5c06\u51fd\u6570fn\u6620\u5c04\u5230\u5143\u7d20 elems \u4e0a\u5e76\u8fd4\u56de\u8f93\u51fa\u3002 \u53c2\u6570 fn : \u5c06\u5728\u6bcf\u4e2a\u5143\u7d20\u4e0a\u8c03\u7528\u7684\u53ef\u8c03\u7528\u51fd\u6570\u3002 elems : \u5f20\u91cf\u3002 name : \u6620\u5c04\u8282\u70b9\u5728\u56fe\u4e2d\u7684\u5b57\u7b26\u4e32\u540d\u79f0\u3002 dtype : \u8f93\u51fa\u6570\u636e\u683c\u5f0f\u3002 \u8fd4\u56de \u6570\u636e\u7c7b\u578b\u4e3a dtype \u7684\u5f20\u91cf\u3002","title":"map_fn"},{"location":"9.backend/#foldl","text":"keras.backend.foldl(fn, elems, initializer=None, name=None) \u4f7f\u7528 fn \u5f52\u7ea6 elems\uff0c\u4ee5\u4ece\u5de6\u5230\u53f3\u7ec4\u5408\u5b83\u4eec\u3002 \u53c2\u6570 fn : \u5c06\u5728\u6bcf\u4e2a\u5143\u7d20\u548c\u4e00\u4e2a\u7d2f\u52a0\u5668\u4e0a\u8c03\u7528\u7684\u53ef\u8c03\u7528\u51fd\u6570\uff0c\u4f8b\u5982 lambda acc, x: acc + x \u3002 elems : \u5f20\u91cf\u3002 initializer : \u7b2c\u4e00\u4e2a\u4f7f\u7528\u7684\u503c (\u5982\u679c\u4e3a None\uff0c\u4f7f\u7528 elems[0] )\u3002 name : foldl \u8282\u70b9\u5728\u56fe\u4e2d\u7684\u5b57\u7b26\u4e32\u540d\u79f0\u3002 \u8fd4\u56de \u4e0e initializer \u7c7b\u578b\u548c\u5c3a\u5bf8\u76f8\u540c\u7684\u5f20\u91cf\u3002","title":"foldl"},{"location":"9.backend/#foldr","text":"keras.backend.foldr(fn, elems, initializer=None, name=None) \u4f7f\u7528 fn \u5f52\u7ea6 elems\uff0c\u4ee5\u4ece\u53f3\u5230\u5de6\u7ec4\u5408\u5b83\u4eec\u3002 \u53c2\u6570 fn : \u5c06\u5728\u6bcf\u4e2a\u5143\u7d20\u548c\u4e00\u4e2a\u7d2f\u52a0\u5668\u4e0a\u8c03\u7528\u7684\u53ef\u8c03\u7528\u51fd\u6570\uff0c\u4f8b\u5982 lambda acc, x: acc + x \u3002 elems : \u5f20\u91cf\u3002 initializer : \u7b2c\u4e00\u4e2a\u4f7f\u7528\u7684\u503c (\u5982\u679c\u4e3a None\uff0c\u4f7f\u7528 elems[-1] )\u3002 name : foldr \u8282\u70b9\u5728\u56fe\u4e2d\u7684\u5b57\u7b26\u4e32\u540d\u79f0\u3002 \u8fd4\u56de \u4e0e initializer \u7c7b\u578b\u548c\u5c3a\u5bf8\u76f8\u540c\u7684\u5f20\u91cf\u3002","title":"foldr"},{"location":"9.backend/#local_conv1d","text":"keras.backend.local_conv1d(inputs, kernel, kernel_size, strides, data_format=None) \u5728\u4e0d\u5171\u4eab\u6743\u503c\u7684\u60c5\u51b5\u4e0b\uff0c\u8fd0\u7528 1D \u5377\u79ef\u3002 \u53c2\u6570 inputs : 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch_size, steps, input_dim) kernel : \u5377\u79ef\u7684\u975e\u5171\u4eab\u6743\u91cd, \u5c3a\u5bf8\u4e3a (output_items, feature_dim, filters) kernel_size : \u4e00\u4e2a\u6574\u6570\u7684\u5143\u7ec4\uff0c \u6307\u5b9a 1D \u5377\u79ef\u7a97\u53e3\u7684\u957f\u5ea6\u3002 strides : \u4e00\u4e2a\u6574\u6570\u7684\u5143\u7ec4\uff0c \u6307\u5b9a\u5377\u79ef\u6b65\u957f\u3002 data_format : \u6570\u636e\u683c\u5f0f\uff0cchannels_first \u6216 channels_last\u3002 \u8fd4\u56de \u8fd0\u7528\u4e0d\u5171\u4eab\u6743\u91cd\u7684 1D \u5377\u79ef\u4e4b\u540e\u7684\u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch_size, output_length, filters)\u3002 \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f channels_last \u4e5f\u4e0d\u662f channels_first \u3002","title":"local_conv1d"},{"location":"9.backend/#local_conv2d","text":"keras.backend.local_conv2d(inputs, kernel, kernel_size, strides, output_shape, data_format=None) \u5728\u4e0d\u5171\u4eab\u6743\u503c\u7684\u60c5\u51b5\u4e0b\uff0c\u8fd0\u7528 2D \u5377\u79ef\u3002 \u53c2\u6570 inputs : \u5982\u679c data_format='channels_first' \uff0c \u5219\u4e3a\u5c3a\u5bf8\u4e3a (batch_size, filters, new_rows, new_cols) \u7684 4D \u5f20\u91cf\u3002 \u5982\u679c data_format='channels_last' \uff0c \u5219\u4e3a\u5c3a\u5bf8\u4e3a (batch_size, new_rows, new_cols, filters) \u7684 4D \u5f20\u91cf\u3002 kernel : \u5377\u79ef\u7684\u975e\u5171\u4eab\u6743\u91cd, \u5c3a\u5bf8\u4e3a (output_items, feature_dim, filters) kernel_size : 2 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\uff0c \u6307\u5b9a 2D \u5377\u79ef\u7a97\u53e3\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u3002 strides : 2 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\uff0c \u6307\u5b9a 2D \u5377\u79ef\u6cbf\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u65b9\u5411\u7684\u6b65\u957f\u3002 output_shape : \u5143\u7ec4 (output_row, output_col) \u3002 data_format : \u6570\u636e\u683c\u5f0f\uff0cchannels_first \u6216 channels_last\u3002 \u8fd4\u56de \u4e00\u4e2a 4D \u5f20\u91cf\u3002 \u5982\u679c data_format='channels_first' \uff0c\u5c3a\u5bf8\u4e3a (batch_size, filters, new_rows, new_cols)\u3002 \u5982\u679c data_format='channels_last' \uff0c\u5c3a\u5bf8\u4e3a (batch_size, new_rows, new_cols, filters) \u5f02\u5e38 ValueError : \u5982\u679c data_format \u65e2\u4e0d\u662f channels_last \u4e5f\u4e0d\u662f channels_first \u3002","title":"local_conv2d"},{"location":"9.backend/#backend","text":"backend.backend() \u516c\u5f00\u53ef\u7528\u7684\u65b9\u6cd5\uff0c\u4ee5\u786e\u5b9a\u5f53\u524d\u540e\u7aef\u3002 \u8fd4\u56de \u5b57\u7b26\u4e32\uff0cKeras \u76ee\u524d\u6b63\u5728\u4f7f\u7528\u7684\u540e\u7aef\u540d\u3002 \u4f8b\u5b50 >>> keras.backend.backend() 'tensorflow'","title":"backend"},{"location":"90.initializers/","text":"\u521d\u59cb\u5316 Initializers \u521d\u59cb\u5316\u5668\u7684\u7528\u6cd5 \u521d\u59cb\u5316\u5b9a\u4e49\u4e86\u8bbe\u7f6e Keras \u5404\u5c42\u6743\u91cd\u968f\u673a\u521d\u59cb\u503c\u7684\u65b9\u6cd5\u3002 \u7528\u6765\u5c06\u521d\u59cb\u5316\u5668\u4f20\u5165 Keras \u5c42\u7684\u53c2\u6570\u540d\u53d6\u51b3\u4e8e\u5177\u4f53\u7684\u5c42\u3002\u901a\u5e38\u5173\u952e\u5b57\u4e3a kernel_initializer \u548c bias_initializer : model.add(Dense(64, kernel_initializer='random_uniform', bias_initializer='zeros')) \u53ef\u7528\u7684\u521d\u59cb\u5316\u5668 \u4e0b\u9762\u8fd9\u4e9b\u662f\u53ef\u7528\u7684\u5185\u7f6e\u521d\u59cb\u5316\u5668\uff0c\u662f keras.initializers \u6a21\u5757\u7684\u4e00\u90e8\u5206: [source] Initializer keras.initializers.Initializer() \u521d\u59cb\u5316\u5668\u57fa\u7c7b\uff1a\u6240\u6709\u521d\u59cb\u5316\u5668\u7ee7\u627f\u8fd9\u4e2a\u7c7b\u3002 [source] Zeros keras.initializers.Zeros() \u5c06\u5f20\u91cf\u521d\u59cb\u503c\u8bbe\u4e3a 0 \u7684\u521d\u59cb\u5316\u5668\u3002 [source] Ones keras.initializers.Ones() \u5c06\u5f20\u91cf\u521d\u59cb\u503c\u8bbe\u4e3a 1 \u7684\u521d\u59cb\u5316\u5668\u3002 [source] Constant keras.initializers.Constant(value=0) \u5c06\u5f20\u91cf\u521d\u59cb\u503c\u8bbe\u4e3a\u4e00\u4e2a\u5e38\u6570\u7684\u521d\u59cb\u5316\u5668\u3002 \u53c2\u6570 value : \u6d6e\u70b9\u6570\uff0c\u751f\u6210\u7684\u5f20\u91cf\u7684\u503c\u3002 [source] RandomNormal keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None) \u6309\u7167\u6b63\u6001\u5206\u5e03\u751f\u6210\u968f\u673a\u5f20\u91cf\u7684\u521d\u59cb\u5316\u5668\u3002 \u53c2\u6570 mean : \u4e00\u4e2a Python \u6807\u91cf\u6216\u8005\u4e00\u4e2a\u6807\u91cf\u5f20\u91cf\u3002\u8981\u751f\u6210\u7684\u968f\u673a\u503c\u7684\u5e73\u5747\u6570\u3002 stddev : \u4e00\u4e2a Python \u6807\u91cf\u6216\u8005\u4e00\u4e2a\u6807\u91cf\u5f20\u91cf\u3002\u8981\u751f\u6210\u7684\u968f\u673a\u503c\u7684\u6807\u51c6\u5dee\u3002 seed : \u4e00\u4e2a Python \u6574\u6570\u3002\u7528\u4e8e\u8bbe\u7f6e\u968f\u673a\u6570\u79cd\u5b50\u3002 [source] RandomUniform keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None) \u6309\u7167\u5747\u5300\u5206\u5e03\u751f\u6210\u968f\u673a\u5f20\u91cf\u7684\u521d\u59cb\u5316\u5668\u3002 \u53c2\u6570 minval : \u4e00\u4e2a Python \u6807\u91cf\u6216\u8005\u4e00\u4e2a\u6807\u91cf\u5f20\u91cf\u3002\u8981\u751f\u6210\u7684\u968f\u673a\u503c\u7684\u8303\u56f4\u4e0b\u9650\u3002 maxval : \u4e00\u4e2a Python \u6807\u91cf\u6216\u8005\u4e00\u4e2a\u6807\u91cf\u5f20\u91cf\u3002\u8981\u751f\u6210\u7684\u968f\u673a\u503c\u7684\u8303\u56f4\u4e0b\u9650\u3002\u9ed8\u8ba4\u4e3a\u6d6e\u70b9\u7c7b\u578b\u7684 1\u3002 seed : \u4e00\u4e2a Python \u6574\u6570\u3002\u7528\u4e8e\u8bbe\u7f6e\u968f\u673a\u6570\u79cd\u5b50\u3002 [source] TruncatedNormal keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=None) \u6309\u7167\u622a\u5c3e\u6b63\u6001\u5206\u5e03\u751f\u6210\u968f\u673a\u5f20\u91cf\u7684\u521d\u59cb\u5316\u5668\u3002 \u751f\u6210\u7684\u968f\u673a\u503c\u4e0e RandomNormal \u751f\u6210\u7684\u7c7b\u4f3c\uff0c\u4f46\u662f\u5728\u8ddd\u79bb\u5e73\u5747\u503c\u4e24\u4e2a\u6807\u51c6\u5dee\u4e4b\u5916\u7684\u968f\u673a\u503c\u5c06\u88ab\u4e22\u5f03\u5e76\u91cd\u65b0\u751f\u6210\u3002\u8fd9\u662f\u7528\u6765\u751f\u6210\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\u548c\u6ee4\u6ce2\u5668\u7684\u63a8\u8350\u521d\u59cb\u5316\u5668\u3002 Arguments mean : \u4e00\u4e2a Python \u6807\u91cf\u6216\u8005\u4e00\u4e2a\u6807\u91cf\u5f20\u91cf\u3002\u8981\u751f\u6210\u7684\u968f\u673a\u503c\u7684\u5e73\u5747\u6570\u3002 stddev : \u4e00\u4e2a Python \u6807\u91cf\u6216\u8005\u4e00\u4e2a\u6807\u91cf\u5f20\u91cf\u3002\u8981\u751f\u6210\u7684\u968f\u673a\u503c\u7684\u6807\u51c6\u5dee\u3002 seed : \u4e00\u4e2a Python \u6574\u6570\u3002\u7528\u4e8e\u8bbe\u7f6e\u968f\u673a\u6570\u79cd\u5b50\u3002 [source] VarianceScaling keras.initializers.VarianceScaling(scale=1.0, mode='fan_in', distribution='normal', seed=None) \u521d\u59cb\u5316\u5668\u80fd\u591f\u6839\u636e\u6743\u503c\u7684\u5c3a\u5bf8\u8c03\u6574\u5176\u89c4\u6a21\u3002 \u4f7f\u7528 distribution=\"normal\" \u65f6\uff0c\u6837\u672c\u662f\u4ece\u4e00\u4e2a\u4ee5 0 \u4e3a\u4e2d\u5fc3\u7684\u622a\u65ad\u6b63\u6001\u5206\u5e03\u4e2d\u62bd\u53d6\u7684\uff0c stddev = sqrt(scale / n) \uff0c\u5176\u4e2d n \u662f\uff1a \u6743\u503c\u5f20\u91cf\u4e2d\u8f93\u5165\u5355\u5143\u7684\u6570\u91cf\uff0c\u5982\u679c mode = \"fan_in\"\u3002 \u8f93\u51fa\u5355\u5143\u7684\u6570\u91cf\uff0c\u5982\u679c mode = \"fan_out\"\u3002 \u8f93\u5165\u548c\u8f93\u51fa\u5355\u4f4d\u6570\u91cf\u7684\u5e73\u5747\u6570\uff0c\u5982\u679c mode = \"fan_avg\"\u3002 \u4f7f\u7528 distribution=\"uniform\" \u65f6\uff0c\u6837\u672c\u662f\u4ece [-limit\uff0climit] \u5185\u7684\u5747\u5300\u5206\u5e03\u4e2d\u62bd\u53d6\u7684\uff0c\u5176\u4e2d limit = sqrt(3 * scale / n) \u3002 \u53c2\u6570 scale : \u7f29\u653e\u56e0\u5b50\uff08\u6b63\u6d6e\u70b9\u6570\uff09\u3002 mode : \"fan_in\", \"fan_out\", \"fan_avg\" \u4e4b\u4e00\u3002 distribution : \u4f7f\u7528\u7684\u968f\u673a\u5206\u5e03\u3002\"normal\", \"uniform\" \u4e4b\u4e00\u3002 seed : \u4e00\u4e2a Python \u6574\u6570\u3002\u4f5c\u4e3a\u968f\u673a\u53d1\u751f\u5668\u7684\u79cd\u5b50\u3002 \u5f02\u5e38 ValueError : \u5982\u679c \"scale\", mode\" \u6216 \"distribution\" \u53c2\u6570\u65e0\u6548\u3002 [source] Orthogonal keras.initializers.Orthogonal(gain=1.0, seed=None) \u751f\u6210\u4e00\u4e2a\u968f\u673a\u6b63\u4ea4\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668\u3002 \u53c2\u6570 gain : \u9002\u7528\u4e8e\u6b63\u4ea4\u77e9\u9635\u7684\u4e58\u6cd5\u56e0\u5b50\u3002 seed : \u4e00\u4e2a Python \u6574\u6570\u3002\u4f5c\u4e3a\u968f\u673a\u53d1\u751f\u5668\u7684\u79cd\u5b50\u3002 \u53c2\u8003\u6587\u732e Saxe et al., http://arxiv.org/abs/1312.6120 [source] Identity keras.initializers.Identity(gain=1.0) \u751f\u6210\u5355\u4f4d\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668\u3002 \u4ec5\u7528\u4e8e 2D \u65b9\u9635\u3002 \u53c2\u6570 gain : \u9002\u7528\u4e8e\u5355\u4f4d\u77e9\u9635\u7684\u4e58\u6cd5\u56e0\u5b50\u3002 lecun_uniform keras.initializers.lecun_uniform(seed=None) LeCun \u5747\u5300\u521d\u59cb\u5316\u5668\u3002 \u5b83\u4ece [-limit\uff0climit] \u4e2d\u7684\u5747\u5300\u5206\u5e03\u4e2d\u62bd\u53d6\u6837\u672c\uff0c \u5176\u4e2d limit \u662f sqrt(3 / fan_in) \uff0c fan_in \u662f\u6743\u503c\u5f20\u91cf\u4e2d\u7684\u8f93\u5165\u5355\u4f4d\u7684\u6570\u91cf\u3002 \u53c2\u6570 seed : \u4e00\u4e2a Python \u6574\u6570\u3002\u4f5c\u4e3a\u968f\u673a\u53d1\u751f\u5668\u7684\u79cd\u5b50\u3002 \u8fd4\u56de \u4e00\u4e2a\u521d\u59cb\u5316\u5668\u3002 \u53c2\u8003\u6587\u732e LeCun 98, Efficient Backprop glorot_normal keras.initializers.glorot_normal(seed=None) Glorot \u6b63\u6001\u5206\u5e03\u521d\u59cb\u5316\u5668\uff0c\u4e5f\u79f0\u4e3a Xavier \u6b63\u6001\u5206\u5e03\u521d\u59cb\u5316\u5668\u3002 \u5b83\u4ece\u4ee5 0 \u4e3a\u4e2d\u5fc3\uff0c\u6807\u51c6\u5dee\u4e3a stddev = sqrt(2 / (fan_in + fan_out)) \u7684\u622a\u65ad\u6b63\u6001\u5206\u5e03\u4e2d\u62bd\u53d6\u6837\u672c\uff0c \u5176\u4e2d fan_in \u662f\u6743\u503c\u5f20\u91cf\u4e2d\u7684\u8f93\u5165\u5355\u4f4d\u7684\u6570\u91cf\uff0c fan_out \u662f\u6743\u503c\u5f20\u91cf\u4e2d\u7684\u8f93\u51fa\u5355\u4f4d\u7684\u6570\u91cf\u3002 \u53c2\u6570 seed : \u4e00\u4e2a Python \u6574\u6570\u3002\u4f5c\u4e3a\u968f\u673a\u53d1\u751f\u5668\u7684\u79cd\u5b50\u3002 \u8fd4\u56de \u4e00\u4e2a\u521d\u59cb\u5316\u5668\u3002 \u53c2\u8003\u6587\u732e Understanding the difficulty of training deep feedforward neural networks glorot_uniform keras.initializers.glorot_uniform(seed=None) Glorot \u5747\u5300\u5206\u5e03\u521d\u59cb\u5316\u5668\uff0c\u4e5f\u79f0\u4e3a Xavier \u5747\u5300\u5206\u5e03\u521d\u59cb\u5316\u5668\u3002 \u5b83\u4ece [-limit\uff0climit] \u4e2d\u7684\u5747\u5300\u5206\u5e03\u4e2d\u62bd\u53d6\u6837\u672c\uff0c \u5176\u4e2d limit \u662f sqrt(6 / (fan_in + fan_out)) \uff0c fan_in \u662f\u6743\u503c\u5f20\u91cf\u4e2d\u7684\u8f93\u5165\u5355\u4f4d\u7684\u6570\u91cf\uff0c fan_out \u662f\u6743\u503c\u5f20\u91cf\u4e2d\u7684\u8f93\u51fa\u5355\u4f4d\u7684\u6570\u91cf\u3002 \u53c2\u6570 seed : \u4e00\u4e2a Python \u6574\u6570\u3002\u4f5c\u4e3a\u968f\u673a\u53d1\u751f\u5668\u7684\u79cd\u5b50\u3002 \u8fd4\u56de \u4e00\u4e2a\u521d\u59cb\u5316\u5668\u3002 \u53c2\u8003\u6587\u732e Understanding the difficulty of training deep feedforward neural networks he_normal keras.initializers.he_normal(seed=None) He \u6b63\u6001\u5206\u5e03\u521d\u59cb\u5316\u5668\u3002 \u5b83\u4ece\u4ee5 0 \u4e3a\u4e2d\u5fc3\uff0c\u6807\u51c6\u5dee\u4e3a stddev = sqrt(2 / fan_in) \u7684\u622a\u65ad\u6b63\u6001\u5206\u5e03\u4e2d\u62bd\u53d6\u6837\u672c\uff0c \u5176\u4e2d fan_in \u662f\u6743\u503c\u5f20\u91cf\u4e2d\u7684\u8f93\u5165\u5355\u4f4d\u7684\u6570\u91cf\uff0c \u53c2\u6570 seed : \u4e00\u4e2a Python \u6574\u6570\u3002\u4f5c\u4e3a\u968f\u673a\u53d1\u751f\u5668\u7684\u79cd\u5b50\u3002 \u8fd4\u56de \u4e00\u4e2a\u521d\u59cb\u5316\u5668\u3002 \u53c2\u8003\u6587\u732e Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification lecun_normal keras.initializers.lecun_normal(seed=None) LeCun \u6b63\u6001\u5206\u5e03\u521d\u59cb\u5316\u5668\u3002 \u5b83\u4ece\u4ee5 0 \u4e3a\u4e2d\u5fc3\uff0c\u6807\u51c6\u5dee\u4e3a stddev = sqrt(1 / fan_in) \u7684\u622a\u65ad\u6b63\u6001\u5206\u5e03\u4e2d\u62bd\u53d6\u6837\u672c\uff0c \u5176\u4e2d fan_in \u662f\u6743\u503c\u5f20\u91cf\u4e2d\u7684\u8f93\u5165\u5355\u4f4d\u7684\u6570\u91cf\u3002 \u53c2\u6570 seed : \u4e00\u4e2a Python \u6574\u6570\u3002\u4f5c\u4e3a\u968f\u673a\u53d1\u751f\u5668\u7684\u79cd\u5b50\u3002 \u8fd4\u56de \u4e00\u4e2a\u521d\u59cb\u5316\u5668\u3002 \u53c2\u8003\u6587\u732e Self-Normalizing Neural Networks Efficient Backprop he_uniform keras.initializers.he_uniform(seed=None) He \u5747\u5300\u65b9\u5dee\u7f29\u653e\u521d\u59cb\u5316\u5668\u3002 \u5b83\u4ece [-limit\uff0climit] \u4e2d\u7684\u5747\u5300\u5206\u5e03\u4e2d\u62bd\u53d6\u6837\u672c\uff0c \u5176\u4e2d limit \u662f sqrt(6 / fan_in) \uff0c \u5176\u4e2d fan_in \u662f\u6743\u503c\u5f20\u91cf\u4e2d\u7684\u8f93\u5165\u5355\u4f4d\u7684\u6570\u91cf\u3002 \u53c2\u6570 seed : \u4e00\u4e2a Python \u6574\u6570\u3002\u4f5c\u4e3a\u968f\u673a\u53d1\u751f\u5668\u7684\u79cd\u5b50\u3002 \u8fd4\u56de \u4e00\u4e2a\u521d\u59cb\u5316\u5668\u3002 \u53c2\u8003\u6587\u732e Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification \u4e00\u4e2a\u521d\u59cb\u5316\u5668\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u4e2a\u5b57\u7b26\u4e32\u4f20\u9012\uff08\u5fc5\u987b\u5339\u914d\u4e0a\u9762\u7684\u4e00\u4e2a\u53ef\u7528\u7684\u521d\u59cb\u5316\u5668\uff09\uff0c\u6216\u8005\u4f5c\u4e3a\u4e00\u4e2a\u53ef\u8c03\u7528\u51fd\u6570\u4f20\u9012\uff1a from keras import initializers model.add(Dense(64, kernel_initializer=initializers.random_normal(stddev=0.01))) # \u540c\u6837\u6709\u6548;\u5c06\u4f7f\u7528\u9ed8\u8ba4\u53c2\u6570\u3002 model.add(Dense(64, kernel_initializer='random_normal')) \u4f7f\u7528\u81ea\u5b9a\u4e49\u521d\u59cb\u5316\u5668 \u5982\u679c\u4f20\u9012\u4e00\u4e2a\u81ea\u5b9a\u4e49\u7684\u53ef\u8c03\u7528\u51fd\u6570\uff0c\u90a3\u4e48\u5b83\u5fc5\u987b\u4f7f\u7528\u53c2\u6570 shape \uff08\u9700\u8981\u521d\u59cb\u5316\u7684\u53d8\u91cf\u7684\u5c3a\u5bf8\uff09\u548c dtype \uff08\u6570\u636e\u7c7b\u578b\uff09\uff1a from keras import backend as K def my_init(shape, dtype=None): return K.random_normal(shape, dtype=dtype) model.add(Dense(64, kernel_initializer=my_init))","title":"\u521d\u59cb\u5316 Initializers"},{"location":"90.initializers/#initializers","text":"","title":"\u521d\u59cb\u5316 Initializers"},{"location":"90.initializers/#_1","text":"\u521d\u59cb\u5316\u5b9a\u4e49\u4e86\u8bbe\u7f6e Keras \u5404\u5c42\u6743\u91cd\u968f\u673a\u521d\u59cb\u503c\u7684\u65b9\u6cd5\u3002 \u7528\u6765\u5c06\u521d\u59cb\u5316\u5668\u4f20\u5165 Keras \u5c42\u7684\u53c2\u6570\u540d\u53d6\u51b3\u4e8e\u5177\u4f53\u7684\u5c42\u3002\u901a\u5e38\u5173\u952e\u5b57\u4e3a kernel_initializer \u548c bias_initializer : model.add(Dense(64, kernel_initializer='random_uniform', bias_initializer='zeros'))","title":"\u521d\u59cb\u5316\u5668\u7684\u7528\u6cd5"},{"location":"90.initializers/#_2","text":"\u4e0b\u9762\u8fd9\u4e9b\u662f\u53ef\u7528\u7684\u5185\u7f6e\u521d\u59cb\u5316\u5668\uff0c\u662f keras.initializers \u6a21\u5757\u7684\u4e00\u90e8\u5206: [source]","title":"\u53ef\u7528\u7684\u521d\u59cb\u5316\u5668"},{"location":"90.initializers/#initializer","text":"keras.initializers.Initializer() \u521d\u59cb\u5316\u5668\u57fa\u7c7b\uff1a\u6240\u6709\u521d\u59cb\u5316\u5668\u7ee7\u627f\u8fd9\u4e2a\u7c7b\u3002 [source]","title":"Initializer"},{"location":"90.initializers/#zeros","text":"keras.initializers.Zeros() \u5c06\u5f20\u91cf\u521d\u59cb\u503c\u8bbe\u4e3a 0 \u7684\u521d\u59cb\u5316\u5668\u3002 [source]","title":"Zeros"},{"location":"90.initializers/#ones","text":"keras.initializers.Ones() \u5c06\u5f20\u91cf\u521d\u59cb\u503c\u8bbe\u4e3a 1 \u7684\u521d\u59cb\u5316\u5668\u3002 [source]","title":"Ones"},{"location":"90.initializers/#constant","text":"keras.initializers.Constant(value=0) \u5c06\u5f20\u91cf\u521d\u59cb\u503c\u8bbe\u4e3a\u4e00\u4e2a\u5e38\u6570\u7684\u521d\u59cb\u5316\u5668\u3002 \u53c2\u6570 value : \u6d6e\u70b9\u6570\uff0c\u751f\u6210\u7684\u5f20\u91cf\u7684\u503c\u3002 [source]","title":"Constant"},{"location":"90.initializers/#randomnormal","text":"keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None) \u6309\u7167\u6b63\u6001\u5206\u5e03\u751f\u6210\u968f\u673a\u5f20\u91cf\u7684\u521d\u59cb\u5316\u5668\u3002 \u53c2\u6570 mean : \u4e00\u4e2a Python \u6807\u91cf\u6216\u8005\u4e00\u4e2a\u6807\u91cf\u5f20\u91cf\u3002\u8981\u751f\u6210\u7684\u968f\u673a\u503c\u7684\u5e73\u5747\u6570\u3002 stddev : \u4e00\u4e2a Python \u6807\u91cf\u6216\u8005\u4e00\u4e2a\u6807\u91cf\u5f20\u91cf\u3002\u8981\u751f\u6210\u7684\u968f\u673a\u503c\u7684\u6807\u51c6\u5dee\u3002 seed : \u4e00\u4e2a Python \u6574\u6570\u3002\u7528\u4e8e\u8bbe\u7f6e\u968f\u673a\u6570\u79cd\u5b50\u3002 [source]","title":"RandomNormal"},{"location":"90.initializers/#randomuniform","text":"keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None) \u6309\u7167\u5747\u5300\u5206\u5e03\u751f\u6210\u968f\u673a\u5f20\u91cf\u7684\u521d\u59cb\u5316\u5668\u3002 \u53c2\u6570 minval : \u4e00\u4e2a Python \u6807\u91cf\u6216\u8005\u4e00\u4e2a\u6807\u91cf\u5f20\u91cf\u3002\u8981\u751f\u6210\u7684\u968f\u673a\u503c\u7684\u8303\u56f4\u4e0b\u9650\u3002 maxval : \u4e00\u4e2a Python \u6807\u91cf\u6216\u8005\u4e00\u4e2a\u6807\u91cf\u5f20\u91cf\u3002\u8981\u751f\u6210\u7684\u968f\u673a\u503c\u7684\u8303\u56f4\u4e0b\u9650\u3002\u9ed8\u8ba4\u4e3a\u6d6e\u70b9\u7c7b\u578b\u7684 1\u3002 seed : \u4e00\u4e2a Python \u6574\u6570\u3002\u7528\u4e8e\u8bbe\u7f6e\u968f\u673a\u6570\u79cd\u5b50\u3002 [source]","title":"RandomUniform"},{"location":"90.initializers/#truncatednormal","text":"keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=None) \u6309\u7167\u622a\u5c3e\u6b63\u6001\u5206\u5e03\u751f\u6210\u968f\u673a\u5f20\u91cf\u7684\u521d\u59cb\u5316\u5668\u3002 \u751f\u6210\u7684\u968f\u673a\u503c\u4e0e RandomNormal \u751f\u6210\u7684\u7c7b\u4f3c\uff0c\u4f46\u662f\u5728\u8ddd\u79bb\u5e73\u5747\u503c\u4e24\u4e2a\u6807\u51c6\u5dee\u4e4b\u5916\u7684\u968f\u673a\u503c\u5c06\u88ab\u4e22\u5f03\u5e76\u91cd\u65b0\u751f\u6210\u3002\u8fd9\u662f\u7528\u6765\u751f\u6210\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\u548c\u6ee4\u6ce2\u5668\u7684\u63a8\u8350\u521d\u59cb\u5316\u5668\u3002 Arguments mean : \u4e00\u4e2a Python \u6807\u91cf\u6216\u8005\u4e00\u4e2a\u6807\u91cf\u5f20\u91cf\u3002\u8981\u751f\u6210\u7684\u968f\u673a\u503c\u7684\u5e73\u5747\u6570\u3002 stddev : \u4e00\u4e2a Python \u6807\u91cf\u6216\u8005\u4e00\u4e2a\u6807\u91cf\u5f20\u91cf\u3002\u8981\u751f\u6210\u7684\u968f\u673a\u503c\u7684\u6807\u51c6\u5dee\u3002 seed : \u4e00\u4e2a Python \u6574\u6570\u3002\u7528\u4e8e\u8bbe\u7f6e\u968f\u673a\u6570\u79cd\u5b50\u3002 [source]","title":"TruncatedNormal"},{"location":"90.initializers/#variancescaling","text":"keras.initializers.VarianceScaling(scale=1.0, mode='fan_in', distribution='normal', seed=None) \u521d\u59cb\u5316\u5668\u80fd\u591f\u6839\u636e\u6743\u503c\u7684\u5c3a\u5bf8\u8c03\u6574\u5176\u89c4\u6a21\u3002 \u4f7f\u7528 distribution=\"normal\" \u65f6\uff0c\u6837\u672c\u662f\u4ece\u4e00\u4e2a\u4ee5 0 \u4e3a\u4e2d\u5fc3\u7684\u622a\u65ad\u6b63\u6001\u5206\u5e03\u4e2d\u62bd\u53d6\u7684\uff0c stddev = sqrt(scale / n) \uff0c\u5176\u4e2d n \u662f\uff1a \u6743\u503c\u5f20\u91cf\u4e2d\u8f93\u5165\u5355\u5143\u7684\u6570\u91cf\uff0c\u5982\u679c mode = \"fan_in\"\u3002 \u8f93\u51fa\u5355\u5143\u7684\u6570\u91cf\uff0c\u5982\u679c mode = \"fan_out\"\u3002 \u8f93\u5165\u548c\u8f93\u51fa\u5355\u4f4d\u6570\u91cf\u7684\u5e73\u5747\u6570\uff0c\u5982\u679c mode = \"fan_avg\"\u3002 \u4f7f\u7528 distribution=\"uniform\" \u65f6\uff0c\u6837\u672c\u662f\u4ece [-limit\uff0climit] \u5185\u7684\u5747\u5300\u5206\u5e03\u4e2d\u62bd\u53d6\u7684\uff0c\u5176\u4e2d limit = sqrt(3 * scale / n) \u3002 \u53c2\u6570 scale : \u7f29\u653e\u56e0\u5b50\uff08\u6b63\u6d6e\u70b9\u6570\uff09\u3002 mode : \"fan_in\", \"fan_out\", \"fan_avg\" \u4e4b\u4e00\u3002 distribution : \u4f7f\u7528\u7684\u968f\u673a\u5206\u5e03\u3002\"normal\", \"uniform\" \u4e4b\u4e00\u3002 seed : \u4e00\u4e2a Python \u6574\u6570\u3002\u4f5c\u4e3a\u968f\u673a\u53d1\u751f\u5668\u7684\u79cd\u5b50\u3002 \u5f02\u5e38 ValueError : \u5982\u679c \"scale\", mode\" \u6216 \"distribution\" \u53c2\u6570\u65e0\u6548\u3002 [source]","title":"VarianceScaling"},{"location":"90.initializers/#orthogonal","text":"keras.initializers.Orthogonal(gain=1.0, seed=None) \u751f\u6210\u4e00\u4e2a\u968f\u673a\u6b63\u4ea4\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668\u3002 \u53c2\u6570 gain : \u9002\u7528\u4e8e\u6b63\u4ea4\u77e9\u9635\u7684\u4e58\u6cd5\u56e0\u5b50\u3002 seed : \u4e00\u4e2a Python \u6574\u6570\u3002\u4f5c\u4e3a\u968f\u673a\u53d1\u751f\u5668\u7684\u79cd\u5b50\u3002 \u53c2\u8003\u6587\u732e Saxe et al., http://arxiv.org/abs/1312.6120 [source]","title":"Orthogonal"},{"location":"90.initializers/#identity","text":"keras.initializers.Identity(gain=1.0) \u751f\u6210\u5355\u4f4d\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668\u3002 \u4ec5\u7528\u4e8e 2D \u65b9\u9635\u3002 \u53c2\u6570 gain : \u9002\u7528\u4e8e\u5355\u4f4d\u77e9\u9635\u7684\u4e58\u6cd5\u56e0\u5b50\u3002","title":"Identity"},{"location":"90.initializers/#lecun_uniform","text":"keras.initializers.lecun_uniform(seed=None) LeCun \u5747\u5300\u521d\u59cb\u5316\u5668\u3002 \u5b83\u4ece [-limit\uff0climit] \u4e2d\u7684\u5747\u5300\u5206\u5e03\u4e2d\u62bd\u53d6\u6837\u672c\uff0c \u5176\u4e2d limit \u662f sqrt(3 / fan_in) \uff0c fan_in \u662f\u6743\u503c\u5f20\u91cf\u4e2d\u7684\u8f93\u5165\u5355\u4f4d\u7684\u6570\u91cf\u3002 \u53c2\u6570 seed : \u4e00\u4e2a Python \u6574\u6570\u3002\u4f5c\u4e3a\u968f\u673a\u53d1\u751f\u5668\u7684\u79cd\u5b50\u3002 \u8fd4\u56de \u4e00\u4e2a\u521d\u59cb\u5316\u5668\u3002 \u53c2\u8003\u6587\u732e LeCun 98, Efficient Backprop","title":"lecun_uniform"},{"location":"90.initializers/#glorot_normal","text":"keras.initializers.glorot_normal(seed=None) Glorot \u6b63\u6001\u5206\u5e03\u521d\u59cb\u5316\u5668\uff0c\u4e5f\u79f0\u4e3a Xavier \u6b63\u6001\u5206\u5e03\u521d\u59cb\u5316\u5668\u3002 \u5b83\u4ece\u4ee5 0 \u4e3a\u4e2d\u5fc3\uff0c\u6807\u51c6\u5dee\u4e3a stddev = sqrt(2 / (fan_in + fan_out)) \u7684\u622a\u65ad\u6b63\u6001\u5206\u5e03\u4e2d\u62bd\u53d6\u6837\u672c\uff0c \u5176\u4e2d fan_in \u662f\u6743\u503c\u5f20\u91cf\u4e2d\u7684\u8f93\u5165\u5355\u4f4d\u7684\u6570\u91cf\uff0c fan_out \u662f\u6743\u503c\u5f20\u91cf\u4e2d\u7684\u8f93\u51fa\u5355\u4f4d\u7684\u6570\u91cf\u3002 \u53c2\u6570 seed : \u4e00\u4e2a Python \u6574\u6570\u3002\u4f5c\u4e3a\u968f\u673a\u53d1\u751f\u5668\u7684\u79cd\u5b50\u3002 \u8fd4\u56de \u4e00\u4e2a\u521d\u59cb\u5316\u5668\u3002 \u53c2\u8003\u6587\u732e Understanding the difficulty of training deep feedforward neural networks","title":"glorot_normal"},{"location":"90.initializers/#glorot_uniform","text":"keras.initializers.glorot_uniform(seed=None) Glorot \u5747\u5300\u5206\u5e03\u521d\u59cb\u5316\u5668\uff0c\u4e5f\u79f0\u4e3a Xavier \u5747\u5300\u5206\u5e03\u521d\u59cb\u5316\u5668\u3002 \u5b83\u4ece [-limit\uff0climit] \u4e2d\u7684\u5747\u5300\u5206\u5e03\u4e2d\u62bd\u53d6\u6837\u672c\uff0c \u5176\u4e2d limit \u662f sqrt(6 / (fan_in + fan_out)) \uff0c fan_in \u662f\u6743\u503c\u5f20\u91cf\u4e2d\u7684\u8f93\u5165\u5355\u4f4d\u7684\u6570\u91cf\uff0c fan_out \u662f\u6743\u503c\u5f20\u91cf\u4e2d\u7684\u8f93\u51fa\u5355\u4f4d\u7684\u6570\u91cf\u3002 \u53c2\u6570 seed : \u4e00\u4e2a Python \u6574\u6570\u3002\u4f5c\u4e3a\u968f\u673a\u53d1\u751f\u5668\u7684\u79cd\u5b50\u3002 \u8fd4\u56de \u4e00\u4e2a\u521d\u59cb\u5316\u5668\u3002 \u53c2\u8003\u6587\u732e Understanding the difficulty of training deep feedforward neural networks","title":"glorot_uniform"},{"location":"90.initializers/#he_normal","text":"keras.initializers.he_normal(seed=None) He \u6b63\u6001\u5206\u5e03\u521d\u59cb\u5316\u5668\u3002 \u5b83\u4ece\u4ee5 0 \u4e3a\u4e2d\u5fc3\uff0c\u6807\u51c6\u5dee\u4e3a stddev = sqrt(2 / fan_in) \u7684\u622a\u65ad\u6b63\u6001\u5206\u5e03\u4e2d\u62bd\u53d6\u6837\u672c\uff0c \u5176\u4e2d fan_in \u662f\u6743\u503c\u5f20\u91cf\u4e2d\u7684\u8f93\u5165\u5355\u4f4d\u7684\u6570\u91cf\uff0c \u53c2\u6570 seed : \u4e00\u4e2a Python \u6574\u6570\u3002\u4f5c\u4e3a\u968f\u673a\u53d1\u751f\u5668\u7684\u79cd\u5b50\u3002 \u8fd4\u56de \u4e00\u4e2a\u521d\u59cb\u5316\u5668\u3002 \u53c2\u8003\u6587\u732e Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification","title":"he_normal"},{"location":"90.initializers/#lecun_normal","text":"keras.initializers.lecun_normal(seed=None) LeCun \u6b63\u6001\u5206\u5e03\u521d\u59cb\u5316\u5668\u3002 \u5b83\u4ece\u4ee5 0 \u4e3a\u4e2d\u5fc3\uff0c\u6807\u51c6\u5dee\u4e3a stddev = sqrt(1 / fan_in) \u7684\u622a\u65ad\u6b63\u6001\u5206\u5e03\u4e2d\u62bd\u53d6\u6837\u672c\uff0c \u5176\u4e2d fan_in \u662f\u6743\u503c\u5f20\u91cf\u4e2d\u7684\u8f93\u5165\u5355\u4f4d\u7684\u6570\u91cf\u3002 \u53c2\u6570 seed : \u4e00\u4e2a Python \u6574\u6570\u3002\u4f5c\u4e3a\u968f\u673a\u53d1\u751f\u5668\u7684\u79cd\u5b50\u3002 \u8fd4\u56de \u4e00\u4e2a\u521d\u59cb\u5316\u5668\u3002 \u53c2\u8003\u6587\u732e Self-Normalizing Neural Networks Efficient Backprop","title":"lecun_normal"},{"location":"90.initializers/#he_uniform","text":"keras.initializers.he_uniform(seed=None) He \u5747\u5300\u65b9\u5dee\u7f29\u653e\u521d\u59cb\u5316\u5668\u3002 \u5b83\u4ece [-limit\uff0climit] \u4e2d\u7684\u5747\u5300\u5206\u5e03\u4e2d\u62bd\u53d6\u6837\u672c\uff0c \u5176\u4e2d limit \u662f sqrt(6 / fan_in) \uff0c \u5176\u4e2d fan_in \u662f\u6743\u503c\u5f20\u91cf\u4e2d\u7684\u8f93\u5165\u5355\u4f4d\u7684\u6570\u91cf\u3002 \u53c2\u6570 seed : \u4e00\u4e2a Python \u6574\u6570\u3002\u4f5c\u4e3a\u968f\u673a\u53d1\u751f\u5668\u7684\u79cd\u5b50\u3002 \u8fd4\u56de \u4e00\u4e2a\u521d\u59cb\u5316\u5668\u3002 \u53c2\u8003\u6587\u732e Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification \u4e00\u4e2a\u521d\u59cb\u5316\u5668\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u4e2a\u5b57\u7b26\u4e32\u4f20\u9012\uff08\u5fc5\u987b\u5339\u914d\u4e0a\u9762\u7684\u4e00\u4e2a\u53ef\u7528\u7684\u521d\u59cb\u5316\u5668\uff09\uff0c\u6216\u8005\u4f5c\u4e3a\u4e00\u4e2a\u53ef\u8c03\u7528\u51fd\u6570\u4f20\u9012\uff1a from keras import initializers model.add(Dense(64, kernel_initializer=initializers.random_normal(stddev=0.01))) # \u540c\u6837\u6709\u6548;\u5c06\u4f7f\u7528\u9ed8\u8ba4\u53c2\u6570\u3002 model.add(Dense(64, kernel_initializer='random_normal'))","title":"he_uniform"},{"location":"90.initializers/#_3","text":"\u5982\u679c\u4f20\u9012\u4e00\u4e2a\u81ea\u5b9a\u4e49\u7684\u53ef\u8c03\u7528\u51fd\u6570\uff0c\u90a3\u4e48\u5b83\u5fc5\u987b\u4f7f\u7528\u53c2\u6570 shape \uff08\u9700\u8981\u521d\u59cb\u5316\u7684\u53d8\u91cf\u7684\u5c3a\u5bf8\uff09\u548c dtype \uff08\u6570\u636e\u7c7b\u578b\uff09\uff1a from keras import backend as K def my_init(shape, dtype=None): return K.random_normal(shape, dtype=dtype) model.add(Dense(64, kernel_initializer=my_init))","title":"\u4f7f\u7528\u81ea\u5b9a\u4e49\u521d\u59cb\u5316\u5668"},{"location":"91.regularizers/","text":"\u6b63\u5219\u5316 Regularizers \u6b63\u5219\u5316\u5668\u7684\u4f7f\u7528 \u6b63\u5219\u5316\u5668\u5141\u8bb8\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u5bf9\u5c42\u7684\u53c2\u6570\u6216\u5c42\u7684\u6fc0\u6d3b\u60c5\u51b5\u8fdb\u884c\u60e9\u7f5a\u3002 \u7f51\u7edc\u4f18\u5316\u7684\u635f\u5931\u51fd\u6570\u4e5f\u5305\u62ec\u8fd9\u4e9b\u60e9\u7f5a\u9879\u3002 \u60e9\u7f5a\u662f\u4ee5\u5c42\u4e3a\u5bf9\u8c61\u8fdb\u884c\u7684\u3002\u5177\u4f53\u7684 API \u56e0\u5c42\u800c\u5f02\uff0c\u4f46 Dense \uff0c Conv1D \uff0c Conv2D \u548c Conv3D \u8fd9\u4e9b\u5c42\u5177\u6709\u7edf\u4e00\u7684 API\u3002 \u6b63\u5219\u5316\u5668\u5f00\u653e 3 \u4e2a\u5173\u952e\u5b57\u53c2\u6570\uff1a kernel_regularizer : keras.regularizers.Regularizer \u7684\u5b9e\u4f8b bias_regularizer : keras.regularizers.Regularizer \u7684\u5b9e\u4f8b activity_regularizer : keras.regularizers.Regularizer \u7684\u5b9e\u4f8b \u4f8b from keras import regularizers model.add(Dense(64, input_dim=64, kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.01))) \u53ef\u7528\u7684\u6b63\u5219\u5316\u5668 keras.regularizers.l1(0.) keras.regularizers.l2(0.) keras.regularizers.l1_l2(l1=0.01, l2=0.01) \u5f00\u53d1\u65b0\u7684\u6b63\u5219\u5316\u5668 \u4efb\u4f55\u8f93\u5165\u4e00\u4e2a\u6743\u91cd\u77e9\u9635\u3001\u8fd4\u56de\u4e00\u4e2a\u635f\u5931\u8d21\u732e\u5f20\u91cf\u7684\u51fd\u6570\uff0c\u90fd\u53ef\u4ee5\u7528\u4f5c\u6b63\u5219\u5316\u5668\uff0c\u4f8b\u5982\uff1a from keras import backend as K def l1_reg(weight_matrix): return 0.01 * K.sum(K.abs(weight_matrix)) model.add(Dense(64, input_dim=64, kernel_regularizer=l1_reg)) \u53e6\u5916\uff0c\u4f60\u4e5f\u53ef\u4ee5\u7528\u9762\u5411\u5bf9\u8c61\u7684\u65b9\u5f0f\u6765\u7f16\u5199\u6b63\u5219\u5316\u5668\u7684\u4ee3\u7801\uff0c\u4f8b\u5b50\u89c1 keras/regularizers.py \u6a21\u5757\u3002","title":"\u6b63\u5219\u5316 Regularizers"},{"location":"91.regularizers/#regularizers","text":"","title":"\u6b63\u5219\u5316 Regularizers"},{"location":"91.regularizers/#_1","text":"\u6b63\u5219\u5316\u5668\u5141\u8bb8\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u5bf9\u5c42\u7684\u53c2\u6570\u6216\u5c42\u7684\u6fc0\u6d3b\u60c5\u51b5\u8fdb\u884c\u60e9\u7f5a\u3002 \u7f51\u7edc\u4f18\u5316\u7684\u635f\u5931\u51fd\u6570\u4e5f\u5305\u62ec\u8fd9\u4e9b\u60e9\u7f5a\u9879\u3002 \u60e9\u7f5a\u662f\u4ee5\u5c42\u4e3a\u5bf9\u8c61\u8fdb\u884c\u7684\u3002\u5177\u4f53\u7684 API \u56e0\u5c42\u800c\u5f02\uff0c\u4f46 Dense \uff0c Conv1D \uff0c Conv2D \u548c Conv3D \u8fd9\u4e9b\u5c42\u5177\u6709\u7edf\u4e00\u7684 API\u3002 \u6b63\u5219\u5316\u5668\u5f00\u653e 3 \u4e2a\u5173\u952e\u5b57\u53c2\u6570\uff1a kernel_regularizer : keras.regularizers.Regularizer \u7684\u5b9e\u4f8b bias_regularizer : keras.regularizers.Regularizer \u7684\u5b9e\u4f8b activity_regularizer : keras.regularizers.Regularizer \u7684\u5b9e\u4f8b","title":"\u6b63\u5219\u5316\u5668\u7684\u4f7f\u7528"},{"location":"91.regularizers/#_2","text":"from keras import regularizers model.add(Dense(64, input_dim=64, kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.01)))","title":"\u4f8b"},{"location":"91.regularizers/#_3","text":"keras.regularizers.l1(0.) keras.regularizers.l2(0.) keras.regularizers.l1_l2(l1=0.01, l2=0.01)","title":"\u53ef\u7528\u7684\u6b63\u5219\u5316\u5668"},{"location":"91.regularizers/#_4","text":"\u4efb\u4f55\u8f93\u5165\u4e00\u4e2a\u6743\u91cd\u77e9\u9635\u3001\u8fd4\u56de\u4e00\u4e2a\u635f\u5931\u8d21\u732e\u5f20\u91cf\u7684\u51fd\u6570\uff0c\u90fd\u53ef\u4ee5\u7528\u4f5c\u6b63\u5219\u5316\u5668\uff0c\u4f8b\u5982\uff1a from keras import backend as K def l1_reg(weight_matrix): return 0.01 * K.sum(K.abs(weight_matrix)) model.add(Dense(64, input_dim=64, kernel_regularizer=l1_reg)) \u53e6\u5916\uff0c\u4f60\u4e5f\u53ef\u4ee5\u7528\u9762\u5411\u5bf9\u8c61\u7684\u65b9\u5f0f\u6765\u7f16\u5199\u6b63\u5219\u5316\u5668\u7684\u4ee3\u7801\uff0c\u4f8b\u5b50\u89c1 keras/regularizers.py \u6a21\u5757\u3002","title":"\u5f00\u53d1\u65b0\u7684\u6b63\u5219\u5316\u5668"},{"location":"92.constraints/","text":"\u7ea6\u675f Constraints \u7ea6\u675f\u9879\u7684\u4f7f\u7528 constraints \u6a21\u5757\u7684\u51fd\u6570\u5141\u8bb8\u5728\u4f18\u5316\u671f\u95f4\u5bf9\u7f51\u7edc\u53c2\u6570\u8bbe\u7f6e\u7ea6\u675f\uff08\u4f8b\u5982\u975e\u8d1f\u6027\uff09\u3002 \u7ea6\u675f\u662f\u4ee5\u5c42\u4e3a\u5bf9\u8c61\u8fdb\u884c\u7684\u3002\u5177\u4f53\u7684 API \u56e0\u5c42\u800c\u5f02\uff0c\u4f46 Dense \uff0c Conv1D \uff0c Conv2D \u548c Conv3D \u8fd9\u4e9b\u5c42\u5177\u6709\u7edf\u4e00\u7684 API\u3002 \u7ea6\u675f\u5c42\u5f00\u653e 2 \u4e2a\u5173\u952e\u5b57\u53c2\u6570\uff1a kernel_constraint \u7528\u4e8e\u4e3b\u6743\u91cd\u77e9\u9635\u3002 bias_constraint \u7528\u4e8e\u504f\u7f6e\u3002 from keras.constraints import max_norm model.add(Dense(64, kernel_constraint=max_norm(2.))) \u53ef\u7528\u7684\u7ea6\u675f [source] MaxNorm keras.constraints.MaxNorm(max_value=2, axis=0) MaxNorm \u6700\u5927\u8303\u6570\u6743\u503c\u7ea6\u675f\u3002 \u6620\u5c04\u5230\u6bcf\u4e2a\u9690\u85cf\u5355\u5143\u7684\u6743\u503c\u7684\u7ea6\u675f\uff0c\u4f7f\u5176\u5177\u6709\u5c0f\u4e8e\u6216\u7b49\u4e8e\u671f\u671b\u503c\u7684\u8303\u6570\u3002 \u53c2\u6570 m : \u8f93\u5165\u6743\u503c\u7684\u6700\u5927\u8303\u6570\u3002 axis : \u6574\u6570\uff0c\u9700\u8981\u8ba1\u7b97\u6743\u503c\u8303\u6570\u7684\u8f74\u3002 \u4f8b\u5982\uff0c\u5728 Dense \u5c42\u4e2d\u6743\u503c\u77e9\u9635\u7684\u5c3a\u5bf8\u4e3a (input_dim, output_dim) \uff0c \u8bbe\u7f6e axis \u4e3a 0 \u4ee5\u7ea6\u675f\u6bcf\u4e2a\u957f\u5ea6\u4e3a (input_dim,) \u7684\u6743\u503c\u5411\u91cf\u3002 \u5728 Conv2D \u5c42\uff08 data_format=\"channels_last\" \uff09\u4e2d\uff0c\u6743\u503c\u5f20\u91cf\u7684\u5c3a\u5bf8\u4e3a (rows, cols, input_depth, output_depth) \uff0c\u8bbe\u7f6e axis \u4e3a [0, 1, 2] \u4ee5\u8d8a\u662f\u6bcf\u4e2a\u5c3a\u5bf8\u4e3a (rows, cols, input_depth) \u7684\u6ee4\u6ce2\u5668\u5f20\u91cf\u7684\u6743\u503c\u3002 \u53c2\u8003\u6587\u732e [Dropout: A Simple Way to Prevent Neural Networks from Overfitting] (http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf) [source] NonNeg keras.constraints.NonNeg() \u6743\u91cd\u975e\u8d1f\u7684\u7ea6\u675f\u3002 [source] UnitNorm keras.constraints.UnitNorm(axis=0) \u6620\u5c04\u5230\u6bcf\u4e2a\u9690\u85cf\u5355\u5143\u7684\u6743\u503c\u7684\u7ea6\u675f\uff0c\u4f7f\u5176\u5177\u6709\u5355\u4f4d\u8303\u6570\u3002 \u53c2\u6570 axis : \u6574\u6570\uff0c\u9700\u8981\u8ba1\u7b97\u6743\u503c\u8303\u6570\u7684\u8f74\u3002 \u4f8b\u5982\uff0c\u5728 Dense \u5c42\u4e2d\u6743\u503c\u77e9\u9635\u7684\u5c3a\u5bf8\u4e3a (input_dim, output_dim) \uff0c \u8bbe\u7f6e axis \u4e3a 0 \u4ee5\u7ea6\u675f\u6bcf\u4e2a\u957f\u5ea6\u4e3a (input_dim,) \u7684\u6743\u503c\u5411\u91cf\u3002 \u5728 Conv2D \u5c42\uff08 data_format=\"channels_last\" \uff09\u4e2d\uff0c\u6743\u503c\u5f20\u91cf\u7684\u5c3a\u5bf8\u4e3a (rows, cols, input_depth, output_depth) \uff0c\u8bbe\u7f6e axis \u4e3a [0, 1, 2] \u4ee5\u8d8a\u662f\u6bcf\u4e2a\u5c3a\u5bf8\u4e3a (rows, cols, input_depth) \u7684\u6ee4\u6ce2\u5668\u5f20\u91cf\u7684\u6743\u503c\u3002 [source] MinMaxNorm keras.constraints.MinMaxNorm(min_value=0.0, max_value=1.0, rate=1.0, axis=0) MinMaxNorm \u6700\u5c0f/\u6700\u5927\u8303\u6570\u6743\u503c\u7ea6\u675f\u3002 \u6620\u5c04\u5230\u6bcf\u4e2a\u9690\u85cf\u5355\u5143\u7684\u6743\u503c\u7684\u7ea6\u675f\uff0c\u4f7f\u5176\u8303\u6570\u5728\u4e0a\u4e0b\u754c\u4e4b\u95f4\u3002 \u53c2\u6570 min_value : \u8f93\u5165\u6743\u503c\u7684\u6700\u5c0f\u8303\u6570\u3002 max_value : \u8f93\u5165\u6743\u503c\u7684\u6700\u5927\u8303\u6570\u3002 rate : \u5f3a\u5236\u6267\u884c\u7ea6\u675f\u7684\u6bd4\u4f8b\uff1a\u6743\u503c\u5c06\u88ab\u91cd\u65b0\u8c03\u6574\u4e3a (1 - rate) * norm + rate * norm.clip(min_value, max_value) \u3002 \u5b9e\u9645\u4e0a\uff0c\u8fd9\u610f\u5473\u7740 rate = 1.0 \u4ee3\u8868\u4e25\u683c\u6267\u884c\u7ea6\u675f\uff0c\u800c rate <1.0 \u610f\u5473\u7740\u6743\u503c \u5c06\u5728\u6bcf\u4e00\u6b65\u91cd\u65b0\u8c03\u6574\u4ee5\u7f13\u6162\u79fb\u52a8\u5230\u6240\u9700\u95f4\u9694\u5185\u7684\u503c\u3002 axis : \u6574\u6570\uff0c\u9700\u8981\u8ba1\u7b97\u6743\u503c\u8303\u6570\u7684\u8f74\u3002 \u4f8b\u5982\uff0c\u5728 Dense \u5c42\u4e2d\u6743\u503c\u77e9\u9635\u7684\u5c3a\u5bf8\u4e3a (input_dim, output_dim) \uff0c \u8bbe\u7f6e axis \u4e3a 0 \u4ee5\u7ea6\u675f\u6bcf\u4e2a\u957f\u5ea6\u4e3a (input_dim,) \u7684\u6743\u503c\u5411\u91cf\u3002 \u5728 Conv2D \u5c42\uff08 data_format=\"channels_last\" \uff09\u4e2d\uff0c\u6743\u503c\u5f20\u91cf\u7684\u5c3a\u5bf8\u4e3a (rows, cols, input_depth, output_depth) \uff0c\u8bbe\u7f6e axis \u4e3a [0, 1, 2] \u4ee5\u8d8a\u662f\u6bcf\u4e2a\u5c3a\u5bf8\u4e3a (rows, cols, input_depth) \u7684\u6ee4\u6ce2\u5668\u5f20\u91cf\u7684\u6743\u503c\u3002","title":"\u7ea6\u675f Constraints"},{"location":"92.constraints/#constraints","text":"","title":"\u7ea6\u675f Constraints"},{"location":"92.constraints/#_1","text":"constraints \u6a21\u5757\u7684\u51fd\u6570\u5141\u8bb8\u5728\u4f18\u5316\u671f\u95f4\u5bf9\u7f51\u7edc\u53c2\u6570\u8bbe\u7f6e\u7ea6\u675f\uff08\u4f8b\u5982\u975e\u8d1f\u6027\uff09\u3002 \u7ea6\u675f\u662f\u4ee5\u5c42\u4e3a\u5bf9\u8c61\u8fdb\u884c\u7684\u3002\u5177\u4f53\u7684 API \u56e0\u5c42\u800c\u5f02\uff0c\u4f46 Dense \uff0c Conv1D \uff0c Conv2D \u548c Conv3D \u8fd9\u4e9b\u5c42\u5177\u6709\u7edf\u4e00\u7684 API\u3002 \u7ea6\u675f\u5c42\u5f00\u653e 2 \u4e2a\u5173\u952e\u5b57\u53c2\u6570\uff1a kernel_constraint \u7528\u4e8e\u4e3b\u6743\u91cd\u77e9\u9635\u3002 bias_constraint \u7528\u4e8e\u504f\u7f6e\u3002 from keras.constraints import max_norm model.add(Dense(64, kernel_constraint=max_norm(2.)))","title":"\u7ea6\u675f\u9879\u7684\u4f7f\u7528"},{"location":"92.constraints/#_2","text":"[source]","title":"\u53ef\u7528\u7684\u7ea6\u675f"},{"location":"92.constraints/#maxnorm","text":"keras.constraints.MaxNorm(max_value=2, axis=0) MaxNorm \u6700\u5927\u8303\u6570\u6743\u503c\u7ea6\u675f\u3002 \u6620\u5c04\u5230\u6bcf\u4e2a\u9690\u85cf\u5355\u5143\u7684\u6743\u503c\u7684\u7ea6\u675f\uff0c\u4f7f\u5176\u5177\u6709\u5c0f\u4e8e\u6216\u7b49\u4e8e\u671f\u671b\u503c\u7684\u8303\u6570\u3002 \u53c2\u6570 m : \u8f93\u5165\u6743\u503c\u7684\u6700\u5927\u8303\u6570\u3002 axis : \u6574\u6570\uff0c\u9700\u8981\u8ba1\u7b97\u6743\u503c\u8303\u6570\u7684\u8f74\u3002 \u4f8b\u5982\uff0c\u5728 Dense \u5c42\u4e2d\u6743\u503c\u77e9\u9635\u7684\u5c3a\u5bf8\u4e3a (input_dim, output_dim) \uff0c \u8bbe\u7f6e axis \u4e3a 0 \u4ee5\u7ea6\u675f\u6bcf\u4e2a\u957f\u5ea6\u4e3a (input_dim,) \u7684\u6743\u503c\u5411\u91cf\u3002 \u5728 Conv2D \u5c42\uff08 data_format=\"channels_last\" \uff09\u4e2d\uff0c\u6743\u503c\u5f20\u91cf\u7684\u5c3a\u5bf8\u4e3a (rows, cols, input_depth, output_depth) \uff0c\u8bbe\u7f6e axis \u4e3a [0, 1, 2] \u4ee5\u8d8a\u662f\u6bcf\u4e2a\u5c3a\u5bf8\u4e3a (rows, cols, input_depth) \u7684\u6ee4\u6ce2\u5668\u5f20\u91cf\u7684\u6743\u503c\u3002 \u53c2\u8003\u6587\u732e [Dropout: A Simple Way to Prevent Neural Networks from Overfitting] (http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf) [source]","title":"MaxNorm"},{"location":"92.constraints/#nonneg","text":"keras.constraints.NonNeg() \u6743\u91cd\u975e\u8d1f\u7684\u7ea6\u675f\u3002 [source]","title":"NonNeg"},{"location":"92.constraints/#unitnorm","text":"keras.constraints.UnitNorm(axis=0) \u6620\u5c04\u5230\u6bcf\u4e2a\u9690\u85cf\u5355\u5143\u7684\u6743\u503c\u7684\u7ea6\u675f\uff0c\u4f7f\u5176\u5177\u6709\u5355\u4f4d\u8303\u6570\u3002 \u53c2\u6570 axis : \u6574\u6570\uff0c\u9700\u8981\u8ba1\u7b97\u6743\u503c\u8303\u6570\u7684\u8f74\u3002 \u4f8b\u5982\uff0c\u5728 Dense \u5c42\u4e2d\u6743\u503c\u77e9\u9635\u7684\u5c3a\u5bf8\u4e3a (input_dim, output_dim) \uff0c \u8bbe\u7f6e axis \u4e3a 0 \u4ee5\u7ea6\u675f\u6bcf\u4e2a\u957f\u5ea6\u4e3a (input_dim,) \u7684\u6743\u503c\u5411\u91cf\u3002 \u5728 Conv2D \u5c42\uff08 data_format=\"channels_last\" \uff09\u4e2d\uff0c\u6743\u503c\u5f20\u91cf\u7684\u5c3a\u5bf8\u4e3a (rows, cols, input_depth, output_depth) \uff0c\u8bbe\u7f6e axis \u4e3a [0, 1, 2] \u4ee5\u8d8a\u662f\u6bcf\u4e2a\u5c3a\u5bf8\u4e3a (rows, cols, input_depth) \u7684\u6ee4\u6ce2\u5668\u5f20\u91cf\u7684\u6743\u503c\u3002 [source]","title":"UnitNorm"},{"location":"92.constraints/#minmaxnorm","text":"keras.constraints.MinMaxNorm(min_value=0.0, max_value=1.0, rate=1.0, axis=0) MinMaxNorm \u6700\u5c0f/\u6700\u5927\u8303\u6570\u6743\u503c\u7ea6\u675f\u3002 \u6620\u5c04\u5230\u6bcf\u4e2a\u9690\u85cf\u5355\u5143\u7684\u6743\u503c\u7684\u7ea6\u675f\uff0c\u4f7f\u5176\u8303\u6570\u5728\u4e0a\u4e0b\u754c\u4e4b\u95f4\u3002 \u53c2\u6570 min_value : \u8f93\u5165\u6743\u503c\u7684\u6700\u5c0f\u8303\u6570\u3002 max_value : \u8f93\u5165\u6743\u503c\u7684\u6700\u5927\u8303\u6570\u3002 rate : \u5f3a\u5236\u6267\u884c\u7ea6\u675f\u7684\u6bd4\u4f8b\uff1a\u6743\u503c\u5c06\u88ab\u91cd\u65b0\u8c03\u6574\u4e3a (1 - rate) * norm + rate * norm.clip(min_value, max_value) \u3002 \u5b9e\u9645\u4e0a\uff0c\u8fd9\u610f\u5473\u7740 rate = 1.0 \u4ee3\u8868\u4e25\u683c\u6267\u884c\u7ea6\u675f\uff0c\u800c rate <1.0 \u610f\u5473\u7740\u6743\u503c \u5c06\u5728\u6bcf\u4e00\u6b65\u91cd\u65b0\u8c03\u6574\u4ee5\u7f13\u6162\u79fb\u52a8\u5230\u6240\u9700\u95f4\u9694\u5185\u7684\u503c\u3002 axis : \u6574\u6570\uff0c\u9700\u8981\u8ba1\u7b97\u6743\u503c\u8303\u6570\u7684\u8f74\u3002 \u4f8b\u5982\uff0c\u5728 Dense \u5c42\u4e2d\u6743\u503c\u77e9\u9635\u7684\u5c3a\u5bf8\u4e3a (input_dim, output_dim) \uff0c \u8bbe\u7f6e axis \u4e3a 0 \u4ee5\u7ea6\u675f\u6bcf\u4e2a\u957f\u5ea6\u4e3a (input_dim,) \u7684\u6743\u503c\u5411\u91cf\u3002 \u5728 Conv2D \u5c42\uff08 data_format=\"channels_last\" \uff09\u4e2d\uff0c\u6743\u503c\u5f20\u91cf\u7684\u5c3a\u5bf8\u4e3a (rows, cols, input_depth, output_depth) \uff0c\u8bbe\u7f6e axis \u4e3a [0, 1, 2] \u4ee5\u8d8a\u662f\u6bcf\u4e2a\u5c3a\u5bf8\u4e3a (rows, cols, input_depth) \u7684\u6ee4\u6ce2\u5668\u5f20\u91cf\u7684\u6743\u503c\u3002","title":"MinMaxNorm"},{"location":"93.visualization/","text":"\u53ef\u89c6\u5316 Visualization \u6a21\u578b\u53ef\u89c6\u5316 keras.utils.vis_utils \u6a21\u5757\u63d0\u4f9b\u4e86\u4e00\u4e9b\u7ed8\u5236 Keras \u6a21\u578b\u7684\u5b9e\u7528\u529f\u80fd(\u4f7f\u7528 graphviz )\u3002 \u4ee5\u4e0b\u5b9e\u4f8b\uff0c\u5c06\u7ed8\u5236\u4e00\u5f20\u6a21\u578b\u56fe\uff0c\u5e76\u4fdd\u5b58\u4e3a\u6587\u4ef6\uff1a from keras.utils import plot_model plot_model(model, to_file='model.png') plot_model \u6709 4 \u4e2a\u53ef\u9009\u53c2\u6570: show_shapes (\u9ed8\u8ba4\u4e3a False) \u63a7\u5236\u662f\u5426\u5728\u56fe\u4e2d\u8f93\u51fa\u5404\u5c42\u7684\u5c3a\u5bf8\u3002 show_layer_names (\u9ed8\u8ba4\u4e3a True) \u63a7\u5236\u662f\u5426\u5728\u56fe\u4e2d\u663e\u793a\u6bcf\u4e00\u5c42\u7684\u540d\u5b57\u3002 expand_dim \uff08\u9ed8\u8ba4\u4e3a False\uff09\u63a7\u5236\u662f\u5426\u5c06\u5d4c\u5957\u6a21\u578b\u6269\u5c55\u4e3a\u56fe\u5f62\u4e2d\u7684\u805a\u7c7b\u3002 dpi \uff08\u9ed8\u8ba4\u4e3a 96\uff09\u63a7\u5236\u56fe\u50cf dpi\u3002 \u6b64\u5916\uff0c\u4f60\u4e5f\u53ef\u4ee5\u76f4\u63a5\u53d6\u5f97 pydot.Graph \u5bf9\u8c61\u5e76\u81ea\u5df1\u6e32\u67d3\u5b83\u3002 \u4f8b\u5982\uff0cipython notebook \u4e2d\u7684\u53ef\u89c6\u5316\u5b9e\u4f8b\u5982\u4e0b\uff1a from IPython.display import SVG from keras.utils.vis_utils import model_to_dot SVG(model_to_dot(model).create(prog='dot', format='svg')) \u8bad\u7ec3\u5386\u53f2\u53ef\u89c6\u5316 Keras Model \u4e0a\u7684 fit() \u65b9\u6cd5\u8fd4\u56de\u4e00\u4e2a History \u5bf9\u8c61\u3002 History.history \u5c5e\u6027\u662f\u4e00\u4e2a\u8bb0\u5f55\u4e86\u8fde\u7eed\u8fed\u4ee3\u7684\u8bad\u7ec3/\u9a8c\u8bc1\uff08\u5982\u679c\u5b58\u5728\uff09\u635f\u5931\u503c\u548c\u8bc4\u4f30\u503c\u7684\u5b57\u5178\u3002\u8fd9\u91cc\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u4f7f\u7528 matplotlib \u6765\u751f\u6210\u8bad\u7ec3/\u9a8c\u8bc1\u96c6\u7684\u635f\u5931\u548c\u51c6\u786e\u7387\u56fe\u8868\u7684\u4f8b\u5b50\uff1a import matplotlib.pyplot as plt history = model.fit(x, y, validation_split=0.25, epochs=50, batch_size=16, verbose=1) # \u7ed8\u5236\u8bad\u7ec3 & \u9a8c\u8bc1\u7684\u51c6\u786e\u7387\u503c plt.plot(history.history['acc']) plt.plot(history.history['val_acc']) plt.title('Model accuracy') plt.ylabel('Accuracy') plt.xlabel('Epoch') plt.legend(['Train', 'Test'], loc='upper left') plt.show() # \u7ed8\u5236\u8bad\u7ec3 & \u9a8c\u8bc1\u7684\u635f\u5931\u503c plt.plot(history.history['loss']) plt.plot(history.history['val_loss']) plt.title('Model loss') plt.ylabel('Loss') plt.xlabel('Epoch') plt.legend(['Train', 'Test'], loc='upper left') plt.show()","title":"\u53ef\u89c6\u5316 Visualization"},{"location":"93.visualization/#visualization","text":"","title":"\u53ef\u89c6\u5316 Visualization"},{"location":"93.visualization/#_1","text":"keras.utils.vis_utils \u6a21\u5757\u63d0\u4f9b\u4e86\u4e00\u4e9b\u7ed8\u5236 Keras \u6a21\u578b\u7684\u5b9e\u7528\u529f\u80fd(\u4f7f\u7528 graphviz )\u3002 \u4ee5\u4e0b\u5b9e\u4f8b\uff0c\u5c06\u7ed8\u5236\u4e00\u5f20\u6a21\u578b\u56fe\uff0c\u5e76\u4fdd\u5b58\u4e3a\u6587\u4ef6\uff1a from keras.utils import plot_model plot_model(model, to_file='model.png') plot_model \u6709 4 \u4e2a\u53ef\u9009\u53c2\u6570: show_shapes (\u9ed8\u8ba4\u4e3a False) \u63a7\u5236\u662f\u5426\u5728\u56fe\u4e2d\u8f93\u51fa\u5404\u5c42\u7684\u5c3a\u5bf8\u3002 show_layer_names (\u9ed8\u8ba4\u4e3a True) \u63a7\u5236\u662f\u5426\u5728\u56fe\u4e2d\u663e\u793a\u6bcf\u4e00\u5c42\u7684\u540d\u5b57\u3002 expand_dim \uff08\u9ed8\u8ba4\u4e3a False\uff09\u63a7\u5236\u662f\u5426\u5c06\u5d4c\u5957\u6a21\u578b\u6269\u5c55\u4e3a\u56fe\u5f62\u4e2d\u7684\u805a\u7c7b\u3002 dpi \uff08\u9ed8\u8ba4\u4e3a 96\uff09\u63a7\u5236\u56fe\u50cf dpi\u3002 \u6b64\u5916\uff0c\u4f60\u4e5f\u53ef\u4ee5\u76f4\u63a5\u53d6\u5f97 pydot.Graph \u5bf9\u8c61\u5e76\u81ea\u5df1\u6e32\u67d3\u5b83\u3002 \u4f8b\u5982\uff0cipython notebook \u4e2d\u7684\u53ef\u89c6\u5316\u5b9e\u4f8b\u5982\u4e0b\uff1a from IPython.display import SVG from keras.utils.vis_utils import model_to_dot SVG(model_to_dot(model).create(prog='dot', format='svg'))","title":"\u6a21\u578b\u53ef\u89c6\u5316"},{"location":"93.visualization/#_2","text":"Keras Model \u4e0a\u7684 fit() \u65b9\u6cd5\u8fd4\u56de\u4e00\u4e2a History \u5bf9\u8c61\u3002 History.history \u5c5e\u6027\u662f\u4e00\u4e2a\u8bb0\u5f55\u4e86\u8fde\u7eed\u8fed\u4ee3\u7684\u8bad\u7ec3/\u9a8c\u8bc1\uff08\u5982\u679c\u5b58\u5728\uff09\u635f\u5931\u503c\u548c\u8bc4\u4f30\u503c\u7684\u5b57\u5178\u3002\u8fd9\u91cc\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u4f7f\u7528 matplotlib \u6765\u751f\u6210\u8bad\u7ec3/\u9a8c\u8bc1\u96c6\u7684\u635f\u5931\u548c\u51c6\u786e\u7387\u56fe\u8868\u7684\u4f8b\u5b50\uff1a import matplotlib.pyplot as plt history = model.fit(x, y, validation_split=0.25, epochs=50, batch_size=16, verbose=1) # \u7ed8\u5236\u8bad\u7ec3 & \u9a8c\u8bc1\u7684\u51c6\u786e\u7387\u503c plt.plot(history.history['acc']) plt.plot(history.history['val_acc']) plt.title('Model accuracy') plt.ylabel('Accuracy') plt.xlabel('Epoch') plt.legend(['Train', 'Test'], loc='upper left') plt.show() # \u7ed8\u5236\u8bad\u7ec3 & \u9a8c\u8bc1\u7684\u635f\u5931\u503c plt.plot(history.history['loss']) plt.plot(history.history['val_loss']) plt.title('Model loss') plt.ylabel('Loss') plt.xlabel('Epoch') plt.legend(['Train', 'Test'], loc='upper left') plt.show()","title":"\u8bad\u7ec3\u5386\u53f2\u53ef\u89c6\u5316"},{"location":"94.scikit-learn-api/","text":"Scikit-Learn API \u7684\u5c01\u88c5\u5668 \u4f60\u53ef\u4ee5\u4f7f\u7528 Keras \u7684 Sequential \u6a21\u578b\uff08\u4ec5\u9650\u5355\u4e00\u8f93\u5165\uff09\u4f5c\u4e3a Scikit-Learn \u5de5\u4f5c\u6d41\u7a0b\u7684\u4e00\u90e8\u5206\uff0c\u901a\u8fc7\u5728\u6b64\u627e\u5230\u7684\u5305\u88c5\u5668: keras.wrappers.scikit_learn.py \u3002 \u6709\u4e24\u4e2a\u5c01\u88c5\u5668\u53ef\u7528: keras.wrappers.scikit_learn.KerasClassifier(build_fn=None, **sk_params) , \u8fd9\u5b9e\u73b0\u4e86Scikit-Learn \u5206\u7c7b\u5668\u63a5\u53e3, keras.wrappers.scikit_learn.KerasRegressor(build_fn=None, **sk_params) , \u8fd9\u5b9e\u73b0\u4e86Scikit-Learn \u56de\u5f52\u63a5\u53e3\u3002 \u53c2\u6570 build_fn : \u53ef\u8c03\u7528\u51fd\u6570\u6216\u7c7b\u5b9e\u4f8b sk_params : \u6a21\u578b\u53c2\u6570\u548c\u62df\u5408\u53c2\u6570 build_fn \u5e94\u8be5\u5efa\u7acb\uff0c\u7f16\u8bd1\uff0c\u5e76\u8fd4\u56de\u4e00\u4e2a Keras \u6a21\u578b\uff0c\u7136\u540e\u88ab\u7528\u6765\u8bad\u7ec3/\u9884\u6d4b\u3002\u4ee5\u4e0b\u4e09\u4e2a\u503c\u4e4b\u4e00\u53ef\u4ee5\u4f20\u9012\u7ed9 build_fn \u4e00\u4e2a\u51fd\u6570\uff1b \u5b9e\u73b0 __call__ \u65b9\u6cd5\u7684\u7c7b\u7684\u5b9e\u4f8b\uff1b None\u3002\u8fd9\u610f\u5473\u7740\u4f60\u5b9e\u73b0\u4e86\u4e00\u4e2a\u7ee7\u627f\u81ea KerasClassifier \u6216 KerasRegressor \u7684\u7c7b\u3002\u5f53\u524d\u7c7b __call__ \u65b9\u6cd5\u5c06\u88ab\u89c6\u4e3a\u9ed8\u8ba4\u7684 build_fn \u3002 sk_params \u540c\u65f6\u5305\u542b\u6a21\u578b\u53c2\u6570\u548c\u62df\u5408\u53c2\u6570\u3002\u5408\u6cd5\u7684\u6a21\u578b\u53c2\u6570\u662f build_fn \u7684\u53c2\u6570\u3002\u8bf7\u6ce8\u610f\uff0c\u4e0e scikit-learn \u4e2d\u7684\u6240\u6709\u5176\u4ed6\u4f30\u7b97\u5668\u4e00\u6837\uff0c build_fn \u5e94\u4e3a\u5176\u53c2\u6570\u63d0\u4f9b\u9ed8\u8ba4\u503c\uff0c \u4ee5\u4fbf\u4f60\u53ef\u4ee5\u521b\u5efa\u4f30\u7b97\u5668\u800c\u4e0d\u5c06\u4efb\u4f55\u503c\u4f20\u9012\u7ed9 sk_params \u3002 sk_params \u8fd8\u53ef\u4ee5\u63a5\u53d7\u7528\u4e8e\u8c03\u7528 fit \uff0c predict \uff0c predict_proba \u548c score \u65b9\u6cd5\u7684\u53c2\u6570\uff08\u4f8b\u5982\uff0c epochs \uff0c batch_size \uff09\u3002\u8bad\u7ec3\uff08\u9884\u6d4b\uff09\u53c2\u6570\u6309\u4ee5\u4e0b\u987a\u5e8f\u9009\u62e9\uff1a \u4f20\u9012\u7ed9 fit \uff0c predict \uff0c predict_proba \u548c score \u51fd\u6570\u7684\u5b57\u5178\u53c2\u6570\u7684\u503c\uff1b \u4f20\u9012\u7ed9 sk_params \u7684\u503c\uff1b keras.models.Sequential \u7684 fit \uff0c predict \uff0c predict_proba \u548c score \u65b9\u6cd5\u7684\u9ed8\u8ba4\u503c\u3002 \u5f53\u4f7f\u7528 scikit-learn \u7684 grid_search API \u65f6\uff0c\u5408\u6cd5\u53ef\u8c03\u53c2\u6570\u662f\u4f60\u53ef\u4ee5\u4f20\u9012\u7ed9 sk_params \u7684\u53c2\u6570\uff0c\u5305\u62ec\u8bad\u7ec3\u53c2\u6570\u3002\u6362\u53e5\u8bdd\u8bf4\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528 grid_search \u6765\u641c\u7d22\u6700\u4f73\u7684 batch_size \u6216 epoch \u4ee5\u53ca\u5176\u4ed6\u6a21\u578b\u53c2\u6570\u3002","title":"Scikit-Learn API \u7684\u5c01\u88c5\u5668"},{"location":"94.scikit-learn-api/#scikit-learn-api","text":"\u4f60\u53ef\u4ee5\u4f7f\u7528 Keras \u7684 Sequential \u6a21\u578b\uff08\u4ec5\u9650\u5355\u4e00\u8f93\u5165\uff09\u4f5c\u4e3a Scikit-Learn \u5de5\u4f5c\u6d41\u7a0b\u7684\u4e00\u90e8\u5206\uff0c\u901a\u8fc7\u5728\u6b64\u627e\u5230\u7684\u5305\u88c5\u5668: keras.wrappers.scikit_learn.py \u3002 \u6709\u4e24\u4e2a\u5c01\u88c5\u5668\u53ef\u7528: keras.wrappers.scikit_learn.KerasClassifier(build_fn=None, **sk_params) , \u8fd9\u5b9e\u73b0\u4e86Scikit-Learn \u5206\u7c7b\u5668\u63a5\u53e3, keras.wrappers.scikit_learn.KerasRegressor(build_fn=None, **sk_params) , \u8fd9\u5b9e\u73b0\u4e86Scikit-Learn \u56de\u5f52\u63a5\u53e3\u3002","title":"Scikit-Learn API \u7684\u5c01\u88c5\u5668"},{"location":"94.scikit-learn-api/#_1","text":"build_fn : \u53ef\u8c03\u7528\u51fd\u6570\u6216\u7c7b\u5b9e\u4f8b sk_params : \u6a21\u578b\u53c2\u6570\u548c\u62df\u5408\u53c2\u6570 build_fn \u5e94\u8be5\u5efa\u7acb\uff0c\u7f16\u8bd1\uff0c\u5e76\u8fd4\u56de\u4e00\u4e2a Keras \u6a21\u578b\uff0c\u7136\u540e\u88ab\u7528\u6765\u8bad\u7ec3/\u9884\u6d4b\u3002\u4ee5\u4e0b\u4e09\u4e2a\u503c\u4e4b\u4e00\u53ef\u4ee5\u4f20\u9012\u7ed9 build_fn \u4e00\u4e2a\u51fd\u6570\uff1b \u5b9e\u73b0 __call__ \u65b9\u6cd5\u7684\u7c7b\u7684\u5b9e\u4f8b\uff1b None\u3002\u8fd9\u610f\u5473\u7740\u4f60\u5b9e\u73b0\u4e86\u4e00\u4e2a\u7ee7\u627f\u81ea KerasClassifier \u6216 KerasRegressor \u7684\u7c7b\u3002\u5f53\u524d\u7c7b __call__ \u65b9\u6cd5\u5c06\u88ab\u89c6\u4e3a\u9ed8\u8ba4\u7684 build_fn \u3002 sk_params \u540c\u65f6\u5305\u542b\u6a21\u578b\u53c2\u6570\u548c\u62df\u5408\u53c2\u6570\u3002\u5408\u6cd5\u7684\u6a21\u578b\u53c2\u6570\u662f build_fn \u7684\u53c2\u6570\u3002\u8bf7\u6ce8\u610f\uff0c\u4e0e scikit-learn \u4e2d\u7684\u6240\u6709\u5176\u4ed6\u4f30\u7b97\u5668\u4e00\u6837\uff0c build_fn \u5e94\u4e3a\u5176\u53c2\u6570\u63d0\u4f9b\u9ed8\u8ba4\u503c\uff0c \u4ee5\u4fbf\u4f60\u53ef\u4ee5\u521b\u5efa\u4f30\u7b97\u5668\u800c\u4e0d\u5c06\u4efb\u4f55\u503c\u4f20\u9012\u7ed9 sk_params \u3002 sk_params \u8fd8\u53ef\u4ee5\u63a5\u53d7\u7528\u4e8e\u8c03\u7528 fit \uff0c predict \uff0c predict_proba \u548c score \u65b9\u6cd5\u7684\u53c2\u6570\uff08\u4f8b\u5982\uff0c epochs \uff0c batch_size \uff09\u3002\u8bad\u7ec3\uff08\u9884\u6d4b\uff09\u53c2\u6570\u6309\u4ee5\u4e0b\u987a\u5e8f\u9009\u62e9\uff1a \u4f20\u9012\u7ed9 fit \uff0c predict \uff0c predict_proba \u548c score \u51fd\u6570\u7684\u5b57\u5178\u53c2\u6570\u7684\u503c\uff1b \u4f20\u9012\u7ed9 sk_params \u7684\u503c\uff1b keras.models.Sequential \u7684 fit \uff0c predict \uff0c predict_proba \u548c score \u65b9\u6cd5\u7684\u9ed8\u8ba4\u503c\u3002 \u5f53\u4f7f\u7528 scikit-learn \u7684 grid_search API \u65f6\uff0c\u5408\u6cd5\u53ef\u8c03\u53c2\u6570\u662f\u4f60\u53ef\u4ee5\u4f20\u9012\u7ed9 sk_params \u7684\u53c2\u6570\uff0c\u5305\u62ec\u8bad\u7ec3\u53c2\u6570\u3002\u6362\u53e5\u8bdd\u8bf4\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528 grid_search \u6765\u641c\u7d22\u6700\u4f73\u7684 batch_size \u6216 epoch \u4ee5\u53ca\u5176\u4ed6\u6a21\u578b\u53c2\u6570\u3002","title":"\u53c2\u6570"},{"location":"96.utils/","text":"\u5de5\u5177 [source] CustomObjectScope keras.utils.CustomObjectScope() \u63d0\u4f9b\u66f4\u6539\u4e3a _GLOBAL_CUSTOM_OBJECTS \u65e0\u6cd5\u8f6c\u4e49\u7684\u8303\u56f4\u3002 with \u8bed\u53e5\u4e2d\u7684\u4ee3\u7801\u5c06\u80fd\u591f\u901a\u8fc7\u540d\u79f0\u8bbf\u95ee\u81ea\u5b9a\u4e49\u5bf9\u8c61\u3002 \u5bf9\u5168\u5c40\u81ea\u5b9a\u4e49\u5bf9\u8c61\u7684\u66f4\u6539\u4f1a\u5728\u5c01\u95ed\u7684 with \u8bed\u53e5\u4e2d\u6301\u7eed\u5b58\u5728\u3002 \u5728 with \u8bed\u53e5\u7ed3\u675f\u65f6\uff0c \u5168\u5c40\u81ea\u5b9a\u4e49\u5bf9\u8c61\u5c06\u6062\u590d\u5230 with \u8bed\u53e5\u5f00\u59cb\u65f6\u7684\u72b6\u6001\u3002 \u4f8b\u5b50 \u8003\u8651\u81ea\u5b9a\u4e49\u5bf9\u8c61 MyObject (\u4f8b\u5982\u4e00\u4e2a\u7c7b)\uff1a with CustomObjectScope({'MyObject':MyObject}): layer = Dense(..., kernel_regularizer='MyObject') # \u4fdd\u5b58\uff0c\u52a0\u8f7d\u7b49\u64cd\u4f5c\u5c06\u6309\u8fd9\u4e2a\u540d\u79f0\u6765\u8bc6\u522b\u81ea\u5b9a\u4e49\u5bf9\u8c61 [source] HDF5Matrix keras.utils.HDF5Matrix(datapath, dataset, start=0, end=None, normalizer=None) \u4f7f\u7528 HDF5 \u6570\u636e\u96c6\u8868\u793a\uff0c\u800c\u4e0d\u662f Numpy \u6570\u7ec4\u3002 \u4f8b\u5b50 x_data = HDF5Matrix('input/file.hdf5', 'data') model.predict(x_data) \u63d0\u4f9b start \u548c end \u5c06\u5141\u8bb8\u4f7f\u7528\u6570\u636e\u96c6\u7684\u4e00\u4e2a\u5207\u7247\u3002 \u4f60\u8fd8\u53ef\u4ee5\u7ed9\u51fa\u6807\u51c6\u5316\u51fd\u6570\uff08\u6216 lambda\uff09\uff08\u53ef\u9009\uff09\u3002 \u8fd9\u5c06\u5728\u68c0\u7d22\u5230\u7684\u6bcf\u4e00\u4e2a\u6570\u636e\u5207\u7247\u4e0a\u8c03\u7528\u5b83\u3002 \u53c2\u6570 datapath : \u5b57\u7b26\u4e32\uff0cHDF5 \u6587\u4ef6\u8def\u5f84\u3002 dataset : \u5b57\u7b26\u4e32\uff0cdatapath\u6307\u5b9a\u7684\u6587\u4ef6\u4e2d\u7684 HDF5 \u6570\u636e\u96c6\u540d\u79f0\u3002 start : \u6574\u6570\uff0c\u6240\u9700\u7684\u6307\u5b9a\u6570\u636e\u96c6\u7684\u5207\u7247\u7684\u5f00\u59cb\u4f4d\u7f6e\u3002 end : \u6574\u6570\uff0c\u6240\u9700\u7684\u6307\u5b9a\u6570\u636e\u96c6\u7684\u5207\u7247\u7684\u7ed3\u675f\u4f4d\u7f6e\u3002 normalizer : \u5728\u68c0\u7d22\u6570\u636e\u65f6\u8c03\u7528\u7684\u51fd\u6570\u3002 \u8fd4\u56de \u4e00\u4e2a\u7c7b\u4f3c\u4e8e\u6570\u7ec4\u7684 HDF5 \u6570\u636e\u96c6\u3002 [source] Sequence keras.utils.Sequence() \u7528\u4e8e\u62df\u5408\u6570\u636e\u5e8f\u5217\u7684\u57fa\u5bf9\u8c61\uff0c\u4f8b\u5982\u4e00\u4e2a\u6570\u636e\u96c6\u3002 \u6bcf\u4e00\u4e2a Sequence \u5fc5\u987b\u5b9e\u73b0 __getitem__ \u548c __len__ \u65b9\u6cd5\u3002 \u5982\u679c\u4f60\u60f3\u5728\u8fed\u4ee3\u4e4b\u95f4\u4fee\u6539\u4f60\u7684\u6570\u636e\u96c6\uff0c\u4f60\u53ef\u4ee5\u5b9e\u73b0 on_epoch_end \u3002 __getitem__ \u65b9\u6cd5\u5e94\u8be5\u8303\u56f4\u4e00\u4e2a\u5b8c\u6574\u7684\u6279\u6b21\u3002 \u6ce8\u610f Sequence \u662f\u8fdb\u884c\u591a\u8fdb\u7a0b\u5904\u7406\u7684\u66f4\u5b89\u5168\u7684\u65b9\u6cd5\u3002\u8fd9\u79cd\u7ed3\u6784\u4fdd\u8bc1\u7f51\u7edc\u5728\u6bcf\u4e2a\u65f6\u671f\u6bcf\u4e2a\u6837\u672c\u53ea\u8bad\u7ec3\u4e00\u6b21\uff0c\u8fd9\u4e0e\u751f\u6210\u5668\u4e0d\u540c\u3002 \u4f8b\u5b50 from skimage.io import imread from skimage.transform import resize import numpy as np # \u8fd9\u91cc\uff0c`x_set` \u662f\u56fe\u50cf\u7684\u8def\u5f84\u5217\u8868 # \u4ee5\u53ca `y_set` \u662f\u5bf9\u5e94\u7684\u7c7b\u522b class CIFAR10Sequence(Sequence): def __init__(self, x_set, y_set, batch_size): self.x, self.y = x_set, y_set self.batch_size = batch_size def __len__(self): return int(np.ceil(len(self.x) / float(self.batch_size))) def __getitem__(self, idx): batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size] batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size] return np.array([ resize(imread(file_name), (200, 200)) for file_name in batch_x]), np.array(batch_y) to_categorical keras.utils.to_categorical(y, num_classes=None, dtype='float32') \u5c06\u7c7b\u5411\u91cf\uff08\u6574\u6570\uff09\u8f6c\u6362\u4e3a\u4e8c\u8fdb\u5236\u7c7b\u77e9\u9635\u3002 \u4f8b\u5982\uff0c\u7528\u4e8e categorical_crossentropy\u3002 \u53c2\u6570 y : \u9700\u8981\u8f6c\u6362\u6210\u77e9\u9635\u7684\u7c7b\u77e2\u91cf (\u4ece 0 \u5230 num_classes \u7684\u6574\u6570)\u3002 num_classes : \u603b\u7c7b\u522b\u6570\u3002 dtype : \u5b57\u7b26\u4e32\uff0c\u8f93\u5165\u6240\u671f\u671b\u7684\u6570\u636e\u7c7b\u578b ( float32 , float64 , int32 ...) \u4f8b\u5b50 # \u8003\u8651\u4e00\u7ec4 3 \u4e2a\u7c7b {0,1,2} \u4e2d\u7684 5 \u4e2a\u6807\u7b7e\u6570\u7ec4\uff1a > labels array([0, 2, 1, 2, 0]) # `to_categorical` \u5c06\u5176\u8f6c\u6362\u4e3a\u5177\u6709\u5c3d\u53ef\u80fd\u591a\u8868\u793a\u7c7b\u522b\u6570\u7684\u5217\u7684\u77e9\u9635\u3002 # \u884c\u6570\u4fdd\u6301\u4e0d\u53d8\u3002 > to_categorical(labels) array([[ 1., 0., 0.], [ 0., 0., 1.], [ 0., 1., 0.], [ 0., 0., 1.], [ 1., 0., 0.]], dtype=float32) \u8fd4\u56de \u8f93\u5165\u7684\u4e8c\u8fdb\u5236\u77e9\u9635\u8868\u793a\u3002 normalize keras.utils.normalize(x, axis=-1, order=2) \u6807\u51c6\u5316\u4e00\u4e2a Numpy \u6570\u7ec4\u3002 \u53c2\u6570 x : \u9700\u8981\u6807\u51c6\u5316\u7684 Numpy \u6570\u7ec4\u3002 axis : \u9700\u8981\u6807\u51c6\u5316\u7684\u8f74\u3002 order : \u6807\u51c6\u5316\u987a\u5e8f(\u4f8b\u5982\uff0c2 \u8868\u793a L2 \u89c4\u8303\u5316)\u3002 Returns \u6570\u7ec4\u7684\u6807\u51c6\u5316\u526f\u672c\u3002 get_file keras.utils.get_file(fname, origin, untar=False, md5_hash=None, file_hash=None, cache_subdir='datasets', hash_algorithm='auto', extract=False, archive_format='auto', cache_dir=None) \u4ece\u4e00\u4e2a URL \u4e0b\u8f7d\u6587\u4ef6\uff0c\u5982\u679c\u5b83\u4e0d\u5b58\u5728\u7f13\u5b58\u4e2d\u3002 \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cURL origin \u5904\u7684\u6587\u4ef6 \u88ab\u4e0b\u8f7d\u5230\u7f13\u5b58\u76ee\u5f55 \u301c/.keras \u4e2d\uff0c \u653e\u5728\u7f13\u5b58\u5b50\u76ee\u5f55 datasets \u4e2d\uff0c\u5e76\u547d\u540d\u4e3a fname \u3002 \u6587\u4ef6 example.txt \u7684\u6700\u7ec8\u4f4d\u7f6e\u4e3a ~/.keras/datasets/example.txt \u3002 tar, tar.gz, tar.bz, \u4ee5\u53ca zip \u683c\u5f0f\u7684\u6587\u4ef6\u4e5f\u53ef\u4ee5\u88ab\u89e3\u538b\u3002 \u4f20\u9012\u4e00\u4e2a\u54c8\u5e0c\u503c\u5c06\u5728\u4e0b\u8f7d\u540e\u6821\u9a8c\u6587\u4ef6\u3002 \u547d\u4ee4\u884c\u7a0b\u5e8f shasum \u548c sha256sum \u53ef\u4ee5\u8ba1\u7b97\u54c8\u5e0c\u3002 \u53c2\u6570 fname : \u6587\u4ef6\u540d\u3002\u5982\u679c\u6307\u5b9a\u4e86\u7edd\u5bf9\u8def\u5f84 /path/to/file.txt \uff0c \u90a3\u4e48\u6587\u4ef6\u5c06\u4f1a\u4fdd\u5b58\u5230\u90a3\u4e2a\u8def\u5f84\u3002 origin : \u6587\u4ef6\u7684\u539f\u59cb URL\u3002 untar : \u7531\u4e8e\u4f7f\u7528 'extract' \u800c\u5df2\u88ab\u5f03\u7528\u3002 \u5e03\u5c14\u503c\uff0c\u662f\u5426\u9700\u8981\u89e3\u538b\u6587\u4ef6\u3002 md5_hash : \u7531\u4e8e\u4f7f\u7528 'file_hash' \u800c\u5df2\u88ab\u5f03\u7528\u3002 \u7528\u4e8e\u6821\u9a8c\u7684 md5 \u54c8\u5e0c\u503c\u3002 file_hash : \u4e0b\u8f7d\u540e\u7684\u6587\u4ef6\u7684\u671f\u671b\u54c8\u5e0c\u5b57\u7b26\u4e32\u3002 \u652f\u6301 sha256 \u548c md5 \u4e24\u4e2a\u54c8\u5e0c\u7b97\u6cd5\u3002 cache_subdir : \u5728 Keras \u7f13\u5b58\u76ee\u5f55\u4e0b\u7684\u4fdd\u5b58\u6587\u4ef6\u7684\u5b50\u76ee\u5f55\u3002 \u5982\u679c\u6307\u5b9a\u4e86\u7edd\u5bf9\u8def\u5f84 /path/to/folder \uff0c\u5219\u6587\u4ef6\u5c06\u88ab\u4fdd\u5b58\u5728\u8be5\u4f4d\u7f6e\u3002 hash_algorithm : \u9009\u62e9\u54c8\u5e0c\u7b97\u6cd5\u6765\u6821\u9a8c\u6587\u4ef6\u3002 \u53ef\u9009\u7684\u6709 'md5', 'sha256', \u4ee5\u53ca 'auto'\u3002 \u9ed8\u8ba4\u7684 'auto' \u5c06\u81ea\u52a8\u68c0\u6d4b\u6240\u4f7f\u7528\u7684\u54c8\u5e0c\u7b97\u6cd5\u3002 extract : True \u7684\u8bdd\u4f1a\u5c1d\u8bd5\u5c06\u89e3\u538b\u7f29\u5b58\u6863\u6587\u4ef6\uff0c\u5982tar\u6216zip\u3002 archive_format : \u5c1d\u8bd5\u63d0\u53d6\u6587\u4ef6\u7684\u5b58\u6863\u683c\u5f0f\u3002 \u53ef\u9009\u7684\u6709 'auto', 'tar', 'zip', \u4ee5\u53ca None\u3002 'tar' \u5305\u542b tar, tar.gz, \u548c tar.bz \u6587\u4ef6\u3002 \u9ed8\u8ba4 'auto' \u4e3a ['tar', 'zip']\u3002 None \u6216 \u7a7a\u5217\u8868\u5c06\u8fd4\u56de\u672a\u627e\u5230\u4efb\u4f55\u5339\u914d\u3002 ke xu az z'auto', 'tar', 'zip', and None. cache_dir : \u5b58\u50a8\u7f13\u5b58\u6587\u4ef6\u7684\u4f4d\u7f6e\uff0c\u4e3a None \u65f6\u9ed8\u8ba4\u4e3a Keras \u76ee\u5f55 . \u8fd4\u56de \u4e0b\u8f7d\u7684\u6587\u4ef6\u7684\u8def\u5f84\u3002 print_summary keras.utils.print_summary(model, line_length=None, positions=None, print_fn=None) \u6253\u5370\u6a21\u578b\u6982\u51b5\u3002 \u53c2\u6570 model : Keras \u6a21\u578b\u5b9e\u4f8b\u3002 line_length : \u6253\u5370\u7684\u6bcf\u884c\u7684\u603b\u957f\u5ea6 (\u4f8b\u5982\uff0c\u8bbe\u7f6e\u6b64\u9879\u4ee5\u4f7f\u5176\u663e\u793a\u9002\u5e94\u4e0d\u540c\u7684\u7ec8\u7aef\u7a97\u53e3\u5927\u5c0f)\u3002 positions : \u6bcf\u884c\u4e2d\u65e5\u5fd7\u5143\u7d20\u7684\u76f8\u5bf9\u6216\u7edd\u5bf9\u4f4d\u7f6e\u3002 \u5982\u679c\u672a\u63d0\u4f9b\uff0c\u9ed8\u8ba4\u4e3a [.33, .55, .67, 1.] \u3002 print_fn : \u9700\u8981\u4f7f\u7528\u7684\u6253\u5370\u51fd\u6570\u3002 \u5b83\u5c06\u5728\u6bcf\u4e00\u884c\u6982\u8ff0\u65f6\u8c03\u7528\u3002 \u60a8\u53ef\u4ee5\u5c06\u5176\u8bbe\u7f6e\u4e3a\u81ea\u5b9a\u4e49\u51fd\u6570\u4ee5\u6355\u83b7\u5b57\u7b26\u4e32\u6982\u8ff0\u3002 \u9ed8\u8ba4\u4e3a print (\u6253\u5370\u5230\u6807\u51c6\u8f93\u51fa)\u3002 plot_model keras.utils.plot_model(model, to_file='model.png', show_shapes=False, show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96) \u5c06 Keras \u6a21\u578b\u8f6c\u6362\u4e3a dot \u683c\u5f0f\u5e76\u4fdd\u5b58\u5230\u6587\u4ef6\u4e2d\u3002 \u53c2\u6570 model : \u4e00\u4e2a Keras \u6a21\u578b\u5b9e\u4f8b\u3002 to_file : \u7ed8\u5236\u56fe\u50cf\u7684\u6587\u4ef6\u540d\u3002 show_shapes : \u662f\u5426\u663e\u793a\u5c3a\u5bf8\u4fe1\u606f\u3002 show_layer_names : \u662f\u5426\u663e\u793a\u5c42\u7684\u540d\u79f0\u3002 rankdir : \u4f20\u9012\u7ed9 PyDot \u7684 rankdir \u53c2\u6570\uff0c \u4e00\u4e2a\u6307\u5b9a\u7ed8\u56fe\u683c\u5f0f\u7684\u5b57\u7b26\u4e32\uff1a 'TB' \u521b\u5efa\u4e00\u4e2a\u5782\u76f4\u7ed8\u56fe\uff1b 'LR' \u521b\u5efa\u4e00\u4e2a\u6c34\u5e73\u7ed8\u56fe\u3002 expand_nested : \u662f\u5426\u6269\u5c55\u5d4c\u5957\u6a21\u578b\u4e3a\u805a\u7c7b\u3002 dpi : \u70b9 DPI\u3002 multi_gpu_model keras.utils.multi_gpu_model(model, gpus=None, cpu_merge=True, cpu_relocation=False) \u5c06\u6a21\u578b\u590d\u5236\u5230\u4e0d\u540c\u7684 GPU \u4e0a\u3002 \u5177\u4f53\u6765\u8bf4\uff0c\u8be5\u529f\u80fd\u5b9e\u73b0\u4e86\u5355\u673a\u591a GPU \u6570\u636e\u5e76\u884c\u6027\u3002 \u5b83\u7684\u5de5\u4f5c\u539f\u7406\u5982\u4e0b\uff1a \u5c06\u6a21\u578b\u7684\u8f93\u5165\u5206\u6210\u591a\u4e2a\u5b50\u6279\u6b21\u3002 \u5728\u6bcf\u4e2a\u5b50\u6279\u6b21\u4e0a\u5e94\u7528\u6a21\u578b\u526f\u672c\u3002 \u6bcf\u4e2a\u6a21\u578b\u526f\u672c\u90fd\u5728\u4e13\u7528 GPU \u4e0a\u6267\u884c\u3002 \u5c06\u7ed3\u679c\uff08\u5728 CPU \u4e0a\uff09\u8fde\u63a5\u6210\u4e00\u4e2a\u5927\u6279\u91cf\u3002 \u4f8b\u5982\uff0c \u5982\u679c\u4f60\u7684 batch_size \u662f 64\uff0c\u4e14\u4f60\u4f7f\u7528 gpus=2 \uff0c \u90a3\u4e48\u6211\u4eec\u5c06\u628a\u8f93\u5165\u5206\u4e3a\u4e24\u4e2a 32 \u4e2a\u6837\u672c\u7684\u5b50\u6279\u6b21\uff0c \u5728 1 \u4e2a GPU \u4e0a\u5904\u7406 1 \u4e2a\u5b50\u6279\u6b21\uff0c\u7136\u540e\u8fd4\u56de\u5b8c\u6574\u6279\u6b21\u7684 64 \u4e2a\u5904\u7406\u8fc7\u7684\u6837\u672c\u3002 \u8fd9\u5b9e\u73b0\u4e86\u591a\u8fbe 8 \u4e2a GPU \u7684\u51c6\u7ebf\u6027\u52a0\u901f\u3002 \u6b64\u529f\u80fd\u76ee\u524d\u4ec5\u9002\u7528\u4e8e TensorFlow \u540e\u7aef\u3002 \u53c2\u6570 model : \u4e00\u4e2a Keras \u6a21\u578b\u5b9e\u4f8b\u3002\u4e3a\u4e86\u907f\u514dOOM\u9519\u8bef\uff0c\u8be5\u6a21\u578b\u53ef\u4ee5\u5efa\u7acb\u5728 CPU \u4e0a\uff0c \u8be6\u89c1\u4e0b\u9762\u7684\u4f7f\u7528\u6837\u4f8b\u3002 gpus : \u6574\u6570 >= 2 \u6216\u6574\u6570\u5217\u8868\uff0c\u521b\u5efa\u6a21\u578b\u526f\u672c\u7684 GPU \u6570\u91cf\uff0c \u6216 GPU ID \u7684\u5217\u8868\u3002 cpu_merge : \u4e00\u4e2a\u5e03\u5c14\u503c\uff0c\u7528\u4e8e\u6807\u8bc6\u662f\u5426\u5f3a\u5236\u5408\u5e76 CPU \u8303\u56f4\u5185\u7684\u6a21\u578b\u6743\u91cd\u3002 cpu_relocation : \u4e00\u4e2a\u5e03\u5c14\u503c\uff0c\u7528\u6765\u786e\u5b9a\u662f\u5426\u5728 CPU \u7684\u8303\u56f4\u5185\u521b\u5efa\u6a21\u578b\u7684\u6743\u91cd\u3002\u5982\u679c\u6a21\u578b\u6ca1\u6709\u5728\u4efb\u4f55\u4e00\u4e2a\u8bbe\u5907\u8303\u56f4\u5185\u5b9a\u4e49\uff0c\u60a8\u4ecd\u7136\u53ef\u4ee5\u901a\u8fc7\u6fc0\u6d3b\u8fd9\u4e2a\u9009\u9879\u6765\u62ef\u6551\u5b83\u3002 \u8fd4\u56de \u4e00\u4e2a Keras Model \u5b9e\u4f8b\uff0c\u5b83\u53ef\u4ee5\u50cf\u521d\u59cb model \u53c2\u6570\u4e00\u6837\u4f7f\u7528\uff0c\u4f46\u5b83\u5c06\u5de5\u4f5c\u8d1f\u8f7d\u5206\u5e03\u5728\u591a\u4e2a GPU \u4e0a\u3002 \u4f8b\u5b50 \u4f8b 1 - \u8bad\u7ec3\u5728 CPU \u4e0a\u5408\u5e76\u6743\u91cd\u7684\u6a21\u578b import tensorflow as tf from keras.applications import Xception from keras.utils import multi_gpu_model import numpy as np num_samples = 1000 height = 224 width = 224 num_classes = 1000 # \u5b9e\u4f8b\u5316\u57fa\u7840\u6a21\u578b\uff08\u6216\u8005\u300c\u6a21\u7248\u300d\u6a21\u578b\uff09\u3002 # \u6211\u4eec\u63a8\u8350\u5728 CPU \u8bbe\u5907\u8303\u56f4\u5185\u505a\u6b64\u64cd\u4f5c\uff0c # \u8fd9\u6837\u6a21\u578b\u7684\u6743\u91cd\u5c31\u4f1a\u5b58\u50a8\u5728 CPU \u5185\u5b58\u4e2d\u3002 # \u5426\u5219\u5b83\u4eec\u4f1a\u5b58\u50a8\u5728 GPU \u4e0a\uff0c\u800c\u5b8c\u5168\u88ab\u5171\u4eab\u3002 with tf.device('/cpu:0'): model = Xception(weights=None, input_shape=(height, width, 3), classes=num_classes) # \u590d\u5236\u6a21\u578b\u5230 8 \u4e2a GPU \u4e0a\u3002 # \u8fd9\u5047\u8bbe\u4f60\u7684\u673a\u5668\u6709 8 \u4e2a\u53ef\u7528 GPU\u3002 parallel_model = multi_gpu_model(model, gpus=8) parallel_model.compile(loss='categorical_crossentropy', optimizer='rmsprop') # \u751f\u6210\u865a\u62df\u6570\u636e x = np.random.random((num_samples, height, width, 3)) y = np.random.random((num_samples, num_classes)) # \u8fd9\u4e2a `fit` \u8c03\u7528\u5c06\u5206\u5e03\u5728 8 \u4e2a GPU \u4e0a\u3002 # \u7531\u4e8e batch size \u662f 256, \u6bcf\u4e2a GPU \u5c06\u5904\u7406 32 \u4e2a\u6837\u672c\u3002 parallel_model.fit(x, y, epochs=20, batch_size=256) # \u901a\u8fc7\u6a21\u7248\u6a21\u578b\u5b58\u50a8\u6a21\u578b\uff08\u5171\u4eab\u76f8\u540c\u6743\u91cd\uff09\uff1a model.save('my_model.h5') \u4f8b 2 - \u8bad\u7ec3\u5728 CPU \u4e0a\u5229\u7528 cpu_relocation \u5408\u5e76\u6743\u91cd\u7684\u6a21\u578b .. # \u4e0d\u9700\u8981\u66f4\u6539\u6a21\u578b\u5b9a\u4e49\u7684\u8bbe\u5907\u8303\u56f4\uff1a model = Xception(weights=None, ..) try: parallel_model = multi_gpu_model(model, cpu_relocation=True) print(\"Training using multiple GPUs..\") except ValueError: parallel_model = model print(\"Training using single GPU or CPU..\") parallel_model.compile(..) .. \u4f8b 3 - \u8bad\u7ec3\u5728 GPU \u4e0a\u5408\u5e76\u6743\u91cd\u7684\u6a21\u578b\uff08\u5efa\u8bae\u7528\u4e8e NV-link\uff09 .. # \u4e0d\u9700\u8981\u66f4\u6539\u6a21\u578b\u5b9a\u4e49\u7684\u8bbe\u5907\u8303\u56f4\uff1a model = Xception(weights=None, ..) try: parallel_model = multi_gpu_model(model, cpu_merge=False) print(\"Training using multiple GPUs..\") except: parallel_model = model print(\"Training using single GPU or CPU..\") parallel_model.compile(..) .. \u5173\u4e8e\u6a21\u578b\u4fdd\u5b58 \u8981\u4fdd\u5b58\u591a GPU \u6a21\u578b\uff0c\u8bf7\u901a\u8fc7\u6a21\u677f\u6a21\u578b\uff08\u4f20\u9012\u7ed9 multi_gpu_model \u7684\u53c2\u6570\uff09\u8c03\u7528 .save(fname) \u6216 .save_weights(fname) \u4ee5\u8fdb\u884c\u5b58\u50a8\uff0c\u800c\u4e0d\u662f\u901a\u8fc7 multi_gpu_model \u8fd4\u56de\u7684\u6a21\u578b\u3002","title":"\u5de5\u5177"},{"location":"96.utils/#_1","text":"[source]","title":"\u5de5\u5177"},{"location":"96.utils/#customobjectscope","text":"keras.utils.CustomObjectScope() \u63d0\u4f9b\u66f4\u6539\u4e3a _GLOBAL_CUSTOM_OBJECTS \u65e0\u6cd5\u8f6c\u4e49\u7684\u8303\u56f4\u3002 with \u8bed\u53e5\u4e2d\u7684\u4ee3\u7801\u5c06\u80fd\u591f\u901a\u8fc7\u540d\u79f0\u8bbf\u95ee\u81ea\u5b9a\u4e49\u5bf9\u8c61\u3002 \u5bf9\u5168\u5c40\u81ea\u5b9a\u4e49\u5bf9\u8c61\u7684\u66f4\u6539\u4f1a\u5728\u5c01\u95ed\u7684 with \u8bed\u53e5\u4e2d\u6301\u7eed\u5b58\u5728\u3002 \u5728 with \u8bed\u53e5\u7ed3\u675f\u65f6\uff0c \u5168\u5c40\u81ea\u5b9a\u4e49\u5bf9\u8c61\u5c06\u6062\u590d\u5230 with \u8bed\u53e5\u5f00\u59cb\u65f6\u7684\u72b6\u6001\u3002 \u4f8b\u5b50 \u8003\u8651\u81ea\u5b9a\u4e49\u5bf9\u8c61 MyObject (\u4f8b\u5982\u4e00\u4e2a\u7c7b)\uff1a with CustomObjectScope({'MyObject':MyObject}): layer = Dense(..., kernel_regularizer='MyObject') # \u4fdd\u5b58\uff0c\u52a0\u8f7d\u7b49\u64cd\u4f5c\u5c06\u6309\u8fd9\u4e2a\u540d\u79f0\u6765\u8bc6\u522b\u81ea\u5b9a\u4e49\u5bf9\u8c61 [source]","title":"CustomObjectScope"},{"location":"96.utils/#hdf5matrix","text":"keras.utils.HDF5Matrix(datapath, dataset, start=0, end=None, normalizer=None) \u4f7f\u7528 HDF5 \u6570\u636e\u96c6\u8868\u793a\uff0c\u800c\u4e0d\u662f Numpy \u6570\u7ec4\u3002 \u4f8b\u5b50 x_data = HDF5Matrix('input/file.hdf5', 'data') model.predict(x_data) \u63d0\u4f9b start \u548c end \u5c06\u5141\u8bb8\u4f7f\u7528\u6570\u636e\u96c6\u7684\u4e00\u4e2a\u5207\u7247\u3002 \u4f60\u8fd8\u53ef\u4ee5\u7ed9\u51fa\u6807\u51c6\u5316\u51fd\u6570\uff08\u6216 lambda\uff09\uff08\u53ef\u9009\uff09\u3002 \u8fd9\u5c06\u5728\u68c0\u7d22\u5230\u7684\u6bcf\u4e00\u4e2a\u6570\u636e\u5207\u7247\u4e0a\u8c03\u7528\u5b83\u3002 \u53c2\u6570 datapath : \u5b57\u7b26\u4e32\uff0cHDF5 \u6587\u4ef6\u8def\u5f84\u3002 dataset : \u5b57\u7b26\u4e32\uff0cdatapath\u6307\u5b9a\u7684\u6587\u4ef6\u4e2d\u7684 HDF5 \u6570\u636e\u96c6\u540d\u79f0\u3002 start : \u6574\u6570\uff0c\u6240\u9700\u7684\u6307\u5b9a\u6570\u636e\u96c6\u7684\u5207\u7247\u7684\u5f00\u59cb\u4f4d\u7f6e\u3002 end : \u6574\u6570\uff0c\u6240\u9700\u7684\u6307\u5b9a\u6570\u636e\u96c6\u7684\u5207\u7247\u7684\u7ed3\u675f\u4f4d\u7f6e\u3002 normalizer : \u5728\u68c0\u7d22\u6570\u636e\u65f6\u8c03\u7528\u7684\u51fd\u6570\u3002 \u8fd4\u56de \u4e00\u4e2a\u7c7b\u4f3c\u4e8e\u6570\u7ec4\u7684 HDF5 \u6570\u636e\u96c6\u3002 [source]","title":"HDF5Matrix"},{"location":"96.utils/#sequence","text":"keras.utils.Sequence() \u7528\u4e8e\u62df\u5408\u6570\u636e\u5e8f\u5217\u7684\u57fa\u5bf9\u8c61\uff0c\u4f8b\u5982\u4e00\u4e2a\u6570\u636e\u96c6\u3002 \u6bcf\u4e00\u4e2a Sequence \u5fc5\u987b\u5b9e\u73b0 __getitem__ \u548c __len__ \u65b9\u6cd5\u3002 \u5982\u679c\u4f60\u60f3\u5728\u8fed\u4ee3\u4e4b\u95f4\u4fee\u6539\u4f60\u7684\u6570\u636e\u96c6\uff0c\u4f60\u53ef\u4ee5\u5b9e\u73b0 on_epoch_end \u3002 __getitem__ \u65b9\u6cd5\u5e94\u8be5\u8303\u56f4\u4e00\u4e2a\u5b8c\u6574\u7684\u6279\u6b21\u3002 \u6ce8\u610f Sequence \u662f\u8fdb\u884c\u591a\u8fdb\u7a0b\u5904\u7406\u7684\u66f4\u5b89\u5168\u7684\u65b9\u6cd5\u3002\u8fd9\u79cd\u7ed3\u6784\u4fdd\u8bc1\u7f51\u7edc\u5728\u6bcf\u4e2a\u65f6\u671f\u6bcf\u4e2a\u6837\u672c\u53ea\u8bad\u7ec3\u4e00\u6b21\uff0c\u8fd9\u4e0e\u751f\u6210\u5668\u4e0d\u540c\u3002 \u4f8b\u5b50 from skimage.io import imread from skimage.transform import resize import numpy as np # \u8fd9\u91cc\uff0c`x_set` \u662f\u56fe\u50cf\u7684\u8def\u5f84\u5217\u8868 # \u4ee5\u53ca `y_set` \u662f\u5bf9\u5e94\u7684\u7c7b\u522b class CIFAR10Sequence(Sequence): def __init__(self, x_set, y_set, batch_size): self.x, self.y = x_set, y_set self.batch_size = batch_size def __len__(self): return int(np.ceil(len(self.x) / float(self.batch_size))) def __getitem__(self, idx): batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size] batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size] return np.array([ resize(imread(file_name), (200, 200)) for file_name in batch_x]), np.array(batch_y)","title":"Sequence"},{"location":"96.utils/#to_categorical","text":"keras.utils.to_categorical(y, num_classes=None, dtype='float32') \u5c06\u7c7b\u5411\u91cf\uff08\u6574\u6570\uff09\u8f6c\u6362\u4e3a\u4e8c\u8fdb\u5236\u7c7b\u77e9\u9635\u3002 \u4f8b\u5982\uff0c\u7528\u4e8e categorical_crossentropy\u3002 \u53c2\u6570 y : \u9700\u8981\u8f6c\u6362\u6210\u77e9\u9635\u7684\u7c7b\u77e2\u91cf (\u4ece 0 \u5230 num_classes \u7684\u6574\u6570)\u3002 num_classes : \u603b\u7c7b\u522b\u6570\u3002 dtype : \u5b57\u7b26\u4e32\uff0c\u8f93\u5165\u6240\u671f\u671b\u7684\u6570\u636e\u7c7b\u578b ( float32 , float64 , int32 ...) \u4f8b\u5b50 # \u8003\u8651\u4e00\u7ec4 3 \u4e2a\u7c7b {0,1,2} \u4e2d\u7684 5 \u4e2a\u6807\u7b7e\u6570\u7ec4\uff1a > labels array([0, 2, 1, 2, 0]) # `to_categorical` \u5c06\u5176\u8f6c\u6362\u4e3a\u5177\u6709\u5c3d\u53ef\u80fd\u591a\u8868\u793a\u7c7b\u522b\u6570\u7684\u5217\u7684\u77e9\u9635\u3002 # \u884c\u6570\u4fdd\u6301\u4e0d\u53d8\u3002 > to_categorical(labels) array([[ 1., 0., 0.], [ 0., 0., 1.], [ 0., 1., 0.], [ 0., 0., 1.], [ 1., 0., 0.]], dtype=float32) \u8fd4\u56de \u8f93\u5165\u7684\u4e8c\u8fdb\u5236\u77e9\u9635\u8868\u793a\u3002","title":"to_categorical"},{"location":"96.utils/#normalize","text":"keras.utils.normalize(x, axis=-1, order=2) \u6807\u51c6\u5316\u4e00\u4e2a Numpy \u6570\u7ec4\u3002 \u53c2\u6570 x : \u9700\u8981\u6807\u51c6\u5316\u7684 Numpy \u6570\u7ec4\u3002 axis : \u9700\u8981\u6807\u51c6\u5316\u7684\u8f74\u3002 order : \u6807\u51c6\u5316\u987a\u5e8f(\u4f8b\u5982\uff0c2 \u8868\u793a L2 \u89c4\u8303\u5316)\u3002 Returns \u6570\u7ec4\u7684\u6807\u51c6\u5316\u526f\u672c\u3002","title":"normalize"},{"location":"96.utils/#get_file","text":"keras.utils.get_file(fname, origin, untar=False, md5_hash=None, file_hash=None, cache_subdir='datasets', hash_algorithm='auto', extract=False, archive_format='auto', cache_dir=None) \u4ece\u4e00\u4e2a URL \u4e0b\u8f7d\u6587\u4ef6\uff0c\u5982\u679c\u5b83\u4e0d\u5b58\u5728\u7f13\u5b58\u4e2d\u3002 \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cURL origin \u5904\u7684\u6587\u4ef6 \u88ab\u4e0b\u8f7d\u5230\u7f13\u5b58\u76ee\u5f55 \u301c/.keras \u4e2d\uff0c \u653e\u5728\u7f13\u5b58\u5b50\u76ee\u5f55 datasets \u4e2d\uff0c\u5e76\u547d\u540d\u4e3a fname \u3002 \u6587\u4ef6 example.txt \u7684\u6700\u7ec8\u4f4d\u7f6e\u4e3a ~/.keras/datasets/example.txt \u3002 tar, tar.gz, tar.bz, \u4ee5\u53ca zip \u683c\u5f0f\u7684\u6587\u4ef6\u4e5f\u53ef\u4ee5\u88ab\u89e3\u538b\u3002 \u4f20\u9012\u4e00\u4e2a\u54c8\u5e0c\u503c\u5c06\u5728\u4e0b\u8f7d\u540e\u6821\u9a8c\u6587\u4ef6\u3002 \u547d\u4ee4\u884c\u7a0b\u5e8f shasum \u548c sha256sum \u53ef\u4ee5\u8ba1\u7b97\u54c8\u5e0c\u3002 \u53c2\u6570 fname : \u6587\u4ef6\u540d\u3002\u5982\u679c\u6307\u5b9a\u4e86\u7edd\u5bf9\u8def\u5f84 /path/to/file.txt \uff0c \u90a3\u4e48\u6587\u4ef6\u5c06\u4f1a\u4fdd\u5b58\u5230\u90a3\u4e2a\u8def\u5f84\u3002 origin : \u6587\u4ef6\u7684\u539f\u59cb URL\u3002 untar : \u7531\u4e8e\u4f7f\u7528 'extract' \u800c\u5df2\u88ab\u5f03\u7528\u3002 \u5e03\u5c14\u503c\uff0c\u662f\u5426\u9700\u8981\u89e3\u538b\u6587\u4ef6\u3002 md5_hash : \u7531\u4e8e\u4f7f\u7528 'file_hash' \u800c\u5df2\u88ab\u5f03\u7528\u3002 \u7528\u4e8e\u6821\u9a8c\u7684 md5 \u54c8\u5e0c\u503c\u3002 file_hash : \u4e0b\u8f7d\u540e\u7684\u6587\u4ef6\u7684\u671f\u671b\u54c8\u5e0c\u5b57\u7b26\u4e32\u3002 \u652f\u6301 sha256 \u548c md5 \u4e24\u4e2a\u54c8\u5e0c\u7b97\u6cd5\u3002 cache_subdir : \u5728 Keras \u7f13\u5b58\u76ee\u5f55\u4e0b\u7684\u4fdd\u5b58\u6587\u4ef6\u7684\u5b50\u76ee\u5f55\u3002 \u5982\u679c\u6307\u5b9a\u4e86\u7edd\u5bf9\u8def\u5f84 /path/to/folder \uff0c\u5219\u6587\u4ef6\u5c06\u88ab\u4fdd\u5b58\u5728\u8be5\u4f4d\u7f6e\u3002 hash_algorithm : \u9009\u62e9\u54c8\u5e0c\u7b97\u6cd5\u6765\u6821\u9a8c\u6587\u4ef6\u3002 \u53ef\u9009\u7684\u6709 'md5', 'sha256', \u4ee5\u53ca 'auto'\u3002 \u9ed8\u8ba4\u7684 'auto' \u5c06\u81ea\u52a8\u68c0\u6d4b\u6240\u4f7f\u7528\u7684\u54c8\u5e0c\u7b97\u6cd5\u3002 extract : True \u7684\u8bdd\u4f1a\u5c1d\u8bd5\u5c06\u89e3\u538b\u7f29\u5b58\u6863\u6587\u4ef6\uff0c\u5982tar\u6216zip\u3002 archive_format : \u5c1d\u8bd5\u63d0\u53d6\u6587\u4ef6\u7684\u5b58\u6863\u683c\u5f0f\u3002 \u53ef\u9009\u7684\u6709 'auto', 'tar', 'zip', \u4ee5\u53ca None\u3002 'tar' \u5305\u542b tar, tar.gz, \u548c tar.bz \u6587\u4ef6\u3002 \u9ed8\u8ba4 'auto' \u4e3a ['tar', 'zip']\u3002 None \u6216 \u7a7a\u5217\u8868\u5c06\u8fd4\u56de\u672a\u627e\u5230\u4efb\u4f55\u5339\u914d\u3002 ke xu az z'auto', 'tar', 'zip', and None. cache_dir : \u5b58\u50a8\u7f13\u5b58\u6587\u4ef6\u7684\u4f4d\u7f6e\uff0c\u4e3a None \u65f6\u9ed8\u8ba4\u4e3a Keras \u76ee\u5f55 . \u8fd4\u56de \u4e0b\u8f7d\u7684\u6587\u4ef6\u7684\u8def\u5f84\u3002","title":"get_file"},{"location":"96.utils/#print_summary","text":"keras.utils.print_summary(model, line_length=None, positions=None, print_fn=None) \u6253\u5370\u6a21\u578b\u6982\u51b5\u3002 \u53c2\u6570 model : Keras \u6a21\u578b\u5b9e\u4f8b\u3002 line_length : \u6253\u5370\u7684\u6bcf\u884c\u7684\u603b\u957f\u5ea6 (\u4f8b\u5982\uff0c\u8bbe\u7f6e\u6b64\u9879\u4ee5\u4f7f\u5176\u663e\u793a\u9002\u5e94\u4e0d\u540c\u7684\u7ec8\u7aef\u7a97\u53e3\u5927\u5c0f)\u3002 positions : \u6bcf\u884c\u4e2d\u65e5\u5fd7\u5143\u7d20\u7684\u76f8\u5bf9\u6216\u7edd\u5bf9\u4f4d\u7f6e\u3002 \u5982\u679c\u672a\u63d0\u4f9b\uff0c\u9ed8\u8ba4\u4e3a [.33, .55, .67, 1.] \u3002 print_fn : \u9700\u8981\u4f7f\u7528\u7684\u6253\u5370\u51fd\u6570\u3002 \u5b83\u5c06\u5728\u6bcf\u4e00\u884c\u6982\u8ff0\u65f6\u8c03\u7528\u3002 \u60a8\u53ef\u4ee5\u5c06\u5176\u8bbe\u7f6e\u4e3a\u81ea\u5b9a\u4e49\u51fd\u6570\u4ee5\u6355\u83b7\u5b57\u7b26\u4e32\u6982\u8ff0\u3002 \u9ed8\u8ba4\u4e3a print (\u6253\u5370\u5230\u6807\u51c6\u8f93\u51fa)\u3002","title":"print_summary"},{"location":"96.utils/#plot_model","text":"keras.utils.plot_model(model, to_file='model.png', show_shapes=False, show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96) \u5c06 Keras \u6a21\u578b\u8f6c\u6362\u4e3a dot \u683c\u5f0f\u5e76\u4fdd\u5b58\u5230\u6587\u4ef6\u4e2d\u3002 \u53c2\u6570 model : \u4e00\u4e2a Keras \u6a21\u578b\u5b9e\u4f8b\u3002 to_file : \u7ed8\u5236\u56fe\u50cf\u7684\u6587\u4ef6\u540d\u3002 show_shapes : \u662f\u5426\u663e\u793a\u5c3a\u5bf8\u4fe1\u606f\u3002 show_layer_names : \u662f\u5426\u663e\u793a\u5c42\u7684\u540d\u79f0\u3002 rankdir : \u4f20\u9012\u7ed9 PyDot \u7684 rankdir \u53c2\u6570\uff0c \u4e00\u4e2a\u6307\u5b9a\u7ed8\u56fe\u683c\u5f0f\u7684\u5b57\u7b26\u4e32\uff1a 'TB' \u521b\u5efa\u4e00\u4e2a\u5782\u76f4\u7ed8\u56fe\uff1b 'LR' \u521b\u5efa\u4e00\u4e2a\u6c34\u5e73\u7ed8\u56fe\u3002 expand_nested : \u662f\u5426\u6269\u5c55\u5d4c\u5957\u6a21\u578b\u4e3a\u805a\u7c7b\u3002 dpi : \u70b9 DPI\u3002","title":"plot_model"},{"location":"96.utils/#multi_gpu_model","text":"keras.utils.multi_gpu_model(model, gpus=None, cpu_merge=True, cpu_relocation=False) \u5c06\u6a21\u578b\u590d\u5236\u5230\u4e0d\u540c\u7684 GPU \u4e0a\u3002 \u5177\u4f53\u6765\u8bf4\uff0c\u8be5\u529f\u80fd\u5b9e\u73b0\u4e86\u5355\u673a\u591a GPU \u6570\u636e\u5e76\u884c\u6027\u3002 \u5b83\u7684\u5de5\u4f5c\u539f\u7406\u5982\u4e0b\uff1a \u5c06\u6a21\u578b\u7684\u8f93\u5165\u5206\u6210\u591a\u4e2a\u5b50\u6279\u6b21\u3002 \u5728\u6bcf\u4e2a\u5b50\u6279\u6b21\u4e0a\u5e94\u7528\u6a21\u578b\u526f\u672c\u3002 \u6bcf\u4e2a\u6a21\u578b\u526f\u672c\u90fd\u5728\u4e13\u7528 GPU \u4e0a\u6267\u884c\u3002 \u5c06\u7ed3\u679c\uff08\u5728 CPU \u4e0a\uff09\u8fde\u63a5\u6210\u4e00\u4e2a\u5927\u6279\u91cf\u3002 \u4f8b\u5982\uff0c \u5982\u679c\u4f60\u7684 batch_size \u662f 64\uff0c\u4e14\u4f60\u4f7f\u7528 gpus=2 \uff0c \u90a3\u4e48\u6211\u4eec\u5c06\u628a\u8f93\u5165\u5206\u4e3a\u4e24\u4e2a 32 \u4e2a\u6837\u672c\u7684\u5b50\u6279\u6b21\uff0c \u5728 1 \u4e2a GPU \u4e0a\u5904\u7406 1 \u4e2a\u5b50\u6279\u6b21\uff0c\u7136\u540e\u8fd4\u56de\u5b8c\u6574\u6279\u6b21\u7684 64 \u4e2a\u5904\u7406\u8fc7\u7684\u6837\u672c\u3002 \u8fd9\u5b9e\u73b0\u4e86\u591a\u8fbe 8 \u4e2a GPU \u7684\u51c6\u7ebf\u6027\u52a0\u901f\u3002 \u6b64\u529f\u80fd\u76ee\u524d\u4ec5\u9002\u7528\u4e8e TensorFlow \u540e\u7aef\u3002 \u53c2\u6570 model : \u4e00\u4e2a Keras \u6a21\u578b\u5b9e\u4f8b\u3002\u4e3a\u4e86\u907f\u514dOOM\u9519\u8bef\uff0c\u8be5\u6a21\u578b\u53ef\u4ee5\u5efa\u7acb\u5728 CPU \u4e0a\uff0c \u8be6\u89c1\u4e0b\u9762\u7684\u4f7f\u7528\u6837\u4f8b\u3002 gpus : \u6574\u6570 >= 2 \u6216\u6574\u6570\u5217\u8868\uff0c\u521b\u5efa\u6a21\u578b\u526f\u672c\u7684 GPU \u6570\u91cf\uff0c \u6216 GPU ID \u7684\u5217\u8868\u3002 cpu_merge : \u4e00\u4e2a\u5e03\u5c14\u503c\uff0c\u7528\u4e8e\u6807\u8bc6\u662f\u5426\u5f3a\u5236\u5408\u5e76 CPU \u8303\u56f4\u5185\u7684\u6a21\u578b\u6743\u91cd\u3002 cpu_relocation : \u4e00\u4e2a\u5e03\u5c14\u503c\uff0c\u7528\u6765\u786e\u5b9a\u662f\u5426\u5728 CPU \u7684\u8303\u56f4\u5185\u521b\u5efa\u6a21\u578b\u7684\u6743\u91cd\u3002\u5982\u679c\u6a21\u578b\u6ca1\u6709\u5728\u4efb\u4f55\u4e00\u4e2a\u8bbe\u5907\u8303\u56f4\u5185\u5b9a\u4e49\uff0c\u60a8\u4ecd\u7136\u53ef\u4ee5\u901a\u8fc7\u6fc0\u6d3b\u8fd9\u4e2a\u9009\u9879\u6765\u62ef\u6551\u5b83\u3002 \u8fd4\u56de \u4e00\u4e2a Keras Model \u5b9e\u4f8b\uff0c\u5b83\u53ef\u4ee5\u50cf\u521d\u59cb model \u53c2\u6570\u4e00\u6837\u4f7f\u7528\uff0c\u4f46\u5b83\u5c06\u5de5\u4f5c\u8d1f\u8f7d\u5206\u5e03\u5728\u591a\u4e2a GPU \u4e0a\u3002 \u4f8b\u5b50 \u4f8b 1 - \u8bad\u7ec3\u5728 CPU \u4e0a\u5408\u5e76\u6743\u91cd\u7684\u6a21\u578b import tensorflow as tf from keras.applications import Xception from keras.utils import multi_gpu_model import numpy as np num_samples = 1000 height = 224 width = 224 num_classes = 1000 # \u5b9e\u4f8b\u5316\u57fa\u7840\u6a21\u578b\uff08\u6216\u8005\u300c\u6a21\u7248\u300d\u6a21\u578b\uff09\u3002 # \u6211\u4eec\u63a8\u8350\u5728 CPU \u8bbe\u5907\u8303\u56f4\u5185\u505a\u6b64\u64cd\u4f5c\uff0c # \u8fd9\u6837\u6a21\u578b\u7684\u6743\u91cd\u5c31\u4f1a\u5b58\u50a8\u5728 CPU \u5185\u5b58\u4e2d\u3002 # \u5426\u5219\u5b83\u4eec\u4f1a\u5b58\u50a8\u5728 GPU \u4e0a\uff0c\u800c\u5b8c\u5168\u88ab\u5171\u4eab\u3002 with tf.device('/cpu:0'): model = Xception(weights=None, input_shape=(height, width, 3), classes=num_classes) # \u590d\u5236\u6a21\u578b\u5230 8 \u4e2a GPU \u4e0a\u3002 # \u8fd9\u5047\u8bbe\u4f60\u7684\u673a\u5668\u6709 8 \u4e2a\u53ef\u7528 GPU\u3002 parallel_model = multi_gpu_model(model, gpus=8) parallel_model.compile(loss='categorical_crossentropy', optimizer='rmsprop') # \u751f\u6210\u865a\u62df\u6570\u636e x = np.random.random((num_samples, height, width, 3)) y = np.random.random((num_samples, num_classes)) # \u8fd9\u4e2a `fit` \u8c03\u7528\u5c06\u5206\u5e03\u5728 8 \u4e2a GPU \u4e0a\u3002 # \u7531\u4e8e batch size \u662f 256, \u6bcf\u4e2a GPU \u5c06\u5904\u7406 32 \u4e2a\u6837\u672c\u3002 parallel_model.fit(x, y, epochs=20, batch_size=256) # \u901a\u8fc7\u6a21\u7248\u6a21\u578b\u5b58\u50a8\u6a21\u578b\uff08\u5171\u4eab\u76f8\u540c\u6743\u91cd\uff09\uff1a model.save('my_model.h5') \u4f8b 2 - \u8bad\u7ec3\u5728 CPU \u4e0a\u5229\u7528 cpu_relocation \u5408\u5e76\u6743\u91cd\u7684\u6a21\u578b .. # \u4e0d\u9700\u8981\u66f4\u6539\u6a21\u578b\u5b9a\u4e49\u7684\u8bbe\u5907\u8303\u56f4\uff1a model = Xception(weights=None, ..) try: parallel_model = multi_gpu_model(model, cpu_relocation=True) print(\"Training using multiple GPUs..\") except ValueError: parallel_model = model print(\"Training using single GPU or CPU..\") parallel_model.compile(..) .. \u4f8b 3 - \u8bad\u7ec3\u5728 GPU \u4e0a\u5408\u5e76\u6743\u91cd\u7684\u6a21\u578b\uff08\u5efa\u8bae\u7528\u4e8e NV-link\uff09 .. # \u4e0d\u9700\u8981\u66f4\u6539\u6a21\u578b\u5b9a\u4e49\u7684\u8bbe\u5907\u8303\u56f4\uff1a model = Xception(weights=None, ..) try: parallel_model = multi_gpu_model(model, cpu_merge=False) print(\"Training using multiple GPUs..\") except: parallel_model = model print(\"Training using single GPU or CPU..\") parallel_model.compile(..) .. \u5173\u4e8e\u6a21\u578b\u4fdd\u5b58 \u8981\u4fdd\u5b58\u591a GPU \u6a21\u578b\uff0c\u8bf7\u901a\u8fc7\u6a21\u677f\u6a21\u578b\uff08\u4f20\u9012\u7ed9 multi_gpu_model \u7684\u53c2\u6570\uff09\u8c03\u7528 .save(fname) \u6216 .save_weights(fname) \u4ee5\u8fdb\u884c\u5b58\u50a8\uff0c\u800c\u4e0d\u662f\u901a\u8fc7 multi_gpu_model \u8fd4\u56de\u7684\u6a21\u578b\u3002","title":"multi_gpu_model"},{"location":"97.contributing/","text":"\u5173\u4e8e Github Issues \u548c Pull Requests \u627e\u5230\u4e00\u4e2a\u6f0f\u6d1e\uff1f\u6709\u4e00\u4e2a\u65b0\u7684\u529f\u80fd\u5efa\u8bae\uff1f\u60f3\u8981\u5bf9\u4ee3\u7801\u5e93\u505a\u51fa\u8d21\u732e\uff1f\u8bf7\u52a1\u5fc5\u5148\u9605\u8bfb\u8fd9\u4e9b\u3002 \u6f0f\u6d1e\u62a5\u544a \u4f60\u7684\u4ee3\u7801\u4e0d\u8d77\u4f5c\u7528\uff0c\u4f60\u786e\u5b9a\u95ee\u9898\u5728\u4e8eKeras\uff1f\u8bf7\u6309\u7167\u4ee5\u4e0b\u6b65\u9aa4\u62a5\u544a\u9519\u8bef\u3002 \u4f60\u7684\u6f0f\u6d1e\u53ef\u80fd\u5df2\u7ecf\u88ab\u4fee\u590d\u4e86\u3002\u786e\u4fdd\u66f4\u65b0\u5230\u76ee\u524d\u7684Keras master\u5206\u652f\uff0c\u4ee5\u53ca\u6700\u65b0\u7684 Theano/TensorFlow/CNTK master \u5206\u652f\u3002 \u8f7b\u677e\u66f4\u65b0 Theano \u7684\u65b9\u6cd5\uff1a pip install git+git://github.com/Theano/Theano.git --upgrade \u641c\u7d22\u76f8\u4f3c\u95ee\u9898\u3002 \u786e\u4fdd\u5728\u641c\u7d22\u5df2\u7ecf\u89e3\u51b3\u7684 Issue \u65f6\u5220\u9664 is:open \u6807\u7b7e\u3002\u6709\u53ef\u80fd\u5df2\u7ecf\u6709\u4eba\u9047\u5230\u4e86\u8fd9\u4e2a\u6f0f\u6d1e\u3002\u540c\u65f6\u8bb0\u5f97\u68c0\u67e5 Keras FAQ \u3002\u4ecd\u7136\u6709\u95ee\u9898\uff1f\u5728 Github \u4e0a\u5f00\u4e00\u4e2a Issue\uff0c\u8ba9\u6211\u4eec\u77e5\u9053\u3002 \u786e\u4fdd\u4f60\u5411\u6211\u4eec\u63d0\u4f9b\u4e86\u6709\u5173\u4f60\u7684\u914d\u7f6e\u7684\u6709\u7528\u4fe1\u606f\uff1a\u4ec0\u4e48\u64cd\u4f5c\u7cfb\u7edf\uff1f\u4ec0\u4e48 Keras \u540e\u7aef\uff1f\u4f60\u662f\u5426\u5728 GPU \u4e0a\u8fd0\u884c\uff0cCuda \u548c cuDNN \u7684\u7248\u672c\u662f\u591a\u5c11\uff1fGPU\u578b\u53f7\u662f\u4ec0\u4e48\uff1f \u4e3a\u6211\u4eec\u63d0\u4f9b\u4e00\u4e2a\u811a\u672c\u6765\u91cd\u73b0\u8fd9\u4e2a\u95ee\u9898\u3002\u8be5\u811a\u672c\u5e94\u8be5\u53ef\u4ee5\u6309\u539f\u6837\u8fd0\u884c\uff0c\u5e76\u4e14\u4e0d\u5e94\u8be5\u8981\u6c42\u4e0b\u8f7d\u5916\u90e8\u6570\u636e\uff08\u5982\u679c\u9700\u8981\u5728\u67d0\u4e9b\u6d4b\u8bd5\u6570\u636e\u4e0a\u8fd0\u884c\u6a21\u578b\uff0c\u8bf7\u4f7f\u7528\u968f\u673a\u751f\u6210\u7684\u6570\u636e\uff09\u3002\u6211\u4eec\u5efa\u8bae\u4f60\u4f7f\u7528 Github Gists \u6765\u5f20\u8d34\u4f60\u7684\u4ee3\u7801\u3002\u4efb\u4f55\u65e0\u6cd5\u91cd\u73b0\u7684\u95ee\u9898\u90fd\u4f1a\u88ab\u5173\u95ed\u3002 \u5982\u679c\u53ef\u80fd\u7684\u8bdd\uff0c\u81ea\u5df1\u52a8\u624b\u4fee\u590d\u8fd9\u4e2a\u6f0f\u6d1e - \u5982\u679c\u53ef\u4ee5\u7684\u8bdd\uff01 \u4f60\u63d0\u4f9b\u7684\u4fe1\u606f\u8d8a\u591a\uff0c\u6211\u4eec\u5c31\u8d8a\u5bb9\u6613\u9a8c\u8bc1\u5b58\u5728\u9519\u8bef\uff0c\u5e76\u4e14\u6211\u4eec\u53ef\u4ee5\u91c7\u53d6\u66f4\u5feb\u7684\u884c\u52a8\u3002\u5982\u679c\u4f60\u60f3\u5feb\u901f\u89e3\u51b3\u4f60\u7684\u95ee\u9898\uff0c\u5c0a\u8bb8\u4e0a\u8ff0\u6b65\u9aa4\u64cd\u4f5c\u662f\u81f3\u5173\u91cd\u8981\u7684\u3002 \u8bf7\u6c42\u65b0\u529f\u80fd \u4f60\u4e5f\u53ef\u4ee5\u4f7f\u7528 Github Issue \u6765\u8bf7\u6c42\u4f60\u5e0c\u671b\u5728 Keras \u4e2d\u770b\u5230\u7684\u529f\u80fd\uff0c\u6216\u8005\u5728 Keras API \u4e2d\u7684\u66f4\u6539\u3002 \u63d0\u4f9b\u4f60\u60f3\u8981\u7684\u529f\u80fd\u7684\u6e05\u6670\u548c\u8be6\u7ec6\u7684\u89e3\u91ca\uff0c\u4ee5\u53ca\u4e3a\u4ec0\u4e48\u6dfb\u52a0\u5b83\u5f88\u91cd\u8981\u3002\u8bf7\u8bb0\u4f4f\uff0c\u6211\u4eec\u9700\u8981\u7684\u529f\u80fd\u662f\u5bf9\u4e8e\u5927\u591a\u6570\u7528\u6237\u800c\u8a00\u7684\uff0c\u4e0d\u4ec5\u4ec5\u662f\u4e00\u5c0f\u90e8\u5206\u4eba\u3002\u5982\u679c\u4f60\u53ea\u662f\u9488\u5bf9\u5c11\u6570\u7528\u6237\uff0c\u8bf7\u8003\u8651\u4e3a Keras \u7f16\u5199\u9644\u52a0\u5e93\u3002\u5bf9 Keras \u6765\u8bf4\uff0c\u907f\u514d\u81c3\u80bf\u7684 API \u548c\u4ee3\u7801\u5e93\u662f\u81f3\u5173\u91cd\u8981\u7684\u3002 \u63d0\u4f9b\u4ee3\u7801\u7247\u6bb5\uff0c\u6f14\u793a\u60a8\u6240\u9700\u7684 API \u5e76\u8bf4\u660e\u60a8\u7684\u529f\u80fd\u7684\u7528\u4f8b\u3002 \u5f53\u7136\uff0c\u5728\u8fd9\u4e00\u70b9\u4e0a\u4f60\u4e0d\u9700\u8981\u5199\u4efb\u4f55\u771f\u6b63\u7684\u4ee3\u7801\uff01 \u8ba8\u8bba\u5b8c\u8be5\u529f\u80fd\u540e\uff0c\u60a8\u53ef\u4ee5\u9009\u62e9\u5c1d\u8bd5\u63d0\u4e00\u4e2a Pull Request\u3002\u5982\u679c\u4f60\u5b8c\u5168\u53ef\u4ee5\uff0c\u5f00\u59cb\u5199\u4e00\u4e9b\u4ee3\u7801\u3002\u76f8\u6bd4\u65f6\u95f4\u4e0a\uff0c\u6211\u4eec\u603b\u662f\u6709\u66f4\u591a\u7684\u5de5\u4f5c\u8981\u505a\u3002\u5982\u679c\u4f60\u53ef\u4ee5\u5199\u4e00\u4e9b\u4ee3\u7801\uff0c\u90a3\u4e48\u8fd9\u5c06\u52a0\u901f\u8fd9\u4e2a\u8fc7\u7a0b\u3002 \u8bf7\u6c42\u8d21\u732e\u4ee3\u7801 \u5728 \u8fd9\u4e2a\u677f\u5757 \u6211\u4eec\u4f1a\u5217\u51fa\u5f53\u524d\u9700\u8981\u6dfb\u52a0\u7684\u51fa\u8272\u7684\u95ee\u9898\u548c\u65b0\u529f\u80fd\u3002\u5982\u679c\u4f60\u60f3\u8981\u4e3a Keras \u505a\u8d21\u732e\uff0c\u8fd9\u5c31\u662f\u53ef\u4ee5\u5f00\u59cb\u7684\u5730\u65b9\u3002 Pull Requests \u5408\u5e76\u8bf7\u6c42 \u6211\u5e94\u8be5\u5728\u54ea\u91cc\u63d0\u4ea4\u6211\u7684\u5408\u5e76\u8bf7\u6c42\uff1f Keras \u6539\u8fdb\u4e0e\u6f0f\u6d1e\u4fee\u590d \uff0c \u8bf7\u5230 Keras master \u5206\u652f \u3002 \u6d4b\u8bd5\u65b0\u529f\u80fd , \u4f8b\u5982\u7f51\u7edc\u5c42\u548c\u6570\u636e\u96c6\uff0c\u8bf7\u5230 keras-contrib \u3002\u9664\u975e\u5b83\u662f\u4e00\u4e2a\u5728 Requests for Contributions \u4e2d\u5217\u51fa\u7684\u65b0\u529f\u80fd\uff0c\u5b83\u5c5e\u4e8e Keras \u7684\u6838\u5fc3\u90e8\u5206\u3002\u5982\u679c\u4f60\u89c9\u5f97\u4f60\u7684\u529f\u80fd\u5c5e\u4e8e Keras \u6838\u5fc3\uff0c\u4f60\u53ef\u4ee5\u63d0\u4ea4\u4e00\u4e2a\u8bbe\u8ba1\u6587\u6863\uff0c\u6765\u89e3\u91ca\u4f60\u7684\u529f\u80fd\uff0c\u5e76\u4e89\u53d6\u5b83\uff08\u8bf7\u770b\u4ee5\u4e0b\u89e3\u91ca\uff09\u3002 \u8bf7\u6ce8\u610f\u4efb\u4f55\u6709\u5173 \u4ee3\u7801\u98ce\u683c \uff08\u800c\u4e0d\u662f\u4fee\u590d\u4fee\u590d\uff0c\u6539\u8fdb\u6587\u6863\u6216\u6dfb\u52a0\u65b0\u529f\u80fd\uff09\u7684 PR \u90fd\u4f1a\u88ab\u62d2\u7edd\u3002 \u4ee5\u4e0b\u662f\u63d0\u4ea4\u4f60\u7684\u6539\u8fdb\u7684\u5feb\u901f\u6307\u5357\uff1a \u5982\u679c\u4f60\u7684 PR \u4ecb\u7ecd\u4e86\u529f\u80fd\u7684\u6539\u53d8\uff0c\u786e\u4fdd\u4f60\u4ece\u64b0\u5199\u8bbe\u8ba1\u6587\u6863\u5e76\u5c06\u5176\u53d1\u7ed9 Keras \u90ae\u4ef6\u5217\u8868\u5f00\u59cb\uff0c\u4ee5\u8ba8\u8bba\u662f\u5426\u5e94\u8be5\u4fee\u6539\uff0c\u4ee5\u53ca\u5982\u4f55\u5904\u7406\u3002\u8fd9\u5c06\u62ef\u6551\u4f60\u4e8e PR \u5173\u95ed\u3002\u5f53\u7136\uff0c\u5982\u679c\u4f60\u7684 PR \u53ea\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u6f0f\u6d1e\u4fee\u590d\uff0c\u90a3\u5c31\u4e0d\u9700\u8981\u8fd9\u6837\u505a\u3002\u64b0\u5199\u4e0e\u63d0\u4ea4\u8bbe\u8ba1\u6587\u6863\u7684\u8fc7\u7a0b\u5982\u4e0b\u6240\u793a\uff1a \u4ece\u8fd9\u4e2a Google \u6587\u6863\u6a21\u7248 \u5f00\u59cb\uff0c\u5c06\u5b83\u590d\u5236\u4e3a\u4e00\u4e2a\u65b0\u7684 Google \u6587\u6863\u3002 \u586b\u5199\u5185\u5bb9\u3002\u6ce8\u610f\u4f60\u9700\u8981\u63d2\u5165\u4ee3\u7801\u6837\u4f8b\u3002\u8981\u63d2\u5165\u4ee3\u7801\uff0c\u8bf7\u4f7f\u7528 Google \u6587\u6863\u63d2\u4ef6\uff0c\u4f8b\u5982 [CodePretty] (https://chrome.google.com/webstore/detail/code-pretty/igjbncgfgnfpbnifnnlcmjfbnidkndnh?hl=en) (\u6709\u8bb8\u591a\u53ef\u7528\u7684\u63d2\u4ef6)\u3002 \u5c06\u5171\u4eab\u8bbe\u7f6e\u4e3a \u300c\u6bcf\u4e2a\u6709\u94fe\u63a5\u7684\u4eba\u90fd\u53ef\u4ee5\u53d1\u8868\u8bc4\u8bba\u300d\u3002 \u5c06\u6587\u6863\u53d1\u7ed9 keras-users@googlegroups.com \uff0c\u4e3b\u9898\u4ece [API DESIGN REVIEW] (\u5168\u5927\u5199) \u5f00\u59cb\uff0c\u8fd9\u6837\u6211\u4eec\u624d\u4f1a\u6ce8\u610f\u5230\u5b83\u3002 \u7b49\u5f85\u8bc4\u8bba\uff0c\u56de\u590d\u8bc4\u8bba\u3002\u5fc5\u8981\u65f6\u4fee\u6539\u63d0\u6848\u3002 - \u8be5\u63d0\u6848\u6700\u7ec8\u5c06\u88ab\u6279\u51c6\u6216\u62d2\u7edd\u3002\u4e00\u65e6\u83b7\u5f97\u6279\u51c6\uff0c\u60a8\u53ef\u4ee5\u53d1\u51fa\u5408\u5e76\u8bf7\u6c42\u6216\u8981\u6c42\u4ed6\u4eba\u64b0\u5199\u5408\u5e76\u8bf7\u6c42\u3002 \u64b0\u5199\u4ee3\u7801\uff08\u6216\u8005\u8ba9\u522b\u4eba\u5199\uff09\u3002\u8fd9\u662f\u6700\u96be\u7684\u4e00\u90e8\u5206\u3002 \u786e\u4fdd\u4f60\u5f15\u5165\u7684\u4efb\u4f55\u65b0\u529f\u80fd\u6216\u7c7b\u90fd\u6709\u9002\u5f53\u7684\u6587\u6863\u3002\u786e\u4fdd\u4f60\u89e6\u6478\u7684\u4efb\u4f55\u4ee3\u7801\u4ecd\u5177\u6709\u6700\u65b0\u7684\u6587\u6863\u3002 \u5e94\u8be5\u4e25\u683c\u9075\u5faa Docstring \u98ce\u683c \u3002\u5c24\u5176\u662f\uff0c\u5b83\u4eec\u5e94\u8be5\u5728 MarkDown \u4e2d\u683c\u5f0f\u5316\uff0c\u5e76\u4e14\u5e94\u8be5\u6709 Arguments \uff0c Returns \uff0c Raises \u90e8\u5206\uff08\u5982\u679c\u9002\u7528\uff09\u3002\u67e5\u770b\u4ee3\u7801\u793a\u4f8b\u4e2d\u7684\u5176\u4ed6\u6587\u6863\u4ee5\u505a\u53c2\u8003\u3002 \u64b0\u5199\u6d4b\u8bd5\u3002\u4f60\u7684\u4ee3\u7801\u5e94\u8be5\u6709\u5b8c\u6574\u7684\u5355\u5143\u6d4b\u8bd5\u8986\u76d6\u3002\u5982\u679c\u4f60\u60f3\u770b\u5230\u4f60\u7684 PR \u8fc5\u901f\u5408\u5e76\uff0c\u8fd9\u662f\u81f3\u5173\u91cd\u8981\u7684\u3002 \u5728\u672c\u5730\u8fd0\u884c\u6d4b\u8bd5\u5957\u4ef6\u3002\u8fd9\u5f88\u7b80\u5355\uff1a\u5728 Keras \u76ee\u5f55\u4e0b\uff0c\u76f4\u63a5\u8fd0\u884c\uff1a py.test tests/ \u3002 \u60a8\u8fd8\u9700\u8981\u5b89\u88c5\u6d4b\u8bd5\u5305\uff1a pip install -e .[tests] \u3002 \u786e\u4fdd\u901a\u8fc7\u6240\u6709\u6d4b\u8bd5\uff1a \u4f7f\u7528 Theano \u540e\u7aef\uff0cPython 2.7 \u548c Python 3.5\u3002\u786e\u4fdd\u4f60\u6709 Theano \u7684\u5f00\u53d1\u7248\u672c\u3002 \u4f7f\u7528 TensorFlow \u540e\u7aef\uff0cPython 2.7 \u548c Python 3.5\u3002\u786e\u4fdd\u4f60\u6709 TensorFlow \u7684\u5f00\u53d1\u7248\u672c\u3002 \u4f7f\u7528 CNTK \u540e\u7aef\uff0c Python 2.7 \u548c Python 3.5\u3002\u786e\u4fdd\u4f60\u6709 CNTK \u7684\u5f00\u53d1\u7248\u672c\u3002 \u6211\u4eec\u4f7f\u7528 PEP8 \u8bed\u6cd5\u7ea6\u5b9a\uff0c\u4f46\u662f\u5f53\u6d89\u53ca\u5230\u884c\u957f\u65f6\uff0c\u6211\u4eec\u4e0d\u662f\u6559\u6761\u5f0f\u7684\u3002\u5c3d\u7ba1\u5982\u6b64\uff0c\u786e\u4fdd\u4f60\u7684\u884c\u4fdd\u6301\u5408\u7406\u7684\u5927\u5c0f\u3002\u4e3a\u4e86\u8ba9\u60a8\u7684\u751f\u6d3b\u66f4\u8f7b\u677e\uff0c\u6211\u4eec\u63a8\u8350\u4f7f\u7528 PEP8 linter\uff1a \u5b89\u88c5 PEP8 \u5305\uff1a pip install pep8 pytest-pep8 autopep8 \u8fd0\u884c\u72ec\u7acb\u7684 PEP8 \u68c0\u67e5\uff1a py.test --pep8 -m pep8 \u4f60\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u8fd9\u4e2a\u547d\u4ee4\u81ea\u52a8\u4fee\u590d\u4e00\u4e9b PEP8 \u9519\u8bef\uff1a autopep8 -i --select <errors> <FILENAME> \u3002 \u4f8b\u5982\uff1a autopep8 -i --select E128 tests/keras/backend/test_backends.py \u63d0\u4ea4\u65f6\uff0c\u8bf7\u4f7f\u7528\u9002\u5f53\u7684\u63cf\u8ff0\u6027\u63d0\u4ea4\u6d88\u606f\u3002 \u66f4\u65b0\u6587\u6863\u3002\u5982\u679c\u5f15\u5165\u65b0\u529f\u80fd\uff0c\u8bf7\u786e\u4fdd\u5305\u542b\u6f14\u793a\u65b0\u529f\u80fd\u7528\u6cd5\u7684\u4ee3\u7801\u7247\u6bb5\u3002 \u63d0\u4ea4\u4f60\u7684 PR\u3002\u5982\u679c\u4f60\u7684\u66f4\u6539\u5df2\u5728\u4e4b\u524d\u7684\u8ba8\u8bba\u4e2d\u83b7\u5f97\u6279\u51c6\uff0c\u5e76\u4e14\u4f60\u6709\u5b8c\u6574\uff08\u5e76\u901a\u8fc7\uff09\u7684\u5355\u5143\u6d4b\u8bd5\u4ee5\u53ca\u6b63\u786e\u7684 docstring/\u6587\u6863\uff0c\u5219\u4f60\u7684 PR \u53ef\u80fd\u4f1a\u7acb\u5373\u5408\u5e76\u3002 \u6dfb\u52a0\u65b0\u7684\u6837\u4f8b \u5373\u4f7f\u4f60\u4e0d\u8d21\u732e Keras \u6e90\u4ee3\u7801\uff0c\u5982\u679c\u4f60\u6709\u4e00\u4e2a\u7b80\u6d01\u800c\u5f3a\u5927\u7684 Keras \u5e94\u7528\uff0c\u8bf7\u8003\u8651\u5c06\u5b83\u6dfb\u52a0\u5230\u6211\u4eec\u7684\u6837\u4f8b\u96c6\u5408\u4e2d\u3002 \u73b0\u6709\u7684\u4f8b\u5b50 \u5c55\u793a\u60ef\u7528\u7684 Keras \u4ee3\u7801\uff1a\u786e\u4fdd\u4fdd\u6301\u81ea\u5df1\u7684\u811a\u672c\u5177\u6709\u76f8\u540c\u7684\u98ce\u683c\u3002","title":"\u5173\u4e8e Github Issues \u548c Pull Requests"},{"location":"97.contributing/#github-issues-pull-requests","text":"\u627e\u5230\u4e00\u4e2a\u6f0f\u6d1e\uff1f\u6709\u4e00\u4e2a\u65b0\u7684\u529f\u80fd\u5efa\u8bae\uff1f\u60f3\u8981\u5bf9\u4ee3\u7801\u5e93\u505a\u51fa\u8d21\u732e\uff1f\u8bf7\u52a1\u5fc5\u5148\u9605\u8bfb\u8fd9\u4e9b\u3002","title":"\u5173\u4e8e Github Issues \u548c Pull Requests"},{"location":"97.contributing/#_1","text":"\u4f60\u7684\u4ee3\u7801\u4e0d\u8d77\u4f5c\u7528\uff0c\u4f60\u786e\u5b9a\u95ee\u9898\u5728\u4e8eKeras\uff1f\u8bf7\u6309\u7167\u4ee5\u4e0b\u6b65\u9aa4\u62a5\u544a\u9519\u8bef\u3002 \u4f60\u7684\u6f0f\u6d1e\u53ef\u80fd\u5df2\u7ecf\u88ab\u4fee\u590d\u4e86\u3002\u786e\u4fdd\u66f4\u65b0\u5230\u76ee\u524d\u7684Keras master\u5206\u652f\uff0c\u4ee5\u53ca\u6700\u65b0\u7684 Theano/TensorFlow/CNTK master \u5206\u652f\u3002 \u8f7b\u677e\u66f4\u65b0 Theano \u7684\u65b9\u6cd5\uff1a pip install git+git://github.com/Theano/Theano.git --upgrade \u641c\u7d22\u76f8\u4f3c\u95ee\u9898\u3002 \u786e\u4fdd\u5728\u641c\u7d22\u5df2\u7ecf\u89e3\u51b3\u7684 Issue \u65f6\u5220\u9664 is:open \u6807\u7b7e\u3002\u6709\u53ef\u80fd\u5df2\u7ecf\u6709\u4eba\u9047\u5230\u4e86\u8fd9\u4e2a\u6f0f\u6d1e\u3002\u540c\u65f6\u8bb0\u5f97\u68c0\u67e5 Keras FAQ \u3002\u4ecd\u7136\u6709\u95ee\u9898\uff1f\u5728 Github \u4e0a\u5f00\u4e00\u4e2a Issue\uff0c\u8ba9\u6211\u4eec\u77e5\u9053\u3002 \u786e\u4fdd\u4f60\u5411\u6211\u4eec\u63d0\u4f9b\u4e86\u6709\u5173\u4f60\u7684\u914d\u7f6e\u7684\u6709\u7528\u4fe1\u606f\uff1a\u4ec0\u4e48\u64cd\u4f5c\u7cfb\u7edf\uff1f\u4ec0\u4e48 Keras \u540e\u7aef\uff1f\u4f60\u662f\u5426\u5728 GPU \u4e0a\u8fd0\u884c\uff0cCuda \u548c cuDNN \u7684\u7248\u672c\u662f\u591a\u5c11\uff1fGPU\u578b\u53f7\u662f\u4ec0\u4e48\uff1f \u4e3a\u6211\u4eec\u63d0\u4f9b\u4e00\u4e2a\u811a\u672c\u6765\u91cd\u73b0\u8fd9\u4e2a\u95ee\u9898\u3002\u8be5\u811a\u672c\u5e94\u8be5\u53ef\u4ee5\u6309\u539f\u6837\u8fd0\u884c\uff0c\u5e76\u4e14\u4e0d\u5e94\u8be5\u8981\u6c42\u4e0b\u8f7d\u5916\u90e8\u6570\u636e\uff08\u5982\u679c\u9700\u8981\u5728\u67d0\u4e9b\u6d4b\u8bd5\u6570\u636e\u4e0a\u8fd0\u884c\u6a21\u578b\uff0c\u8bf7\u4f7f\u7528\u968f\u673a\u751f\u6210\u7684\u6570\u636e\uff09\u3002\u6211\u4eec\u5efa\u8bae\u4f60\u4f7f\u7528 Github Gists \u6765\u5f20\u8d34\u4f60\u7684\u4ee3\u7801\u3002\u4efb\u4f55\u65e0\u6cd5\u91cd\u73b0\u7684\u95ee\u9898\u90fd\u4f1a\u88ab\u5173\u95ed\u3002 \u5982\u679c\u53ef\u80fd\u7684\u8bdd\uff0c\u81ea\u5df1\u52a8\u624b\u4fee\u590d\u8fd9\u4e2a\u6f0f\u6d1e - \u5982\u679c\u53ef\u4ee5\u7684\u8bdd\uff01 \u4f60\u63d0\u4f9b\u7684\u4fe1\u606f\u8d8a\u591a\uff0c\u6211\u4eec\u5c31\u8d8a\u5bb9\u6613\u9a8c\u8bc1\u5b58\u5728\u9519\u8bef\uff0c\u5e76\u4e14\u6211\u4eec\u53ef\u4ee5\u91c7\u53d6\u66f4\u5feb\u7684\u884c\u52a8\u3002\u5982\u679c\u4f60\u60f3\u5feb\u901f\u89e3\u51b3\u4f60\u7684\u95ee\u9898\uff0c\u5c0a\u8bb8\u4e0a\u8ff0\u6b65\u9aa4\u64cd\u4f5c\u662f\u81f3\u5173\u91cd\u8981\u7684\u3002","title":"\u6f0f\u6d1e\u62a5\u544a"},{"location":"97.contributing/#_2","text":"\u4f60\u4e5f\u53ef\u4ee5\u4f7f\u7528 Github Issue \u6765\u8bf7\u6c42\u4f60\u5e0c\u671b\u5728 Keras \u4e2d\u770b\u5230\u7684\u529f\u80fd\uff0c\u6216\u8005\u5728 Keras API \u4e2d\u7684\u66f4\u6539\u3002 \u63d0\u4f9b\u4f60\u60f3\u8981\u7684\u529f\u80fd\u7684\u6e05\u6670\u548c\u8be6\u7ec6\u7684\u89e3\u91ca\uff0c\u4ee5\u53ca\u4e3a\u4ec0\u4e48\u6dfb\u52a0\u5b83\u5f88\u91cd\u8981\u3002\u8bf7\u8bb0\u4f4f\uff0c\u6211\u4eec\u9700\u8981\u7684\u529f\u80fd\u662f\u5bf9\u4e8e\u5927\u591a\u6570\u7528\u6237\u800c\u8a00\u7684\uff0c\u4e0d\u4ec5\u4ec5\u662f\u4e00\u5c0f\u90e8\u5206\u4eba\u3002\u5982\u679c\u4f60\u53ea\u662f\u9488\u5bf9\u5c11\u6570\u7528\u6237\uff0c\u8bf7\u8003\u8651\u4e3a Keras \u7f16\u5199\u9644\u52a0\u5e93\u3002\u5bf9 Keras \u6765\u8bf4\uff0c\u907f\u514d\u81c3\u80bf\u7684 API \u548c\u4ee3\u7801\u5e93\u662f\u81f3\u5173\u91cd\u8981\u7684\u3002 \u63d0\u4f9b\u4ee3\u7801\u7247\u6bb5\uff0c\u6f14\u793a\u60a8\u6240\u9700\u7684 API \u5e76\u8bf4\u660e\u60a8\u7684\u529f\u80fd\u7684\u7528\u4f8b\u3002 \u5f53\u7136\uff0c\u5728\u8fd9\u4e00\u70b9\u4e0a\u4f60\u4e0d\u9700\u8981\u5199\u4efb\u4f55\u771f\u6b63\u7684\u4ee3\u7801\uff01 \u8ba8\u8bba\u5b8c\u8be5\u529f\u80fd\u540e\uff0c\u60a8\u53ef\u4ee5\u9009\u62e9\u5c1d\u8bd5\u63d0\u4e00\u4e2a Pull Request\u3002\u5982\u679c\u4f60\u5b8c\u5168\u53ef\u4ee5\uff0c\u5f00\u59cb\u5199\u4e00\u4e9b\u4ee3\u7801\u3002\u76f8\u6bd4\u65f6\u95f4\u4e0a\uff0c\u6211\u4eec\u603b\u662f\u6709\u66f4\u591a\u7684\u5de5\u4f5c\u8981\u505a\u3002\u5982\u679c\u4f60\u53ef\u4ee5\u5199\u4e00\u4e9b\u4ee3\u7801\uff0c\u90a3\u4e48\u8fd9\u5c06\u52a0\u901f\u8fd9\u4e2a\u8fc7\u7a0b\u3002","title":"\u8bf7\u6c42\u65b0\u529f\u80fd"},{"location":"97.contributing/#_3","text":"\u5728 \u8fd9\u4e2a\u677f\u5757 \u6211\u4eec\u4f1a\u5217\u51fa\u5f53\u524d\u9700\u8981\u6dfb\u52a0\u7684\u51fa\u8272\u7684\u95ee\u9898\u548c\u65b0\u529f\u80fd\u3002\u5982\u679c\u4f60\u60f3\u8981\u4e3a Keras \u505a\u8d21\u732e\uff0c\u8fd9\u5c31\u662f\u53ef\u4ee5\u5f00\u59cb\u7684\u5730\u65b9\u3002","title":"\u8bf7\u6c42\u8d21\u732e\u4ee3\u7801"},{"location":"97.contributing/#pull-requests","text":"\u6211\u5e94\u8be5\u5728\u54ea\u91cc\u63d0\u4ea4\u6211\u7684\u5408\u5e76\u8bf7\u6c42\uff1f Keras \u6539\u8fdb\u4e0e\u6f0f\u6d1e\u4fee\u590d \uff0c \u8bf7\u5230 Keras master \u5206\u652f \u3002 \u6d4b\u8bd5\u65b0\u529f\u80fd , \u4f8b\u5982\u7f51\u7edc\u5c42\u548c\u6570\u636e\u96c6\uff0c\u8bf7\u5230 keras-contrib \u3002\u9664\u975e\u5b83\u662f\u4e00\u4e2a\u5728 Requests for Contributions \u4e2d\u5217\u51fa\u7684\u65b0\u529f\u80fd\uff0c\u5b83\u5c5e\u4e8e Keras \u7684\u6838\u5fc3\u90e8\u5206\u3002\u5982\u679c\u4f60\u89c9\u5f97\u4f60\u7684\u529f\u80fd\u5c5e\u4e8e Keras \u6838\u5fc3\uff0c\u4f60\u53ef\u4ee5\u63d0\u4ea4\u4e00\u4e2a\u8bbe\u8ba1\u6587\u6863\uff0c\u6765\u89e3\u91ca\u4f60\u7684\u529f\u80fd\uff0c\u5e76\u4e89\u53d6\u5b83\uff08\u8bf7\u770b\u4ee5\u4e0b\u89e3\u91ca\uff09\u3002 \u8bf7\u6ce8\u610f\u4efb\u4f55\u6709\u5173 \u4ee3\u7801\u98ce\u683c \uff08\u800c\u4e0d\u662f\u4fee\u590d\u4fee\u590d\uff0c\u6539\u8fdb\u6587\u6863\u6216\u6dfb\u52a0\u65b0\u529f\u80fd\uff09\u7684 PR \u90fd\u4f1a\u88ab\u62d2\u7edd\u3002 \u4ee5\u4e0b\u662f\u63d0\u4ea4\u4f60\u7684\u6539\u8fdb\u7684\u5feb\u901f\u6307\u5357\uff1a \u5982\u679c\u4f60\u7684 PR \u4ecb\u7ecd\u4e86\u529f\u80fd\u7684\u6539\u53d8\uff0c\u786e\u4fdd\u4f60\u4ece\u64b0\u5199\u8bbe\u8ba1\u6587\u6863\u5e76\u5c06\u5176\u53d1\u7ed9 Keras \u90ae\u4ef6\u5217\u8868\u5f00\u59cb\uff0c\u4ee5\u8ba8\u8bba\u662f\u5426\u5e94\u8be5\u4fee\u6539\uff0c\u4ee5\u53ca\u5982\u4f55\u5904\u7406\u3002\u8fd9\u5c06\u62ef\u6551\u4f60\u4e8e PR \u5173\u95ed\u3002\u5f53\u7136\uff0c\u5982\u679c\u4f60\u7684 PR \u53ea\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u6f0f\u6d1e\u4fee\u590d\uff0c\u90a3\u5c31\u4e0d\u9700\u8981\u8fd9\u6837\u505a\u3002\u64b0\u5199\u4e0e\u63d0\u4ea4\u8bbe\u8ba1\u6587\u6863\u7684\u8fc7\u7a0b\u5982\u4e0b\u6240\u793a\uff1a \u4ece\u8fd9\u4e2a Google \u6587\u6863\u6a21\u7248 \u5f00\u59cb\uff0c\u5c06\u5b83\u590d\u5236\u4e3a\u4e00\u4e2a\u65b0\u7684 Google \u6587\u6863\u3002 \u586b\u5199\u5185\u5bb9\u3002\u6ce8\u610f\u4f60\u9700\u8981\u63d2\u5165\u4ee3\u7801\u6837\u4f8b\u3002\u8981\u63d2\u5165\u4ee3\u7801\uff0c\u8bf7\u4f7f\u7528 Google \u6587\u6863\u63d2\u4ef6\uff0c\u4f8b\u5982 [CodePretty] (https://chrome.google.com/webstore/detail/code-pretty/igjbncgfgnfpbnifnnlcmjfbnidkndnh?hl=en) (\u6709\u8bb8\u591a\u53ef\u7528\u7684\u63d2\u4ef6)\u3002 \u5c06\u5171\u4eab\u8bbe\u7f6e\u4e3a \u300c\u6bcf\u4e2a\u6709\u94fe\u63a5\u7684\u4eba\u90fd\u53ef\u4ee5\u53d1\u8868\u8bc4\u8bba\u300d\u3002 \u5c06\u6587\u6863\u53d1\u7ed9 keras-users@googlegroups.com \uff0c\u4e3b\u9898\u4ece [API DESIGN REVIEW] (\u5168\u5927\u5199) \u5f00\u59cb\uff0c\u8fd9\u6837\u6211\u4eec\u624d\u4f1a\u6ce8\u610f\u5230\u5b83\u3002 \u7b49\u5f85\u8bc4\u8bba\uff0c\u56de\u590d\u8bc4\u8bba\u3002\u5fc5\u8981\u65f6\u4fee\u6539\u63d0\u6848\u3002 - \u8be5\u63d0\u6848\u6700\u7ec8\u5c06\u88ab\u6279\u51c6\u6216\u62d2\u7edd\u3002\u4e00\u65e6\u83b7\u5f97\u6279\u51c6\uff0c\u60a8\u53ef\u4ee5\u53d1\u51fa\u5408\u5e76\u8bf7\u6c42\u6216\u8981\u6c42\u4ed6\u4eba\u64b0\u5199\u5408\u5e76\u8bf7\u6c42\u3002 \u64b0\u5199\u4ee3\u7801\uff08\u6216\u8005\u8ba9\u522b\u4eba\u5199\uff09\u3002\u8fd9\u662f\u6700\u96be\u7684\u4e00\u90e8\u5206\u3002 \u786e\u4fdd\u4f60\u5f15\u5165\u7684\u4efb\u4f55\u65b0\u529f\u80fd\u6216\u7c7b\u90fd\u6709\u9002\u5f53\u7684\u6587\u6863\u3002\u786e\u4fdd\u4f60\u89e6\u6478\u7684\u4efb\u4f55\u4ee3\u7801\u4ecd\u5177\u6709\u6700\u65b0\u7684\u6587\u6863\u3002 \u5e94\u8be5\u4e25\u683c\u9075\u5faa Docstring \u98ce\u683c \u3002\u5c24\u5176\u662f\uff0c\u5b83\u4eec\u5e94\u8be5\u5728 MarkDown \u4e2d\u683c\u5f0f\u5316\uff0c\u5e76\u4e14\u5e94\u8be5\u6709 Arguments \uff0c Returns \uff0c Raises \u90e8\u5206\uff08\u5982\u679c\u9002\u7528\uff09\u3002\u67e5\u770b\u4ee3\u7801\u793a\u4f8b\u4e2d\u7684\u5176\u4ed6\u6587\u6863\u4ee5\u505a\u53c2\u8003\u3002 \u64b0\u5199\u6d4b\u8bd5\u3002\u4f60\u7684\u4ee3\u7801\u5e94\u8be5\u6709\u5b8c\u6574\u7684\u5355\u5143\u6d4b\u8bd5\u8986\u76d6\u3002\u5982\u679c\u4f60\u60f3\u770b\u5230\u4f60\u7684 PR \u8fc5\u901f\u5408\u5e76\uff0c\u8fd9\u662f\u81f3\u5173\u91cd\u8981\u7684\u3002 \u5728\u672c\u5730\u8fd0\u884c\u6d4b\u8bd5\u5957\u4ef6\u3002\u8fd9\u5f88\u7b80\u5355\uff1a\u5728 Keras \u76ee\u5f55\u4e0b\uff0c\u76f4\u63a5\u8fd0\u884c\uff1a py.test tests/ \u3002 \u60a8\u8fd8\u9700\u8981\u5b89\u88c5\u6d4b\u8bd5\u5305\uff1a pip install -e .[tests] \u3002 \u786e\u4fdd\u901a\u8fc7\u6240\u6709\u6d4b\u8bd5\uff1a \u4f7f\u7528 Theano \u540e\u7aef\uff0cPython 2.7 \u548c Python 3.5\u3002\u786e\u4fdd\u4f60\u6709 Theano \u7684\u5f00\u53d1\u7248\u672c\u3002 \u4f7f\u7528 TensorFlow \u540e\u7aef\uff0cPython 2.7 \u548c Python 3.5\u3002\u786e\u4fdd\u4f60\u6709 TensorFlow \u7684\u5f00\u53d1\u7248\u672c\u3002 \u4f7f\u7528 CNTK \u540e\u7aef\uff0c Python 2.7 \u548c Python 3.5\u3002\u786e\u4fdd\u4f60\u6709 CNTK \u7684\u5f00\u53d1\u7248\u672c\u3002 \u6211\u4eec\u4f7f\u7528 PEP8 \u8bed\u6cd5\u7ea6\u5b9a\uff0c\u4f46\u662f\u5f53\u6d89\u53ca\u5230\u884c\u957f\u65f6\uff0c\u6211\u4eec\u4e0d\u662f\u6559\u6761\u5f0f\u7684\u3002\u5c3d\u7ba1\u5982\u6b64\uff0c\u786e\u4fdd\u4f60\u7684\u884c\u4fdd\u6301\u5408\u7406\u7684\u5927\u5c0f\u3002\u4e3a\u4e86\u8ba9\u60a8\u7684\u751f\u6d3b\u66f4\u8f7b\u677e\uff0c\u6211\u4eec\u63a8\u8350\u4f7f\u7528 PEP8 linter\uff1a \u5b89\u88c5 PEP8 \u5305\uff1a pip install pep8 pytest-pep8 autopep8 \u8fd0\u884c\u72ec\u7acb\u7684 PEP8 \u68c0\u67e5\uff1a py.test --pep8 -m pep8 \u4f60\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u8fd9\u4e2a\u547d\u4ee4\u81ea\u52a8\u4fee\u590d\u4e00\u4e9b PEP8 \u9519\u8bef\uff1a autopep8 -i --select <errors> <FILENAME> \u3002 \u4f8b\u5982\uff1a autopep8 -i --select E128 tests/keras/backend/test_backends.py \u63d0\u4ea4\u65f6\uff0c\u8bf7\u4f7f\u7528\u9002\u5f53\u7684\u63cf\u8ff0\u6027\u63d0\u4ea4\u6d88\u606f\u3002 \u66f4\u65b0\u6587\u6863\u3002\u5982\u679c\u5f15\u5165\u65b0\u529f\u80fd\uff0c\u8bf7\u786e\u4fdd\u5305\u542b\u6f14\u793a\u65b0\u529f\u80fd\u7528\u6cd5\u7684\u4ee3\u7801\u7247\u6bb5\u3002 \u63d0\u4ea4\u4f60\u7684 PR\u3002\u5982\u679c\u4f60\u7684\u66f4\u6539\u5df2\u5728\u4e4b\u524d\u7684\u8ba8\u8bba\u4e2d\u83b7\u5f97\u6279\u51c6\uff0c\u5e76\u4e14\u4f60\u6709\u5b8c\u6574\uff08\u5e76\u901a\u8fc7\uff09\u7684\u5355\u5143\u6d4b\u8bd5\u4ee5\u53ca\u6b63\u786e\u7684 docstring/\u6587\u6863\uff0c\u5219\u4f60\u7684 PR \u53ef\u80fd\u4f1a\u7acb\u5373\u5408\u5e76\u3002","title":"Pull Requests \u5408\u5e76\u8bf7\u6c42"},{"location":"97.contributing/#_4","text":"\u5373\u4f7f\u4f60\u4e0d\u8d21\u732e Keras \u6e90\u4ee3\u7801\uff0c\u5982\u679c\u4f60\u6709\u4e00\u4e2a\u7b80\u6d01\u800c\u5f3a\u5927\u7684 Keras \u5e94\u7528\uff0c\u8bf7\u8003\u8651\u5c06\u5b83\u6dfb\u52a0\u5230\u6211\u4eec\u7684\u6837\u4f8b\u96c6\u5408\u4e2d\u3002 \u73b0\u6709\u7684\u4f8b\u5b50 \u5c55\u793a\u60ef\u7528\u7684 Keras \u4ee3\u7801\uff1a\u786e\u4fdd\u4fdd\u6301\u81ea\u5df1\u7684\u811a\u672c\u5177\u6709\u76f8\u540c\u7684\u98ce\u683c\u3002","title":"\u6dfb\u52a0\u65b0\u7684\u6837\u4f8b"},{"location":"0-Getting-Started/0.sequential-model-guide/","text":"\u5f00\u59cb\u4f7f\u7528 Keras Sequential \u987a\u5e8f\u6a21\u578b \u987a\u5e8f\u6a21\u578b\u662f\u591a\u4e2a\u7f51\u7edc\u5c42\u7684\u7ebf\u6027\u5806\u53e0\u3002 \u4f60\u53ef\u4ee5\u901a\u8fc7\u5c06\u7f51\u7edc\u5c42\u5b9e\u4f8b\u7684\u5217\u8868\u4f20\u9012\u7ed9 Sequential \u7684\u6784\u9020\u5668\uff0c\u6765\u521b\u5efa\u4e00\u4e2a Sequential \u6a21\u578b\uff1a from keras.models import Sequential from keras.layers import Dense, Activation model = Sequential([ Dense(32, input_shape=(784,)), Activation('relu'), Dense(10), Activation('softmax'), ]) \u4e5f\u53ef\u4ee5\u7b80\u5355\u5730\u4f7f\u7528 .add() \u65b9\u6cd5\u5c06\u5404\u5c42\u6dfb\u52a0\u5230\u6a21\u578b\u4e2d\uff1a model = Sequential() model.add(Dense(32, input_dim=784)) model.add(Activation('relu')) \u6307\u5b9a\u8f93\u5165\u6570\u636e\u7684\u5c3a\u5bf8 \u6a21\u578b\u9700\u8981\u77e5\u9053\u5b83\u6240\u671f\u671b\u7684\u8f93\u5165\u7684\u5c3a\u5bf8\u3002\u51fa\u4e8e\u8fd9\u4e2a\u539f\u56e0\uff0c\u987a\u5e8f\u6a21\u578b\u4e2d\u7684\u7b2c\u4e00\u5c42\uff08\u4e14\u53ea\u6709\u7b2c\u4e00\u5c42\uff0c\u56e0\u4e3a\u4e0b\u9762\u7684\u5c42\u53ef\u4ee5\u81ea\u52a8\u5730\u63a8\u65ad\u5c3a\u5bf8\uff09\u9700\u8981\u63a5\u6536\u5173\u4e8e\u5176\u8f93\u5165\u5c3a\u5bf8\u7684\u4fe1\u606f\u3002\u6709\u51e0\u79cd\u65b9\u6cd5\u6765\u505a\u5230\u8fd9\u4e00\u70b9\uff1a \u4f20\u9012\u4e00\u4e2a input_shape \u53c2\u6570\u7ed9\u7b2c\u4e00\u5c42\u3002\u5b83\u662f\u4e00\u4e2a\u8868\u793a\u5c3a\u5bf8\u7684\u5143\u7ec4 (\u4e00\u4e2a\u6574\u6570\u6216 None \u7684\u5143\u7ec4\uff0c\u5176\u4e2d None \u8868\u793a\u53ef\u80fd\u4e3a\u4efb\u4f55\u6b63\u6574\u6570)\u3002\u5728 input_shape \u4e2d\u4e0d\u5305\u542b\u6570\u636e\u7684 batch \u5927\u5c0f\u3002 \u67d0\u4e9b 2D \u5c42\uff0c\u4f8b\u5982 Dense \uff0c\u652f\u6301\u901a\u8fc7\u53c2\u6570 input_dim \u6307\u5b9a\u8f93\u5165\u5c3a\u5bf8\uff0c\u67d0\u4e9b 3D \u65f6\u5e8f\u5c42\u652f\u6301 input_dim \u548c input_length \u53c2\u6570\u3002 \u5982\u679c\u4f60\u9700\u8981\u4e3a\u4f60\u7684\u8f93\u5165\u6307\u5b9a\u4e00\u4e2a\u56fa\u5b9a\u7684 batch \u5927\u5c0f\uff08\u8fd9\u5bf9 stateful RNNs \u5f88\u6709\u7528\uff09\uff0c\u4f60\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a batch_size \u53c2\u6570\u7ed9\u4e00\u4e2a\u5c42\u3002\u5982\u679c\u4f60\u540c\u65f6\u5c06 batch_size=32 \u548c input_shape=(6, 8) \u4f20\u9012\u7ed9\u4e00\u4e2a\u5c42\uff0c\u90a3\u4e48\u6bcf\u4e00\u6279\u8f93\u5165\u7684\u5c3a\u5bf8\u5c31\u4e3a (32\uff0c6\uff0c8) \u3002 \u56e0\u6b64\uff0c\u4e0b\u9762\u7684\u4ee3\u7801\u7247\u6bb5\u662f\u7b49\u4ef7\u7684\uff1a model = Sequential() model.add(Dense(32, input_shape=(784,))) model = Sequential() model.add(Dense(32, input_dim=784)) \u6a21\u578b\u7f16\u8bd1 \u5728\u8bad\u7ec3\u6a21\u578b\u4e4b\u524d\uff0c\u60a8\u9700\u8981\u914d\u7f6e\u5b66\u4e60\u8fc7\u7a0b\uff0c\u8fd9\u662f\u901a\u8fc7 compile \u65b9\u6cd5\u5b8c\u6210\u7684\u3002\u5b83\u63a5\u6536\u4e09\u4e2a\u53c2\u6570\uff1a \u4f18\u5316\u5668 optimizer\u3002\u5b83\u53ef\u4ee5\u662f\u73b0\u6709\u4f18\u5316\u5668\u7684\u5b57\u7b26\u4e32\u6807\u8bc6\u7b26\uff0c\u5982 rmsprop \u6216 adagrad \uff0c\u4e5f\u53ef\u4ee5\u662f Optimizer \u7c7b\u7684\u5b9e\u4f8b\u3002\u8be6\u89c1\uff1a optimizers \u3002 \u635f\u5931\u51fd\u6570 loss\uff0c\u6a21\u578b\u8bd5\u56fe\u6700\u5c0f\u5316\u7684\u76ee\u6807\u51fd\u6570\u3002\u5b83\u53ef\u4ee5\u662f\u73b0\u6709\u635f\u5931\u51fd\u6570\u7684\u5b57\u7b26\u4e32\u6807\u8bc6\u7b26\uff0c\u5982 categorical_crossentropy \u6216 mse \uff0c\u4e5f\u53ef\u4ee5\u662f\u4e00\u4e2a\u76ee\u6807\u51fd\u6570\u3002\u8be6\u89c1\uff1a losses \u3002 \u8bc4\u4f30\u6807\u51c6 metrics\u3002\u5bf9\u4e8e\u4efb\u4f55\u5206\u7c7b\u95ee\u9898\uff0c\u4f60\u90fd\u5e0c\u671b\u5c06\u5176\u8bbe\u7f6e\u4e3a metrics = ['accuracy'] \u3002\u8bc4\u4f30\u6807\u51c6\u53ef\u4ee5\u662f\u73b0\u6709\u7684\u6807\u51c6\u7684\u5b57\u7b26\u4e32\u6807\u8bc6\u7b26\uff0c\u4e5f\u53ef\u4ee5\u662f\u81ea\u5b9a\u4e49\u7684\u8bc4\u4f30\u6807\u51c6\u51fd\u6570\u3002 # \u591a\u5206\u7c7b\u95ee\u9898 model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) # \u4e8c\u5206\u7c7b\u95ee\u9898 model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy']) # \u5747\u65b9\u8bef\u5dee\u56de\u5f52\u95ee\u9898 model.compile(optimizer='rmsprop', loss='mse') # \u81ea\u5b9a\u4e49\u8bc4\u4f30\u6807\u51c6\u51fd\u6570 import keras.backend as K def mean_pred(y_true, y_pred): return K.mean(y_pred) model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', mean_pred]) \u6a21\u578b\u8bad\u7ec3 Keras \u6a21\u578b\u5728\u8f93\u5165\u6570\u636e\u548c\u6807\u7b7e\u7684 Numpy \u77e9\u9635\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u4e3a\u4e86\u8bad\u7ec3\u4e00\u4e2a\u6a21\u578b\uff0c\u4f60\u901a\u5e38\u4f1a\u4f7f\u7528 fit \u51fd\u6570\u3002 \u6587\u6863\u8be6\u89c1\u6b64\u5904 \u3002 # \u5bf9\u4e8e\u5177\u6709 2 \u4e2a\u7c7b\u7684\u5355\u8f93\u5165\u6a21\u578b\uff08\u4e8c\u8fdb\u5236\u5206\u7c7b\uff09\uff1a model = Sequential() model.add(Dense(32, activation='relu', input_dim=100)) model.add(Dense(1, activation='sigmoid')) model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy']) # \u751f\u6210\u865a\u62df\u6570\u636e import numpy as np data = np.random.random((1000, 100)) labels = np.random.randint(2, size=(1000, 1)) # \u8bad\u7ec3\u6a21\u578b\uff0c\u4ee5 32 \u4e2a\u6837\u672c\u4e3a\u4e00\u4e2a batch \u8fdb\u884c\u8fed\u4ee3 model.fit(data, labels, epochs=10, batch_size=32) # \u5bf9\u4e8e\u5177\u6709 10 \u4e2a\u7c7b\u7684\u5355\u8f93\u5165\u6a21\u578b\uff08\u591a\u5206\u7c7b\u5206\u7c7b\uff09\uff1a model = Sequential() model.add(Dense(32, activation='relu', input_dim=100)) model.add(Dense(10, activation='softmax')) model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) # \u751f\u6210\u865a\u62df\u6570\u636e import numpy as np data = np.random.random((1000, 100)) labels = np.random.randint(10, size=(1000, 1)) # \u5c06\u6807\u7b7e\u8f6c\u6362\u4e3a\u5206\u7c7b\u7684 one-hot \u7f16\u7801 one_hot_labels = keras.utils.to_categorical(labels, num_classes=10) # \u8bad\u7ec3\u6a21\u578b\uff0c\u4ee5 32 \u4e2a\u6837\u672c\u4e3a\u4e00\u4e2a batch \u8fdb\u884c\u8fed\u4ee3 model.fit(data, one_hot_labels, epochs=10, batch_size=32) \u6837\u4f8b \u8fd9\u91cc\u6709\u51e0\u4e2a\u53ef\u4ee5\u5e2e\u52a9\u4f60\u8d77\u6b65\u7684\u4f8b\u5b50\uff01 \u5728 examples \u76ee\u5f55 \u4e2d\uff0c\u4f60\u53ef\u4ee5\u627e\u5230\u771f\u5b9e\u6570\u636e\u96c6\u7684\u793a\u4f8b\u6a21\u578b\uff1a CIFAR10 \u5c0f\u56fe\u7247\u5206\u7c7b\uff1a\u5177\u6709\u5b9e\u65f6\u6570\u636e\u589e\u5f3a\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) IMDB \u7535\u5f71\u8bc4\u8bba\u60c5\u611f\u5206\u7c7b\uff1a\u57fa\u4e8e\u8bcd\u5e8f\u5217\u7684 LSTM Reuters \u65b0\u95fb\u4e3b\u9898\u5206\u7c7b\uff1a\u591a\u5c42\u611f\u77e5\u5668 (MLP) MNIST \u624b\u5199\u6570\u5b57\u5206\u7c7b\uff1aMLP & CNN \u57fa\u4e8e LSTM \u7684\u5b57\u7b26\u7ea7\u6587\u672c\u751f\u6210 ...\u4ee5\u53ca\u66f4\u591a\u3002 \u57fa\u4e8e\u591a\u5c42\u611f\u77e5\u5668 (MLP) \u7684 softmax \u591a\u5206\u7c7b\uff1a import keras from keras.models import Sequential from keras.layers import Dense, Dropout, Activation from keras.optimizers import SGD # \u751f\u6210\u865a\u62df\u6570\u636e import numpy as np x_train = np.random.random((1000, 20)) y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10) x_test = np.random.random((100, 20)) y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10) model = Sequential() # Dense(64) \u662f\u4e00\u4e2a\u5177\u6709 64 \u4e2a\u9690\u85cf\u795e\u7ecf\u5143\u7684\u5168\u8fde\u63a5\u5c42\u3002 # \u5728\u7b2c\u4e00\u5c42\u5fc5\u987b\u6307\u5b9a\u6240\u671f\u671b\u7684\u8f93\u5165\u6570\u636e\u5c3a\u5bf8\uff1a # \u5728\u8fd9\u91cc\uff0c\u662f\u4e00\u4e2a 20 \u7ef4\u7684\u5411\u91cf\u3002 model.add(Dense(64, activation='relu', input_dim=20)) model.add(Dropout(0.5)) model.add(Dense(64, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(10, activation='softmax')) sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy']) model.fit(x_train, y_train, epochs=20, batch_size=128) score = model.evaluate(x_test, y_test, batch_size=128) \u57fa\u4e8e\u591a\u5c42\u611f\u77e5\u5668\u7684\u4e8c\u5206\u7c7b\uff1a import numpy as np from keras.models import Sequential from keras.layers import Dense, Dropout # \u751f\u6210\u865a\u62df\u6570\u636e x_train = np.random.random((1000, 20)) y_train = np.random.randint(2, size=(1000, 1)) x_test = np.random.random((100, 20)) y_test = np.random.randint(2, size=(100, 1)) model = Sequential() model.add(Dense(64, input_dim=20, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(64, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(1, activation='sigmoid')) model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) model.fit(x_train, y_train, epochs=20, batch_size=128) score = model.evaluate(x_test, y_test, batch_size=128) \u7c7b\u4f3c VGG \u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff1a import numpy as np import keras from keras.models import Sequential from keras.layers import Dense, Dropout, Flatten from keras.layers import Conv2D, MaxPooling2D from keras.optimizers import SGD # \u751f\u6210\u865a\u62df\u6570\u636e x_train = np.random.random((100, 100, 100, 3)) y_train = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10) x_test = np.random.random((20, 100, 100, 3)) y_test = keras.utils.to_categorical(np.random.randint(10, size=(20, 1)), num_classes=10) model = Sequential() # \u8f93\u5165: 3 \u901a\u9053 100x100 \u50cf\u7d20\u56fe\u50cf -> (100, 100, 3) \u5f20\u91cf\u3002 # \u4f7f\u7528 32 \u4e2a\u5927\u5c0f\u4e3a 3x3 \u7684\u5377\u79ef\u6ee4\u6ce2\u5668\u3002 model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3))) model.add(Conv2D(32, (3, 3), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Conv2D(64, (3, 3), activation='relu')) model.add(Conv2D(64, (3, 3), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Flatten()) model.add(Dense(256, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(10, activation='softmax')) sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss='categorical_crossentropy', optimizer=sgd) model.fit(x_train, y_train, batch_size=32, epochs=10) score = model.evaluate(x_test, y_test, batch_size=32) \u57fa\u4e8e LSTM \u7684\u5e8f\u5217\u5206\u7c7b\uff1a from keras.models import Sequential from keras.layers import Dense, Dropout from keras.layers import Embedding from keras.layers import LSTM max_features = 1024 model = Sequential() model.add(Embedding(max_features, output_dim=256)) model.add(LSTM(128)) model.add(Dropout(0.5)) model.add(Dense(1, activation='sigmoid')) model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) model.fit(x_train, y_train, batch_size=16, epochs=10) score = model.evaluate(x_test, y_test, batch_size=16) \u57fa\u4e8e 1D \u5377\u79ef\u7684\u5e8f\u5217\u5206\u7c7b\uff1a from keras.models import Sequential from keras.layers import Dense, Dropout from keras.layers import Embedding from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D seq_length = 64 model = Sequential() model.add(Conv1D(64, 3, activation='relu', input_shape=(seq_length, 100))) model.add(Conv1D(64, 3, activation='relu')) model.add(MaxPooling1D(3)) model.add(Conv1D(128, 3, activation='relu')) model.add(Conv1D(128, 3, activation='relu')) model.add(GlobalAveragePooling1D()) model.add(Dropout(0.5)) model.add(Dense(1, activation='sigmoid')) model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) model.fit(x_train, y_train, batch_size=16, epochs=10) score = model.evaluate(x_test, y_test, batch_size=16) \u57fa\u4e8e\u6808\u5f0f LSTM \u7684\u5e8f\u5217\u5206\u7c7b \u5728\u8fd9\u4e2a\u6a21\u578b\u4e2d\uff0c\u6211\u4eec\u5c06 3 \u4e2a LSTM \u5c42\u53e0\u5728\u4e00\u8d77\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u66f4\u9ad8\u5c42\u6b21\u7684\u65f6\u95f4\u8868\u793a\u3002 \u524d\u4e24\u4e2a LSTM \u8fd4\u56de\u5b8c\u6574\u7684\u8f93\u51fa\u5e8f\u5217\uff0c\u4f46\u6700\u540e\u4e00\u4e2a\u53ea\u8fd4\u56de\u8f93\u51fa\u5e8f\u5217\u7684\u6700\u540e\u4e00\u6b65\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u65f6\u95f4\u7ef4\u5ea6\uff08\u5373\u5c06\u8f93\u5165\u5e8f\u5217\u8f6c\u6362\u6210\u5355\u4e2a\u5411\u91cf\uff09\u3002 from keras.models import Sequential from keras.layers import LSTM, Dense import numpy as np data_dim = 16 timesteps = 8 num_classes = 10 # \u671f\u671b\u8f93\u5165\u6570\u636e\u5c3a\u5bf8: (batch_size, timesteps, data_dim) model = Sequential() model.add(LSTM(32, return_sequences=True, input_shape=(timesteps, data_dim))) # \u8fd4\u56de\u7ef4\u5ea6\u4e3a 32 \u7684\u5411\u91cf\u5e8f\u5217 model.add(LSTM(32, return_sequences=True)) # \u8fd4\u56de\u7ef4\u5ea6\u4e3a 32 \u7684\u5411\u91cf\u5e8f\u5217 model.add(LSTM(32)) # \u8fd4\u56de\u7ef4\u5ea6\u4e3a 32 \u7684\u5355\u4e2a\u5411\u91cf model.add(Dense(10, activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) # \u751f\u6210\u865a\u62df\u8bad\u7ec3\u6570\u636e x_train = np.random.random((1000, timesteps, data_dim)) y_train = np.random.random((1000, num_classes)) # \u751f\u6210\u865a\u62df\u9a8c\u8bc1\u6570\u636e x_val = np.random.random((100, timesteps, data_dim)) y_val = np.random.random((100, num_classes)) model.fit(x_train, y_train, batch_size=64, epochs=5, validation_data=(x_val, y_val)) \"stateful\" \u6e32\u67d3\u7684\u7684\u6808\u5f0f LSTM \u6a21\u578b \u6709\u72b6\u6001 (stateful) \u7684\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e2d\uff0c\u5728\u4e00\u4e2a batch \u7684\u6837\u672c\u5904\u7406\u5b8c\u6210\u540e\uff0c\u5176\u5185\u90e8\u72b6\u6001\uff08\u8bb0\u5fc6\uff09\u4f1a\u88ab\u8bb0\u5f55\u5e76\u4f5c\u4e3a\u4e0b\u4e00\u4e2a batch \u7684\u6837\u672c\u7684\u521d\u59cb\u72b6\u6001\u3002\u8fd9\u5141\u8bb8\u5904\u7406\u66f4\u957f\u7684\u5e8f\u5217\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u53ef\u63a7\u6027\u3002 \u4f60\u53ef\u4ee5\u5728 FAQ \u4e2d\u67e5\u627e\u66f4\u591a\u5173\u4e8e stateful RNNs \u7684\u4fe1\u606f\u3002 from keras.models import Sequential from keras.layers import LSTM, Dense import numpy as np data_dim = 16 timesteps = 8 num_classes = 10 batch_size = 32 # \u671f\u671b\u8f93\u5165\u6570\u636e\u5c3a\u5bf8: (batch_size, timesteps, data_dim) # \u8bf7\u6ce8\u610f\uff0c\u6211\u4eec\u5fc5\u987b\u63d0\u4f9b\u5b8c\u6574\u7684 batch_input_shape\uff0c\u56e0\u4e3a\u7f51\u7edc\u662f\u6709\u72b6\u6001\u7684\u3002 # \u7b2c k \u6279\u6570\u636e\u7684\u7b2c i \u4e2a\u6837\u672c\u662f\u7b2c k-1 \u6279\u6570\u636e\u7684\u7b2c i \u4e2a\u6837\u672c\u7684\u540e\u7eed\u3002 model = Sequential() model.add(LSTM(32, return_sequences=True, stateful=True, batch_input_shape=(batch_size, timesteps, data_dim))) model.add(LSTM(32, return_sequences=True, stateful=True)) model.add(LSTM(32, stateful=True)) model.add(Dense(10, activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) # \u751f\u6210\u865a\u62df\u8bad\u7ec3\u6570\u636e x_train = np.random.random((batch_size * 10, timesteps, data_dim)) y_train = np.random.random((batch_size * 10, num_classes)) # \u751f\u6210\u865a\u62df\u9a8c\u8bc1\u6570\u636e x_val = np.random.random((batch_size * 3, timesteps, data_dim)) y_val = np.random.random((batch_size * 3, num_classes)) model.fit(x_train, y_train, batch_size=batch_size, epochs=5, shuffle=False, validation_data=(x_val, y_val))","title":"\u5f00\u59cb\u4f7f\u7528 Keras Sequential \u987a\u5e8f\u6a21\u578b"},{"location":"0-Getting-Started/0.sequential-model-guide/#keras-sequential","text":"\u987a\u5e8f\u6a21\u578b\u662f\u591a\u4e2a\u7f51\u7edc\u5c42\u7684\u7ebf\u6027\u5806\u53e0\u3002 \u4f60\u53ef\u4ee5\u901a\u8fc7\u5c06\u7f51\u7edc\u5c42\u5b9e\u4f8b\u7684\u5217\u8868\u4f20\u9012\u7ed9 Sequential \u7684\u6784\u9020\u5668\uff0c\u6765\u521b\u5efa\u4e00\u4e2a Sequential \u6a21\u578b\uff1a from keras.models import Sequential from keras.layers import Dense, Activation model = Sequential([ Dense(32, input_shape=(784,)), Activation('relu'), Dense(10), Activation('softmax'), ]) \u4e5f\u53ef\u4ee5\u7b80\u5355\u5730\u4f7f\u7528 .add() \u65b9\u6cd5\u5c06\u5404\u5c42\u6dfb\u52a0\u5230\u6a21\u578b\u4e2d\uff1a model = Sequential() model.add(Dense(32, input_dim=784)) model.add(Activation('relu'))","title":"\u5f00\u59cb\u4f7f\u7528 Keras Sequential \u987a\u5e8f\u6a21\u578b"},{"location":"0-Getting-Started/0.sequential-model-guide/#_1","text":"\u6a21\u578b\u9700\u8981\u77e5\u9053\u5b83\u6240\u671f\u671b\u7684\u8f93\u5165\u7684\u5c3a\u5bf8\u3002\u51fa\u4e8e\u8fd9\u4e2a\u539f\u56e0\uff0c\u987a\u5e8f\u6a21\u578b\u4e2d\u7684\u7b2c\u4e00\u5c42\uff08\u4e14\u53ea\u6709\u7b2c\u4e00\u5c42\uff0c\u56e0\u4e3a\u4e0b\u9762\u7684\u5c42\u53ef\u4ee5\u81ea\u52a8\u5730\u63a8\u65ad\u5c3a\u5bf8\uff09\u9700\u8981\u63a5\u6536\u5173\u4e8e\u5176\u8f93\u5165\u5c3a\u5bf8\u7684\u4fe1\u606f\u3002\u6709\u51e0\u79cd\u65b9\u6cd5\u6765\u505a\u5230\u8fd9\u4e00\u70b9\uff1a \u4f20\u9012\u4e00\u4e2a input_shape \u53c2\u6570\u7ed9\u7b2c\u4e00\u5c42\u3002\u5b83\u662f\u4e00\u4e2a\u8868\u793a\u5c3a\u5bf8\u7684\u5143\u7ec4 (\u4e00\u4e2a\u6574\u6570\u6216 None \u7684\u5143\u7ec4\uff0c\u5176\u4e2d None \u8868\u793a\u53ef\u80fd\u4e3a\u4efb\u4f55\u6b63\u6574\u6570)\u3002\u5728 input_shape \u4e2d\u4e0d\u5305\u542b\u6570\u636e\u7684 batch \u5927\u5c0f\u3002 \u67d0\u4e9b 2D \u5c42\uff0c\u4f8b\u5982 Dense \uff0c\u652f\u6301\u901a\u8fc7\u53c2\u6570 input_dim \u6307\u5b9a\u8f93\u5165\u5c3a\u5bf8\uff0c\u67d0\u4e9b 3D \u65f6\u5e8f\u5c42\u652f\u6301 input_dim \u548c input_length \u53c2\u6570\u3002 \u5982\u679c\u4f60\u9700\u8981\u4e3a\u4f60\u7684\u8f93\u5165\u6307\u5b9a\u4e00\u4e2a\u56fa\u5b9a\u7684 batch \u5927\u5c0f\uff08\u8fd9\u5bf9 stateful RNNs \u5f88\u6709\u7528\uff09\uff0c\u4f60\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a batch_size \u53c2\u6570\u7ed9\u4e00\u4e2a\u5c42\u3002\u5982\u679c\u4f60\u540c\u65f6\u5c06 batch_size=32 \u548c input_shape=(6, 8) \u4f20\u9012\u7ed9\u4e00\u4e2a\u5c42\uff0c\u90a3\u4e48\u6bcf\u4e00\u6279\u8f93\u5165\u7684\u5c3a\u5bf8\u5c31\u4e3a (32\uff0c6\uff0c8) \u3002 \u56e0\u6b64\uff0c\u4e0b\u9762\u7684\u4ee3\u7801\u7247\u6bb5\u662f\u7b49\u4ef7\u7684\uff1a model = Sequential() model.add(Dense(32, input_shape=(784,))) model = Sequential() model.add(Dense(32, input_dim=784))","title":"\u6307\u5b9a\u8f93\u5165\u6570\u636e\u7684\u5c3a\u5bf8"},{"location":"0-Getting-Started/0.sequential-model-guide/#_2","text":"\u5728\u8bad\u7ec3\u6a21\u578b\u4e4b\u524d\uff0c\u60a8\u9700\u8981\u914d\u7f6e\u5b66\u4e60\u8fc7\u7a0b\uff0c\u8fd9\u662f\u901a\u8fc7 compile \u65b9\u6cd5\u5b8c\u6210\u7684\u3002\u5b83\u63a5\u6536\u4e09\u4e2a\u53c2\u6570\uff1a \u4f18\u5316\u5668 optimizer\u3002\u5b83\u53ef\u4ee5\u662f\u73b0\u6709\u4f18\u5316\u5668\u7684\u5b57\u7b26\u4e32\u6807\u8bc6\u7b26\uff0c\u5982 rmsprop \u6216 adagrad \uff0c\u4e5f\u53ef\u4ee5\u662f Optimizer \u7c7b\u7684\u5b9e\u4f8b\u3002\u8be6\u89c1\uff1a optimizers \u3002 \u635f\u5931\u51fd\u6570 loss\uff0c\u6a21\u578b\u8bd5\u56fe\u6700\u5c0f\u5316\u7684\u76ee\u6807\u51fd\u6570\u3002\u5b83\u53ef\u4ee5\u662f\u73b0\u6709\u635f\u5931\u51fd\u6570\u7684\u5b57\u7b26\u4e32\u6807\u8bc6\u7b26\uff0c\u5982 categorical_crossentropy \u6216 mse \uff0c\u4e5f\u53ef\u4ee5\u662f\u4e00\u4e2a\u76ee\u6807\u51fd\u6570\u3002\u8be6\u89c1\uff1a losses \u3002 \u8bc4\u4f30\u6807\u51c6 metrics\u3002\u5bf9\u4e8e\u4efb\u4f55\u5206\u7c7b\u95ee\u9898\uff0c\u4f60\u90fd\u5e0c\u671b\u5c06\u5176\u8bbe\u7f6e\u4e3a metrics = ['accuracy'] \u3002\u8bc4\u4f30\u6807\u51c6\u53ef\u4ee5\u662f\u73b0\u6709\u7684\u6807\u51c6\u7684\u5b57\u7b26\u4e32\u6807\u8bc6\u7b26\uff0c\u4e5f\u53ef\u4ee5\u662f\u81ea\u5b9a\u4e49\u7684\u8bc4\u4f30\u6807\u51c6\u51fd\u6570\u3002 # \u591a\u5206\u7c7b\u95ee\u9898 model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) # \u4e8c\u5206\u7c7b\u95ee\u9898 model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy']) # \u5747\u65b9\u8bef\u5dee\u56de\u5f52\u95ee\u9898 model.compile(optimizer='rmsprop', loss='mse') # \u81ea\u5b9a\u4e49\u8bc4\u4f30\u6807\u51c6\u51fd\u6570 import keras.backend as K def mean_pred(y_true, y_pred): return K.mean(y_pred) model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', mean_pred])","title":"\u6a21\u578b\u7f16\u8bd1"},{"location":"0-Getting-Started/0.sequential-model-guide/#_3","text":"Keras \u6a21\u578b\u5728\u8f93\u5165\u6570\u636e\u548c\u6807\u7b7e\u7684 Numpy \u77e9\u9635\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u4e3a\u4e86\u8bad\u7ec3\u4e00\u4e2a\u6a21\u578b\uff0c\u4f60\u901a\u5e38\u4f1a\u4f7f\u7528 fit \u51fd\u6570\u3002 \u6587\u6863\u8be6\u89c1\u6b64\u5904 \u3002 # \u5bf9\u4e8e\u5177\u6709 2 \u4e2a\u7c7b\u7684\u5355\u8f93\u5165\u6a21\u578b\uff08\u4e8c\u8fdb\u5236\u5206\u7c7b\uff09\uff1a model = Sequential() model.add(Dense(32, activation='relu', input_dim=100)) model.add(Dense(1, activation='sigmoid')) model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy']) # \u751f\u6210\u865a\u62df\u6570\u636e import numpy as np data = np.random.random((1000, 100)) labels = np.random.randint(2, size=(1000, 1)) # \u8bad\u7ec3\u6a21\u578b\uff0c\u4ee5 32 \u4e2a\u6837\u672c\u4e3a\u4e00\u4e2a batch \u8fdb\u884c\u8fed\u4ee3 model.fit(data, labels, epochs=10, batch_size=32) # \u5bf9\u4e8e\u5177\u6709 10 \u4e2a\u7c7b\u7684\u5355\u8f93\u5165\u6a21\u578b\uff08\u591a\u5206\u7c7b\u5206\u7c7b\uff09\uff1a model = Sequential() model.add(Dense(32, activation='relu', input_dim=100)) model.add(Dense(10, activation='softmax')) model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) # \u751f\u6210\u865a\u62df\u6570\u636e import numpy as np data = np.random.random((1000, 100)) labels = np.random.randint(10, size=(1000, 1)) # \u5c06\u6807\u7b7e\u8f6c\u6362\u4e3a\u5206\u7c7b\u7684 one-hot \u7f16\u7801 one_hot_labels = keras.utils.to_categorical(labels, num_classes=10) # \u8bad\u7ec3\u6a21\u578b\uff0c\u4ee5 32 \u4e2a\u6837\u672c\u4e3a\u4e00\u4e2a batch \u8fdb\u884c\u8fed\u4ee3 model.fit(data, one_hot_labels, epochs=10, batch_size=32)","title":"\u6a21\u578b\u8bad\u7ec3"},{"location":"0-Getting-Started/0.sequential-model-guide/#_4","text":"\u8fd9\u91cc\u6709\u51e0\u4e2a\u53ef\u4ee5\u5e2e\u52a9\u4f60\u8d77\u6b65\u7684\u4f8b\u5b50\uff01 \u5728 examples \u76ee\u5f55 \u4e2d\uff0c\u4f60\u53ef\u4ee5\u627e\u5230\u771f\u5b9e\u6570\u636e\u96c6\u7684\u793a\u4f8b\u6a21\u578b\uff1a CIFAR10 \u5c0f\u56fe\u7247\u5206\u7c7b\uff1a\u5177\u6709\u5b9e\u65f6\u6570\u636e\u589e\u5f3a\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) IMDB \u7535\u5f71\u8bc4\u8bba\u60c5\u611f\u5206\u7c7b\uff1a\u57fa\u4e8e\u8bcd\u5e8f\u5217\u7684 LSTM Reuters \u65b0\u95fb\u4e3b\u9898\u5206\u7c7b\uff1a\u591a\u5c42\u611f\u77e5\u5668 (MLP) MNIST \u624b\u5199\u6570\u5b57\u5206\u7c7b\uff1aMLP & CNN \u57fa\u4e8e LSTM \u7684\u5b57\u7b26\u7ea7\u6587\u672c\u751f\u6210 ...\u4ee5\u53ca\u66f4\u591a\u3002","title":"\u6837\u4f8b"},{"location":"0-Getting-Started/0.sequential-model-guide/#mlp-softmax","text":"import keras from keras.models import Sequential from keras.layers import Dense, Dropout, Activation from keras.optimizers import SGD # \u751f\u6210\u865a\u62df\u6570\u636e import numpy as np x_train = np.random.random((1000, 20)) y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10) x_test = np.random.random((100, 20)) y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10) model = Sequential() # Dense(64) \u662f\u4e00\u4e2a\u5177\u6709 64 \u4e2a\u9690\u85cf\u795e\u7ecf\u5143\u7684\u5168\u8fde\u63a5\u5c42\u3002 # \u5728\u7b2c\u4e00\u5c42\u5fc5\u987b\u6307\u5b9a\u6240\u671f\u671b\u7684\u8f93\u5165\u6570\u636e\u5c3a\u5bf8\uff1a # \u5728\u8fd9\u91cc\uff0c\u662f\u4e00\u4e2a 20 \u7ef4\u7684\u5411\u91cf\u3002 model.add(Dense(64, activation='relu', input_dim=20)) model.add(Dropout(0.5)) model.add(Dense(64, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(10, activation='softmax')) sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy']) model.fit(x_train, y_train, epochs=20, batch_size=128) score = model.evaluate(x_test, y_test, batch_size=128)","title":"\u57fa\u4e8e\u591a\u5c42\u611f\u77e5\u5668 (MLP) \u7684 softmax \u591a\u5206\u7c7b\uff1a"},{"location":"0-Getting-Started/0.sequential-model-guide/#_5","text":"import numpy as np from keras.models import Sequential from keras.layers import Dense, Dropout # \u751f\u6210\u865a\u62df\u6570\u636e x_train = np.random.random((1000, 20)) y_train = np.random.randint(2, size=(1000, 1)) x_test = np.random.random((100, 20)) y_test = np.random.randint(2, size=(100, 1)) model = Sequential() model.add(Dense(64, input_dim=20, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(64, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(1, activation='sigmoid')) model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) model.fit(x_train, y_train, epochs=20, batch_size=128) score = model.evaluate(x_test, y_test, batch_size=128)","title":"\u57fa\u4e8e\u591a\u5c42\u611f\u77e5\u5668\u7684\u4e8c\u5206\u7c7b\uff1a"},{"location":"0-Getting-Started/0.sequential-model-guide/#vgg","text":"import numpy as np import keras from keras.models import Sequential from keras.layers import Dense, Dropout, Flatten from keras.layers import Conv2D, MaxPooling2D from keras.optimizers import SGD # \u751f\u6210\u865a\u62df\u6570\u636e x_train = np.random.random((100, 100, 100, 3)) y_train = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10) x_test = np.random.random((20, 100, 100, 3)) y_test = keras.utils.to_categorical(np.random.randint(10, size=(20, 1)), num_classes=10) model = Sequential() # \u8f93\u5165: 3 \u901a\u9053 100x100 \u50cf\u7d20\u56fe\u50cf -> (100, 100, 3) \u5f20\u91cf\u3002 # \u4f7f\u7528 32 \u4e2a\u5927\u5c0f\u4e3a 3x3 \u7684\u5377\u79ef\u6ee4\u6ce2\u5668\u3002 model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3))) model.add(Conv2D(32, (3, 3), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Conv2D(64, (3, 3), activation='relu')) model.add(Conv2D(64, (3, 3), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Flatten()) model.add(Dense(256, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(10, activation='softmax')) sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss='categorical_crossentropy', optimizer=sgd) model.fit(x_train, y_train, batch_size=32, epochs=10) score = model.evaluate(x_test, y_test, batch_size=32)","title":"\u7c7b\u4f3c VGG \u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff1a"},{"location":"0-Getting-Started/0.sequential-model-guide/#lstm","text":"from keras.models import Sequential from keras.layers import Dense, Dropout from keras.layers import Embedding from keras.layers import LSTM max_features = 1024 model = Sequential() model.add(Embedding(max_features, output_dim=256)) model.add(LSTM(128)) model.add(Dropout(0.5)) model.add(Dense(1, activation='sigmoid')) model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) model.fit(x_train, y_train, batch_size=16, epochs=10) score = model.evaluate(x_test, y_test, batch_size=16)","title":"\u57fa\u4e8e LSTM \u7684\u5e8f\u5217\u5206\u7c7b\uff1a"},{"location":"0-Getting-Started/0.sequential-model-guide/#1d","text":"from keras.models import Sequential from keras.layers import Dense, Dropout from keras.layers import Embedding from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D seq_length = 64 model = Sequential() model.add(Conv1D(64, 3, activation='relu', input_shape=(seq_length, 100))) model.add(Conv1D(64, 3, activation='relu')) model.add(MaxPooling1D(3)) model.add(Conv1D(128, 3, activation='relu')) model.add(Conv1D(128, 3, activation='relu')) model.add(GlobalAveragePooling1D()) model.add(Dropout(0.5)) model.add(Dense(1, activation='sigmoid')) model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) model.fit(x_train, y_train, batch_size=16, epochs=10) score = model.evaluate(x_test, y_test, batch_size=16)","title":"\u57fa\u4e8e 1D \u5377\u79ef\u7684\u5e8f\u5217\u5206\u7c7b\uff1a"},{"location":"0-Getting-Started/0.sequential-model-guide/#lstm_1","text":"\u5728\u8fd9\u4e2a\u6a21\u578b\u4e2d\uff0c\u6211\u4eec\u5c06 3 \u4e2a LSTM \u5c42\u53e0\u5728\u4e00\u8d77\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u66f4\u9ad8\u5c42\u6b21\u7684\u65f6\u95f4\u8868\u793a\u3002 \u524d\u4e24\u4e2a LSTM \u8fd4\u56de\u5b8c\u6574\u7684\u8f93\u51fa\u5e8f\u5217\uff0c\u4f46\u6700\u540e\u4e00\u4e2a\u53ea\u8fd4\u56de\u8f93\u51fa\u5e8f\u5217\u7684\u6700\u540e\u4e00\u6b65\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u65f6\u95f4\u7ef4\u5ea6\uff08\u5373\u5c06\u8f93\u5165\u5e8f\u5217\u8f6c\u6362\u6210\u5355\u4e2a\u5411\u91cf\uff09\u3002 from keras.models import Sequential from keras.layers import LSTM, Dense import numpy as np data_dim = 16 timesteps = 8 num_classes = 10 # \u671f\u671b\u8f93\u5165\u6570\u636e\u5c3a\u5bf8: (batch_size, timesteps, data_dim) model = Sequential() model.add(LSTM(32, return_sequences=True, input_shape=(timesteps, data_dim))) # \u8fd4\u56de\u7ef4\u5ea6\u4e3a 32 \u7684\u5411\u91cf\u5e8f\u5217 model.add(LSTM(32, return_sequences=True)) # \u8fd4\u56de\u7ef4\u5ea6\u4e3a 32 \u7684\u5411\u91cf\u5e8f\u5217 model.add(LSTM(32)) # \u8fd4\u56de\u7ef4\u5ea6\u4e3a 32 \u7684\u5355\u4e2a\u5411\u91cf model.add(Dense(10, activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) # \u751f\u6210\u865a\u62df\u8bad\u7ec3\u6570\u636e x_train = np.random.random((1000, timesteps, data_dim)) y_train = np.random.random((1000, num_classes)) # \u751f\u6210\u865a\u62df\u9a8c\u8bc1\u6570\u636e x_val = np.random.random((100, timesteps, data_dim)) y_val = np.random.random((100, num_classes)) model.fit(x_train, y_train, batch_size=64, epochs=5, validation_data=(x_val, y_val))","title":"\u57fa\u4e8e\u6808\u5f0f LSTM \u7684\u5e8f\u5217\u5206\u7c7b"},{"location":"0-Getting-Started/0.sequential-model-guide/#stateful-lstm","text":"\u6709\u72b6\u6001 (stateful) \u7684\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e2d\uff0c\u5728\u4e00\u4e2a batch \u7684\u6837\u672c\u5904\u7406\u5b8c\u6210\u540e\uff0c\u5176\u5185\u90e8\u72b6\u6001\uff08\u8bb0\u5fc6\uff09\u4f1a\u88ab\u8bb0\u5f55\u5e76\u4f5c\u4e3a\u4e0b\u4e00\u4e2a batch \u7684\u6837\u672c\u7684\u521d\u59cb\u72b6\u6001\u3002\u8fd9\u5141\u8bb8\u5904\u7406\u66f4\u957f\u7684\u5e8f\u5217\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u53ef\u63a7\u6027\u3002 \u4f60\u53ef\u4ee5\u5728 FAQ \u4e2d\u67e5\u627e\u66f4\u591a\u5173\u4e8e stateful RNNs \u7684\u4fe1\u606f\u3002 from keras.models import Sequential from keras.layers import LSTM, Dense import numpy as np data_dim = 16 timesteps = 8 num_classes = 10 batch_size = 32 # \u671f\u671b\u8f93\u5165\u6570\u636e\u5c3a\u5bf8: (batch_size, timesteps, data_dim) # \u8bf7\u6ce8\u610f\uff0c\u6211\u4eec\u5fc5\u987b\u63d0\u4f9b\u5b8c\u6574\u7684 batch_input_shape\uff0c\u56e0\u4e3a\u7f51\u7edc\u662f\u6709\u72b6\u6001\u7684\u3002 # \u7b2c k \u6279\u6570\u636e\u7684\u7b2c i \u4e2a\u6837\u672c\u662f\u7b2c k-1 \u6279\u6570\u636e\u7684\u7b2c i \u4e2a\u6837\u672c\u7684\u540e\u7eed\u3002 model = Sequential() model.add(LSTM(32, return_sequences=True, stateful=True, batch_input_shape=(batch_size, timesteps, data_dim))) model.add(LSTM(32, return_sequences=True, stateful=True)) model.add(LSTM(32, stateful=True)) model.add(Dense(10, activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) # \u751f\u6210\u865a\u62df\u8bad\u7ec3\u6570\u636e x_train = np.random.random((batch_size * 10, timesteps, data_dim)) y_train = np.random.random((batch_size * 10, num_classes)) # \u751f\u6210\u865a\u62df\u9a8c\u8bc1\u6570\u636e x_val = np.random.random((batch_size * 3, timesteps, data_dim)) y_val = np.random.random((batch_size * 3, num_classes)) model.fit(x_train, y_train, batch_size=batch_size, epochs=5, shuffle=False, validation_data=(x_val, y_val))","title":"\"stateful\" \u6e32\u67d3\u7684\u7684\u6808\u5f0f LSTM \u6a21\u578b"},{"location":"0-Getting-Started/1.functional-api-guide/","text":"\u5f00\u59cb\u4f7f\u7528 Keras \u51fd\u6570\u5f0f API Keras \u51fd\u6570\u5f0f API \u662f\u5b9a\u4e49\u590d\u6742\u6a21\u578b\uff08\u5982\u591a\u8f93\u51fa\u6a21\u578b\u3001\u6709\u5411\u65e0\u73af\u56fe\uff0c\u6216\u5177\u6709\u5171\u4eab\u5c42\u7684\u6a21\u578b\uff09\u7684\u65b9\u6cd5\u3002 \u8fd9\u90e8\u5206\u6587\u6863\u5047\u8bbe\u4f60\u5df2\u7ecf\u5bf9 Sequential \u987a\u5e8f\u6a21\u578b\u6bd4\u8f83\u719f\u6089\u3002 \u8ba9\u6211\u4eec\u5148\u4ece\u4e00\u4e9b\u7b80\u5355\u7684\u4f8b\u5b50\u5f00\u59cb\u3002 \u4f8b\u4e00\uff1a\u5168\u8fde\u63a5\u7f51\u7edc Sequential \u6a21\u578b\u53ef\u80fd\u662f\u5b9e\u73b0\u8fd9\u79cd\u7f51\u7edc\u7684\u4e00\u4e2a\u66f4\u597d\u9009\u62e9\uff0c\u4f46\u8fd9\u4e2a\u4f8b\u5b50\u80fd\u591f\u5e2e\u52a9\u6211\u4eec\u8fdb\u884c\u4e00\u4e9b\u7b80\u5355\u7684\u7406\u89e3\u3002 \u7f51\u7edc\u5c42\u7684\u5b9e\u4f8b\u662f\u53ef\u8c03\u7528\u7684\uff0c\u5b83\u4ee5\u5f20\u91cf\u4e3a\u53c2\u6570\uff0c\u5e76\u4e14\u8fd4\u56de\u4e00\u4e2a\u5f20\u91cf \u8f93\u5165\u548c\u8f93\u51fa\u5747\u4e3a\u5f20\u91cf\uff0c\u5b83\u4eec\u90fd\u53ef\u4ee5\u7528\u6765\u5b9a\u4e49\u4e00\u4e2a\u6a21\u578b\uff08 Model \uff09 \u8fd9\u6837\u7684\u6a21\u578b\u540c Keras \u7684 Sequential \u6a21\u578b\u4e00\u6837\uff0c\u90fd\u53ef\u4ee5\u88ab\u8bad\u7ec3 from keras.layers import Input, Dense from keras.models import Model # \u8fd9\u90e8\u5206\u8fd4\u56de\u4e00\u4e2a\u5f20\u91cf inputs = Input(shape=(784,)) # \u5c42\u7684\u5b9e\u4f8b\u662f\u53ef\u8c03\u7528\u7684\uff0c\u5b83\u4ee5\u5f20\u91cf\u4e3a\u53c2\u6570\uff0c\u5e76\u4e14\u8fd4\u56de\u4e00\u4e2a\u5f20\u91cf x = Dense(64, activation='relu')(inputs) x = Dense(64, activation='relu')(x) predictions = Dense(10, activation='softmax')(x) # \u8fd9\u90e8\u5206\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u8f93\u5165\u5c42\u548c\u4e09\u4e2a\u5168\u8fde\u63a5\u5c42\u7684\u6a21\u578b model = Model(inputs=inputs, outputs=predictions) model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) model.fit(data, labels) # \u5f00\u59cb\u8bad\u7ec3 \u6240\u6709\u7684\u6a21\u578b\u90fd\u53ef\u8c03\u7528\uff0c\u5c31\u50cf\u7f51\u7edc\u5c42\u4e00\u6837 \u5229\u7528\u51fd\u6570\u5f0f API\uff0c\u53ef\u4ee5\u8f7b\u6613\u5730\u91cd\u7528\u8bad\u7ec3\u597d\u7684\u6a21\u578b\uff1a\u53ef\u4ee5\u5c06\u4efb\u4f55\u6a21\u578b\u770b\u4f5c\u662f\u4e00\u4e2a\u5c42\uff0c\u7136\u540e\u901a\u8fc7\u4f20\u9012\u4e00\u4e2a\u5f20\u91cf\u6765\u8c03\u7528\u5b83\u3002\u6ce8\u610f\uff0c\u5728\u8c03\u7528\u6a21\u578b\u65f6\uff0c\u60a8\u4e0d\u4ec5\u91cd\u7528\u6a21\u578b\u7684 \u7ed3\u6784 \uff0c\u8fd8\u91cd\u7528\u4e86\u5b83\u7684\u6743\u91cd\u3002 x = Input(shape=(784,)) # \u8fd9\u662f\u53ef\u884c\u7684\uff0c\u5e76\u4e14\u8fd4\u56de\u4e0a\u9762\u5b9a\u4e49\u7684 10-way softmax\u3002 y = model(x) \u8fd9\u79cd\u65b9\u5f0f\u80fd\u5141\u8bb8\u6211\u4eec\u5feb\u901f\u521b\u5efa\u53ef\u4ee5\u5904\u7406 \u5e8f\u5217\u8f93\u5165 \u7684\u6a21\u578b\u3002\u53ea\u9700\u4e00\u884c\u4ee3\u7801\uff0c\u4f60\u5c31\u5c06\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u8f6c\u6362\u4e3a\u89c6\u9891\u5206\u7c7b\u6a21\u578b\u3002 from keras.layers import TimeDistributed # \u8f93\u5165\u5f20\u91cf\u662f 20 \u4e2a\u65f6\u95f4\u6b65\u7684\u5e8f\u5217\uff0c # \u6bcf\u4e00\u4e2a\u65f6\u95f4\u4e3a\u4e00\u4e2a 784 \u7ef4\u7684\u5411\u91cf input_sequences = Input(shape=(20, 784)) # \u8fd9\u90e8\u5206\u5c06\u6211\u4eec\u4e4b\u524d\u5b9a\u4e49\u7684\u6a21\u578b\u5e94\u7528\u4e8e\u8f93\u5165\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2a\u65f6\u95f4\u6b65\u3002 # \u4e4b\u524d\u5b9a\u4e49\u7684\u6a21\u578b\u7684\u8f93\u51fa\u662f\u4e00\u4e2a 10-way softmax\uff0c # \u56e0\u800c\u4e0b\u9762\u7684\u5c42\u7684\u8f93\u51fa\u5c06\u662f\u7ef4\u5ea6\u4e3a 10 \u7684 20 \u4e2a\u5411\u91cf\u7684\u5e8f\u5217\u3002 processed_sequences = TimeDistributed(model)(input_sequences) \u591a\u8f93\u5165\u591a\u8f93\u51fa\u6a21\u578b \u4ee5\u4e0b\u662f\u51fd\u6570\u5f0f API \u7684\u4e00\u4e2a\u5f88\u597d\u7684\u4f8b\u5b50\uff1a\u5177\u6709\u591a\u4e2a\u8f93\u5165\u548c\u8f93\u51fa\u7684\u6a21\u578b\u3002\u51fd\u6570\u5f0f API \u4f7f\u5904\u7406\u5927\u91cf\u4ea4\u7ec7\u7684\u6570\u636e\u6d41\u53d8\u5f97\u5bb9\u6613\u3002 \u6765\u8003\u8651\u4e0b\u9762\u7684\u6a21\u578b\u3002\u6211\u4eec\u8bd5\u56fe\u9884\u6d4b Twitter \u4e0a\u7684\u4e00\u6761\u65b0\u95fb\u6807\u9898\u6709\u591a\u5c11\u8f6c\u53d1\u548c\u70b9\u8d5e\u6570\u3002\u6a21\u578b\u7684\u4e3b\u8981\u8f93\u5165\u5c06\u662f\u65b0\u95fb\u6807\u9898\u672c\u8eab\uff0c\u5373\u4e00\u7cfb\u5217\u8bcd\u8bed\uff0c\u4f46\u662f\u4e3a\u4e86\u589e\u6dfb\u8da3\u5473\uff0c\u6211\u4eec\u7684\u6a21\u578b\u8fd8\u6dfb\u52a0\u4e86\u5176\u4ed6\u7684\u8f85\u52a9\u8f93\u5165\u6765\u63a5\u6536\u989d\u5916\u7684\u6570\u636e\uff0c\u4f8b\u5982\u65b0\u95fb\u6807\u9898\u7684\u53d1\u5e03\u7684\u65f6\u95f4\u7b49\u3002 \u8be5\u6a21\u578b\u4e5f\u5c06\u901a\u8fc7\u4e24\u4e2a\u635f\u5931\u51fd\u6570\u8fdb\u884c\u76d1\u7763\u5b66\u4e60\u3002\u8f83\u65e9\u5730\u5728\u6a21\u578b\u4e2d\u4f7f\u7528\u4e3b\u635f\u5931\u51fd\u6570\uff0c\u662f\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u4e00\u4e2a\u826f\u597d\u6b63\u5219\u65b9\u6cd5\u3002 \u6a21\u578b\u7ed3\u6784\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u8ba9\u6211\u4eec\u7528\u51fd\u6570\u5f0f API \u6765\u5b9e\u73b0\u5b83\u3002 \u4e3b\u8981\u8f93\u5165\u63a5\u6536\u65b0\u95fb\u6807\u9898\u672c\u8eab\uff0c\u5373\u4e00\u4e2a\u6574\u6570\u5e8f\u5217\uff08\u6bcf\u4e2a\u6574\u6570\u7f16\u7801\u4e00\u4e2a\u8bcd\uff09\u3002 \u8fd9\u4e9b\u6574\u6570\u5728 1 \u5230 10,000 \u4e4b\u95f4\uff0810,000 \u4e2a\u8bcd\u7684\u8bcd\u6c47\u8868\uff09\uff0c\u4e14\u5e8f\u5217\u957f\u5ea6\u4e3a 100 \u4e2a\u8bcd\u3002 from keras.layers import Input, Embedding, LSTM, Dense from keras.models import Model # \u6807\u9898\u8f93\u5165\uff1a\u63a5\u6536\u4e00\u4e2a\u542b\u6709 100 \u4e2a\u6574\u6570\u7684\u5e8f\u5217\uff0c\u6bcf\u4e2a\u6574\u6570\u5728 1 \u5230 10000 \u4e4b\u95f4\u3002 # \u6ce8\u610f\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4f20\u9012\u4e00\u4e2a \"name\" \u53c2\u6570\u6765\u547d\u540d\u4efb\u4f55\u5c42\u3002 main_input = Input(shape=(100,), dtype='int32', name='main_input') # Embedding \u5c42\u5c06\u8f93\u5165\u5e8f\u5217\u7f16\u7801\u4e3a\u4e00\u4e2a\u7a20\u5bc6\u5411\u91cf\u7684\u5e8f\u5217\uff0c # \u6bcf\u4e2a\u5411\u91cf\u7ef4\u5ea6\u4e3a 512\u3002 x = Embedding(output_dim=512, input_dim=10000, input_length=100)(main_input) # LSTM \u5c42\u628a\u5411\u91cf\u5e8f\u5217\u8f6c\u6362\u6210\u5355\u4e2a\u5411\u91cf\uff0c # \u5b83\u5305\u542b\u6574\u4e2a\u5e8f\u5217\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f lstm_out = LSTM(32)(x) \u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u63d2\u5165\u8f85\u52a9\u635f\u5931\uff0c\u4f7f\u5f97\u5373\u4f7f\u5728\u6a21\u578b\u4e3b\u635f\u5931\u5f88\u9ad8\u7684\u60c5\u51b5\u4e0b\uff0cLSTM \u5c42\u548c Embedding \u5c42\u90fd\u80fd\u88ab\u5e73\u7a33\u5730\u8bad\u7ec3\u3002 auxiliary_output = Dense(1, activation='sigmoid', name='aux_output')(lstm_out) \u6b64\u65f6\uff0c\u6211\u4eec\u5c06\u8f85\u52a9\u8f93\u5165\u6570\u636e\u4e0e LSTM \u5c42\u7684\u8f93\u51fa\u8fde\u63a5\u8d77\u6765\uff0c\u8f93\u5165\u5230\u6a21\u578b\u4e2d\uff1a auxiliary_input = Input(shape=(5,), name='aux_input') x = keras.layers.concatenate([lstm_out, auxiliary_input]) # \u5806\u53e0\u591a\u4e2a\u5168\u8fde\u63a5\u7f51\u7edc\u5c42 x = Dense(64, activation='relu')(x) x = Dense(64, activation='relu')(x) x = Dense(64, activation='relu')(x) # \u6700\u540e\u6dfb\u52a0\u4e3b\u8981\u7684\u903b\u8f91\u56de\u5f52\u5c42 main_output = Dense(1, activation='sigmoid', name='main_output')(x) \u7136\u540e\u5b9a\u4e49\u4e00\u4e2a\u5177\u6709\u4e24\u4e2a\u8f93\u5165\u548c\u4e24\u4e2a\u8f93\u51fa\u7684\u6a21\u578b\uff1a model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output, auxiliary_output]) \u73b0\u5728\u7f16\u8bd1\u6a21\u578b\uff0c\u5e76\u7ed9\u8f85\u52a9\u635f\u5931\u5206\u914d\u4e00\u4e2a 0.2 \u7684\u6743\u91cd\u3002\u5982\u679c\u8981\u4e3a\u4e0d\u540c\u7684\u8f93\u51fa\u6307\u5b9a\u4e0d\u540c\u7684 loss_weights \u6216 loss \uff0c\u53ef\u4ee5\u4f7f\u7528\u5217\u8868\u6216\u5b57\u5178\u3002 \u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u7ed9 loss \u53c2\u6570\u4f20\u9012\u5355\u4e2a\u635f\u5931\u51fd\u6570\uff0c\u8fd9\u4e2a\u635f\u5931\u5c06\u7528\u4e8e\u6240\u6709\u7684\u8f93\u51fa\u3002 model.compile(optimizer='rmsprop', loss='binary_crossentropy', loss_weights=[1., 0.2]) \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4f20\u9012\u8f93\u5165\u6570\u7ec4\u548c\u76ee\u6807\u6570\u7ec4\u7684\u5217\u8868\u6765\u8bad\u7ec3\u6a21\u578b\uff1a model.fit([headline_data, additional_data], [labels, labels], epochs=50, batch_size=32) \u7531\u4e8e\u8f93\u5165\u548c\u8f93\u51fa\u5747\u88ab\u547d\u540d\u4e86\uff08\u5728\u5b9a\u4e49\u65f6\u4f20\u9012\u4e86\u4e00\u4e2a name \u53c2\u6570\uff09\uff0c\u6211\u4eec\u4e5f\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u7f16\u8bd1\u6a21\u578b\uff1a model.compile(optimizer='rmsprop', loss={'main_output': 'binary_crossentropy', 'aux_output': 'binary_crossentropy'}, loss_weights={'main_output': 1., 'aux_output': 0.2}) # \u7136\u540e\u4f7f\u7528\u4ee5\u4e0b\u65b9\u5f0f\u8bad\u7ec3\uff1a model.fit({'main_input': headline_data, 'aux_input': additional_data}, {'main_output': labels, 'aux_output': labels}, epochs=50, batch_size=32) \u5171\u4eab\u7f51\u7edc\u5c42 \u51fd\u6570\u5f0f API \u7684\u53e6\u4e00\u4e2a\u7528\u9014\u662f\u4f7f\u7528\u5171\u4eab\u7f51\u7edc\u5c42\u7684\u6a21\u578b\u3002\u6211\u4eec\u6765\u770b\u770b\u5171\u4eab\u5c42\u3002 \u6765\u8003\u8651\u63a8\u7279\u63a8\u6587\u6570\u636e\u96c6\u3002\u6211\u4eec\u60f3\u8981\u5efa\u7acb\u4e00\u4e2a\u6a21\u578b\u6765\u5206\u8fa8\u4e24\u6761\u63a8\u6587\u662f\u5426\u6765\u81ea\u540c\u4e00\u4e2a\u4eba\uff08\u4f8b\u5982\uff0c\u901a\u8fc7\u63a8\u6587\u7684\u76f8\u4f3c\u6027\u6765\u5bf9\u7528\u6237\u8fdb\u884c\u6bd4\u8f83\uff09\u3002 \u5b9e\u73b0\u8fd9\u4e2a\u76ee\u6807\u7684\u4e00\u79cd\u65b9\u6cd5\u662f\u5efa\u7acb\u4e00\u4e2a\u6a21\u578b\uff0c\u5c06\u4e24\u6761\u63a8\u6587\u7f16\u7801\u6210\u4e24\u4e2a\u5411\u91cf\uff0c\u8fde\u63a5\u5411\u91cf\uff0c\u7136\u540e\u6dfb\u52a0\u903b\u8f91\u56de\u5f52\u5c42\uff1b\u8fd9\u5c06\u8f93\u51fa\u4e24\u6761\u63a8\u6587\u6765\u81ea\u540c\u4e00\u4f5c\u8005\u7684\u6982\u7387\u3002\u6a21\u578b\u5c06\u63a5\u6536\u4e00\u5bf9\u5bf9\u6b63\u8d1f\u8868\u793a\u7684\u63a8\u7279\u6570\u636e\u3002 \u7531\u4e8e\u8fd9\u4e2a\u95ee\u9898\u662f\u5bf9\u79f0\u7684\uff0c\u7f16\u7801\u7b2c\u4e00\u6761\u63a8\u6587\u7684\u673a\u5236\u5e94\u8be5\u88ab\u5b8c\u5168\u91cd\u7528\u6765\u7f16\u7801\u7b2c\u4e8c\u6761\u63a8\u6587\uff08\u6743\u91cd\u53ca\u5176\u4ed6\u5168\u90e8\uff09\u3002\u8fd9\u91cc\u6211\u4eec\u4f7f\u7528\u4e00\u4e2a\u5171\u4eab\u7684 LSTM \u5c42\u6765\u7f16\u7801\u63a8\u6587\u3002 \u8ba9\u6211\u4eec\u4f7f\u7528\u51fd\u6570\u5f0f API \u6765\u6784\u5efa\u5b83\u3002\u9996\u5148\u6211\u4eec\u5c06\u4e00\u6761\u63a8\u7279\u8f6c\u6362\u4e3a\u4e00\u4e2a\u5c3a\u5bf8\u4e3a (280, 256) \u7684\u77e9\u9635\uff0c\u5373\u6bcf\u6761\u63a8\u7279 280 \u5b57\u7b26\uff0c\u6bcf\u4e2a\u5b57\u7b26\u4e3a 256 \u7ef4\u7684 one-hot \u7f16\u7801\u5411\u91cf \uff08\u53d6 256 \u4e2a\u5e38\u7528\u5b57\u7b26\uff09\u3002 import keras from keras.layers import Input, LSTM, Dense from keras.models import Model tweet_a = Input(shape=(280, 256)) tweet_b = Input(shape=(280, 256)) \u8981\u5728\u4e0d\u540c\u7684\u8f93\u5165\u4e0a\u5171\u4eab\u540c\u4e00\u4e2a\u5c42\uff0c\u53ea\u9700\u5b9e\u4f8b\u5316\u8be5\u5c42\u4e00\u6b21\uff0c\u7136\u540e\u6839\u636e\u9700\u8981\u4f20\u5165\u4f60\u60f3\u8981\u7684\u8f93\u5165\u5373\u53ef\uff1a # \u8fd9\u4e00\u5c42\u53ef\u4ee5\u8f93\u5165\u4e00\u4e2a\u77e9\u9635\uff0c\u5e76\u8fd4\u56de\u4e00\u4e2a 64 \u7ef4\u7684\u5411\u91cf shared_lstm = LSTM(64) # \u5f53\u6211\u4eec\u91cd\u7528\u76f8\u540c\u7684\u56fe\u5c42\u5b9e\u4f8b\u591a\u6b21\uff0c\u56fe\u5c42\u7684\u6743\u91cd\u4e5f\u4f1a\u88ab\u91cd\u7528 (\u5b83\u5176\u5b9e\u5c31\u662f\u540c\u4e00\u5c42) encoded_a = shared_lstm(tweet_a) encoded_b = shared_lstm(tweet_b) # \u7136\u540e\u518d\u8fde\u63a5\u4e24\u4e2a\u5411\u91cf\uff1a merged_vector = keras.layers.concatenate([encoded_a, encoded_b], axis=-1) # \u518d\u5728\u4e0a\u9762\u6dfb\u52a0\u4e00\u4e2a\u903b\u8f91\u56de\u5f52\u5c42 predictions = Dense(1, activation='sigmoid')(merged_vector) # \u5b9a\u4e49\u4e00\u4e2a\u8fde\u63a5\u63a8\u7279\u8f93\u5165\u548c\u9884\u6d4b\u7684\u53ef\u8bad\u7ec3\u7684\u6a21\u578b model = Model(inputs=[tweet_a, tweet_b], outputs=predictions) model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy']) model.fit([data_a, data_b], labels, epochs=10) \u8ba9\u6211\u4eec\u6682\u505c\u4e00\u4f1a\uff0c\u770b\u770b\u5982\u4f55\u8bfb\u53d6\u5171\u4eab\u5c42\u7684\u8f93\u51fa\u6216\u8f93\u51fa\u5c3a\u5bf8\u3002 \u5c42\u300c\u8282\u70b9\u300d\u7684\u6982\u5ff5 \u6bcf\u5f53\u4f60\u5728\u67d0\u4e2a\u8f93\u5165\u4e0a\u8c03\u7528\u4e00\u4e2a\u5c42\u65f6\uff0c\u90fd\u5c06\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\uff08\u5c42\u7684\u8f93\u51fa\uff09\uff0c\u5e76\u4e14\u4e3a\u8be5\u5c42\u6dfb\u52a0\u4e00\u4e2a\u300c\u8282\u70b9\u300d\uff0c\u5c06\u8f93\u5165\u5f20\u91cf\u8fde\u63a5\u5230\u8f93\u51fa\u5f20\u91cf\u3002\u5f53\u591a\u6b21\u8c03\u7528\u540c\u4e00\u4e2a\u56fe\u5c42\u65f6\uff0c\u8be5\u56fe\u5c42\u5c06\u62e5\u6709\u591a\u4e2a\u8282\u70b9\u7d22\u5f15 (0, 1, 2...)\u3002 \u5728\u4e4b\u524d\u7248\u672c\u7684 Keras \u4e2d\uff0c\u53ef\u4ee5\u901a\u8fc7 layer.get_output() \u6765\u83b7\u5f97\u5c42\u5b9e\u4f8b\u7684\u8f93\u51fa\u5f20\u91cf\uff0c\u6216\u8005\u901a\u8fc7 layer.output_shape \u6765\u83b7\u53d6\u5176\u8f93\u51fa\u5f62\u72b6\u3002\u73b0\u5728\u4f60\u4f9d\u7136\u53ef\u4ee5\u8fd9\u4e48\u505a\uff08\u9664\u4e86 get_output() \u5df2\u7ecf\u88ab output \u5c5e\u6027\u66ff\u4ee3\uff09\u3002\u4f46\u662f\u5982\u679c\u4e00\u4e2a\u5c42\u4e0e\u591a\u4e2a\u8f93\u5165\u8fde\u63a5\u5462\uff1f \u53ea\u8981\u4e00\u4e2a\u5c42\u4ec5\u4ec5\u8fde\u63a5\u5230\u4e00\u4e2a\u8f93\u5165\uff0c\u5c31\u4e0d\u4f1a\u6709\u56f0\u60d1\uff0c .output \u4f1a\u8fd4\u56de\u5c42\u7684\u552f\u4e00\u8f93\u51fa\uff1a a = Input(shape=(280, 256)) lstm = LSTM(32) encoded_a = lstm(a) assert lstm.output == encoded_a \u4f46\u662f\u5982\u679c\u8be5\u5c42\u6709\u591a\u4e2a\u8f93\u5165\uff0c\u90a3\u5c31\u4f1a\u51fa\u73b0\u95ee\u9898\uff1a a = Input(shape=(280, 256)) b = Input(shape=(280, 256)) lstm = LSTM(32) encoded_a = lstm(a) encoded_b = lstm(b) lstm.output >> AttributeError: Layer lstm_1 has multiple inbound nodes, hence the notion of \"layer output\" is ill-defined. Use `get_output_at(node_index)` instead. \u597d\u5427\uff0c\u901a\u8fc7\u4e0b\u9762\u7684\u65b9\u6cd5\u53ef\u4ee5\u89e3\u51b3\uff1a assert lstm.get_output_at(0) == encoded_a assert lstm.get_output_at(1) == encoded_b \u591f\u7b80\u5355\uff0c\u5bf9\u5427\uff1f input_shape \u548c output_shape \u8fd9\u4e24\u4e2a\u5c5e\u6027\u4e5f\u662f\u5982\u6b64\uff1a\u53ea\u8981\u8be5\u5c42\u53ea\u6709\u4e00\u4e2a\u8282\u70b9\uff0c\u6216\u8005\u53ea\u8981\u6240\u6709\u8282\u70b9\u5177\u6709\u76f8\u540c\u7684\u8f93\u5165/\u8f93\u51fa\u5c3a\u5bf8\uff0c\u90a3\u4e48\u300c\u5c42\u8f93\u51fa/\u8f93\u5165\u5c3a\u5bf8\u300d\u7684\u6982\u5ff5\u5c31\u88ab\u5f88\u597d\u5730\u5b9a\u4e49\uff0c\u5e76\u4e14\u5c06\u7531 layer.output_shape / layer.input_shape \u8fd4\u56de\u3002\u4f46\u662f\u6bd4\u5982\u8bf4\uff0c\u5982\u679c\u5c06\u4e00\u4e2a Conv2D \u5c42\u5148\u5e94\u7528\u4e8e\u5c3a\u5bf8\u4e3a (32\uff0c32\uff0c3) \u7684\u8f93\u5165\uff0c\u518d\u5e94\u7528\u4e8e\u5c3a\u5bf8\u4e3a (64, 64, 3) \u7684\u8f93\u5165\uff0c\u90a3\u4e48\u8fd9\u4e2a\u5c42\u5c31\u4f1a\u6709\u591a\u4e2a\u8f93\u5165/\u8f93\u51fa\u5c3a\u5bf8\uff0c\u4f60\u5c06\u4e0d\u5f97\u4e0d\u901a\u8fc7\u6307\u5b9a\u5b83\u4eec\u6240\u5c5e\u8282\u70b9\u7684\u7d22\u5f15\u6765\u83b7\u53d6\u5b83\u4eec\uff1a a = Input(shape=(32, 32, 3)) b = Input(shape=(64, 64, 3)) conv = Conv2D(16, (3, 3), padding='same') conved_a = conv(a) # \u5230\u76ee\u524d\u4e3a\u6b62\u53ea\u6709\u4e00\u4e2a\u8f93\u5165\uff0c\u4ee5\u4e0b\u53ef\u884c\uff1a assert conv.input_shape == (None, 32, 32, 3) conved_b = conv(b) # \u73b0\u5728 `.input_shape` \u5c5e\u6027\u4e0d\u53ef\u884c\uff0c\u4f46\u662f\u8fd9\u6837\u53ef\u4ee5\uff1a assert conv.get_input_shape_at(0) == (None, 32, 32, 3) assert conv.get_input_shape_at(1) == (None, 64, 64, 3) \u66f4\u591a\u7684\u4f8b\u5b50 \u4ee3\u7801\u793a\u4f8b\u4ecd\u7136\u662f\u8d77\u6b65\u7684\u6700\u4f73\u65b9\u5f0f\uff0c\u6240\u4ee5\u8fd9\u91cc\u8fd8\u6709\u66f4\u591a\u7684\u4f8b\u5b50\u3002 Inception \u6a21\u578b \u6709\u5173 Inception \u7ed3\u6784\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 Going Deeper with Convolutions \u3002 from keras.layers import Conv2D, MaxPooling2D, Input input_img = Input(shape=(256, 256, 3)) tower_1 = Conv2D(64, (1, 1), padding='same', activation='relu')(input_img) tower_1 = Conv2D(64, (3, 3), padding='same', activation='relu')(tower_1) tower_2 = Conv2D(64, (1, 1), padding='same', activation='relu')(input_img) tower_2 = Conv2D(64, (5, 5), padding='same', activation='relu')(tower_2) tower_3 = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(input_img) tower_3 = Conv2D(64, (1, 1), padding='same', activation='relu')(tower_3) output = keras.layers.concatenate([tower_1, tower_2, tower_3], axis=1) \u5377\u79ef\u5c42\u4e0a\u7684\u6b8b\u5dee\u8fde\u63a5 \u6709\u5173\u6b8b\u5dee\u7f51\u7edc (Residual Network) \u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 Deep Residual Learning for Image Recognition \u3002 from keras.layers import Conv2D, Input # \u8f93\u5165\u5f20\u91cf\u4e3a 3 \u901a\u9053 256x256 \u56fe\u50cf x = Input(shape=(256, 256, 3)) # 3 \u8f93\u51fa\u901a\u9053\uff08\u4e0e\u8f93\u5165\u901a\u9053\u76f8\u540c\uff09\u7684 3x3 \u5377\u79ef\u6838 y = Conv2D(3, (3, 3), padding='same')(x) # \u8fd4\u56de x + y z = keras.layers.add([x, y]) \u5171\u4eab\u89c6\u89c9\u6a21\u578b \u8be5\u6a21\u578b\u5728\u4e24\u4e2a\u8f93\u5165\u4e0a\u91cd\u590d\u4f7f\u7528\u540c\u4e00\u4e2a\u56fe\u50cf\u5904\u7406\u6a21\u5757\uff0c\u4ee5\u5224\u65ad\u4e24\u4e2a MNIST \u6570\u5b57\u662f\u5426\u4e3a\u76f8\u540c\u7684\u6570\u5b57\u3002 from keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten from keras.models import Model # \u9996\u5148\uff0c\u5b9a\u4e49\u89c6\u89c9\u6a21\u578b digit_input = Input(shape=(27, 27, 1)) x = Conv2D(64, (3, 3))(digit_input) x = Conv2D(64, (3, 3))(x) x = MaxPooling2D((2, 2))(x) out = Flatten()(x) vision_model = Model(digit_input, out) # \u7136\u540e\uff0c\u5b9a\u4e49\u533a\u5206\u6570\u5b57\u7684\u6a21\u578b digit_a = Input(shape=(27, 27, 1)) digit_b = Input(shape=(27, 27, 1)) # \u89c6\u89c9\u6a21\u578b\u5c06\u88ab\u5171\u4eab\uff0c\u5305\u62ec\u6743\u91cd\u548c\u5176\u4ed6\u6240\u6709 out_a = vision_model(digit_a) out_b = vision_model(digit_b) concatenated = keras.layers.concatenate([out_a, out_b]) out = Dense(1, activation='sigmoid')(concatenated) classification_model = Model([digit_a, digit_b], out) \u89c6\u89c9\u95ee\u7b54\u6a21\u578b \u5f53\u88ab\u95ee\u53ca\u5173\u4e8e\u56fe\u7247\u7684\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u65f6\uff0c\u8be5\u6a21\u578b\u53ef\u4ee5\u9009\u62e9\u6b63\u786e\u7684\u5355\u8bcd\u4f5c\u7b54\u3002 \u5b83\u901a\u8fc7\u5c06\u95ee\u9898\u548c\u56fe\u50cf\u7f16\u7801\u6210\u5411\u91cf\uff0c\u7136\u540e\u8fde\u63a5\u4e24\u8005\uff0c\u5728\u4e0a\u9762\u8bad\u7ec3\u4e00\u4e2a\u903b\u8f91\u56de\u5f52\uff0c\u6765\u4ece\u8bcd\u6c47\u8868\u4e2d\u6311\u9009\u4e00\u4e2a\u53ef\u80fd\u7684\u5355\u8bcd\u4f5c\u7b54\u3002 from keras.layers import Conv2D, MaxPooling2D, Flatten from keras.layers import Input, LSTM, Embedding, Dense from keras.models import Model, Sequential # \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u7528 Sequential \u6765\u5b9a\u4e49\u4e00\u4e2a\u89c6\u89c9\u6a21\u578b\u3002 # \u8fd9\u4e2a\u6a21\u578b\u4f1a\u628a\u4e00\u5f20\u56fe\u50cf\u7f16\u7801\u4e3a\u5411\u91cf\u3002 vision_model = Sequential() vision_model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(224, 224, 3))) vision_model.add(Conv2D(64, (3, 3), activation='relu')) vision_model.add(MaxPooling2D((2, 2))) vision_model.add(Conv2D(128, (3, 3), activation='relu', padding='same')) vision_model.add(Conv2D(128, (3, 3), activation='relu')) vision_model.add(MaxPooling2D((2, 2))) vision_model.add(Conv2D(256, (3, 3), activation='relu', padding='same')) vision_model.add(Conv2D(256, (3, 3), activation='relu')) vision_model.add(Conv2D(256, (3, 3), activation='relu')) vision_model.add(MaxPooling2D((2, 2))) vision_model.add(Flatten()) # \u73b0\u5728\u8ba9\u6211\u4eec\u7528\u89c6\u89c9\u6a21\u578b\u6765\u5f97\u5230\u4e00\u4e2a\u8f93\u51fa\u5f20\u91cf\uff1a image_input = Input(shape=(224, 224, 3)) encoded_image = vision_model(image_input) # \u63a5\u4e0b\u6765\uff0c\u5b9a\u4e49\u4e00\u4e2a\u8bed\u8a00\u6a21\u578b\u6765\u5c06\u95ee\u9898\u7f16\u7801\u6210\u4e00\u4e2a\u5411\u91cf\u3002 # \u6bcf\u4e2a\u95ee\u9898\u6700\u957f 100 \u4e2a\u8bcd\uff0c\u8bcd\u7684\u7d22\u5f15\u4ece 1 \u5230 9999. question_input = Input(shape=(100,), dtype='int32') embedded_question = Embedding(input_dim=10000, output_dim=256, input_length=100)(question_input) encoded_question = LSTM(256)(embedded_question) # \u8fde\u63a5\u95ee\u9898\u5411\u91cf\u548c\u56fe\u50cf\u5411\u91cf\uff1a merged = keras.layers.concatenate([encoded_question, encoded_image]) # \u7136\u540e\u5728\u4e0a\u9762\u8bad\u7ec3\u4e00\u4e2a 1000 \u8bcd\u7684\u903b\u8f91\u56de\u5f52\u6a21\u578b\uff1a output = Dense(1000, activation='softmax')(merged) # \u6700\u7ec8\u6a21\u578b\uff1a vqa_model = Model(inputs=[image_input, question_input], outputs=output) # \u4e0b\u4e00\u6b65\u5c31\u662f\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u8bad\u7ec3\u6a21\u578b\u3002 \u89c6\u9891\u95ee\u7b54\u6a21\u578b \u73b0\u5728\u6211\u4eec\u5df2\u7ecf\u8bad\u7ec3\u4e86\u56fe\u50cf\u95ee\u7b54\u6a21\u578b\uff0c\u6211\u4eec\u53ef\u4ee5\u5f88\u5feb\u5730\u5c06\u5b83\u8f6c\u6362\u4e3a\u89c6\u9891\u95ee\u7b54\u6a21\u578b\u3002\u5728\u9002\u5f53\u7684\u8bad\u7ec3\u4e0b\uff0c\u4f60\u53ef\u4ee5\u7ed9\u5b83\u5c55\u793a\u4e00\u5c0f\u6bb5\u89c6\u9891\uff08\u4f8b\u5982 100 \u5e27\u7684\u4eba\u4f53\u52a8\u4f5c\uff09\uff0c\u7136\u540e\u95ee\u5b83\u4e00\u4e2a\u5173\u4e8e\u8fd9\u6bb5\u89c6\u9891\u7684\u95ee\u9898\uff08\u4f8b\u5982\uff0c\u300c\u8fd9\u4e2a\u4eba\u5728\u505a\u4ec0\u4e48\u8fd0\u52a8\uff1f\u300d -> \u300c\u8db3\u7403\u300d\uff09\u3002 from keras.layers import TimeDistributed video_input = Input(shape=(100, 224, 224, 3)) # \u8fd9\u662f\u57fa\u4e8e\u4e4b\u524d\u5b9a\u4e49\u7684\u89c6\u89c9\u6a21\u578b\uff08\u6743\u91cd\u88ab\u91cd\u7528\uff09\u6784\u5efa\u7684\u89c6\u9891\u7f16\u7801 encoded_frame_sequence = TimeDistributed(vision_model)(video_input) # \u8f93\u51fa\u4e3a\u5411\u91cf\u7684\u5e8f\u5217 encoded_video = LSTM(256)(encoded_frame_sequence) # \u8f93\u51fa\u4e3a\u4e00\u4e2a\u5411\u91cf # \u8fd9\u662f\u95ee\u9898\u7f16\u7801\u5668\u7684\u6a21\u578b\u7ea7\u8868\u793a\uff0c\u91cd\u590d\u4f7f\u7528\u4e0e\u4e4b\u524d\u76f8\u540c\u7684\u6743\u91cd\uff1a question_encoder = Model(inputs=question_input, outputs=encoded_question) # \u8ba9\u6211\u4eec\u7528\u5b83\u6765\u7f16\u7801\u8fd9\u4e2a\u95ee\u9898\uff1a video_question_input = Input(shape=(100,), dtype='int32') encoded_video_question = question_encoder(video_question_input) # \u8fd9\u5c31\u662f\u6211\u4eec\u7684\u89c6\u9891\u95ee\u7b54\u6a21\u5f0f\uff1a merged = keras.layers.concatenate([encoded_video, encoded_video_question]) output = Dense(1000, activation='softmax')(merged) video_qa_model = Model(inputs=[video_input, video_question_input], outputs=output)","title":"\u5f00\u59cb\u4f7f\u7528 Keras \u51fd\u6570\u5f0f API"},{"location":"0-Getting-Started/1.functional-api-guide/#keras-api","text":"Keras \u51fd\u6570\u5f0f API \u662f\u5b9a\u4e49\u590d\u6742\u6a21\u578b\uff08\u5982\u591a\u8f93\u51fa\u6a21\u578b\u3001\u6709\u5411\u65e0\u73af\u56fe\uff0c\u6216\u5177\u6709\u5171\u4eab\u5c42\u7684\u6a21\u578b\uff09\u7684\u65b9\u6cd5\u3002 \u8fd9\u90e8\u5206\u6587\u6863\u5047\u8bbe\u4f60\u5df2\u7ecf\u5bf9 Sequential \u987a\u5e8f\u6a21\u578b\u6bd4\u8f83\u719f\u6089\u3002 \u8ba9\u6211\u4eec\u5148\u4ece\u4e00\u4e9b\u7b80\u5355\u7684\u4f8b\u5b50\u5f00\u59cb\u3002","title":"\u5f00\u59cb\u4f7f\u7528 Keras \u51fd\u6570\u5f0f API"},{"location":"0-Getting-Started/1.functional-api-guide/#_1","text":"Sequential \u6a21\u578b\u53ef\u80fd\u662f\u5b9e\u73b0\u8fd9\u79cd\u7f51\u7edc\u7684\u4e00\u4e2a\u66f4\u597d\u9009\u62e9\uff0c\u4f46\u8fd9\u4e2a\u4f8b\u5b50\u80fd\u591f\u5e2e\u52a9\u6211\u4eec\u8fdb\u884c\u4e00\u4e9b\u7b80\u5355\u7684\u7406\u89e3\u3002 \u7f51\u7edc\u5c42\u7684\u5b9e\u4f8b\u662f\u53ef\u8c03\u7528\u7684\uff0c\u5b83\u4ee5\u5f20\u91cf\u4e3a\u53c2\u6570\uff0c\u5e76\u4e14\u8fd4\u56de\u4e00\u4e2a\u5f20\u91cf \u8f93\u5165\u548c\u8f93\u51fa\u5747\u4e3a\u5f20\u91cf\uff0c\u5b83\u4eec\u90fd\u53ef\u4ee5\u7528\u6765\u5b9a\u4e49\u4e00\u4e2a\u6a21\u578b\uff08 Model \uff09 \u8fd9\u6837\u7684\u6a21\u578b\u540c Keras \u7684 Sequential \u6a21\u578b\u4e00\u6837\uff0c\u90fd\u53ef\u4ee5\u88ab\u8bad\u7ec3 from keras.layers import Input, Dense from keras.models import Model # \u8fd9\u90e8\u5206\u8fd4\u56de\u4e00\u4e2a\u5f20\u91cf inputs = Input(shape=(784,)) # \u5c42\u7684\u5b9e\u4f8b\u662f\u53ef\u8c03\u7528\u7684\uff0c\u5b83\u4ee5\u5f20\u91cf\u4e3a\u53c2\u6570\uff0c\u5e76\u4e14\u8fd4\u56de\u4e00\u4e2a\u5f20\u91cf x = Dense(64, activation='relu')(inputs) x = Dense(64, activation='relu')(x) predictions = Dense(10, activation='softmax')(x) # \u8fd9\u90e8\u5206\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u8f93\u5165\u5c42\u548c\u4e09\u4e2a\u5168\u8fde\u63a5\u5c42\u7684\u6a21\u578b model = Model(inputs=inputs, outputs=predictions) model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) model.fit(data, labels) # \u5f00\u59cb\u8bad\u7ec3","title":"\u4f8b\u4e00\uff1a\u5168\u8fde\u63a5\u7f51\u7edc"},{"location":"0-Getting-Started/1.functional-api-guide/#_2","text":"\u5229\u7528\u51fd\u6570\u5f0f API\uff0c\u53ef\u4ee5\u8f7b\u6613\u5730\u91cd\u7528\u8bad\u7ec3\u597d\u7684\u6a21\u578b\uff1a\u53ef\u4ee5\u5c06\u4efb\u4f55\u6a21\u578b\u770b\u4f5c\u662f\u4e00\u4e2a\u5c42\uff0c\u7136\u540e\u901a\u8fc7\u4f20\u9012\u4e00\u4e2a\u5f20\u91cf\u6765\u8c03\u7528\u5b83\u3002\u6ce8\u610f\uff0c\u5728\u8c03\u7528\u6a21\u578b\u65f6\uff0c\u60a8\u4e0d\u4ec5\u91cd\u7528\u6a21\u578b\u7684 \u7ed3\u6784 \uff0c\u8fd8\u91cd\u7528\u4e86\u5b83\u7684\u6743\u91cd\u3002 x = Input(shape=(784,)) # \u8fd9\u662f\u53ef\u884c\u7684\uff0c\u5e76\u4e14\u8fd4\u56de\u4e0a\u9762\u5b9a\u4e49\u7684 10-way softmax\u3002 y = model(x) \u8fd9\u79cd\u65b9\u5f0f\u80fd\u5141\u8bb8\u6211\u4eec\u5feb\u901f\u521b\u5efa\u53ef\u4ee5\u5904\u7406 \u5e8f\u5217\u8f93\u5165 \u7684\u6a21\u578b\u3002\u53ea\u9700\u4e00\u884c\u4ee3\u7801\uff0c\u4f60\u5c31\u5c06\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u8f6c\u6362\u4e3a\u89c6\u9891\u5206\u7c7b\u6a21\u578b\u3002 from keras.layers import TimeDistributed # \u8f93\u5165\u5f20\u91cf\u662f 20 \u4e2a\u65f6\u95f4\u6b65\u7684\u5e8f\u5217\uff0c # \u6bcf\u4e00\u4e2a\u65f6\u95f4\u4e3a\u4e00\u4e2a 784 \u7ef4\u7684\u5411\u91cf input_sequences = Input(shape=(20, 784)) # \u8fd9\u90e8\u5206\u5c06\u6211\u4eec\u4e4b\u524d\u5b9a\u4e49\u7684\u6a21\u578b\u5e94\u7528\u4e8e\u8f93\u5165\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2a\u65f6\u95f4\u6b65\u3002 # \u4e4b\u524d\u5b9a\u4e49\u7684\u6a21\u578b\u7684\u8f93\u51fa\u662f\u4e00\u4e2a 10-way softmax\uff0c # \u56e0\u800c\u4e0b\u9762\u7684\u5c42\u7684\u8f93\u51fa\u5c06\u662f\u7ef4\u5ea6\u4e3a 10 \u7684 20 \u4e2a\u5411\u91cf\u7684\u5e8f\u5217\u3002 processed_sequences = TimeDistributed(model)(input_sequences)","title":"\u6240\u6709\u7684\u6a21\u578b\u90fd\u53ef\u8c03\u7528\uff0c\u5c31\u50cf\u7f51\u7edc\u5c42\u4e00\u6837"},{"location":"0-Getting-Started/1.functional-api-guide/#_3","text":"\u4ee5\u4e0b\u662f\u51fd\u6570\u5f0f API \u7684\u4e00\u4e2a\u5f88\u597d\u7684\u4f8b\u5b50\uff1a\u5177\u6709\u591a\u4e2a\u8f93\u5165\u548c\u8f93\u51fa\u7684\u6a21\u578b\u3002\u51fd\u6570\u5f0f API \u4f7f\u5904\u7406\u5927\u91cf\u4ea4\u7ec7\u7684\u6570\u636e\u6d41\u53d8\u5f97\u5bb9\u6613\u3002 \u6765\u8003\u8651\u4e0b\u9762\u7684\u6a21\u578b\u3002\u6211\u4eec\u8bd5\u56fe\u9884\u6d4b Twitter \u4e0a\u7684\u4e00\u6761\u65b0\u95fb\u6807\u9898\u6709\u591a\u5c11\u8f6c\u53d1\u548c\u70b9\u8d5e\u6570\u3002\u6a21\u578b\u7684\u4e3b\u8981\u8f93\u5165\u5c06\u662f\u65b0\u95fb\u6807\u9898\u672c\u8eab\uff0c\u5373\u4e00\u7cfb\u5217\u8bcd\u8bed\uff0c\u4f46\u662f\u4e3a\u4e86\u589e\u6dfb\u8da3\u5473\uff0c\u6211\u4eec\u7684\u6a21\u578b\u8fd8\u6dfb\u52a0\u4e86\u5176\u4ed6\u7684\u8f85\u52a9\u8f93\u5165\u6765\u63a5\u6536\u989d\u5916\u7684\u6570\u636e\uff0c\u4f8b\u5982\u65b0\u95fb\u6807\u9898\u7684\u53d1\u5e03\u7684\u65f6\u95f4\u7b49\u3002 \u8be5\u6a21\u578b\u4e5f\u5c06\u901a\u8fc7\u4e24\u4e2a\u635f\u5931\u51fd\u6570\u8fdb\u884c\u76d1\u7763\u5b66\u4e60\u3002\u8f83\u65e9\u5730\u5728\u6a21\u578b\u4e2d\u4f7f\u7528\u4e3b\u635f\u5931\u51fd\u6570\uff0c\u662f\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u4e00\u4e2a\u826f\u597d\u6b63\u5219\u65b9\u6cd5\u3002 \u6a21\u578b\u7ed3\u6784\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u8ba9\u6211\u4eec\u7528\u51fd\u6570\u5f0f API \u6765\u5b9e\u73b0\u5b83\u3002 \u4e3b\u8981\u8f93\u5165\u63a5\u6536\u65b0\u95fb\u6807\u9898\u672c\u8eab\uff0c\u5373\u4e00\u4e2a\u6574\u6570\u5e8f\u5217\uff08\u6bcf\u4e2a\u6574\u6570\u7f16\u7801\u4e00\u4e2a\u8bcd\uff09\u3002 \u8fd9\u4e9b\u6574\u6570\u5728 1 \u5230 10,000 \u4e4b\u95f4\uff0810,000 \u4e2a\u8bcd\u7684\u8bcd\u6c47\u8868\uff09\uff0c\u4e14\u5e8f\u5217\u957f\u5ea6\u4e3a 100 \u4e2a\u8bcd\u3002 from keras.layers import Input, Embedding, LSTM, Dense from keras.models import Model # \u6807\u9898\u8f93\u5165\uff1a\u63a5\u6536\u4e00\u4e2a\u542b\u6709 100 \u4e2a\u6574\u6570\u7684\u5e8f\u5217\uff0c\u6bcf\u4e2a\u6574\u6570\u5728 1 \u5230 10000 \u4e4b\u95f4\u3002 # \u6ce8\u610f\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4f20\u9012\u4e00\u4e2a \"name\" \u53c2\u6570\u6765\u547d\u540d\u4efb\u4f55\u5c42\u3002 main_input = Input(shape=(100,), dtype='int32', name='main_input') # Embedding \u5c42\u5c06\u8f93\u5165\u5e8f\u5217\u7f16\u7801\u4e3a\u4e00\u4e2a\u7a20\u5bc6\u5411\u91cf\u7684\u5e8f\u5217\uff0c # \u6bcf\u4e2a\u5411\u91cf\u7ef4\u5ea6\u4e3a 512\u3002 x = Embedding(output_dim=512, input_dim=10000, input_length=100)(main_input) # LSTM \u5c42\u628a\u5411\u91cf\u5e8f\u5217\u8f6c\u6362\u6210\u5355\u4e2a\u5411\u91cf\uff0c # \u5b83\u5305\u542b\u6574\u4e2a\u5e8f\u5217\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f lstm_out = LSTM(32)(x) \u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u63d2\u5165\u8f85\u52a9\u635f\u5931\uff0c\u4f7f\u5f97\u5373\u4f7f\u5728\u6a21\u578b\u4e3b\u635f\u5931\u5f88\u9ad8\u7684\u60c5\u51b5\u4e0b\uff0cLSTM \u5c42\u548c Embedding \u5c42\u90fd\u80fd\u88ab\u5e73\u7a33\u5730\u8bad\u7ec3\u3002 auxiliary_output = Dense(1, activation='sigmoid', name='aux_output')(lstm_out) \u6b64\u65f6\uff0c\u6211\u4eec\u5c06\u8f85\u52a9\u8f93\u5165\u6570\u636e\u4e0e LSTM \u5c42\u7684\u8f93\u51fa\u8fde\u63a5\u8d77\u6765\uff0c\u8f93\u5165\u5230\u6a21\u578b\u4e2d\uff1a auxiliary_input = Input(shape=(5,), name='aux_input') x = keras.layers.concatenate([lstm_out, auxiliary_input]) # \u5806\u53e0\u591a\u4e2a\u5168\u8fde\u63a5\u7f51\u7edc\u5c42 x = Dense(64, activation='relu')(x) x = Dense(64, activation='relu')(x) x = Dense(64, activation='relu')(x) # \u6700\u540e\u6dfb\u52a0\u4e3b\u8981\u7684\u903b\u8f91\u56de\u5f52\u5c42 main_output = Dense(1, activation='sigmoid', name='main_output')(x) \u7136\u540e\u5b9a\u4e49\u4e00\u4e2a\u5177\u6709\u4e24\u4e2a\u8f93\u5165\u548c\u4e24\u4e2a\u8f93\u51fa\u7684\u6a21\u578b\uff1a model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output, auxiliary_output]) \u73b0\u5728\u7f16\u8bd1\u6a21\u578b\uff0c\u5e76\u7ed9\u8f85\u52a9\u635f\u5931\u5206\u914d\u4e00\u4e2a 0.2 \u7684\u6743\u91cd\u3002\u5982\u679c\u8981\u4e3a\u4e0d\u540c\u7684\u8f93\u51fa\u6307\u5b9a\u4e0d\u540c\u7684 loss_weights \u6216 loss \uff0c\u53ef\u4ee5\u4f7f\u7528\u5217\u8868\u6216\u5b57\u5178\u3002 \u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u7ed9 loss \u53c2\u6570\u4f20\u9012\u5355\u4e2a\u635f\u5931\u51fd\u6570\uff0c\u8fd9\u4e2a\u635f\u5931\u5c06\u7528\u4e8e\u6240\u6709\u7684\u8f93\u51fa\u3002 model.compile(optimizer='rmsprop', loss='binary_crossentropy', loss_weights=[1., 0.2]) \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4f20\u9012\u8f93\u5165\u6570\u7ec4\u548c\u76ee\u6807\u6570\u7ec4\u7684\u5217\u8868\u6765\u8bad\u7ec3\u6a21\u578b\uff1a model.fit([headline_data, additional_data], [labels, labels], epochs=50, batch_size=32) \u7531\u4e8e\u8f93\u5165\u548c\u8f93\u51fa\u5747\u88ab\u547d\u540d\u4e86\uff08\u5728\u5b9a\u4e49\u65f6\u4f20\u9012\u4e86\u4e00\u4e2a name \u53c2\u6570\uff09\uff0c\u6211\u4eec\u4e5f\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u7f16\u8bd1\u6a21\u578b\uff1a model.compile(optimizer='rmsprop', loss={'main_output': 'binary_crossentropy', 'aux_output': 'binary_crossentropy'}, loss_weights={'main_output': 1., 'aux_output': 0.2}) # \u7136\u540e\u4f7f\u7528\u4ee5\u4e0b\u65b9\u5f0f\u8bad\u7ec3\uff1a model.fit({'main_input': headline_data, 'aux_input': additional_data}, {'main_output': labels, 'aux_output': labels}, epochs=50, batch_size=32)","title":"\u591a\u8f93\u5165\u591a\u8f93\u51fa\u6a21\u578b"},{"location":"0-Getting-Started/1.functional-api-guide/#_4","text":"\u51fd\u6570\u5f0f API \u7684\u53e6\u4e00\u4e2a\u7528\u9014\u662f\u4f7f\u7528\u5171\u4eab\u7f51\u7edc\u5c42\u7684\u6a21\u578b\u3002\u6211\u4eec\u6765\u770b\u770b\u5171\u4eab\u5c42\u3002 \u6765\u8003\u8651\u63a8\u7279\u63a8\u6587\u6570\u636e\u96c6\u3002\u6211\u4eec\u60f3\u8981\u5efa\u7acb\u4e00\u4e2a\u6a21\u578b\u6765\u5206\u8fa8\u4e24\u6761\u63a8\u6587\u662f\u5426\u6765\u81ea\u540c\u4e00\u4e2a\u4eba\uff08\u4f8b\u5982\uff0c\u901a\u8fc7\u63a8\u6587\u7684\u76f8\u4f3c\u6027\u6765\u5bf9\u7528\u6237\u8fdb\u884c\u6bd4\u8f83\uff09\u3002 \u5b9e\u73b0\u8fd9\u4e2a\u76ee\u6807\u7684\u4e00\u79cd\u65b9\u6cd5\u662f\u5efa\u7acb\u4e00\u4e2a\u6a21\u578b\uff0c\u5c06\u4e24\u6761\u63a8\u6587\u7f16\u7801\u6210\u4e24\u4e2a\u5411\u91cf\uff0c\u8fde\u63a5\u5411\u91cf\uff0c\u7136\u540e\u6dfb\u52a0\u903b\u8f91\u56de\u5f52\u5c42\uff1b\u8fd9\u5c06\u8f93\u51fa\u4e24\u6761\u63a8\u6587\u6765\u81ea\u540c\u4e00\u4f5c\u8005\u7684\u6982\u7387\u3002\u6a21\u578b\u5c06\u63a5\u6536\u4e00\u5bf9\u5bf9\u6b63\u8d1f\u8868\u793a\u7684\u63a8\u7279\u6570\u636e\u3002 \u7531\u4e8e\u8fd9\u4e2a\u95ee\u9898\u662f\u5bf9\u79f0\u7684\uff0c\u7f16\u7801\u7b2c\u4e00\u6761\u63a8\u6587\u7684\u673a\u5236\u5e94\u8be5\u88ab\u5b8c\u5168\u91cd\u7528\u6765\u7f16\u7801\u7b2c\u4e8c\u6761\u63a8\u6587\uff08\u6743\u91cd\u53ca\u5176\u4ed6\u5168\u90e8\uff09\u3002\u8fd9\u91cc\u6211\u4eec\u4f7f\u7528\u4e00\u4e2a\u5171\u4eab\u7684 LSTM \u5c42\u6765\u7f16\u7801\u63a8\u6587\u3002 \u8ba9\u6211\u4eec\u4f7f\u7528\u51fd\u6570\u5f0f API \u6765\u6784\u5efa\u5b83\u3002\u9996\u5148\u6211\u4eec\u5c06\u4e00\u6761\u63a8\u7279\u8f6c\u6362\u4e3a\u4e00\u4e2a\u5c3a\u5bf8\u4e3a (280, 256) \u7684\u77e9\u9635\uff0c\u5373\u6bcf\u6761\u63a8\u7279 280 \u5b57\u7b26\uff0c\u6bcf\u4e2a\u5b57\u7b26\u4e3a 256 \u7ef4\u7684 one-hot \u7f16\u7801\u5411\u91cf \uff08\u53d6 256 \u4e2a\u5e38\u7528\u5b57\u7b26\uff09\u3002 import keras from keras.layers import Input, LSTM, Dense from keras.models import Model tweet_a = Input(shape=(280, 256)) tweet_b = Input(shape=(280, 256)) \u8981\u5728\u4e0d\u540c\u7684\u8f93\u5165\u4e0a\u5171\u4eab\u540c\u4e00\u4e2a\u5c42\uff0c\u53ea\u9700\u5b9e\u4f8b\u5316\u8be5\u5c42\u4e00\u6b21\uff0c\u7136\u540e\u6839\u636e\u9700\u8981\u4f20\u5165\u4f60\u60f3\u8981\u7684\u8f93\u5165\u5373\u53ef\uff1a # \u8fd9\u4e00\u5c42\u53ef\u4ee5\u8f93\u5165\u4e00\u4e2a\u77e9\u9635\uff0c\u5e76\u8fd4\u56de\u4e00\u4e2a 64 \u7ef4\u7684\u5411\u91cf shared_lstm = LSTM(64) # \u5f53\u6211\u4eec\u91cd\u7528\u76f8\u540c\u7684\u56fe\u5c42\u5b9e\u4f8b\u591a\u6b21\uff0c\u56fe\u5c42\u7684\u6743\u91cd\u4e5f\u4f1a\u88ab\u91cd\u7528 (\u5b83\u5176\u5b9e\u5c31\u662f\u540c\u4e00\u5c42) encoded_a = shared_lstm(tweet_a) encoded_b = shared_lstm(tweet_b) # \u7136\u540e\u518d\u8fde\u63a5\u4e24\u4e2a\u5411\u91cf\uff1a merged_vector = keras.layers.concatenate([encoded_a, encoded_b], axis=-1) # \u518d\u5728\u4e0a\u9762\u6dfb\u52a0\u4e00\u4e2a\u903b\u8f91\u56de\u5f52\u5c42 predictions = Dense(1, activation='sigmoid')(merged_vector) # \u5b9a\u4e49\u4e00\u4e2a\u8fde\u63a5\u63a8\u7279\u8f93\u5165\u548c\u9884\u6d4b\u7684\u53ef\u8bad\u7ec3\u7684\u6a21\u578b model = Model(inputs=[tweet_a, tweet_b], outputs=predictions) model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy']) model.fit([data_a, data_b], labels, epochs=10) \u8ba9\u6211\u4eec\u6682\u505c\u4e00\u4f1a\uff0c\u770b\u770b\u5982\u4f55\u8bfb\u53d6\u5171\u4eab\u5c42\u7684\u8f93\u51fa\u6216\u8f93\u51fa\u5c3a\u5bf8\u3002","title":"\u5171\u4eab\u7f51\u7edc\u5c42"},{"location":"0-Getting-Started/1.functional-api-guide/#_5","text":"\u6bcf\u5f53\u4f60\u5728\u67d0\u4e2a\u8f93\u5165\u4e0a\u8c03\u7528\u4e00\u4e2a\u5c42\u65f6\uff0c\u90fd\u5c06\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\uff08\u5c42\u7684\u8f93\u51fa\uff09\uff0c\u5e76\u4e14\u4e3a\u8be5\u5c42\u6dfb\u52a0\u4e00\u4e2a\u300c\u8282\u70b9\u300d\uff0c\u5c06\u8f93\u5165\u5f20\u91cf\u8fde\u63a5\u5230\u8f93\u51fa\u5f20\u91cf\u3002\u5f53\u591a\u6b21\u8c03\u7528\u540c\u4e00\u4e2a\u56fe\u5c42\u65f6\uff0c\u8be5\u56fe\u5c42\u5c06\u62e5\u6709\u591a\u4e2a\u8282\u70b9\u7d22\u5f15 (0, 1, 2...)\u3002 \u5728\u4e4b\u524d\u7248\u672c\u7684 Keras \u4e2d\uff0c\u53ef\u4ee5\u901a\u8fc7 layer.get_output() \u6765\u83b7\u5f97\u5c42\u5b9e\u4f8b\u7684\u8f93\u51fa\u5f20\u91cf\uff0c\u6216\u8005\u901a\u8fc7 layer.output_shape \u6765\u83b7\u53d6\u5176\u8f93\u51fa\u5f62\u72b6\u3002\u73b0\u5728\u4f60\u4f9d\u7136\u53ef\u4ee5\u8fd9\u4e48\u505a\uff08\u9664\u4e86 get_output() \u5df2\u7ecf\u88ab output \u5c5e\u6027\u66ff\u4ee3\uff09\u3002\u4f46\u662f\u5982\u679c\u4e00\u4e2a\u5c42\u4e0e\u591a\u4e2a\u8f93\u5165\u8fde\u63a5\u5462\uff1f \u53ea\u8981\u4e00\u4e2a\u5c42\u4ec5\u4ec5\u8fde\u63a5\u5230\u4e00\u4e2a\u8f93\u5165\uff0c\u5c31\u4e0d\u4f1a\u6709\u56f0\u60d1\uff0c .output \u4f1a\u8fd4\u56de\u5c42\u7684\u552f\u4e00\u8f93\u51fa\uff1a a = Input(shape=(280, 256)) lstm = LSTM(32) encoded_a = lstm(a) assert lstm.output == encoded_a \u4f46\u662f\u5982\u679c\u8be5\u5c42\u6709\u591a\u4e2a\u8f93\u5165\uff0c\u90a3\u5c31\u4f1a\u51fa\u73b0\u95ee\u9898\uff1a a = Input(shape=(280, 256)) b = Input(shape=(280, 256)) lstm = LSTM(32) encoded_a = lstm(a) encoded_b = lstm(b) lstm.output >> AttributeError: Layer lstm_1 has multiple inbound nodes, hence the notion of \"layer output\" is ill-defined. Use `get_output_at(node_index)` instead. \u597d\u5427\uff0c\u901a\u8fc7\u4e0b\u9762\u7684\u65b9\u6cd5\u53ef\u4ee5\u89e3\u51b3\uff1a assert lstm.get_output_at(0) == encoded_a assert lstm.get_output_at(1) == encoded_b \u591f\u7b80\u5355\uff0c\u5bf9\u5427\uff1f input_shape \u548c output_shape \u8fd9\u4e24\u4e2a\u5c5e\u6027\u4e5f\u662f\u5982\u6b64\uff1a\u53ea\u8981\u8be5\u5c42\u53ea\u6709\u4e00\u4e2a\u8282\u70b9\uff0c\u6216\u8005\u53ea\u8981\u6240\u6709\u8282\u70b9\u5177\u6709\u76f8\u540c\u7684\u8f93\u5165/\u8f93\u51fa\u5c3a\u5bf8\uff0c\u90a3\u4e48\u300c\u5c42\u8f93\u51fa/\u8f93\u5165\u5c3a\u5bf8\u300d\u7684\u6982\u5ff5\u5c31\u88ab\u5f88\u597d\u5730\u5b9a\u4e49\uff0c\u5e76\u4e14\u5c06\u7531 layer.output_shape / layer.input_shape \u8fd4\u56de\u3002\u4f46\u662f\u6bd4\u5982\u8bf4\uff0c\u5982\u679c\u5c06\u4e00\u4e2a Conv2D \u5c42\u5148\u5e94\u7528\u4e8e\u5c3a\u5bf8\u4e3a (32\uff0c32\uff0c3) \u7684\u8f93\u5165\uff0c\u518d\u5e94\u7528\u4e8e\u5c3a\u5bf8\u4e3a (64, 64, 3) \u7684\u8f93\u5165\uff0c\u90a3\u4e48\u8fd9\u4e2a\u5c42\u5c31\u4f1a\u6709\u591a\u4e2a\u8f93\u5165/\u8f93\u51fa\u5c3a\u5bf8\uff0c\u4f60\u5c06\u4e0d\u5f97\u4e0d\u901a\u8fc7\u6307\u5b9a\u5b83\u4eec\u6240\u5c5e\u8282\u70b9\u7684\u7d22\u5f15\u6765\u83b7\u53d6\u5b83\u4eec\uff1a a = Input(shape=(32, 32, 3)) b = Input(shape=(64, 64, 3)) conv = Conv2D(16, (3, 3), padding='same') conved_a = conv(a) # \u5230\u76ee\u524d\u4e3a\u6b62\u53ea\u6709\u4e00\u4e2a\u8f93\u5165\uff0c\u4ee5\u4e0b\u53ef\u884c\uff1a assert conv.input_shape == (None, 32, 32, 3) conved_b = conv(b) # \u73b0\u5728 `.input_shape` \u5c5e\u6027\u4e0d\u53ef\u884c\uff0c\u4f46\u662f\u8fd9\u6837\u53ef\u4ee5\uff1a assert conv.get_input_shape_at(0) == (None, 32, 32, 3) assert conv.get_input_shape_at(1) == (None, 64, 64, 3)","title":"\u5c42\u300c\u8282\u70b9\u300d\u7684\u6982\u5ff5"},{"location":"0-Getting-Started/1.functional-api-guide/#_6","text":"\u4ee3\u7801\u793a\u4f8b\u4ecd\u7136\u662f\u8d77\u6b65\u7684\u6700\u4f73\u65b9\u5f0f\uff0c\u6240\u4ee5\u8fd9\u91cc\u8fd8\u6709\u66f4\u591a\u7684\u4f8b\u5b50\u3002","title":"\u66f4\u591a\u7684\u4f8b\u5b50"},{"location":"0-Getting-Started/1.functional-api-guide/#inception","text":"\u6709\u5173 Inception \u7ed3\u6784\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 Going Deeper with Convolutions \u3002 from keras.layers import Conv2D, MaxPooling2D, Input input_img = Input(shape=(256, 256, 3)) tower_1 = Conv2D(64, (1, 1), padding='same', activation='relu')(input_img) tower_1 = Conv2D(64, (3, 3), padding='same', activation='relu')(tower_1) tower_2 = Conv2D(64, (1, 1), padding='same', activation='relu')(input_img) tower_2 = Conv2D(64, (5, 5), padding='same', activation='relu')(tower_2) tower_3 = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(input_img) tower_3 = Conv2D(64, (1, 1), padding='same', activation='relu')(tower_3) output = keras.layers.concatenate([tower_1, tower_2, tower_3], axis=1)","title":"Inception \u6a21\u578b"},{"location":"0-Getting-Started/1.functional-api-guide/#_7","text":"\u6709\u5173\u6b8b\u5dee\u7f51\u7edc (Residual Network) \u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 Deep Residual Learning for Image Recognition \u3002 from keras.layers import Conv2D, Input # \u8f93\u5165\u5f20\u91cf\u4e3a 3 \u901a\u9053 256x256 \u56fe\u50cf x = Input(shape=(256, 256, 3)) # 3 \u8f93\u51fa\u901a\u9053\uff08\u4e0e\u8f93\u5165\u901a\u9053\u76f8\u540c\uff09\u7684 3x3 \u5377\u79ef\u6838 y = Conv2D(3, (3, 3), padding='same')(x) # \u8fd4\u56de x + y z = keras.layers.add([x, y])","title":"\u5377\u79ef\u5c42\u4e0a\u7684\u6b8b\u5dee\u8fde\u63a5"},{"location":"0-Getting-Started/1.functional-api-guide/#_8","text":"\u8be5\u6a21\u578b\u5728\u4e24\u4e2a\u8f93\u5165\u4e0a\u91cd\u590d\u4f7f\u7528\u540c\u4e00\u4e2a\u56fe\u50cf\u5904\u7406\u6a21\u5757\uff0c\u4ee5\u5224\u65ad\u4e24\u4e2a MNIST \u6570\u5b57\u662f\u5426\u4e3a\u76f8\u540c\u7684\u6570\u5b57\u3002 from keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten from keras.models import Model # \u9996\u5148\uff0c\u5b9a\u4e49\u89c6\u89c9\u6a21\u578b digit_input = Input(shape=(27, 27, 1)) x = Conv2D(64, (3, 3))(digit_input) x = Conv2D(64, (3, 3))(x) x = MaxPooling2D((2, 2))(x) out = Flatten()(x) vision_model = Model(digit_input, out) # \u7136\u540e\uff0c\u5b9a\u4e49\u533a\u5206\u6570\u5b57\u7684\u6a21\u578b digit_a = Input(shape=(27, 27, 1)) digit_b = Input(shape=(27, 27, 1)) # \u89c6\u89c9\u6a21\u578b\u5c06\u88ab\u5171\u4eab\uff0c\u5305\u62ec\u6743\u91cd\u548c\u5176\u4ed6\u6240\u6709 out_a = vision_model(digit_a) out_b = vision_model(digit_b) concatenated = keras.layers.concatenate([out_a, out_b]) out = Dense(1, activation='sigmoid')(concatenated) classification_model = Model([digit_a, digit_b], out)","title":"\u5171\u4eab\u89c6\u89c9\u6a21\u578b"},{"location":"0-Getting-Started/1.functional-api-guide/#_9","text":"\u5f53\u88ab\u95ee\u53ca\u5173\u4e8e\u56fe\u7247\u7684\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u65f6\uff0c\u8be5\u6a21\u578b\u53ef\u4ee5\u9009\u62e9\u6b63\u786e\u7684\u5355\u8bcd\u4f5c\u7b54\u3002 \u5b83\u901a\u8fc7\u5c06\u95ee\u9898\u548c\u56fe\u50cf\u7f16\u7801\u6210\u5411\u91cf\uff0c\u7136\u540e\u8fde\u63a5\u4e24\u8005\uff0c\u5728\u4e0a\u9762\u8bad\u7ec3\u4e00\u4e2a\u903b\u8f91\u56de\u5f52\uff0c\u6765\u4ece\u8bcd\u6c47\u8868\u4e2d\u6311\u9009\u4e00\u4e2a\u53ef\u80fd\u7684\u5355\u8bcd\u4f5c\u7b54\u3002 from keras.layers import Conv2D, MaxPooling2D, Flatten from keras.layers import Input, LSTM, Embedding, Dense from keras.models import Model, Sequential # \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u7528 Sequential \u6765\u5b9a\u4e49\u4e00\u4e2a\u89c6\u89c9\u6a21\u578b\u3002 # \u8fd9\u4e2a\u6a21\u578b\u4f1a\u628a\u4e00\u5f20\u56fe\u50cf\u7f16\u7801\u4e3a\u5411\u91cf\u3002 vision_model = Sequential() vision_model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(224, 224, 3))) vision_model.add(Conv2D(64, (3, 3), activation='relu')) vision_model.add(MaxPooling2D((2, 2))) vision_model.add(Conv2D(128, (3, 3), activation='relu', padding='same')) vision_model.add(Conv2D(128, (3, 3), activation='relu')) vision_model.add(MaxPooling2D((2, 2))) vision_model.add(Conv2D(256, (3, 3), activation='relu', padding='same')) vision_model.add(Conv2D(256, (3, 3), activation='relu')) vision_model.add(Conv2D(256, (3, 3), activation='relu')) vision_model.add(MaxPooling2D((2, 2))) vision_model.add(Flatten()) # \u73b0\u5728\u8ba9\u6211\u4eec\u7528\u89c6\u89c9\u6a21\u578b\u6765\u5f97\u5230\u4e00\u4e2a\u8f93\u51fa\u5f20\u91cf\uff1a image_input = Input(shape=(224, 224, 3)) encoded_image = vision_model(image_input) # \u63a5\u4e0b\u6765\uff0c\u5b9a\u4e49\u4e00\u4e2a\u8bed\u8a00\u6a21\u578b\u6765\u5c06\u95ee\u9898\u7f16\u7801\u6210\u4e00\u4e2a\u5411\u91cf\u3002 # \u6bcf\u4e2a\u95ee\u9898\u6700\u957f 100 \u4e2a\u8bcd\uff0c\u8bcd\u7684\u7d22\u5f15\u4ece 1 \u5230 9999. question_input = Input(shape=(100,), dtype='int32') embedded_question = Embedding(input_dim=10000, output_dim=256, input_length=100)(question_input) encoded_question = LSTM(256)(embedded_question) # \u8fde\u63a5\u95ee\u9898\u5411\u91cf\u548c\u56fe\u50cf\u5411\u91cf\uff1a merged = keras.layers.concatenate([encoded_question, encoded_image]) # \u7136\u540e\u5728\u4e0a\u9762\u8bad\u7ec3\u4e00\u4e2a 1000 \u8bcd\u7684\u903b\u8f91\u56de\u5f52\u6a21\u578b\uff1a output = Dense(1000, activation='softmax')(merged) # \u6700\u7ec8\u6a21\u578b\uff1a vqa_model = Model(inputs=[image_input, question_input], outputs=output) # \u4e0b\u4e00\u6b65\u5c31\u662f\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u8bad\u7ec3\u6a21\u578b\u3002","title":"\u89c6\u89c9\u95ee\u7b54\u6a21\u578b"},{"location":"0-Getting-Started/1.functional-api-guide/#_10","text":"\u73b0\u5728\u6211\u4eec\u5df2\u7ecf\u8bad\u7ec3\u4e86\u56fe\u50cf\u95ee\u7b54\u6a21\u578b\uff0c\u6211\u4eec\u53ef\u4ee5\u5f88\u5feb\u5730\u5c06\u5b83\u8f6c\u6362\u4e3a\u89c6\u9891\u95ee\u7b54\u6a21\u578b\u3002\u5728\u9002\u5f53\u7684\u8bad\u7ec3\u4e0b\uff0c\u4f60\u53ef\u4ee5\u7ed9\u5b83\u5c55\u793a\u4e00\u5c0f\u6bb5\u89c6\u9891\uff08\u4f8b\u5982 100 \u5e27\u7684\u4eba\u4f53\u52a8\u4f5c\uff09\uff0c\u7136\u540e\u95ee\u5b83\u4e00\u4e2a\u5173\u4e8e\u8fd9\u6bb5\u89c6\u9891\u7684\u95ee\u9898\uff08\u4f8b\u5982\uff0c\u300c\u8fd9\u4e2a\u4eba\u5728\u505a\u4ec0\u4e48\u8fd0\u52a8\uff1f\u300d -> \u300c\u8db3\u7403\u300d\uff09\u3002 from keras.layers import TimeDistributed video_input = Input(shape=(100, 224, 224, 3)) # \u8fd9\u662f\u57fa\u4e8e\u4e4b\u524d\u5b9a\u4e49\u7684\u89c6\u89c9\u6a21\u578b\uff08\u6743\u91cd\u88ab\u91cd\u7528\uff09\u6784\u5efa\u7684\u89c6\u9891\u7f16\u7801 encoded_frame_sequence = TimeDistributed(vision_model)(video_input) # \u8f93\u51fa\u4e3a\u5411\u91cf\u7684\u5e8f\u5217 encoded_video = LSTM(256)(encoded_frame_sequence) # \u8f93\u51fa\u4e3a\u4e00\u4e2a\u5411\u91cf # \u8fd9\u662f\u95ee\u9898\u7f16\u7801\u5668\u7684\u6a21\u578b\u7ea7\u8868\u793a\uff0c\u91cd\u590d\u4f7f\u7528\u4e0e\u4e4b\u524d\u76f8\u540c\u7684\u6743\u91cd\uff1a question_encoder = Model(inputs=question_input, outputs=encoded_question) # \u8ba9\u6211\u4eec\u7528\u5b83\u6765\u7f16\u7801\u8fd9\u4e2a\u95ee\u9898\uff1a video_question_input = Input(shape=(100,), dtype='int32') encoded_video_question = question_encoder(video_question_input) # \u8fd9\u5c31\u662f\u6211\u4eec\u7684\u89c6\u9891\u95ee\u7b54\u6a21\u5f0f\uff1a merged = keras.layers.concatenate([encoded_video, encoded_video_question]) output = Dense(1000, activation='softmax')(merged) video_qa_model = Model(inputs=[video_input, video_question_input], outputs=output)","title":"\u89c6\u9891\u95ee\u7b54\u6a21\u578b"},{"location":"0-Getting-Started/2.faq/","text":"Keras FAQ: \u5e38\u89c1\u95ee\u9898\u89e3\u7b54 \u5982\u4f55\u5f15\u7528 Keras? \u5982\u4f55\u5728 GPU \u4e0a\u8fd0\u884c Keras? \u5982\u4f55\u5728\u591a GPU \u4e0a\u8fd0\u884c Keras \u6a21\u578b\uff1f \"sample\", \"batch\", \"epoch\" \u5206\u522b\u662f\u4ec0\u4e48\uff1f \u5982\u4f55\u4fdd\u5b58 Keras \u6a21\u578b\uff1f \u4e3a\u4ec0\u4e48\u8bad\u7ec3\u96c6\u8bef\u5dee\u6bd4\u6d4b\u8bd5\u96c6\u7684\u8bef\u5dee\u9ad8\u5f88\u591a\uff1f \u5982\u4f55\u83b7\u53d6\u4e2d\u95f4\u5c42\u7684\u8f93\u51fa\uff1f \u5982\u4f55\u7528 Keras \u5904\u7406\u8d85\u8fc7\u5185\u5b58\u7684\u6570\u636e\u96c6\uff1f \u5728\u9a8c\u8bc1\u96c6\u7684\u8bef\u5dee\u4e0d\u518d\u4e0b\u964d\u65f6\uff0c\u5982\u4f55\u4e2d\u65ad\u8bad\u7ec3\uff1f \u9a8c\u8bc1\u96c6\u5212\u5206\u662f\u5982\u4f55\u8ba1\u7b97\u7684\uff1f \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6570\u636e\u662f\u5426\u4f1a\u6df7\u6d17\uff1f \u5982\u4f55\u5728\u6bcf\u4e2a epoch \u540e\u8bb0\u5f55\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u7684\u8bef\u5dee\u548c\u51c6\u786e\u7387\uff1f \u5982\u4f55\u300c\u51bb\u7ed3\u300d\u7f51\u7edc\u5c42\uff1f \u5982\u4f55\u4f7f\u7528\u72b6\u6001 RNNs (stateful RNNs)? \u5982\u4f55\u4ece Sequential \u6a21\u578b\u4e2d\u79fb\u9664\u4e00\u4e2a\u5c42\uff1f \u5982\u4f55\u5728 Keras \u4e2d\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u6a21\u578b\uff1f \u5982\u4f55\u5728 Keras \u4e2d\u4f7f\u7528 HDF5 \u8f93\u5165\uff1f Keras \u914d\u7f6e\u6587\u4ef6\u4fdd\u5b58\u5728\u54ea\u91cc\uff1f \u5982\u4f55\u5728 Keras \u5f00\u53d1\u8fc7\u7a0b\u4e2d\u83b7\u53d6\u53ef\u590d\u73b0\u7684\u7ed3\u679c\uff1f \u5982\u4f55\u5728 Keras \u4e2d\u5b89\u88c5 HDF5 \u6216 h5py \u6765\u4fdd\u5b58\u6211\u7684\u6a21\u578b\uff1f \u5982\u4f55\u5f15\u7528 Keras? \u5982\u679c Keras \u6709\u52a9\u4e8e\u60a8\u7684\u7814\u7a76\uff0c\u8bf7\u5728\u4f60\u7684\u51fa\u7248\u7269\u4e2d\u5f15\u7528\u5b83\u3002\u4ee5\u4e0b\u662f BibTeX \u6761\u76ee\u5f15\u7528\u7684\u793a\u4f8b\uff1a @misc{chollet2015keras, title={Keras}, author={Chollet, Fran\\c{c}ois and others}, year={2015}, publisher={GitHub}, howpublished={\\url{https://github.com/keras-team/keras}}, } \u5982\u4f55\u5728 GPU \u4e0a\u8fd0\u884c Keras? \u5982\u679c\u4f60\u4ee5 TensorFlow \u6216 CNTK \u540e\u7aef\u8fd0\u884c\uff0c\u53ea\u8981\u68c0\u6d4b\u5230\u4efb\u4f55\u53ef\u7528\u7684 GPU\uff0c\u90a3\u4e48\u4ee3\u7801\u5c06\u81ea\u52a8\u5728 GPU \u4e0a\u8fd0\u884c\u3002 \u5982\u679c\u4f60\u4ee5 Theano \u540e\u7aef\u8fd0\u884c\uff0c\u5219\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u65b9\u6cd5\u4e4b\u4e00\uff1a \u65b9\u6cd5 1 : \u4f7f\u7528 Theano flags\u3002 THEANO_FLAGS=device=gpu,floatX=float32 python my_keras_script.py \"gpu\" \u53ef\u80fd\u9700\u8981\u6839\u636e\u4f60\u7684\u8bbe\u5907\u6807\u8bc6\u7b26\uff08\u4f8b\u5982gpu0\uff0cgpu1\u7b49\uff09\u8fdb\u884c\u66f4\u6539\u3002 \u65b9\u6cd5 2 : \u521b\u5efa .theanorc : \u6307\u5bfc\u6559\u7a0b \u65b9\u6cd5 3 : \u5728\u4ee3\u7801\u7684\u5f00\u5934\u624b\u52a8\u8bbe\u7f6e theano.config.device , theano.config.floatX \uff1a import theano theano.config.device = 'gpu' theano.config.floatX = 'float32' \u5982\u4f55\u5728\u591a GPU \u4e0a\u8fd0\u884c Keras \u6a21\u578b? \u6211\u4eec\u5efa\u8bae\u4f7f\u7528 TensorFlow \u540e\u7aef\u6765\u6267\u884c\u8fd9\u9879\u4efb\u52a1\u3002\u6709\u4e24\u79cd\u65b9\u6cd5\u53ef\u5728\u591a\u4e2a GPU \u4e0a\u8fd0\u884c\u5355\u4e2a\u6a21\u578b\uff1a \u6570\u636e\u5e76\u884c \u548c \u8bbe\u5907\u5e76\u884c \u3002 \u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\uff0c\u4f60\u6700\u9700\u8981\u7684\u662f\u6570\u636e\u5e76\u884c\u3002 \u6570\u636e\u5e76\u884c \u6570\u636e\u5e76\u884c\u5305\u62ec\u5728\u6bcf\u4e2a\u8bbe\u5907\u4e0a\u590d\u5236\u4e00\u6b21\u76ee\u6807\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u6bcf\u4e2a\u6a21\u578b\u526f\u672c\u5904\u7406\u4e0d\u540c\u90e8\u5206\u7684\u8f93\u5165\u6570\u636e\u3002Keras \u6709\u4e00\u4e2a\u5185\u7f6e\u7684\u5b9e\u7528\u51fd\u6570 keras.utils.multi_gpu_model \uff0c\u5b83\u53ef\u4ee5\u751f\u6210\u4efb\u4f55\u6a21\u578b\u7684\u6570\u636e\u5e76\u884c\u7248\u672c\uff0c\u5728\u591a\u8fbe 8 \u4e2a GPU \u4e0a\u5b9e\u73b0\u51c6\u7ebf\u6027\u52a0\u901f\u3002 \u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 multi_gpu_model \u7684\u6587\u6863\u3002\u8fd9\u91cc\u662f\u4e00\u4e2a\u5feb\u901f\u7684\u4f8b\u5b50\uff1a from keras.utils import multi_gpu_model # \u5c06 `model` \u590d\u5236\u5230 8 \u4e2a GPU \u4e0a\u3002 # \u5047\u5b9a\u4f60\u7684\u673a\u5668\u6709 8 \u4e2a\u53ef\u7528\u7684 GPU\u3002 parallel_model = multi_gpu_model(model, gpus=8) parallel_model.compile(loss='categorical_crossentropy', optimizer='rmsprop') # \u8fd9\u4e2a `fit` \u8c03\u7528\u5c06\u5206\u5e03\u5728 8 \u4e2a GPU \u4e0a\u3002 # \u7531\u4e8e batch size \u4e3a 256\uff0c\u6bcf\u4e2a GPU \u5c06\u5904\u7406 32 \u4e2a\u6837\u672c\u3002 parallel_model.fit(x, y, epochs=20, batch_size=256) \u8bbe\u5907\u5e76\u884c \u8bbe\u5907\u5e76\u884c\u6027\u5305\u62ec\u5728\u4e0d\u540c\u8bbe\u5907\u4e0a\u8fd0\u884c\u540c\u4e00\u6a21\u578b\u7684\u4e0d\u540c\u90e8\u5206\u3002\u5bf9\u4e8e\u5177\u6709\u5e76\u884c\u4f53\u7cfb\u7ed3\u6784\u7684\u6a21\u578b\uff0c\u4f8b\u5982\u6709\u4e24\u4e2a\u5206\u652f\u7684\u6a21\u578b\uff0c\u8fd9\u79cd\u65b9\u5f0f\u5f88\u5408\u9002\u3002 \u8fd9\u79cd\u5e76\u884c\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 TensorFlow device scopes \u6765\u5b9e\u73b0\u3002\u8fd9\u91cc\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\uff1a # \u6a21\u578b\u4e2d\u5171\u4eab\u7684 LSTM \u7528\u4e8e\u5e76\u884c\u7f16\u7801\u4e24\u4e2a\u4e0d\u540c\u7684\u5e8f\u5217 input_a = keras.Input(shape=(140, 256)) input_b = keras.Input(shape=(140, 256)) shared_lstm = keras.layers.LSTM(64) # \u5728\u4e00\u4e2a GPU \u4e0a\u5904\u7406\u7b2c\u4e00\u4e2a\u5e8f\u5217 with tf.device_scope('/gpu:0'): encoded_a = shared_lstm(tweet_a) # \u5728\u53e6\u4e00\u4e2a GPU\u4e0a \u5904\u7406\u4e0b\u4e00\u4e2a\u5e8f\u5217 with tf.device_scope('/gpu:1'): encoded_b = shared_lstm(tweet_b) # \u5728 CPU \u4e0a\u8fde\u63a5\u7ed3\u679c with tf.device_scope('/cpu:0'): merged_vector = keras.layers.concatenate([encoded_a, encoded_b], axis=-1) \"sample\", \"batch\", \"epoch\" \u5206\u522b\u662f\u4ec0\u4e48\uff1f \u4e3a\u4e86\u6b63\u786e\u5730\u4f7f\u7528 Keras\uff0c\u4ee5\u4e0b\u662f\u5fc5\u987b\u4e86\u89e3\u548c\u7406\u89e3\u7684\u4e00\u4e9b\u5e38\u89c1\u5b9a\u4e49\uff1a Sample : \u6837\u672c\uff0c\u6570\u636e\u96c6\u4e2d\u7684\u4e00\u4e2a\u5143\u7d20\uff0c\u4e00\u6761\u6570\u636e\u3002 \u4f8b1: \u5728\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4e2d\uff0c\u4e00\u5f20\u56fe\u50cf\u662f\u4e00\u4e2a\u6837\u672c\u3002 \u4f8b2: \u5728\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u4e2d\uff0c\u4e00\u6bb5\u97f3\u9891\u662f\u4e00\u4e2a\u6837\u672c\u3002 Batch : \u6279\uff0c\u542b\u6709 N \u4e2a\u6837\u672c\u7684\u96c6\u5408\u3002\u6bcf\u4e00\u4e2a batch \u7684\u6837\u672c\u90fd\u662f\u72ec\u7acb\u5e76\u884c\u5904\u7406\u7684\u3002\u5728\u8bad\u7ec3\u65f6\uff0c\u4e00\u4e2a batch \u7684\u7ed3\u679c\u53ea\u4f1a\u7528\u6765\u66f4\u65b0\u4e00\u6b21\u6a21\u578b\u3002 \u4e00\u4e2a batch \u7684\u6837\u672c\u901a\u5e38\u6bd4\u5355\u4e2a\u8f93\u5165\u66f4\u63a5\u8fd1\u4e8e\u603b\u4f53\u8f93\u5165\u6570\u636e\u7684\u5206\u5e03\uff0cbatch \u8d8a\u5927\u5c31\u8d8a\u8fd1\u4f3c\u3002\u7136\u800c\uff0c\u6bcf\u4e2a batch \u5c06\u82b1\u8d39\u66f4\u957f\u7684\u65f6\u95f4\u6765\u5904\u7406\uff0c\u5e76\u4e14\u4ecd\u7136\u53ea\u66f4\u65b0\u6a21\u578b\u4e00\u6b21\u3002\u5728\u63a8\u7406\uff08\u8bc4\u4f30/\u9884\u6d4b\uff09\u65f6\uff0c\u5efa\u8bae\u6761\u4ef6\u5141\u8bb8\u7684\u60c5\u51b5\u4e0b\u9009\u62e9\u4e00\u4e2a\u5c3d\u53ef\u80fd\u5927\u7684 batch\uff0c\uff08\u56e0\u4e3a\u8f83\u5927\u7684 batch \u901a\u5e38\u8bc4\u4f30/\u9884\u6d4b\u7684\u901f\u5ea6\u4f1a\u66f4\u5feb\uff09\u3002 Epoch : \u8f6e\u6b21\uff0c\u901a\u5e38\u88ab\u5b9a\u4e49\u4e3a \u300c\u5728\u6574\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u4e00\u8f6e\u8fed\u4ee3\u300d\uff0c\u7528\u4e8e\u8bad\u7ec3\u7684\u4e0d\u540c\u7684\u9636\u6bb5\uff0c\u8fd9\u6709\u5229\u4e8e\u8bb0\u5f55\u548c\u5b9a\u671f\u8bc4\u4f30\u3002 \u5f53\u5728 Keras \u6a21\u578b\u7684 fit \u65b9\u6cd5\u4e2d\u4f7f\u7528 validation_data \u6216 validation_split \u65f6\uff0c\u8bc4\u4f30\u5c06\u5728\u6bcf\u4e2a epoch \u7ed3\u675f\u65f6\u8fd0\u884c\u3002 \u5728 Keras \u4e2d\uff0c\u53ef\u4ee5\u6dfb\u52a0\u4e13\u95e8\u7684\u7528\u4e8e\u5728 epoch \u7ed3\u675f\u65f6\u8fd0\u884c\u7684 callbacks \u56de\u8c03 \u3002\u4f8b\u5982\u5b66\u4e60\u7387\u53d8\u5316\u548c\u6a21\u578b\u68c0\u67e5\u70b9\uff08\u4fdd\u5b58\uff09\u3002 \u5982\u4f55\u4fdd\u5b58 Keras \u6a21\u578b\uff1f \u4fdd\u5b58/\u52a0\u8f7d\u6574\u4e2a\u6a21\u578b\uff08\u7ed3\u6784 + \u6743\u91cd + \u4f18\u5316\u5668\u72b6\u6001\uff09 \u4e0d\u5efa\u8bae\u4f7f\u7528 pickle \u6216 cPickle \u6765\u4fdd\u5b58 Keras \u6a21\u578b\u3002 \u4f60\u53ef\u4ee5\u4f7f\u7528 model.save(filepath) \u5c06 Keras \u6a21\u578b\u4fdd\u5b58\u5230\u5355\u4e2a HDF5 \u6587\u4ef6\u4e2d\uff0c\u8be5\u6587\u4ef6\u5c06\u5305\u542b\uff1a \u6a21\u578b\u7684\u7ed3\u6784\uff0c\u5141\u8bb8\u91cd\u65b0\u521b\u5efa\u6a21\u578b \u6a21\u578b\u7684\u6743\u91cd \u8bad\u7ec3\u914d\u7f6e\u9879\uff08\u635f\u5931\u51fd\u6570\uff0c\u4f18\u5316\u5668\uff09 \u4f18\u5316\u5668\u72b6\u6001\uff0c\u5141\u8bb8\u51c6\u786e\u5730\u4ece\u4f60\u4e0a\u6b21\u7ed3\u675f\u7684\u5730\u65b9\u7ee7\u7eed\u8bad\u7ec3\u3002 \u4f60\u53ef\u4ee5\u4f7f\u7528 keras.models.load_model(filepath) \u91cd\u65b0\u5b9e\u4f8b\u5316\u6a21\u578b\u3002 load_model \u8fd8\u5c06\u8d1f\u8d23\u4f7f\u7528\u4fdd\u5b58\u7684\u8bad\u7ec3\u914d\u7f6e\u9879\u6765\u7f16\u8bd1\u6a21\u578b\uff08\u9664\u975e\u6a21\u578b\u4ece\u672a\u7f16\u8bd1\u8fc7\uff09\u3002 \u4f8b\u5b50\uff1a from keras.models import load_model model.save('my_model.h5') # \u521b\u5efa HDF5 \u6587\u4ef6 'my_model.h5' del model # \u5220\u9664\u73b0\u6709\u6a21\u578b # \u8fd4\u56de\u4e00\u4e2a\u7f16\u8bd1\u597d\u7684\u6a21\u578b # \u4e0e\u4e4b\u524d\u90a3\u4e2a\u76f8\u540c model = load_model('my_model.h5') \u53e6\u8bf7\u53c2\u9605 \u5982\u4f55\u5b89\u88c5 HDF5 \u6216 h5py \u4ee5\u5728 Keras \u4e2d\u4fdd\u5b58\u6211\u7684\u6a21\u578b\uff1f \uff0c\u67e5\u770b\u6709\u5173\u5982\u4f55\u5b89\u88c5 h5py \u7684\u8bf4\u660e\u3002 \u53ea\u4fdd\u5b58/\u52a0\u8f7d\u6a21\u578b\u7684\u7ed3\u6784 \u5982\u679c\u60a8\u53ea\u9700\u8981\u4fdd\u5b58 \u6a21\u578b\u7684\u7ed3\u6784 \uff0c\u800c\u975e\u5176\u6743\u91cd\u6216\u8bad\u7ec3\u914d\u7f6e\u9879\uff0c\u5219\u53ef\u4ee5\u6267\u884c\u4ee5\u4e0b\u64cd\u4f5c\uff1a # \u4fdd\u5b58\u4e3a JSON json_string = model.to_json() # \u4fdd\u5b58\u4e3a YAML yaml_string = model.to_yaml() \u751f\u6210\u7684 JSON/YAML \u6587\u4ef6\u662f\u4eba\u7c7b\u53ef\u8bfb\u7684\uff0c\u5982\u679c\u9700\u8981\u8fd8\u53ef\u4ee5\u624b\u52a8\u7f16\u8f91\u3002 \u4f60\u53ef\u4ee5\u4ece\u8fd9\u4e9b\u6570\u636e\u5efa\u7acb\u4e00\u4e2a\u65b0\u7684\u6a21\u578b\uff1a # \u4ece JSON \u91cd\u5efa\u6a21\u578b\uff1a from keras.models import model_from_json model = model_from_json(json_string) # \u4ece YAML \u91cd\u5efa\u6a21\u578b\uff1a from keras.models import model_from_yaml model = model_from_yaml(yaml_string) \u53ea\u4fdd\u5b58/\u52a0\u8f7d\u6a21\u578b\u7684\u6743\u91cd \u5982\u679c\u60a8\u53ea\u9700\u8981 \u6a21\u578b\u7684\u6743\u91cd \uff0c\u53ef\u4ee5\u4f7f\u7528\u4e0b\u9762\u7684\u4ee3\u7801\u4ee5 HDF5 \u683c\u5f0f\u8fdb\u884c\u4fdd\u5b58\u3002 \u8bf7\u6ce8\u610f\uff0c\u6211\u4eec\u9996\u5148\u9700\u8981\u5b89\u88c5 HDF5 \u548c Python \u5e93 h5py\uff0c\u5b83\u4eec\u4e0d\u5305\u542b\u5728 Keras \u4e2d\u3002 model.save_weights('my_model_weights.h5') \u5047\u8bbe\u4f60\u6709\u7528\u4e8e\u5b9e\u4f8b\u5316\u6a21\u578b\u7684\u4ee3\u7801\uff0c\u5219\u53ef\u4ee5\u5c06\u4fdd\u5b58\u7684\u6743\u91cd\u52a0\u8f7d\u5230\u5177\u6709\u76f8\u540c\u7ed3\u6784\u7684\u6a21\u578b\u4e2d\uff1a model.load_weights('my_model_weights.h5') \u5982\u679c\u4f60\u9700\u8981\u5c06\u6743\u91cd\u52a0\u8f7d\u5230\u4e0d\u540c\u7684\u7ed3\u6784\uff08\u6709\u4e00\u4e9b\u5171\u540c\u5c42\uff09\u7684\u6a21\u578b\u4e2d\uff0c\u4f8b\u5982\u5fae\u8c03\u6216\u8fc1\u79fb\u5b66\u4e60\uff0c\u5219\u53ef\u4ee5\u6309\u5c42\u7684\u540d\u5b57\u6765\u52a0\u8f7d\u6743\u91cd\uff1a model.load_weights('my_model_weights.h5', by_name=True) \u4f8b\u5b50\uff1a \"\"\" \u5047\u8bbe\u539f\u59cb\u6a21\u578b\u5982\u4e0b\u6240\u793a\uff1a model = Sequential() model.add(Dense(2, input_dim=3, name='dense_1')) model.add(Dense(3, name='dense_2')) ... model.save_weights(fname) \"\"\" # \u65b0\u6a21\u578b model = Sequential() model.add(Dense(2, input_dim=3, name='dense_1')) # \u5c06\u88ab\u52a0\u8f7d model.add(Dense(10, name='new_dense')) # \u5c06\u4e0d\u88ab\u52a0\u8f7d # \u4ece\u7b2c\u4e00\u4e2a\u6a21\u578b\u52a0\u8f7d\u6743\u91cd\uff1b\u53ea\u4f1a\u5f71\u54cd\u7b2c\u4e00\u5c42\uff0cdense_1 model.load_weights(fname, by_name=True) \u5904\u7406\u5df2\u4fdd\u5b58\u6a21\u578b\u4e2d\u7684\u81ea\u5b9a\u4e49\u5c42\uff08\u6216\u5176\u4ed6\u81ea\u5b9a\u4e49\u5bf9\u8c61\uff09 \u5982\u679c\u8981\u52a0\u8f7d\u7684\u6a21\u578b\u5305\u542b\u81ea\u5b9a\u4e49\u5c42\u6216\u5176\u4ed6\u81ea\u5b9a\u4e49\u7c7b\u6216\u51fd\u6570\uff0c\u5219\u53ef\u4ee5\u901a\u8fc7 custom_objects \u53c2\u6570\u5c06\u5b83\u4eec\u4f20\u9012\u7ed9\u52a0\u8f7d\u673a\u5236\uff1a from keras.models import load_model # \u5047\u8bbe\u4f60\u7684\u6a21\u578b\u5305\u542b\u4e00\u4e2a AttentionLayer \u7c7b\u7684\u5b9e\u4f8b model = load_model('my_model.h5', custom_objects={'AttentionLayer': AttentionLayer}) \u6216\u8005\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528 \u81ea\u5b9a\u4e49\u5bf9\u8c61\u4f5c\u7528\u57df \uff1a from keras.utils import CustomObjectScope with CustomObjectScope({'AttentionLayer': AttentionLayer}): model = load_model('my_model.h5') \u81ea\u5b9a\u4e49\u5bf9\u8c61\u7684\u5904\u7406\u4e0e load_model , model_from_json , model_from_yaml \u7684\u5de5\u4f5c\u65b9\u5f0f\u76f8\u540c\uff1a from keras.models import model_from_json model = model_from_json(json_string, custom_objects={'AttentionLayer': AttentionLayer}) \u4e3a\u4ec0\u4e48\u8bad\u7ec3\u8bef\u5dee\u6bd4\u6d4b\u8bd5\u8bef\u5dee\u9ad8\u5f88\u591a\uff1f Keras \u6a21\u578b\u6709\u4e24\u79cd\u6a21\u5f0f\uff1a\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002\u6b63\u5219\u5316\u673a\u5236\uff0c\u5982 Dropout \u548c L1/L2 \u6743\u91cd\u6b63\u5219\u5316\uff0c\u5728\u6d4b\u8bd5\u65f6\u662f\u5173\u95ed\u7684\u3002 \u6b64\u5916\uff0c\u8bad\u7ec3\u8bef\u5dee\u662f\u6bcf\u6279\u8bad\u7ec3\u6570\u636e\u7684\u5e73\u5747\u8bef\u5dee\u3002\u7531\u4e8e\u4f60\u7684\u6a21\u578b\u662f\u968f\u7740\u65f6\u95f4\u800c\u53d8\u5316\u7684\uff0c\u4e00\u4e2a epoch \u4e2d\u7684\u7b2c\u4e00\u6279\u6570\u636e\u7684\u8bef\u5dee\u901a\u5e38\u6bd4\u6700\u540e\u4e00\u6279\u7684\u8981\u9ad8\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u6d4b\u8bd5\u8bef\u5dee\u662f\u6a21\u578b\u5728\u4e00\u4e2a epoch \u8bad\u7ec3\u5b8c\u540e\u8ba1\u7b97\u7684\uff0c\u56e0\u800c\u8bef\u5dee\u8f83\u5c0f\u3002 \u5982\u4f55\u83b7\u53d6\u4e2d\u95f4\u5c42\u7684\u8f93\u51fa\uff1f \u4e00\u4e2a\u7b80\u5355\u7684\u65b9\u6cd5\u662f\u521b\u5efa\u4e00\u4e2a\u65b0\u7684 Model \u6765\u8f93\u51fa\u4f60\u6240\u611f\u5174\u8da3\u7684\u5c42\uff1a from keras.models import Model model = ... # \u521b\u5efa\u539f\u59cb\u6a21\u578b layer_name = 'my_layer' intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output) intermediate_output = intermediate_layer_model.predict(data) \u6216\u8005\uff0c\u4f60\u4e5f\u53ef\u4ee5\u6784\u5efa\u4e00\u4e2a Keras \u51fd\u6570\uff0c\u8be5\u51fd\u6570\u5c06\u5728\u7ed9\u5b9a\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\u8fd4\u56de\u67d0\u4e2a\u5c42\u7684\u8f93\u51fa\uff0c\u4f8b\u5982\uff1a from keras import backend as K # \u4ee5 Sequential \u6a21\u578b\u4e3a\u4f8b get_3rd_layer_output = K.function([model.layers[0].input], [model.layers[3].output]) layer_output = get_3rd_layer_output([x])[0] \u540c\u6837\uff0c\u4f60\u53ef\u4ee5\u76f4\u63a5\u5efa\u7acb\u4e00\u4e2a Theano \u6216 TensorFlow \u51fd\u6570\u3002 \u6ce8\u610f\uff0c\u5982\u679c\u4f60\u7684\u6a21\u578b\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u9636\u6bb5\u6709\u4e0d\u540c\u7684\u884c\u4e3a\uff08\u4f8b\u5982\uff0c\u4f7f\u7528 Dropout , BatchNormalization \u7b49\uff09\uff0c\u5219\u9700\u8981\u5c06\u5b66\u4e60\u9636\u6bb5\u6807\u5fd7\u4f20\u9012\u7ed9\u4f60\u7684\u51fd\u6570\uff1a get_3rd_layer_output = K.function([model.layers[0].input, K.learning_phase()], [model.layers[3].output]) # \u6d4b\u8bd5\u6a21\u5f0f = 0 \u65f6\u7684\u8f93\u51fa layer_output = get_3rd_layer_output([x, 0])[0] # \u6d4b\u8bd5\u6a21\u5f0f = 1 \u65f6\u7684\u8f93\u51fa layer_output = get_3rd_layer_output([x, 1])[0] \u5982\u4f55\u7528 Keras \u5904\u7406\u8d85\u8fc7\u5185\u5b58\u7684\u6570\u636e\u96c6\uff1f \u4f60\u53ef\u4ee5\u4f7f\u7528 model.train_on_batch(x\uff0cy) \u548c model.test_on_batch(x\uff0cy) \u8fdb\u884c\u6279\u91cf\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u3002\u8bf7\u53c2\u9605 \u6a21\u578b\u6587\u6863 \u3002 \u6216\u8005\uff0c\u4f60\u53ef\u4ee5\u7f16\u5199\u4e00\u4e2a\u751f\u6210\u6279\u5904\u7406\u8bad\u7ec3\u6570\u636e\u7684\u751f\u6210\u5668\uff0c\u7136\u540e\u4f7f\u7528 model.fit_generator(data_generator\uff0csteps_per_epoch\uff0cepochs) \u65b9\u6cd5\u3002 \u4f60\u53ef\u4ee5\u5728 CIFAR10 example \u4e2d\u627e\u5230\u5b9e\u8df5\u4ee3\u7801\u3002 \u5728\u9a8c\u8bc1\u96c6\u7684\u8bef\u5dee\u4e0d\u518d\u4e0b\u964d\u65f6\uff0c\u5982\u4f55\u4e2d\u65ad\u8bad\u7ec3\uff1f \u4f60\u53ef\u4ee5\u4f7f\u7528 EarlyStopping \u56de\u8c03\uff1a from keras.callbacks import EarlyStopping early_stopping = EarlyStopping(monitor='val_loss', patience=2) model.fit(x, y, validation_split=0.2, callbacks=[early_stopping]) \u66f4\u591a\u4fe1\u606f\u8bf7\u67e5\u770b callbacks \u6587\u6863 \u3002 \u9a8c\u8bc1\u96c6\u5212\u5206\u662f\u5982\u4f55\u8ba1\u7b97\u7684\uff1f \u5982\u679c\u60a8\u5c06 model.fit \u4e2d\u7684 validation_split \u53c2\u6570\u8bbe\u7f6e\u4e3a 0.1\uff0c\u90a3\u4e48\u4f7f\u7528\u7684\u9a8c\u8bc1\u6570\u636e\u5c06\u662f\u6700\u540e 10\uff05 \u7684\u6570\u636e\u3002\u5982\u679c\u8bbe\u7f6e\u4e3a 0.25\uff0c\u5c31\u662f\u6700\u540e 25% \u7684\u6570\u636e\u3002\u6ce8\u610f\uff0c\u5728\u63d0\u53d6\u5206\u5272\u9a8c\u8bc1\u96c6\u4e4b\u524d\uff0c\u6570\u636e\u4e0d\u4f1a\u88ab\u6df7\u6d17\uff0c\u56e0\u6b64\u9a8c\u8bc1\u96c6\u4ec5\u4ec5\u662f\u4f20\u9012\u7684\u8f93\u5165\u4e2d\u6700\u540e\u4e00\u4e2a x\uff05 \u7684\u6837\u672c\u3002 \u6240\u6709 epoch \u90fd\u4f7f\u7528\u76f8\u540c\u7684\u9a8c\u8bc1\u96c6\uff08\u5728\u540c\u4e00\u4e2a fit \u4e2d\u8c03\u7528\uff09\u3002 \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6570\u636e\u662f\u5426\u4f1a\u6df7\u6d17\uff1f \u662f\u7684\uff0c\u5982\u679c model.fit \u4e2d\u7684 shuffle \u53c2\u6570\u8bbe\u7f6e\u4e3a True\uff08\u9ed8\u8ba4\u503c\uff09\uff0c\u5219\u8bad\u7ec3\u6570\u636e\u5c06\u5728\u6bcf\u4e2a epoch \u6df7\u6d17\u3002 \u9a8c\u8bc1\u96c6\u6c38\u8fdc\u4e0d\u4f1a\u6df7\u6d17\u3002 \u5982\u4f55\u5728\u6bcf\u4e2a epoch \u540e\u8bb0\u5f55\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u7684\u8bef\u5dee\u548c\u51c6\u786e\u7387\uff1f model.fit \u65b9\u6cd5\u8fd4\u56de\u4e00\u4e2a History \u56de\u8c03\uff0c\u5b83\u5177\u6709\u5305\u542b\u8fde\u7eed\u8bef\u5dee\u7684\u5217\u8868\u548c\u5176\u4ed6\u5ea6\u91cf\u7684 history \u5c5e\u6027\u3002 hist = model.fit(x, y, validation_split=0.2) print(hist.history) \u5982\u4f55\u300c\u51bb\u7ed3\u300d\u7f51\u7edc\u5c42\uff1f \u300c\u51bb\u7ed3\u300d\u4e00\u4e2a\u5c42\u610f\u5473\u7740\u5c06\u5176\u6392\u9664\u5728\u8bad\u7ec3\u4e4b\u5916\uff0c\u5373\u5176\u6743\u91cd\u5c06\u6c38\u8fdc\u4e0d\u4f1a\u66f4\u65b0\u3002\u8fd9\u5728\u5fae\u8c03\u6a21\u578b\u6216\u4f7f\u7528\u56fa\u5b9a\u7684\u8bcd\u5411\u91cf\u8fdb\u884c\u6587\u672c\u8f93\u5165\u4e2d\u5f88\u6709\u7528\u3002 \u60a8\u53ef\u4ee5\u5c06 trainable \u53c2\u6570\uff08\u5e03\u5c14\u503c\uff09\u4f20\u9012\u7ed9\u4e00\u4e2a\u5c42\u7684\u6784\u9020\u5668\uff0c\u4ee5\u5c06\u8be5\u5c42\u8bbe\u7f6e\u4e3a\u4e0d\u53ef\u8bad\u7ec3\u7684\uff1a frozen_layer = Dense(32, trainable=False) \u53e6\u5916\uff0c\u53ef\u4ee5\u5728\u5b9e\u4f8b\u5316\u4e4b\u540e\u5c06\u7f51\u7edc\u5c42\u7684 trainable \u5c5e\u6027\u8bbe\u7f6e\u4e3a True \u6216 False\u3002\u4e3a\u4e86\u4f7f\u4e4b\u751f\u6548\uff0c\u5728\u4fee\u6539 trainable \u5c5e\u6027\u4e4b\u540e\uff0c\u9700\u8981\u5728\u6a21\u578b\u4e0a\u8c03\u7528 compile() \u3002\u8fd9\u662f\u4e00\u4e2a\u4f8b\u5b50\uff1a x = Input(shape=(32,)) layer = Dense(32) layer.trainable = False y = layer(x) frozen_model = Model(x, y) # \u5728\u4e0b\u9762\u7684\u6a21\u578b\u4e2d\uff0c\u8bad\u7ec3\u671f\u95f4\u4e0d\u4f1a\u66f4\u65b0\u5c42\u7684\u6743\u91cd frozen_model.compile(optimizer='rmsprop', loss='mse') layer.trainable = True trainable_model = Model(x, y) # \u4f7f\u7528\u8fd9\u4e2a\u6a21\u578b\uff0c\u8bad\u7ec3\u671f\u95f4 `layer` \u7684\u6743\u91cd\u5c06\u88ab\u66f4\u65b0 # (\u8fd9\u4e5f\u4f1a\u5f71\u54cd\u4e0a\u9762\u7684\u6a21\u578b\uff0c\u56e0\u4e3a\u5b83\u4f7f\u7528\u4e86\u540c\u4e00\u4e2a\u7f51\u7edc\u5c42\u5b9e\u4f8b) trainable_model.compile(optimizer='rmsprop', loss='mse') frozen_model.fit(data, labels) # \u8fd9\u4e0d\u4f1a\u66f4\u65b0 `layer` \u7684\u6743\u91cd trainable_model.fit(data, labels) # \u8fd9\u4f1a\u66f4\u65b0 `layer` \u7684\u6743\u91cd \u5982\u4f55\u4f7f\u7528\u6709\u72b6\u6001 RNN (stateful RNNs)? \u4f7f RNN \u5177\u6709\u72b6\u6001\u610f\u5473\u7740\u6bcf\u6279\u6837\u54c1\u7684\u72b6\u6001\u5c06\u88ab\u91cd\u65b0\u7528\u4f5c\u4e0b\u4e00\u6279\u6837\u54c1\u7684\u521d\u59cb\u72b6\u6001\u3002 \u5f53\u4f7f\u7528\u6709\u72b6\u6001 RNN \u65f6\uff0c\u5047\u5b9a\uff1a \u6240\u6709\u7684\u6279\u6b21\u90fd\u6709\u76f8\u540c\u6570\u91cf\u7684\u6837\u672c \u5982\u679c x1 \u548c x2 \u662f\u8fde\u7eed\u6279\u6b21\u7684\u6837\u672c\uff0c\u5219 x2[i] \u662f x1[i] \u7684\u540e\u7eed\u5e8f\u5217\uff0c\u5bf9\u4e8e\u6bcf\u4e2a i \u3002 \u8981\u5728 RNN \u4e2d\u4f7f\u7528\u72b6\u6001\uff0c\u4f60\u9700\u8981: \u901a\u8fc7\u5c06 batch_size \u53c2\u6570\u4f20\u9012\u7ed9\u6a21\u578b\u7684\u7b2c\u4e00\u5c42\u6765\u663e\u5f0f\u6307\u5b9a\u4f60\u6b63\u5728\u4f7f\u7528\u7684\u6279\u5927\u5c0f\u3002\u4f8b\u5982\uff0c\u5bf9\u4e8e 10 \u4e2a\u65f6\u95f4\u6b65\u957f\u7684 32 \u6837\u672c\u7684 batch\uff0c\u6bcf\u4e2a\u65f6\u95f4\u6b65\u957f\u5177\u6709 16 \u4e2a\u7279\u5f81\uff0c batch_size = 32 \u3002 \u5728 RNN \u5c42\u4e2d\u8bbe\u7f6e stateful = True \u3002 \u5728\u8c03\u7528 fit() \u65f6\u6307\u5b9a shuffle = False \u3002 \u91cd\u7f6e\u7d2f\u79ef\u72b6\u6001\uff1a \u4f7f\u7528 model.reset_states() \u6765\u91cd\u7f6e\u6a21\u578b\u4e2d\u6240\u6709\u5c42\u7684\u72b6\u6001 \u4f7f\u7528 layer.reset_states() \u6765\u91cd\u7f6e\u6307\u5b9a\u6709\u72b6\u6001 RNN \u5c42\u7684\u72b6\u6001 \u4f8b\u5b50\uff1a x # \u8f93\u5165\u6570\u636e\uff0c\u5c3a\u5bf8\u4e3a (32, 21, 16) # \u5c06\u6b65\u957f\u4e3a 10 \u7684\u5e8f\u5217\u8f93\u9001\u5230\u6a21\u578b\u4e2d model = Sequential() model.add(LSTM(32, input_shape=(10, 16), batch_size=32, stateful=True)) model.add(Dense(16, activation='softmax')) model.compile(optimizer='rmsprop', loss='categorical_crossentropy') # \u8bad\u7ec3\u7f51\u7edc\uff0c\u6839\u636e\u7ed9\u5b9a\u7684\u524d 10 \u4e2a\u65f6\u95f4\u6b65\uff0c\u6765\u9884\u6d4b\u7b2c 11 \u4e2a\u65f6\u95f4\u6b65\uff1a model.train_on_batch(x[:, :10, :], np.reshape(x[:, 10, :], (32, 16))) # \u7f51\u7edc\u7684\u72b6\u6001\u5df2\u7ecf\u6539\u53d8\u3002\u6211\u4eec\u53ef\u4ee5\u63d0\u4f9b\u540e\u7eed\u5e8f\u5217\uff1a model.train_on_batch(x[:, 10:20, :], np.reshape(x[:, 20, :], (32, 16))) # \u91cd\u7f6e LSTM \u5c42\u7684\u72b6\u6001\uff1a model.reset_states() # \u53e6\u4e00\u79cd\u91cd\u7f6e\u65b9\u6cd5\uff1a model.layers[0].reset_states() \u8bf7\u6ce8\u610f\uff0c predict , fit , train_on_batch , predict_classes \u7b49\u65b9\u6cd5 \u5168\u90e8 \u90fd\u4f1a\u66f4\u65b0\u6a21\u578b\u4e2d\u6709\u72b6\u6001\u5c42\u7684\u72b6\u6001\u3002\u8fd9\u4f7f\u4f60\u4e0d\u4ec5\u53ef\u4ee5\u8fdb\u884c\u6709\u72b6\u6001\u7684\u8bad\u7ec3\uff0c\u8fd8\u53ef\u4ee5\u8fdb\u884c\u6709\u72b6\u6001\u7684\u9884\u6d4b\u3002 \u5982\u4f55\u4ece Sequential \u6a21\u578b\u4e2d\u79fb\u9664\u4e00\u4e2a\u5c42\uff1f \u4f60\u53ef\u4ee5\u901a\u8fc7\u8c03\u7528 .pop() \u6765\u5220\u9664 Sequential \u6a21\u578b\u4e2d\u6700\u540e\u6dfb\u52a0\u7684\u5c42\uff1a model = Sequential() model.add(Dense(32, activation='relu', input_dim=784)) model.add(Dense(32, activation='relu')) print(len(model.layers)) # \"2\" model.pop() print(len(model.layers)) # \"1\" \u5982\u4f55\u5728 Keras \u4e2d\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u6a21\u578b\uff1f \u6211\u4eec\u63d0\u4f9b\u4e86\u4ee5\u4e0b\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u7684\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u7684\u6743\u91cd\uff1a Xception VGG16 VGG19 ResNet50 Inception v3 Inception-ResNet v2 MobileNet v1 DenseNet NASNet MobileNet v2 \u5b83\u4eec\u53ef\u4ee5\u4f7f\u7528 keras.applications \u6a21\u5757\u8fdb\u884c\u5bfc\u5165\uff1a from keras.applications.xception import Xception from keras.applications.vgg16 import VGG16 from keras.applications.vgg19 import VGG19 from keras.applications.resnet50 import ResNet50 from keras.applications.inception_v3 import InceptionV3 from keras.applications.inception_resnet_v2 import InceptionResNetV2 from keras.applications.mobilenet import MobileNet from keras.applications.densenet import DenseNet121 from keras.applications.densenet import DenseNet169 from keras.applications.densenet import DenseNet201 from keras.applications.nasnet import NASNetLarge from keras.applications.nasnet import NASNetMobile from keras.applications.mobilenet_v2 import MobileNetV2 model = VGG16(weights='imagenet', include_top=True) \u6709\u5173\u4e00\u4e9b\u7b80\u5355\u7684\u7528\u6cd5\u793a\u4f8b\uff0c\u8bf7\u53c2\u9605 Applications \u6a21\u5757\u7684\u6587\u6863 \u3002 \u6709\u5173\u5982\u4f55\u4f7f\u7528\u6b64\u7c7b\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u6216\u5fae\u8c03\u7684\u8be6\u7ec6\u793a\u4f8b\uff0c\u8bf7\u53c2\u9605 \u6b64\u535a\u5ba2\u6587\u7ae0 \u3002 VGG16 \u6a21\u578b\u4e5f\u662f\u4ee5\u4e0b\u51e0\u4e2a Keras \u793a\u4f8b\u811a\u672c\u7684\u57fa\u7840\uff1a Style transfer Feature visualization Deep dream \u5982\u4f55\u5728 Keras \u4e2d\u4f7f\u7528 HDF5 \u8f93\u5165\uff1f \u4f60\u53ef\u4ee5\u4f7f\u7528 keras.utils.io_utils \u4e2d\u7684 HDF5Matrix \u7c7b\u3002\u6709\u5173\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 HDF5Matrix\u6587\u6863 \u3002 \u4f60\u4e5f\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 HDF5 \u6570\u636e\u96c6\uff1a import h5py with h5py.File('input/file.hdf5', 'r') as f: x_data = f['x_data'] model.predict(x_data) Keras \u914d\u7f6e\u6587\u4ef6\u4fdd\u5b58\u5728\u54ea\u91cc\uff1f \u6240\u6709 Keras \u6570\u636e\u5b58\u50a8\u7684\u9ed8\u8ba4\u76ee\u5f55\u662f\uff1a $HOME/.keras/ \u6ce8\u610f\uff0cWindows \u7528\u6237\u5e94\u8be5\u5c06 $HOME \u66ff\u6362\u4e3a \uff05USERPROFILE\uff05 \u3002\u5982\u679c Keras \u65e0\u6cd5\u521b\u5efa\u4e0a\u8ff0\u76ee\u5f55\uff08\u4f8b\u5982\uff0c\u7531\u4e8e\u6743\u9650\u95ee\u9898\uff09\uff0c\u5219\u4f7f\u7528 /tmp/.keras/ \u4f5c\u4e3a\u5907\u4efd\u3002 Keras\u914d\u7f6e\u6587\u4ef6\u662f\u5b58\u50a8\u5728 $HOME/.keras/keras.json \u4e2d\u7684 JSON \u6587\u4ef6\u3002\u9ed8\u8ba4\u7684\u914d\u7f6e\u6587\u4ef6\u5982\u4e0b\u6240\u793a\uff1a { \"image_data_format\": \"channels_last\", \"epsilon\": 1e-07, \"floatx\": \"float32\", \"backend\": \"tensorflow\" } \u5b83\u5305\u542b\u4ee5\u4e0b\u5b57\u6bb5\uff1a \u56fe\u50cf\u5904\u7406\u5c42\u548c\u5b9e\u7528\u7a0b\u5e8f\u6240\u4f7f\u7528\u7684\u9ed8\u8ba4\u503c\u56fe\u50cf\u6570\u636e\u683c\u5f0f\uff08 channels_last \u6216 channels_first \uff09\u3002 \u7528\u4e8e\u9632\u6b62\u5728\u67d0\u4e9b\u64cd\u4f5c\u4e2d\u88ab\u96f6\u9664\u7684 epsilon \u6a21\u7cca\u56e0\u5b50\u3002 \u9ed8\u8ba4\u6d6e\u70b9\u6570\u636e\u7c7b\u578b\u3002 \u9ed8\u8ba4\u540e\u7aef\u3002\u8be6\u89c1 backend \u6587\u6863 \u3002 \u540c\u6837\uff0c\u7f13\u5b58\u7684\u6570\u636e\u96c6\u6587\u4ef6\uff08\u5982\u4f7f\u7528 get_file() \u4e0b\u8f7d\u7684\u6587\u4ef6\uff09\u9ed8\u8ba4\u5b58\u50a8\u5728 $HOME/.keras/datasets/ \u4e2d\u3002 \u5982\u4f55\u5728 Keras \u5f00\u53d1\u8fc7\u7a0b\u4e2d\u83b7\u53d6\u53ef\u590d\u73b0\u7684\u7ed3\u679c\uff1f \u5728\u6a21\u578b\u7684\u5f00\u53d1\u8fc7\u7a0b\u4e2d\uff0c\u80fd\u591f\u5728\u4e00\u6b21\u6b21\u7684\u8fd0\u884c\u4e2d\u83b7\u5f97\u53ef\u590d\u73b0\u7684\u7ed3\u679c\uff0c\u4ee5\u786e\u5b9a\u6027\u80fd\u7684\u53d8\u5316\u662f\u6765\u81ea\u6a21\u578b\u8fd8\u662f\u6570\u636e\u96c6\u7684\u53d8\u5316\uff0c\u6216\u8005\u4ec5\u4ec5\u662f\u4e00\u4e9b\u65b0\u7684\u968f\u673a\u6837\u672c\u70b9\u5e26\u6765\u7684\u7ed3\u679c\uff0c\u6709\u65f6\u5019\u662f\u5f88\u6709\u7528\u5904\u7684\u3002 \u9996\u5148\uff0c\u4f60\u9700\u8981\u5728\u7a0b\u5e8f\u542f\u52a8\u4e4b\u524d\u5c06 PYTHONHASHSEED \u73af\u5883\u53d8\u91cf\u8bbe\u7f6e\u4e3a 0\uff08\u4e0d\u5728\u7a0b\u5e8f\u672c\u8eab\u5185\uff09\u3002\u5bf9\u4e8e Python 3.2.3 \u4ee5\u4e0a\u7248\u672c\uff0c\u5b83\u5bf9\u4e8e\u67d0\u4e9b\u57fa\u4e8e\u6563\u5217\u7684\u64cd\u4f5c\u5177\u6709\u53ef\u91cd\u73b0\u7684\u884c\u4e3a\u662f\u5fc5\u8981\u7684\uff08\u4f8b\u5982\uff0c\u96c6\u5408\u548c\u5b57\u5178\u7684 item \u987a\u5e8f\uff0c\u8bf7\u53c2\u9605 Python \u6587\u6863 \u548c issue #2280 \u83b7\u53d6\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff09\u3002\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\u7684\u4e00\u79cd\u65b9\u6cd5\u662f\uff0c\u5728\u8fd9\u6837\u542f\u52a8 python \u65f6\uff1a $ cat test_hash.py print(hash(\"keras\")) $ python3 test_hash.py # \u65e0\u6cd5\u590d\u73b0\u7684 hash (Python 3.2.3+) -8127205062320133199 $ python3 test_hash.py # \u65e0\u6cd5\u590d\u73b0\u7684 hash (Python 3.2.3+) 3204480642156461591 $ PYTHONHASHSEED=0 python3 test_hash.py # \u53ef\u590d\u73b0\u7684 hash 4883664951434749476 $ PYTHONHASHSEED=0 python3 test_hash.py # \u53ef\u590d\u73b0\u7684 hash 4883664951434749476 \u6b64\u5916\uff0c\u5f53\u4f7f\u7528 TensorFlow \u540e\u7aef\u5e76\u5728 GPU \u4e0a\u8fd0\u884c\u65f6\uff0c\u67d0\u4e9b\u64cd\u4f5c\u5177\u6709\u975e\u786e\u5b9a\u6027\u8f93\u51fa\uff0c\u7279\u522b\u662f tf.reduce_sum() \u3002\u8fd9\u662f\u56e0\u4e3a GPU \u5e76\u884c\u8fd0\u884c\u8bb8\u591a\u64cd\u4f5c\uff0c\u56e0\u6b64\u5e76\u4e0d\u603b\u80fd\u4fdd\u8bc1\u6267\u884c\u987a\u5e8f\u3002\u7531\u4e8e\u6d6e\u70b9\u6570\u7684\u7cbe\u5ea6\u6709\u9650\uff0c\u5373\u4f7f\u6dfb\u52a0\u51e0\u4e2a\u6570\u5b57\uff0c\u4e5f\u53ef\u80fd\u4f1a\u4ea7\u751f\u7565\u6709\u4e0d\u540c\u7684\u7ed3\u679c\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u6dfb\u52a0\u5b83\u4eec\u7684\u987a\u5e8f\u3002\u4f60\u53ef\u4ee5\u5c1d\u8bd5\u907f\u514d\u67d0\u4e9b\u975e\u786e\u5b9a\u6027\u64cd\u4f5c\uff0c\u4f46\u6709\u4e9b\u64cd\u4f5c\u53ef\u80fd\u662f\u7531 TensorFlow \u5728\u8ba1\u7b97\u68af\u5ea6\u65f6\u81ea\u52a8\u521b\u5efa\u7684\uff0c\u56e0\u6b64\u5728 CPU \u4e0a\u8fd0\u884c\u4ee3\u7801\u8981\u7b80\u5355\u5f97\u591a\u3002\u4e3a\u6b64\uff0c\u4f60\u53ef\u4ee5\u5c06 CUDA_VISIBLE_DEVICES \u73af\u5883\u53d8\u91cf\u8bbe\u7f6e\u4e3a\u7a7a\u5b57\u7b26\u4e32\uff0c\u4f8b\u5982\uff1a $ CUDA_VISIBLE_DEVICES=\"\" PYTHONHASHSEED=0 python your_program.py \u4e0b\u9762\u7684\u4ee3\u7801\u7247\u6bb5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5982\u4f55\u83b7\u5f97\u53ef\u590d\u73b0\u7ed3\u679c\u7684\u4f8b\u5b50 - \u9488\u5bf9 Python 3 \u73af\u5883\u7684 TensorFlow \u540e\u7aef\u3002 import numpy as np import tensorflow as tf import random as rn # \u4ee5\u4e0b\u662f Numpy \u5728\u4e00\u4e2a\u660e\u786e\u7684\u521d\u59cb\u72b6\u6001\u751f\u6210\u56fa\u5b9a\u968f\u673a\u6570\u5b57\u6240\u5fc5\u9700\u7684\u3002 np.random.seed(42) # \u4ee5\u4e0b\u662f Python \u5728\u4e00\u4e2a\u660e\u786e\u7684\u521d\u59cb\u72b6\u6001\u751f\u6210\u56fa\u5b9a\u968f\u673a\u6570\u5b57\u6240\u5fc5\u9700\u7684\u3002 rn.seed(12345) # \u5f3a\u5236 TensorFlow \u4f7f\u7528\u5355\u7ebf\u7a0b\u3002 # \u591a\u7ebf\u7a0b\u662f\u7ed3\u679c\u4e0d\u53ef\u590d\u73b0\u7684\u4e00\u4e2a\u6f5c\u5728\u56e0\u7d20\u3002 # \u66f4\u591a\u8be6\u60c5\uff0c\u89c1: https://stackoverflow.com/questions/42022950/ session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1) from keras import backend as K # `tf.set_random_seed()` \u5c06\u4f1a\u4ee5 TensorFlow \u4e3a\u540e\u7aef\uff0c # \u5728\u4e00\u4e2a\u660e\u786e\u7684\u521d\u59cb\u72b6\u6001\u4e0b\u751f\u6210\u56fa\u5b9a\u968f\u673a\u6570\u5b57\u3002 # \u66f4\u591a\u8be6\u60c5\uff0c\u89c1: https://www.tensorflow.org/api_docs/python/tf/set_random_seed tf.set_random_seed(1234) sess = tf.Session(graph=tf.get_default_graph(), config=session_conf) K.set_session(sess) # \u5269\u4f59\u4ee3\u7801 ... \u5982\u4f55\u5728 Keras \u4e2d\u5b89\u88c5 HDF5 \u6216 h5py \u6765\u4fdd\u5b58\u6211\u7684\u6a21\u578b\uff1f \u4e3a\u4e86\u5c06\u4f60\u7684 Keras \u6a21\u578b\u4fdd\u5b58\u4e3a HDF5 \u6587\u4ef6\uff0c\u4f8b\u5982\u901a\u8fc7 keras.callbacks.ModelCheckpoint \uff0cKeras \u4f7f\u7528\u4e86 h5py Python \u5305\u3002h5py \u662f Keras \u7684\u4f9d\u8d56\u9879\uff0c\u5e94\u9ed8\u8ba4\u88ab\u5b89\u88c5\u3002\u5728\u57fa\u4e8e Debian \u7684\u53d1\u884c\u7248\u672c\u4e0a\uff0c\u4f60\u9700\u8981\u518d\u989d\u5916\u5b89\u88c5 libhdf5 \uff1a sudo apt-get install libhdf5-serial-dev \u5982\u679c\u4f60\u4e0d\u786e\u5b9a\u662f\u5426\u5b89\u88c5\u4e86 h5py\uff0c\u5219\u53ef\u4ee5\u6253\u5f00 Python shell \u5e76\u901a\u8fc7\u4e0b\u9762\u7684\u547d\u4ee4\u52a0\u8f7d\u6a21\u5757 import h5py \u5982\u679c\u6a21\u5757\u5bfc\u5165\u6ca1\u6709\u9519\u8bef\uff0c\u90a3\u4e48\u8bf4\u660e\u6a21\u5757\u5df2\u7ecf\u5b89\u88c5\u6210\u529f\uff0c\u5426\u5219\u4f60\u53ef\u4ee5\u5728 http://docs.h5py.org/en/latest/build.html \u4e2d\u627e\u5230\u8be6\u7ec6\u7684\u5b89\u88c5\u8bf4\u660e\u3002","title":"Keras FAQ: \u5e38\u89c1\u95ee\u9898\u89e3\u7b54"},{"location":"0-Getting-Started/2.faq/#keras-faq","text":"\u5982\u4f55\u5f15\u7528 Keras? \u5982\u4f55\u5728 GPU \u4e0a\u8fd0\u884c Keras? \u5982\u4f55\u5728\u591a GPU \u4e0a\u8fd0\u884c Keras \u6a21\u578b\uff1f \"sample\", \"batch\", \"epoch\" \u5206\u522b\u662f\u4ec0\u4e48\uff1f \u5982\u4f55\u4fdd\u5b58 Keras \u6a21\u578b\uff1f \u4e3a\u4ec0\u4e48\u8bad\u7ec3\u96c6\u8bef\u5dee\u6bd4\u6d4b\u8bd5\u96c6\u7684\u8bef\u5dee\u9ad8\u5f88\u591a\uff1f \u5982\u4f55\u83b7\u53d6\u4e2d\u95f4\u5c42\u7684\u8f93\u51fa\uff1f \u5982\u4f55\u7528 Keras \u5904\u7406\u8d85\u8fc7\u5185\u5b58\u7684\u6570\u636e\u96c6\uff1f \u5728\u9a8c\u8bc1\u96c6\u7684\u8bef\u5dee\u4e0d\u518d\u4e0b\u964d\u65f6\uff0c\u5982\u4f55\u4e2d\u65ad\u8bad\u7ec3\uff1f \u9a8c\u8bc1\u96c6\u5212\u5206\u662f\u5982\u4f55\u8ba1\u7b97\u7684\uff1f \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6570\u636e\u662f\u5426\u4f1a\u6df7\u6d17\uff1f \u5982\u4f55\u5728\u6bcf\u4e2a epoch \u540e\u8bb0\u5f55\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u7684\u8bef\u5dee\u548c\u51c6\u786e\u7387\uff1f \u5982\u4f55\u300c\u51bb\u7ed3\u300d\u7f51\u7edc\u5c42\uff1f \u5982\u4f55\u4f7f\u7528\u72b6\u6001 RNNs (stateful RNNs)? \u5982\u4f55\u4ece Sequential \u6a21\u578b\u4e2d\u79fb\u9664\u4e00\u4e2a\u5c42\uff1f \u5982\u4f55\u5728 Keras \u4e2d\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u6a21\u578b\uff1f \u5982\u4f55\u5728 Keras \u4e2d\u4f7f\u7528 HDF5 \u8f93\u5165\uff1f Keras \u914d\u7f6e\u6587\u4ef6\u4fdd\u5b58\u5728\u54ea\u91cc\uff1f \u5982\u4f55\u5728 Keras \u5f00\u53d1\u8fc7\u7a0b\u4e2d\u83b7\u53d6\u53ef\u590d\u73b0\u7684\u7ed3\u679c\uff1f \u5982\u4f55\u5728 Keras \u4e2d\u5b89\u88c5 HDF5 \u6216 h5py \u6765\u4fdd\u5b58\u6211\u7684\u6a21\u578b\uff1f","title":"Keras FAQ: \u5e38\u89c1\u95ee\u9898\u89e3\u7b54"},{"location":"0-Getting-Started/2.faq/#keras","text":"\u5982\u679c Keras \u6709\u52a9\u4e8e\u60a8\u7684\u7814\u7a76\uff0c\u8bf7\u5728\u4f60\u7684\u51fa\u7248\u7269\u4e2d\u5f15\u7528\u5b83\u3002\u4ee5\u4e0b\u662f BibTeX \u6761\u76ee\u5f15\u7528\u7684\u793a\u4f8b\uff1a @misc{chollet2015keras, title={Keras}, author={Chollet, Fran\\c{c}ois and others}, year={2015}, publisher={GitHub}, howpublished={\\url{https://github.com/keras-team/keras}}, }","title":"\u5982\u4f55\u5f15\u7528 Keras?"},{"location":"0-Getting-Started/2.faq/#gpu-keras","text":"\u5982\u679c\u4f60\u4ee5 TensorFlow \u6216 CNTK \u540e\u7aef\u8fd0\u884c\uff0c\u53ea\u8981\u68c0\u6d4b\u5230\u4efb\u4f55\u53ef\u7528\u7684 GPU\uff0c\u90a3\u4e48\u4ee3\u7801\u5c06\u81ea\u52a8\u5728 GPU \u4e0a\u8fd0\u884c\u3002 \u5982\u679c\u4f60\u4ee5 Theano \u540e\u7aef\u8fd0\u884c\uff0c\u5219\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u65b9\u6cd5\u4e4b\u4e00\uff1a \u65b9\u6cd5 1 : \u4f7f\u7528 Theano flags\u3002 THEANO_FLAGS=device=gpu,floatX=float32 python my_keras_script.py \"gpu\" \u53ef\u80fd\u9700\u8981\u6839\u636e\u4f60\u7684\u8bbe\u5907\u6807\u8bc6\u7b26\uff08\u4f8b\u5982gpu0\uff0cgpu1\u7b49\uff09\u8fdb\u884c\u66f4\u6539\u3002 \u65b9\u6cd5 2 : \u521b\u5efa .theanorc : \u6307\u5bfc\u6559\u7a0b \u65b9\u6cd5 3 : \u5728\u4ee3\u7801\u7684\u5f00\u5934\u624b\u52a8\u8bbe\u7f6e theano.config.device , theano.config.floatX \uff1a import theano theano.config.device = 'gpu' theano.config.floatX = 'float32'","title":"\u5982\u4f55\u5728 GPU \u4e0a\u8fd0\u884c Keras?"},{"location":"0-Getting-Started/2.faq/#gpu-keras_1","text":"\u6211\u4eec\u5efa\u8bae\u4f7f\u7528 TensorFlow \u540e\u7aef\u6765\u6267\u884c\u8fd9\u9879\u4efb\u52a1\u3002\u6709\u4e24\u79cd\u65b9\u6cd5\u53ef\u5728\u591a\u4e2a GPU \u4e0a\u8fd0\u884c\u5355\u4e2a\u6a21\u578b\uff1a \u6570\u636e\u5e76\u884c \u548c \u8bbe\u5907\u5e76\u884c \u3002 \u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\uff0c\u4f60\u6700\u9700\u8981\u7684\u662f\u6570\u636e\u5e76\u884c\u3002","title":"\u5982\u4f55\u5728\u591a GPU \u4e0a\u8fd0\u884c Keras \u6a21\u578b?"},{"location":"0-Getting-Started/2.faq/#_1","text":"\u6570\u636e\u5e76\u884c\u5305\u62ec\u5728\u6bcf\u4e2a\u8bbe\u5907\u4e0a\u590d\u5236\u4e00\u6b21\u76ee\u6807\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u6bcf\u4e2a\u6a21\u578b\u526f\u672c\u5904\u7406\u4e0d\u540c\u90e8\u5206\u7684\u8f93\u5165\u6570\u636e\u3002Keras \u6709\u4e00\u4e2a\u5185\u7f6e\u7684\u5b9e\u7528\u51fd\u6570 keras.utils.multi_gpu_model \uff0c\u5b83\u53ef\u4ee5\u751f\u6210\u4efb\u4f55\u6a21\u578b\u7684\u6570\u636e\u5e76\u884c\u7248\u672c\uff0c\u5728\u591a\u8fbe 8 \u4e2a GPU \u4e0a\u5b9e\u73b0\u51c6\u7ebf\u6027\u52a0\u901f\u3002 \u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 multi_gpu_model \u7684\u6587\u6863\u3002\u8fd9\u91cc\u662f\u4e00\u4e2a\u5feb\u901f\u7684\u4f8b\u5b50\uff1a from keras.utils import multi_gpu_model # \u5c06 `model` \u590d\u5236\u5230 8 \u4e2a GPU \u4e0a\u3002 # \u5047\u5b9a\u4f60\u7684\u673a\u5668\u6709 8 \u4e2a\u53ef\u7528\u7684 GPU\u3002 parallel_model = multi_gpu_model(model, gpus=8) parallel_model.compile(loss='categorical_crossentropy', optimizer='rmsprop') # \u8fd9\u4e2a `fit` \u8c03\u7528\u5c06\u5206\u5e03\u5728 8 \u4e2a GPU \u4e0a\u3002 # \u7531\u4e8e batch size \u4e3a 256\uff0c\u6bcf\u4e2a GPU \u5c06\u5904\u7406 32 \u4e2a\u6837\u672c\u3002 parallel_model.fit(x, y, epochs=20, batch_size=256)","title":"\u6570\u636e\u5e76\u884c"},{"location":"0-Getting-Started/2.faq/#_2","text":"\u8bbe\u5907\u5e76\u884c\u6027\u5305\u62ec\u5728\u4e0d\u540c\u8bbe\u5907\u4e0a\u8fd0\u884c\u540c\u4e00\u6a21\u578b\u7684\u4e0d\u540c\u90e8\u5206\u3002\u5bf9\u4e8e\u5177\u6709\u5e76\u884c\u4f53\u7cfb\u7ed3\u6784\u7684\u6a21\u578b\uff0c\u4f8b\u5982\u6709\u4e24\u4e2a\u5206\u652f\u7684\u6a21\u578b\uff0c\u8fd9\u79cd\u65b9\u5f0f\u5f88\u5408\u9002\u3002 \u8fd9\u79cd\u5e76\u884c\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 TensorFlow device scopes \u6765\u5b9e\u73b0\u3002\u8fd9\u91cc\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\uff1a # \u6a21\u578b\u4e2d\u5171\u4eab\u7684 LSTM \u7528\u4e8e\u5e76\u884c\u7f16\u7801\u4e24\u4e2a\u4e0d\u540c\u7684\u5e8f\u5217 input_a = keras.Input(shape=(140, 256)) input_b = keras.Input(shape=(140, 256)) shared_lstm = keras.layers.LSTM(64) # \u5728\u4e00\u4e2a GPU \u4e0a\u5904\u7406\u7b2c\u4e00\u4e2a\u5e8f\u5217 with tf.device_scope('/gpu:0'): encoded_a = shared_lstm(tweet_a) # \u5728\u53e6\u4e00\u4e2a GPU\u4e0a \u5904\u7406\u4e0b\u4e00\u4e2a\u5e8f\u5217 with tf.device_scope('/gpu:1'): encoded_b = shared_lstm(tweet_b) # \u5728 CPU \u4e0a\u8fde\u63a5\u7ed3\u679c with tf.device_scope('/cpu:0'): merged_vector = keras.layers.concatenate([encoded_a, encoded_b], axis=-1)","title":"\u8bbe\u5907\u5e76\u884c"},{"location":"0-Getting-Started/2.faq/#sample-batch-epoch","text":"\u4e3a\u4e86\u6b63\u786e\u5730\u4f7f\u7528 Keras\uff0c\u4ee5\u4e0b\u662f\u5fc5\u987b\u4e86\u89e3\u548c\u7406\u89e3\u7684\u4e00\u4e9b\u5e38\u89c1\u5b9a\u4e49\uff1a Sample : \u6837\u672c\uff0c\u6570\u636e\u96c6\u4e2d\u7684\u4e00\u4e2a\u5143\u7d20\uff0c\u4e00\u6761\u6570\u636e\u3002 \u4f8b1: \u5728\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4e2d\uff0c\u4e00\u5f20\u56fe\u50cf\u662f\u4e00\u4e2a\u6837\u672c\u3002 \u4f8b2: \u5728\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u4e2d\uff0c\u4e00\u6bb5\u97f3\u9891\u662f\u4e00\u4e2a\u6837\u672c\u3002 Batch : \u6279\uff0c\u542b\u6709 N \u4e2a\u6837\u672c\u7684\u96c6\u5408\u3002\u6bcf\u4e00\u4e2a batch \u7684\u6837\u672c\u90fd\u662f\u72ec\u7acb\u5e76\u884c\u5904\u7406\u7684\u3002\u5728\u8bad\u7ec3\u65f6\uff0c\u4e00\u4e2a batch \u7684\u7ed3\u679c\u53ea\u4f1a\u7528\u6765\u66f4\u65b0\u4e00\u6b21\u6a21\u578b\u3002 \u4e00\u4e2a batch \u7684\u6837\u672c\u901a\u5e38\u6bd4\u5355\u4e2a\u8f93\u5165\u66f4\u63a5\u8fd1\u4e8e\u603b\u4f53\u8f93\u5165\u6570\u636e\u7684\u5206\u5e03\uff0cbatch \u8d8a\u5927\u5c31\u8d8a\u8fd1\u4f3c\u3002\u7136\u800c\uff0c\u6bcf\u4e2a batch \u5c06\u82b1\u8d39\u66f4\u957f\u7684\u65f6\u95f4\u6765\u5904\u7406\uff0c\u5e76\u4e14\u4ecd\u7136\u53ea\u66f4\u65b0\u6a21\u578b\u4e00\u6b21\u3002\u5728\u63a8\u7406\uff08\u8bc4\u4f30/\u9884\u6d4b\uff09\u65f6\uff0c\u5efa\u8bae\u6761\u4ef6\u5141\u8bb8\u7684\u60c5\u51b5\u4e0b\u9009\u62e9\u4e00\u4e2a\u5c3d\u53ef\u80fd\u5927\u7684 batch\uff0c\uff08\u56e0\u4e3a\u8f83\u5927\u7684 batch \u901a\u5e38\u8bc4\u4f30/\u9884\u6d4b\u7684\u901f\u5ea6\u4f1a\u66f4\u5feb\uff09\u3002 Epoch : \u8f6e\u6b21\uff0c\u901a\u5e38\u88ab\u5b9a\u4e49\u4e3a \u300c\u5728\u6574\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u4e00\u8f6e\u8fed\u4ee3\u300d\uff0c\u7528\u4e8e\u8bad\u7ec3\u7684\u4e0d\u540c\u7684\u9636\u6bb5\uff0c\u8fd9\u6709\u5229\u4e8e\u8bb0\u5f55\u548c\u5b9a\u671f\u8bc4\u4f30\u3002 \u5f53\u5728 Keras \u6a21\u578b\u7684 fit \u65b9\u6cd5\u4e2d\u4f7f\u7528 validation_data \u6216 validation_split \u65f6\uff0c\u8bc4\u4f30\u5c06\u5728\u6bcf\u4e2a epoch \u7ed3\u675f\u65f6\u8fd0\u884c\u3002 \u5728 Keras \u4e2d\uff0c\u53ef\u4ee5\u6dfb\u52a0\u4e13\u95e8\u7684\u7528\u4e8e\u5728 epoch \u7ed3\u675f\u65f6\u8fd0\u884c\u7684 callbacks \u56de\u8c03 \u3002\u4f8b\u5982\u5b66\u4e60\u7387\u53d8\u5316\u548c\u6a21\u578b\u68c0\u67e5\u70b9\uff08\u4fdd\u5b58\uff09\u3002","title":"\"sample\", \"batch\", \"epoch\" \u5206\u522b\u662f\u4ec0\u4e48\uff1f"},{"location":"0-Getting-Started/2.faq/#keras_1","text":"","title":"\u5982\u4f55\u4fdd\u5b58 Keras \u6a21\u578b\uff1f"},{"location":"0-Getting-Started/2.faq/#_3","text":"\u4e0d\u5efa\u8bae\u4f7f\u7528 pickle \u6216 cPickle \u6765\u4fdd\u5b58 Keras \u6a21\u578b\u3002 \u4f60\u53ef\u4ee5\u4f7f\u7528 model.save(filepath) \u5c06 Keras \u6a21\u578b\u4fdd\u5b58\u5230\u5355\u4e2a HDF5 \u6587\u4ef6\u4e2d\uff0c\u8be5\u6587\u4ef6\u5c06\u5305\u542b\uff1a \u6a21\u578b\u7684\u7ed3\u6784\uff0c\u5141\u8bb8\u91cd\u65b0\u521b\u5efa\u6a21\u578b \u6a21\u578b\u7684\u6743\u91cd \u8bad\u7ec3\u914d\u7f6e\u9879\uff08\u635f\u5931\u51fd\u6570\uff0c\u4f18\u5316\u5668\uff09 \u4f18\u5316\u5668\u72b6\u6001\uff0c\u5141\u8bb8\u51c6\u786e\u5730\u4ece\u4f60\u4e0a\u6b21\u7ed3\u675f\u7684\u5730\u65b9\u7ee7\u7eed\u8bad\u7ec3\u3002 \u4f60\u53ef\u4ee5\u4f7f\u7528 keras.models.load_model(filepath) \u91cd\u65b0\u5b9e\u4f8b\u5316\u6a21\u578b\u3002 load_model \u8fd8\u5c06\u8d1f\u8d23\u4f7f\u7528\u4fdd\u5b58\u7684\u8bad\u7ec3\u914d\u7f6e\u9879\u6765\u7f16\u8bd1\u6a21\u578b\uff08\u9664\u975e\u6a21\u578b\u4ece\u672a\u7f16\u8bd1\u8fc7\uff09\u3002 \u4f8b\u5b50\uff1a from keras.models import load_model model.save('my_model.h5') # \u521b\u5efa HDF5 \u6587\u4ef6 'my_model.h5' del model # \u5220\u9664\u73b0\u6709\u6a21\u578b # \u8fd4\u56de\u4e00\u4e2a\u7f16\u8bd1\u597d\u7684\u6a21\u578b # \u4e0e\u4e4b\u524d\u90a3\u4e2a\u76f8\u540c model = load_model('my_model.h5') \u53e6\u8bf7\u53c2\u9605 \u5982\u4f55\u5b89\u88c5 HDF5 \u6216 h5py \u4ee5\u5728 Keras \u4e2d\u4fdd\u5b58\u6211\u7684\u6a21\u578b\uff1f \uff0c\u67e5\u770b\u6709\u5173\u5982\u4f55\u5b89\u88c5 h5py \u7684\u8bf4\u660e\u3002","title":"\u4fdd\u5b58/\u52a0\u8f7d\u6574\u4e2a\u6a21\u578b\uff08\u7ed3\u6784 + \u6743\u91cd + \u4f18\u5316\u5668\u72b6\u6001\uff09"},{"location":"0-Getting-Started/2.faq/#_4","text":"\u5982\u679c\u60a8\u53ea\u9700\u8981\u4fdd\u5b58 \u6a21\u578b\u7684\u7ed3\u6784 \uff0c\u800c\u975e\u5176\u6743\u91cd\u6216\u8bad\u7ec3\u914d\u7f6e\u9879\uff0c\u5219\u53ef\u4ee5\u6267\u884c\u4ee5\u4e0b\u64cd\u4f5c\uff1a # \u4fdd\u5b58\u4e3a JSON json_string = model.to_json() # \u4fdd\u5b58\u4e3a YAML yaml_string = model.to_yaml() \u751f\u6210\u7684 JSON/YAML \u6587\u4ef6\u662f\u4eba\u7c7b\u53ef\u8bfb\u7684\uff0c\u5982\u679c\u9700\u8981\u8fd8\u53ef\u4ee5\u624b\u52a8\u7f16\u8f91\u3002 \u4f60\u53ef\u4ee5\u4ece\u8fd9\u4e9b\u6570\u636e\u5efa\u7acb\u4e00\u4e2a\u65b0\u7684\u6a21\u578b\uff1a # \u4ece JSON \u91cd\u5efa\u6a21\u578b\uff1a from keras.models import model_from_json model = model_from_json(json_string) # \u4ece YAML \u91cd\u5efa\u6a21\u578b\uff1a from keras.models import model_from_yaml model = model_from_yaml(yaml_string)","title":"\u53ea\u4fdd\u5b58/\u52a0\u8f7d\u6a21\u578b\u7684\u7ed3\u6784"},{"location":"0-Getting-Started/2.faq/#_5","text":"\u5982\u679c\u60a8\u53ea\u9700\u8981 \u6a21\u578b\u7684\u6743\u91cd \uff0c\u53ef\u4ee5\u4f7f\u7528\u4e0b\u9762\u7684\u4ee3\u7801\u4ee5 HDF5 \u683c\u5f0f\u8fdb\u884c\u4fdd\u5b58\u3002 \u8bf7\u6ce8\u610f\uff0c\u6211\u4eec\u9996\u5148\u9700\u8981\u5b89\u88c5 HDF5 \u548c Python \u5e93 h5py\uff0c\u5b83\u4eec\u4e0d\u5305\u542b\u5728 Keras \u4e2d\u3002 model.save_weights('my_model_weights.h5') \u5047\u8bbe\u4f60\u6709\u7528\u4e8e\u5b9e\u4f8b\u5316\u6a21\u578b\u7684\u4ee3\u7801\uff0c\u5219\u53ef\u4ee5\u5c06\u4fdd\u5b58\u7684\u6743\u91cd\u52a0\u8f7d\u5230\u5177\u6709\u76f8\u540c\u7ed3\u6784\u7684\u6a21\u578b\u4e2d\uff1a model.load_weights('my_model_weights.h5') \u5982\u679c\u4f60\u9700\u8981\u5c06\u6743\u91cd\u52a0\u8f7d\u5230\u4e0d\u540c\u7684\u7ed3\u6784\uff08\u6709\u4e00\u4e9b\u5171\u540c\u5c42\uff09\u7684\u6a21\u578b\u4e2d\uff0c\u4f8b\u5982\u5fae\u8c03\u6216\u8fc1\u79fb\u5b66\u4e60\uff0c\u5219\u53ef\u4ee5\u6309\u5c42\u7684\u540d\u5b57\u6765\u52a0\u8f7d\u6743\u91cd\uff1a model.load_weights('my_model_weights.h5', by_name=True) \u4f8b\u5b50\uff1a \"\"\" \u5047\u8bbe\u539f\u59cb\u6a21\u578b\u5982\u4e0b\u6240\u793a\uff1a model = Sequential() model.add(Dense(2, input_dim=3, name='dense_1')) model.add(Dense(3, name='dense_2')) ... model.save_weights(fname) \"\"\" # \u65b0\u6a21\u578b model = Sequential() model.add(Dense(2, input_dim=3, name='dense_1')) # \u5c06\u88ab\u52a0\u8f7d model.add(Dense(10, name='new_dense')) # \u5c06\u4e0d\u88ab\u52a0\u8f7d # \u4ece\u7b2c\u4e00\u4e2a\u6a21\u578b\u52a0\u8f7d\u6743\u91cd\uff1b\u53ea\u4f1a\u5f71\u54cd\u7b2c\u4e00\u5c42\uff0cdense_1 model.load_weights(fname, by_name=True)","title":"\u53ea\u4fdd\u5b58/\u52a0\u8f7d\u6a21\u578b\u7684\u6743\u91cd"},{"location":"0-Getting-Started/2.faq/#_6","text":"\u5982\u679c\u8981\u52a0\u8f7d\u7684\u6a21\u578b\u5305\u542b\u81ea\u5b9a\u4e49\u5c42\u6216\u5176\u4ed6\u81ea\u5b9a\u4e49\u7c7b\u6216\u51fd\u6570\uff0c\u5219\u53ef\u4ee5\u901a\u8fc7 custom_objects \u53c2\u6570\u5c06\u5b83\u4eec\u4f20\u9012\u7ed9\u52a0\u8f7d\u673a\u5236\uff1a from keras.models import load_model # \u5047\u8bbe\u4f60\u7684\u6a21\u578b\u5305\u542b\u4e00\u4e2a AttentionLayer \u7c7b\u7684\u5b9e\u4f8b model = load_model('my_model.h5', custom_objects={'AttentionLayer': AttentionLayer}) \u6216\u8005\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528 \u81ea\u5b9a\u4e49\u5bf9\u8c61\u4f5c\u7528\u57df \uff1a from keras.utils import CustomObjectScope with CustomObjectScope({'AttentionLayer': AttentionLayer}): model = load_model('my_model.h5') \u81ea\u5b9a\u4e49\u5bf9\u8c61\u7684\u5904\u7406\u4e0e load_model , model_from_json , model_from_yaml \u7684\u5de5\u4f5c\u65b9\u5f0f\u76f8\u540c\uff1a from keras.models import model_from_json model = model_from_json(json_string, custom_objects={'AttentionLayer': AttentionLayer})","title":"\u5904\u7406\u5df2\u4fdd\u5b58\u6a21\u578b\u4e2d\u7684\u81ea\u5b9a\u4e49\u5c42\uff08\u6216\u5176\u4ed6\u81ea\u5b9a\u4e49\u5bf9\u8c61\uff09"},{"location":"0-Getting-Started/2.faq/#_7","text":"Keras \u6a21\u578b\u6709\u4e24\u79cd\u6a21\u5f0f\uff1a\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002\u6b63\u5219\u5316\u673a\u5236\uff0c\u5982 Dropout \u548c L1/L2 \u6743\u91cd\u6b63\u5219\u5316\uff0c\u5728\u6d4b\u8bd5\u65f6\u662f\u5173\u95ed\u7684\u3002 \u6b64\u5916\uff0c\u8bad\u7ec3\u8bef\u5dee\u662f\u6bcf\u6279\u8bad\u7ec3\u6570\u636e\u7684\u5e73\u5747\u8bef\u5dee\u3002\u7531\u4e8e\u4f60\u7684\u6a21\u578b\u662f\u968f\u7740\u65f6\u95f4\u800c\u53d8\u5316\u7684\uff0c\u4e00\u4e2a epoch \u4e2d\u7684\u7b2c\u4e00\u6279\u6570\u636e\u7684\u8bef\u5dee\u901a\u5e38\u6bd4\u6700\u540e\u4e00\u6279\u7684\u8981\u9ad8\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u6d4b\u8bd5\u8bef\u5dee\u662f\u6a21\u578b\u5728\u4e00\u4e2a epoch \u8bad\u7ec3\u5b8c\u540e\u8ba1\u7b97\u7684\uff0c\u56e0\u800c\u8bef\u5dee\u8f83\u5c0f\u3002","title":"\u4e3a\u4ec0\u4e48\u8bad\u7ec3\u8bef\u5dee\u6bd4\u6d4b\u8bd5\u8bef\u5dee\u9ad8\u5f88\u591a\uff1f"},{"location":"0-Getting-Started/2.faq/#_8","text":"\u4e00\u4e2a\u7b80\u5355\u7684\u65b9\u6cd5\u662f\u521b\u5efa\u4e00\u4e2a\u65b0\u7684 Model \u6765\u8f93\u51fa\u4f60\u6240\u611f\u5174\u8da3\u7684\u5c42\uff1a from keras.models import Model model = ... # \u521b\u5efa\u539f\u59cb\u6a21\u578b layer_name = 'my_layer' intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output) intermediate_output = intermediate_layer_model.predict(data) \u6216\u8005\uff0c\u4f60\u4e5f\u53ef\u4ee5\u6784\u5efa\u4e00\u4e2a Keras \u51fd\u6570\uff0c\u8be5\u51fd\u6570\u5c06\u5728\u7ed9\u5b9a\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\u8fd4\u56de\u67d0\u4e2a\u5c42\u7684\u8f93\u51fa\uff0c\u4f8b\u5982\uff1a from keras import backend as K # \u4ee5 Sequential \u6a21\u578b\u4e3a\u4f8b get_3rd_layer_output = K.function([model.layers[0].input], [model.layers[3].output]) layer_output = get_3rd_layer_output([x])[0] \u540c\u6837\uff0c\u4f60\u53ef\u4ee5\u76f4\u63a5\u5efa\u7acb\u4e00\u4e2a Theano \u6216 TensorFlow \u51fd\u6570\u3002 \u6ce8\u610f\uff0c\u5982\u679c\u4f60\u7684\u6a21\u578b\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u9636\u6bb5\u6709\u4e0d\u540c\u7684\u884c\u4e3a\uff08\u4f8b\u5982\uff0c\u4f7f\u7528 Dropout , BatchNormalization \u7b49\uff09\uff0c\u5219\u9700\u8981\u5c06\u5b66\u4e60\u9636\u6bb5\u6807\u5fd7\u4f20\u9012\u7ed9\u4f60\u7684\u51fd\u6570\uff1a get_3rd_layer_output = K.function([model.layers[0].input, K.learning_phase()], [model.layers[3].output]) # \u6d4b\u8bd5\u6a21\u5f0f = 0 \u65f6\u7684\u8f93\u51fa layer_output = get_3rd_layer_output([x, 0])[0] # \u6d4b\u8bd5\u6a21\u5f0f = 1 \u65f6\u7684\u8f93\u51fa layer_output = get_3rd_layer_output([x, 1])[0]","title":"\u5982\u4f55\u83b7\u53d6\u4e2d\u95f4\u5c42\u7684\u8f93\u51fa\uff1f"},{"location":"0-Getting-Started/2.faq/#keras_2","text":"\u4f60\u53ef\u4ee5\u4f7f\u7528 model.train_on_batch(x\uff0cy) \u548c model.test_on_batch(x\uff0cy) \u8fdb\u884c\u6279\u91cf\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u3002\u8bf7\u53c2\u9605 \u6a21\u578b\u6587\u6863 \u3002 \u6216\u8005\uff0c\u4f60\u53ef\u4ee5\u7f16\u5199\u4e00\u4e2a\u751f\u6210\u6279\u5904\u7406\u8bad\u7ec3\u6570\u636e\u7684\u751f\u6210\u5668\uff0c\u7136\u540e\u4f7f\u7528 model.fit_generator(data_generator\uff0csteps_per_epoch\uff0cepochs) \u65b9\u6cd5\u3002 \u4f60\u53ef\u4ee5\u5728 CIFAR10 example \u4e2d\u627e\u5230\u5b9e\u8df5\u4ee3\u7801\u3002","title":"\u5982\u4f55\u7528 Keras \u5904\u7406\u8d85\u8fc7\u5185\u5b58\u7684\u6570\u636e\u96c6\uff1f"},{"location":"0-Getting-Started/2.faq/#_9","text":"\u4f60\u53ef\u4ee5\u4f7f\u7528 EarlyStopping \u56de\u8c03\uff1a from keras.callbacks import EarlyStopping early_stopping = EarlyStopping(monitor='val_loss', patience=2) model.fit(x, y, validation_split=0.2, callbacks=[early_stopping]) \u66f4\u591a\u4fe1\u606f\u8bf7\u67e5\u770b callbacks \u6587\u6863 \u3002","title":"\u5728\u9a8c\u8bc1\u96c6\u7684\u8bef\u5dee\u4e0d\u518d\u4e0b\u964d\u65f6\uff0c\u5982\u4f55\u4e2d\u65ad\u8bad\u7ec3\uff1f"},{"location":"0-Getting-Started/2.faq/#_10","text":"\u5982\u679c\u60a8\u5c06 model.fit \u4e2d\u7684 validation_split \u53c2\u6570\u8bbe\u7f6e\u4e3a 0.1\uff0c\u90a3\u4e48\u4f7f\u7528\u7684\u9a8c\u8bc1\u6570\u636e\u5c06\u662f\u6700\u540e 10\uff05 \u7684\u6570\u636e\u3002\u5982\u679c\u8bbe\u7f6e\u4e3a 0.25\uff0c\u5c31\u662f\u6700\u540e 25% \u7684\u6570\u636e\u3002\u6ce8\u610f\uff0c\u5728\u63d0\u53d6\u5206\u5272\u9a8c\u8bc1\u96c6\u4e4b\u524d\uff0c\u6570\u636e\u4e0d\u4f1a\u88ab\u6df7\u6d17\uff0c\u56e0\u6b64\u9a8c\u8bc1\u96c6\u4ec5\u4ec5\u662f\u4f20\u9012\u7684\u8f93\u5165\u4e2d\u6700\u540e\u4e00\u4e2a x\uff05 \u7684\u6837\u672c\u3002 \u6240\u6709 epoch \u90fd\u4f7f\u7528\u76f8\u540c\u7684\u9a8c\u8bc1\u96c6\uff08\u5728\u540c\u4e00\u4e2a fit \u4e2d\u8c03\u7528\uff09\u3002","title":"\u9a8c\u8bc1\u96c6\u5212\u5206\u662f\u5982\u4f55\u8ba1\u7b97\u7684\uff1f"},{"location":"0-Getting-Started/2.faq/#_11","text":"\u662f\u7684\uff0c\u5982\u679c model.fit \u4e2d\u7684 shuffle \u53c2\u6570\u8bbe\u7f6e\u4e3a True\uff08\u9ed8\u8ba4\u503c\uff09\uff0c\u5219\u8bad\u7ec3\u6570\u636e\u5c06\u5728\u6bcf\u4e2a epoch \u6df7\u6d17\u3002 \u9a8c\u8bc1\u96c6\u6c38\u8fdc\u4e0d\u4f1a\u6df7\u6d17\u3002","title":"\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6570\u636e\u662f\u5426\u4f1a\u6df7\u6d17\uff1f"},{"location":"0-Getting-Started/2.faq/#epoch","text":"model.fit \u65b9\u6cd5\u8fd4\u56de\u4e00\u4e2a History \u56de\u8c03\uff0c\u5b83\u5177\u6709\u5305\u542b\u8fde\u7eed\u8bef\u5dee\u7684\u5217\u8868\u548c\u5176\u4ed6\u5ea6\u91cf\u7684 history \u5c5e\u6027\u3002 hist = model.fit(x, y, validation_split=0.2) print(hist.history)","title":"\u5982\u4f55\u5728\u6bcf\u4e2a epoch \u540e\u8bb0\u5f55\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u7684\u8bef\u5dee\u548c\u51c6\u786e\u7387\uff1f"},{"location":"0-Getting-Started/2.faq/#_12","text":"\u300c\u51bb\u7ed3\u300d\u4e00\u4e2a\u5c42\u610f\u5473\u7740\u5c06\u5176\u6392\u9664\u5728\u8bad\u7ec3\u4e4b\u5916\uff0c\u5373\u5176\u6743\u91cd\u5c06\u6c38\u8fdc\u4e0d\u4f1a\u66f4\u65b0\u3002\u8fd9\u5728\u5fae\u8c03\u6a21\u578b\u6216\u4f7f\u7528\u56fa\u5b9a\u7684\u8bcd\u5411\u91cf\u8fdb\u884c\u6587\u672c\u8f93\u5165\u4e2d\u5f88\u6709\u7528\u3002 \u60a8\u53ef\u4ee5\u5c06 trainable \u53c2\u6570\uff08\u5e03\u5c14\u503c\uff09\u4f20\u9012\u7ed9\u4e00\u4e2a\u5c42\u7684\u6784\u9020\u5668\uff0c\u4ee5\u5c06\u8be5\u5c42\u8bbe\u7f6e\u4e3a\u4e0d\u53ef\u8bad\u7ec3\u7684\uff1a frozen_layer = Dense(32, trainable=False) \u53e6\u5916\uff0c\u53ef\u4ee5\u5728\u5b9e\u4f8b\u5316\u4e4b\u540e\u5c06\u7f51\u7edc\u5c42\u7684 trainable \u5c5e\u6027\u8bbe\u7f6e\u4e3a True \u6216 False\u3002\u4e3a\u4e86\u4f7f\u4e4b\u751f\u6548\uff0c\u5728\u4fee\u6539 trainable \u5c5e\u6027\u4e4b\u540e\uff0c\u9700\u8981\u5728\u6a21\u578b\u4e0a\u8c03\u7528 compile() \u3002\u8fd9\u662f\u4e00\u4e2a\u4f8b\u5b50\uff1a x = Input(shape=(32,)) layer = Dense(32) layer.trainable = False y = layer(x) frozen_model = Model(x, y) # \u5728\u4e0b\u9762\u7684\u6a21\u578b\u4e2d\uff0c\u8bad\u7ec3\u671f\u95f4\u4e0d\u4f1a\u66f4\u65b0\u5c42\u7684\u6743\u91cd frozen_model.compile(optimizer='rmsprop', loss='mse') layer.trainable = True trainable_model = Model(x, y) # \u4f7f\u7528\u8fd9\u4e2a\u6a21\u578b\uff0c\u8bad\u7ec3\u671f\u95f4 `layer` \u7684\u6743\u91cd\u5c06\u88ab\u66f4\u65b0 # (\u8fd9\u4e5f\u4f1a\u5f71\u54cd\u4e0a\u9762\u7684\u6a21\u578b\uff0c\u56e0\u4e3a\u5b83\u4f7f\u7528\u4e86\u540c\u4e00\u4e2a\u7f51\u7edc\u5c42\u5b9e\u4f8b) trainable_model.compile(optimizer='rmsprop', loss='mse') frozen_model.fit(data, labels) # \u8fd9\u4e0d\u4f1a\u66f4\u65b0 `layer` \u7684\u6743\u91cd trainable_model.fit(data, labels) # \u8fd9\u4f1a\u66f4\u65b0 `layer` \u7684\u6743\u91cd","title":"\u5982\u4f55\u300c\u51bb\u7ed3\u300d\u7f51\u7edc\u5c42\uff1f"},{"location":"0-Getting-Started/2.faq/#rnn-stateful-rnns","text":"\u4f7f RNN \u5177\u6709\u72b6\u6001\u610f\u5473\u7740\u6bcf\u6279\u6837\u54c1\u7684\u72b6\u6001\u5c06\u88ab\u91cd\u65b0\u7528\u4f5c\u4e0b\u4e00\u6279\u6837\u54c1\u7684\u521d\u59cb\u72b6\u6001\u3002 \u5f53\u4f7f\u7528\u6709\u72b6\u6001 RNN \u65f6\uff0c\u5047\u5b9a\uff1a \u6240\u6709\u7684\u6279\u6b21\u90fd\u6709\u76f8\u540c\u6570\u91cf\u7684\u6837\u672c \u5982\u679c x1 \u548c x2 \u662f\u8fde\u7eed\u6279\u6b21\u7684\u6837\u672c\uff0c\u5219 x2[i] \u662f x1[i] \u7684\u540e\u7eed\u5e8f\u5217\uff0c\u5bf9\u4e8e\u6bcf\u4e2a i \u3002 \u8981\u5728 RNN \u4e2d\u4f7f\u7528\u72b6\u6001\uff0c\u4f60\u9700\u8981: \u901a\u8fc7\u5c06 batch_size \u53c2\u6570\u4f20\u9012\u7ed9\u6a21\u578b\u7684\u7b2c\u4e00\u5c42\u6765\u663e\u5f0f\u6307\u5b9a\u4f60\u6b63\u5728\u4f7f\u7528\u7684\u6279\u5927\u5c0f\u3002\u4f8b\u5982\uff0c\u5bf9\u4e8e 10 \u4e2a\u65f6\u95f4\u6b65\u957f\u7684 32 \u6837\u672c\u7684 batch\uff0c\u6bcf\u4e2a\u65f6\u95f4\u6b65\u957f\u5177\u6709 16 \u4e2a\u7279\u5f81\uff0c batch_size = 32 \u3002 \u5728 RNN \u5c42\u4e2d\u8bbe\u7f6e stateful = True \u3002 \u5728\u8c03\u7528 fit() \u65f6\u6307\u5b9a shuffle = False \u3002 \u91cd\u7f6e\u7d2f\u79ef\u72b6\u6001\uff1a \u4f7f\u7528 model.reset_states() \u6765\u91cd\u7f6e\u6a21\u578b\u4e2d\u6240\u6709\u5c42\u7684\u72b6\u6001 \u4f7f\u7528 layer.reset_states() \u6765\u91cd\u7f6e\u6307\u5b9a\u6709\u72b6\u6001 RNN \u5c42\u7684\u72b6\u6001 \u4f8b\u5b50\uff1a x # \u8f93\u5165\u6570\u636e\uff0c\u5c3a\u5bf8\u4e3a (32, 21, 16) # \u5c06\u6b65\u957f\u4e3a 10 \u7684\u5e8f\u5217\u8f93\u9001\u5230\u6a21\u578b\u4e2d model = Sequential() model.add(LSTM(32, input_shape=(10, 16), batch_size=32, stateful=True)) model.add(Dense(16, activation='softmax')) model.compile(optimizer='rmsprop', loss='categorical_crossentropy') # \u8bad\u7ec3\u7f51\u7edc\uff0c\u6839\u636e\u7ed9\u5b9a\u7684\u524d 10 \u4e2a\u65f6\u95f4\u6b65\uff0c\u6765\u9884\u6d4b\u7b2c 11 \u4e2a\u65f6\u95f4\u6b65\uff1a model.train_on_batch(x[:, :10, :], np.reshape(x[:, 10, :], (32, 16))) # \u7f51\u7edc\u7684\u72b6\u6001\u5df2\u7ecf\u6539\u53d8\u3002\u6211\u4eec\u53ef\u4ee5\u63d0\u4f9b\u540e\u7eed\u5e8f\u5217\uff1a model.train_on_batch(x[:, 10:20, :], np.reshape(x[:, 20, :], (32, 16))) # \u91cd\u7f6e LSTM \u5c42\u7684\u72b6\u6001\uff1a model.reset_states() # \u53e6\u4e00\u79cd\u91cd\u7f6e\u65b9\u6cd5\uff1a model.layers[0].reset_states() \u8bf7\u6ce8\u610f\uff0c predict , fit , train_on_batch , predict_classes \u7b49\u65b9\u6cd5 \u5168\u90e8 \u90fd\u4f1a\u66f4\u65b0\u6a21\u578b\u4e2d\u6709\u72b6\u6001\u5c42\u7684\u72b6\u6001\u3002\u8fd9\u4f7f\u4f60\u4e0d\u4ec5\u53ef\u4ee5\u8fdb\u884c\u6709\u72b6\u6001\u7684\u8bad\u7ec3\uff0c\u8fd8\u53ef\u4ee5\u8fdb\u884c\u6709\u72b6\u6001\u7684\u9884\u6d4b\u3002","title":"\u5982\u4f55\u4f7f\u7528\u6709\u72b6\u6001 RNN (stateful RNNs)?"},{"location":"0-Getting-Started/2.faq/#sequential","text":"\u4f60\u53ef\u4ee5\u901a\u8fc7\u8c03\u7528 .pop() \u6765\u5220\u9664 Sequential \u6a21\u578b\u4e2d\u6700\u540e\u6dfb\u52a0\u7684\u5c42\uff1a model = Sequential() model.add(Dense(32, activation='relu', input_dim=784)) model.add(Dense(32, activation='relu')) print(len(model.layers)) # \"2\" model.pop() print(len(model.layers)) # \"1\"","title":"\u5982\u4f55\u4ece Sequential \u6a21\u578b\u4e2d\u79fb\u9664\u4e00\u4e2a\u5c42\uff1f"},{"location":"0-Getting-Started/2.faq/#keras_3","text":"\u6211\u4eec\u63d0\u4f9b\u4e86\u4ee5\u4e0b\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u7684\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u7684\u6743\u91cd\uff1a Xception VGG16 VGG19 ResNet50 Inception v3 Inception-ResNet v2 MobileNet v1 DenseNet NASNet MobileNet v2 \u5b83\u4eec\u53ef\u4ee5\u4f7f\u7528 keras.applications \u6a21\u5757\u8fdb\u884c\u5bfc\u5165\uff1a from keras.applications.xception import Xception from keras.applications.vgg16 import VGG16 from keras.applications.vgg19 import VGG19 from keras.applications.resnet50 import ResNet50 from keras.applications.inception_v3 import InceptionV3 from keras.applications.inception_resnet_v2 import InceptionResNetV2 from keras.applications.mobilenet import MobileNet from keras.applications.densenet import DenseNet121 from keras.applications.densenet import DenseNet169 from keras.applications.densenet import DenseNet201 from keras.applications.nasnet import NASNetLarge from keras.applications.nasnet import NASNetMobile from keras.applications.mobilenet_v2 import MobileNetV2 model = VGG16(weights='imagenet', include_top=True) \u6709\u5173\u4e00\u4e9b\u7b80\u5355\u7684\u7528\u6cd5\u793a\u4f8b\uff0c\u8bf7\u53c2\u9605 Applications \u6a21\u5757\u7684\u6587\u6863 \u3002 \u6709\u5173\u5982\u4f55\u4f7f\u7528\u6b64\u7c7b\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u6216\u5fae\u8c03\u7684\u8be6\u7ec6\u793a\u4f8b\uff0c\u8bf7\u53c2\u9605 \u6b64\u535a\u5ba2\u6587\u7ae0 \u3002 VGG16 \u6a21\u578b\u4e5f\u662f\u4ee5\u4e0b\u51e0\u4e2a Keras \u793a\u4f8b\u811a\u672c\u7684\u57fa\u7840\uff1a Style transfer Feature visualization Deep dream","title":"\u5982\u4f55\u5728 Keras \u4e2d\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u6a21\u578b\uff1f"},{"location":"0-Getting-Started/2.faq/#keras-hdf5","text":"\u4f60\u53ef\u4ee5\u4f7f\u7528 keras.utils.io_utils \u4e2d\u7684 HDF5Matrix \u7c7b\u3002\u6709\u5173\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 HDF5Matrix\u6587\u6863 \u3002 \u4f60\u4e5f\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 HDF5 \u6570\u636e\u96c6\uff1a import h5py with h5py.File('input/file.hdf5', 'r') as f: x_data = f['x_data'] model.predict(x_data)","title":"\u5982\u4f55\u5728 Keras \u4e2d\u4f7f\u7528 HDF5 \u8f93\u5165\uff1f"},{"location":"0-Getting-Started/2.faq/#keras_4","text":"\u6240\u6709 Keras \u6570\u636e\u5b58\u50a8\u7684\u9ed8\u8ba4\u76ee\u5f55\u662f\uff1a $HOME/.keras/ \u6ce8\u610f\uff0cWindows \u7528\u6237\u5e94\u8be5\u5c06 $HOME \u66ff\u6362\u4e3a \uff05USERPROFILE\uff05 \u3002\u5982\u679c Keras \u65e0\u6cd5\u521b\u5efa\u4e0a\u8ff0\u76ee\u5f55\uff08\u4f8b\u5982\uff0c\u7531\u4e8e\u6743\u9650\u95ee\u9898\uff09\uff0c\u5219\u4f7f\u7528 /tmp/.keras/ \u4f5c\u4e3a\u5907\u4efd\u3002 Keras\u914d\u7f6e\u6587\u4ef6\u662f\u5b58\u50a8\u5728 $HOME/.keras/keras.json \u4e2d\u7684 JSON \u6587\u4ef6\u3002\u9ed8\u8ba4\u7684\u914d\u7f6e\u6587\u4ef6\u5982\u4e0b\u6240\u793a\uff1a { \"image_data_format\": \"channels_last\", \"epsilon\": 1e-07, \"floatx\": \"float32\", \"backend\": \"tensorflow\" } \u5b83\u5305\u542b\u4ee5\u4e0b\u5b57\u6bb5\uff1a \u56fe\u50cf\u5904\u7406\u5c42\u548c\u5b9e\u7528\u7a0b\u5e8f\u6240\u4f7f\u7528\u7684\u9ed8\u8ba4\u503c\u56fe\u50cf\u6570\u636e\u683c\u5f0f\uff08 channels_last \u6216 channels_first \uff09\u3002 \u7528\u4e8e\u9632\u6b62\u5728\u67d0\u4e9b\u64cd\u4f5c\u4e2d\u88ab\u96f6\u9664\u7684 epsilon \u6a21\u7cca\u56e0\u5b50\u3002 \u9ed8\u8ba4\u6d6e\u70b9\u6570\u636e\u7c7b\u578b\u3002 \u9ed8\u8ba4\u540e\u7aef\u3002\u8be6\u89c1 backend \u6587\u6863 \u3002 \u540c\u6837\uff0c\u7f13\u5b58\u7684\u6570\u636e\u96c6\u6587\u4ef6\uff08\u5982\u4f7f\u7528 get_file() \u4e0b\u8f7d\u7684\u6587\u4ef6\uff09\u9ed8\u8ba4\u5b58\u50a8\u5728 $HOME/.keras/datasets/ \u4e2d\u3002","title":"Keras \u914d\u7f6e\u6587\u4ef6\u4fdd\u5b58\u5728\u54ea\u91cc\uff1f"},{"location":"0-Getting-Started/2.faq/#keras_5","text":"\u5728\u6a21\u578b\u7684\u5f00\u53d1\u8fc7\u7a0b\u4e2d\uff0c\u80fd\u591f\u5728\u4e00\u6b21\u6b21\u7684\u8fd0\u884c\u4e2d\u83b7\u5f97\u53ef\u590d\u73b0\u7684\u7ed3\u679c\uff0c\u4ee5\u786e\u5b9a\u6027\u80fd\u7684\u53d8\u5316\u662f\u6765\u81ea\u6a21\u578b\u8fd8\u662f\u6570\u636e\u96c6\u7684\u53d8\u5316\uff0c\u6216\u8005\u4ec5\u4ec5\u662f\u4e00\u4e9b\u65b0\u7684\u968f\u673a\u6837\u672c\u70b9\u5e26\u6765\u7684\u7ed3\u679c\uff0c\u6709\u65f6\u5019\u662f\u5f88\u6709\u7528\u5904\u7684\u3002 \u9996\u5148\uff0c\u4f60\u9700\u8981\u5728\u7a0b\u5e8f\u542f\u52a8\u4e4b\u524d\u5c06 PYTHONHASHSEED \u73af\u5883\u53d8\u91cf\u8bbe\u7f6e\u4e3a 0\uff08\u4e0d\u5728\u7a0b\u5e8f\u672c\u8eab\u5185\uff09\u3002\u5bf9\u4e8e Python 3.2.3 \u4ee5\u4e0a\u7248\u672c\uff0c\u5b83\u5bf9\u4e8e\u67d0\u4e9b\u57fa\u4e8e\u6563\u5217\u7684\u64cd\u4f5c\u5177\u6709\u53ef\u91cd\u73b0\u7684\u884c\u4e3a\u662f\u5fc5\u8981\u7684\uff08\u4f8b\u5982\uff0c\u96c6\u5408\u548c\u5b57\u5178\u7684 item \u987a\u5e8f\uff0c\u8bf7\u53c2\u9605 Python \u6587\u6863 \u548c issue #2280 \u83b7\u53d6\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff09\u3002\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\u7684\u4e00\u79cd\u65b9\u6cd5\u662f\uff0c\u5728\u8fd9\u6837\u542f\u52a8 python \u65f6\uff1a $ cat test_hash.py print(hash(\"keras\")) $ python3 test_hash.py # \u65e0\u6cd5\u590d\u73b0\u7684 hash (Python 3.2.3+) -8127205062320133199 $ python3 test_hash.py # \u65e0\u6cd5\u590d\u73b0\u7684 hash (Python 3.2.3+) 3204480642156461591 $ PYTHONHASHSEED=0 python3 test_hash.py # \u53ef\u590d\u73b0\u7684 hash 4883664951434749476 $ PYTHONHASHSEED=0 python3 test_hash.py # \u53ef\u590d\u73b0\u7684 hash 4883664951434749476 \u6b64\u5916\uff0c\u5f53\u4f7f\u7528 TensorFlow \u540e\u7aef\u5e76\u5728 GPU \u4e0a\u8fd0\u884c\u65f6\uff0c\u67d0\u4e9b\u64cd\u4f5c\u5177\u6709\u975e\u786e\u5b9a\u6027\u8f93\u51fa\uff0c\u7279\u522b\u662f tf.reduce_sum() \u3002\u8fd9\u662f\u56e0\u4e3a GPU \u5e76\u884c\u8fd0\u884c\u8bb8\u591a\u64cd\u4f5c\uff0c\u56e0\u6b64\u5e76\u4e0d\u603b\u80fd\u4fdd\u8bc1\u6267\u884c\u987a\u5e8f\u3002\u7531\u4e8e\u6d6e\u70b9\u6570\u7684\u7cbe\u5ea6\u6709\u9650\uff0c\u5373\u4f7f\u6dfb\u52a0\u51e0\u4e2a\u6570\u5b57\uff0c\u4e5f\u53ef\u80fd\u4f1a\u4ea7\u751f\u7565\u6709\u4e0d\u540c\u7684\u7ed3\u679c\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u6dfb\u52a0\u5b83\u4eec\u7684\u987a\u5e8f\u3002\u4f60\u53ef\u4ee5\u5c1d\u8bd5\u907f\u514d\u67d0\u4e9b\u975e\u786e\u5b9a\u6027\u64cd\u4f5c\uff0c\u4f46\u6709\u4e9b\u64cd\u4f5c\u53ef\u80fd\u662f\u7531 TensorFlow \u5728\u8ba1\u7b97\u68af\u5ea6\u65f6\u81ea\u52a8\u521b\u5efa\u7684\uff0c\u56e0\u6b64\u5728 CPU \u4e0a\u8fd0\u884c\u4ee3\u7801\u8981\u7b80\u5355\u5f97\u591a\u3002\u4e3a\u6b64\uff0c\u4f60\u53ef\u4ee5\u5c06 CUDA_VISIBLE_DEVICES \u73af\u5883\u53d8\u91cf\u8bbe\u7f6e\u4e3a\u7a7a\u5b57\u7b26\u4e32\uff0c\u4f8b\u5982\uff1a $ CUDA_VISIBLE_DEVICES=\"\" PYTHONHASHSEED=0 python your_program.py \u4e0b\u9762\u7684\u4ee3\u7801\u7247\u6bb5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5982\u4f55\u83b7\u5f97\u53ef\u590d\u73b0\u7ed3\u679c\u7684\u4f8b\u5b50 - \u9488\u5bf9 Python 3 \u73af\u5883\u7684 TensorFlow \u540e\u7aef\u3002 import numpy as np import tensorflow as tf import random as rn # \u4ee5\u4e0b\u662f Numpy \u5728\u4e00\u4e2a\u660e\u786e\u7684\u521d\u59cb\u72b6\u6001\u751f\u6210\u56fa\u5b9a\u968f\u673a\u6570\u5b57\u6240\u5fc5\u9700\u7684\u3002 np.random.seed(42) # \u4ee5\u4e0b\u662f Python \u5728\u4e00\u4e2a\u660e\u786e\u7684\u521d\u59cb\u72b6\u6001\u751f\u6210\u56fa\u5b9a\u968f\u673a\u6570\u5b57\u6240\u5fc5\u9700\u7684\u3002 rn.seed(12345) # \u5f3a\u5236 TensorFlow \u4f7f\u7528\u5355\u7ebf\u7a0b\u3002 # \u591a\u7ebf\u7a0b\u662f\u7ed3\u679c\u4e0d\u53ef\u590d\u73b0\u7684\u4e00\u4e2a\u6f5c\u5728\u56e0\u7d20\u3002 # \u66f4\u591a\u8be6\u60c5\uff0c\u89c1: https://stackoverflow.com/questions/42022950/ session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1) from keras import backend as K # `tf.set_random_seed()` \u5c06\u4f1a\u4ee5 TensorFlow \u4e3a\u540e\u7aef\uff0c # \u5728\u4e00\u4e2a\u660e\u786e\u7684\u521d\u59cb\u72b6\u6001\u4e0b\u751f\u6210\u56fa\u5b9a\u968f\u673a\u6570\u5b57\u3002 # \u66f4\u591a\u8be6\u60c5\uff0c\u89c1: https://www.tensorflow.org/api_docs/python/tf/set_random_seed tf.set_random_seed(1234) sess = tf.Session(graph=tf.get_default_graph(), config=session_conf) K.set_session(sess) # \u5269\u4f59\u4ee3\u7801 ...","title":"\u5982\u4f55\u5728 Keras \u5f00\u53d1\u8fc7\u7a0b\u4e2d\u83b7\u53d6\u53ef\u590d\u73b0\u7684\u7ed3\u679c\uff1f"},{"location":"0-Getting-Started/2.faq/#keras-hdf5-h5py","text":"\u4e3a\u4e86\u5c06\u4f60\u7684 Keras \u6a21\u578b\u4fdd\u5b58\u4e3a HDF5 \u6587\u4ef6\uff0c\u4f8b\u5982\u901a\u8fc7 keras.callbacks.ModelCheckpoint \uff0cKeras \u4f7f\u7528\u4e86 h5py Python \u5305\u3002h5py \u662f Keras \u7684\u4f9d\u8d56\u9879\uff0c\u5e94\u9ed8\u8ba4\u88ab\u5b89\u88c5\u3002\u5728\u57fa\u4e8e Debian \u7684\u53d1\u884c\u7248\u672c\u4e0a\uff0c\u4f60\u9700\u8981\u518d\u989d\u5916\u5b89\u88c5 libhdf5 \uff1a sudo apt-get install libhdf5-serial-dev \u5982\u679c\u4f60\u4e0d\u786e\u5b9a\u662f\u5426\u5b89\u88c5\u4e86 h5py\uff0c\u5219\u53ef\u4ee5\u6253\u5f00 Python shell \u5e76\u901a\u8fc7\u4e0b\u9762\u7684\u547d\u4ee4\u52a0\u8f7d\u6a21\u5757 import h5py \u5982\u679c\u6a21\u5757\u5bfc\u5165\u6ca1\u6709\u9519\u8bef\uff0c\u90a3\u4e48\u8bf4\u660e\u6a21\u5757\u5df2\u7ecf\u5b89\u88c5\u6210\u529f\uff0c\u5426\u5219\u4f60\u53ef\u4ee5\u5728 http://docs.h5py.org/en/latest/build.html \u4e2d\u627e\u5230\u8be6\u7ec6\u7684\u5b89\u88c5\u8bf4\u660e\u3002","title":"\u5982\u4f55\u5728 Keras \u4e2d\u5b89\u88c5 HDF5 \u6216 h5py \u6765\u4fdd\u5b58\u6211\u7684\u6a21\u578b\uff1f"},{"location":"1-Models/0.about-keras-models/","text":"\u5173\u4e8e Keras \u6a21\u578b \u5728 Keras \u4e2d\u6709\u4e24\u7c7b\u4e3b\u8981\u7684\u6a21\u578b\uff1a Sequential \u987a\u5e8f\u6a21\u578b \u548c \u4f7f\u7528\u51fd\u6570\u5f0f API \u7684 Model \u7c7b\u6a21\u578b \u3002 \u8fd9\u4e9b\u6a21\u578b\u6709\u8bb8\u591a\u5171\u540c\u7684\u65b9\u6cd5\u548c\u5c5e\u6027\uff1a model.layers \u662f\u5305\u542b\u6a21\u578b\u7f51\u7edc\u5c42\u7684\u5c55\u5e73\u5217\u8868\u3002 model.inputs \u662f\u6a21\u578b\u8f93\u5165\u5f20\u91cf\u7684\u5217\u8868\u3002 model.outputs \u662f\u6a21\u578b\u8f93\u51fa\u5f20\u91cf\u7684\u5217\u8868\u3002 model.summary() \u6253\u5370\u51fa\u6a21\u578b\u6982\u8ff0\u4fe1\u606f\u3002 \u5b83\u662f utils.print_summary \u7684\u7b80\u6377\u8c03\u7528\u3002 model.get_config() \u8fd4\u56de\u5305\u542b\u6a21\u578b\u914d\u7f6e\u4fe1\u606f\u7684\u5b57\u5178\u3002\u901a\u8fc7\u4ee5\u4e0b\u4ee3\u7801\uff0c\u5c31\u53ef\u4ee5\u6839\u636e\u8fd9\u4e9b\u914d\u7f6e\u4fe1\u606f\u91cd\u65b0\u5b9e\u4f8b\u5316\u6a21\u578b\uff1a config = model.get_config() model = Model.from_config(config) # \u6216\u8005\uff0c\u5bf9\u4e8e Sequential: model = Sequential.from_config(config) model.get_weights() \u8fd4\u56de\u6a21\u578b\u4e2d\u6240\u6709\u6743\u91cd\u5f20\u91cf\u7684\u5217\u8868\uff0c\u7c7b\u578b\u4e3a Numpy \u6570\u7ec4\u3002 model.set_weights(weights) \u4ece Numpy \u6570\u7ec4\u4e2d\u4e3a\u6a21\u578b\u8bbe\u7f6e\u6743\u91cd\u3002\u5217\u8868\u4e2d\u7684\u6570\u7ec4\u5fc5\u987b\u4e0e get_weights() \u8fd4\u56de\u7684\u6743\u91cd\u5177\u6709\u76f8\u540c\u7684\u5c3a\u5bf8\u3002 model.to_json() \u4ee5 JSON \u5b57\u7b26\u4e32\u7684\u5f62\u5f0f\u8fd4\u56de\u6a21\u578b\u7684\u8868\u793a\u3002\u8bf7\u6ce8\u610f\uff0c\u8be5\u8868\u793a\u4e0d\u5305\u62ec\u6743\u91cd\uff0c\u4ec5\u5305\u542b\u7ed3\u6784\u3002\u4f60\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u4ece JSON \u5b57\u7b26\u4e32\u91cd\u65b0\u5b9e\u4f8b\u5316\u540c\u4e00\u6a21\u578b\uff08\u4f7f\u7528\u91cd\u65b0\u521d\u59cb\u5316\u7684\u6743\u91cd\uff09\uff1a from keras.models import model_from_json json_string = model.to_json() model = model_from_json(json_string) model.to_yaml() \u4ee5 YAML \u5b57\u7b26\u4e32\u7684\u5f62\u5f0f\u8fd4\u56de\u6a21\u578b\u7684\u8868\u793a\u3002\u8bf7\u6ce8\u610f\uff0c\u8be5\u8868\u793a\u4e0d\u5305\u62ec\u6743\u91cd\uff0c\u53ea\u5305\u542b\u7ed3\u6784\u3002\u4f60\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u4ee3\u7801\uff0c\u4ece YAML \u5b57\u7b26\u4e32\u4e2d\u91cd\u65b0\u5b9e\u4f8b\u5316\u76f8\u540c\u7684\u6a21\u578b\uff08\u4f7f\u7528\u91cd\u65b0\u521d\u59cb\u5316\u7684\u6743\u91cd\uff09\uff1a from keras.models import model_from_yaml yaml_string = model.to_yaml() model = model_from_yaml(yaml_string) model.save_weights(filepath) \u5c06\u6a21\u578b\u6743\u91cd\u5b58\u50a8\u4e3a HDF5 \u6587\u4ef6\u3002 model.load_weights(filepath, by_name=False) : \u4ece HDF5 \u6587\u4ef6\uff08\u7531 save_weights \u521b\u5efa\uff09\u4e2d\u52a0\u8f7d\u6743\u91cd\u3002\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u7684\u7ed3\u6784\u5e94\u8be5\u662f\u4e0d\u53d8\u7684\u3002 \u5982\u679c\u60f3\u5c06\u6743\u91cd\u8f7d\u5165\u4e0d\u540c\u7684\u6a21\u578b\uff08\u90e8\u5206\u5c42\u76f8\u540c\uff09\uff0c \u8bbe\u7f6e by_name=True \u6765\u8f7d\u5165\u90a3\u4e9b\u540d\u5b57\u76f8\u540c\u7684\u5c42\u7684\u6743\u91cd\u3002 \u6ce8\u610f\uff1a\u53e6\u8bf7\u53c2\u9605 \u5982\u4f55\u5b89\u88c5 HDF5 \u6216 h5py \u4ee5\u4fdd\u5b58 Keras \u6a21\u578b \uff0c\u5728\u5e38\u89c1\u95ee\u9898\u4e2d\u4e86\u89e3\u5982\u4f55\u5b89\u88c5 h5py \u7684\u8bf4\u660e\u3002 Model \u7c7b\u7ee7\u627f \u9664\u4e86\u8fd9\u4e24\u7c7b\u6a21\u578b\u4e4b\u5916\uff0c\u4f60\u8fd8\u53ef\u4ee5\u901a\u8fc7\u7ee7\u627f Model \u7c7b\u5e76\u5728 call \u65b9\u6cd5\u4e2d\u5b9e\u73b0\u4f60\u81ea\u5df1\u7684\u524d\u5411\u4f20\u64ad\uff0c\u4ee5\u521b\u5efa\u4f60\u81ea\u5df1\u7684\u5b8c\u5168\u5b9a\u5236\u5316\u7684\u6a21\u578b\uff0c\uff08 Model \u7c7b\u7ee7\u627f API \u5f15\u5165\u4e8e Keras 2.2.0\uff09\u3002 \u8fd9\u91cc\u662f\u4e00\u4e2a\u7528 Model \u7c7b\u7ee7\u627f\u5199\u7684\u7b80\u5355\u7684\u591a\u5c42\u611f\u77e5\u5668\u7684\u4f8b\u5b50\uff1a import keras class SimpleMLP(keras.Model): def __init__(self, use_bn=False, use_dp=False, num_classes=10): super(SimpleMLP, self).__init__(name='mlp') self.use_bn = use_bn self.use_dp = use_dp self.num_classes = num_classes self.dense1 = keras.layers.Dense(32, activation='relu') self.dense2 = keras.layers.Dense(num_classes, activation='softmax') if self.use_dp: self.dp = keras.layers.Dropout(0.5) if self.use_bn: self.bn = keras.layers.BatchNormalization(axis=-1) def call(self, inputs): x = self.dense1(inputs) if self.use_dp: x = self.dp(x) if self.use_bn: x = self.bn(x) return self.dense2(x) model = SimpleMLP() model.compile(...) model.fit(...) \u7f51\u7edc\u5c42\u5b9a\u4e49\u5728 __init__(self, ...) \u4e2d\uff0c\u524d\u5411\u4f20\u64ad\u5728 call(self, inputs) \u4e2d\u6307\u5b9a\u3002\u5728 call \u4e2d\uff0c\u4f60\u53ef\u4ee5\u6307\u5b9a\u81ea\u5b9a\u4e49\u7684\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u8c03\u7528 self.add_loss(loss_tensor) \uff08\u5c31\u50cf\u4f60\u5728\u81ea\u5b9a\u4e49\u5c42\u4e2d\u4e00\u6837\uff09\u3002 \u5728\u7c7b\u7ee7\u627f\u6a21\u578b\u4e2d\uff0c\u6a21\u578b\u7684\u62d3\u6251\u7ed3\u6784\u662f\u7531 Python \u4ee3\u7801\u5b9a\u4e49\u7684\uff08\u800c\u4e0d\u662f\u7f51\u7edc\u5c42\u7684\u9759\u6001\u56fe\uff09\u3002\u8fd9\u610f\u5473\u7740\u8be5\u6a21\u578b\u7684\u62d3\u6251\u7ed3\u6784\u4e0d\u80fd\u88ab\u68c0\u67e5\u6216\u5e8f\u5217\u5316\u3002\u56e0\u6b64\uff0c\u4ee5\u4e0b\u65b9\u6cd5\u548c\u5c5e\u6027 \u4e0d\u9002\u7528\u4e8e\u7c7b\u7ee7\u627f\u6a21\u578b \uff1a model.inputs \u548c model.outputs \u3002 model.to_yaml() \u548c model.to_json() \u3002 model.get_config() \u548c model.save() \u3002 \u5173\u952e\u70b9 \uff1a\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u4f7f\u7528\u6b63\u786e\u7684 API\u3002 Model \u7c7b\u7ee7\u627f API \u53ef\u4ee5\u4e3a\u5b9e\u73b0\u590d\u6742\u6a21\u578b\u63d0\u4f9b\u66f4\u5927\u7684\u7075\u6d3b\u6027\uff0c\u4f46\u5b83\u9700\u8981\u4ed8\u51fa\u4ee3\u4ef7\uff08\u6bd4\u5982\u7f3a\u5931\u7684\u7279\u6027\uff09\uff1a\u5b83\u66f4\u5197\u957f\uff0c\u66f4\u590d\u6742\uff0c\u5e76\u4e14\u6709\u66f4\u591a\u7684\u7528\u6237\u9519\u8bef\u673a\u4f1a\u3002\u5982\u679c\u53ef\u80fd\u7684\u8bdd\uff0c\u5c3d\u53ef\u80fd\u4f7f\u7528\u51fd\u6570\u5f0f API\uff0c\u8fd9\u5bf9\u7528\u6237\u66f4\u53cb\u597d\u3002","title":"\u5173\u4e8e Keras \u6a21\u578b"},{"location":"1-Models/0.about-keras-models/#keras","text":"\u5728 Keras \u4e2d\u6709\u4e24\u7c7b\u4e3b\u8981\u7684\u6a21\u578b\uff1a Sequential \u987a\u5e8f\u6a21\u578b \u548c \u4f7f\u7528\u51fd\u6570\u5f0f API \u7684 Model \u7c7b\u6a21\u578b \u3002 \u8fd9\u4e9b\u6a21\u578b\u6709\u8bb8\u591a\u5171\u540c\u7684\u65b9\u6cd5\u548c\u5c5e\u6027\uff1a model.layers \u662f\u5305\u542b\u6a21\u578b\u7f51\u7edc\u5c42\u7684\u5c55\u5e73\u5217\u8868\u3002 model.inputs \u662f\u6a21\u578b\u8f93\u5165\u5f20\u91cf\u7684\u5217\u8868\u3002 model.outputs \u662f\u6a21\u578b\u8f93\u51fa\u5f20\u91cf\u7684\u5217\u8868\u3002 model.summary() \u6253\u5370\u51fa\u6a21\u578b\u6982\u8ff0\u4fe1\u606f\u3002 \u5b83\u662f utils.print_summary \u7684\u7b80\u6377\u8c03\u7528\u3002 model.get_config() \u8fd4\u56de\u5305\u542b\u6a21\u578b\u914d\u7f6e\u4fe1\u606f\u7684\u5b57\u5178\u3002\u901a\u8fc7\u4ee5\u4e0b\u4ee3\u7801\uff0c\u5c31\u53ef\u4ee5\u6839\u636e\u8fd9\u4e9b\u914d\u7f6e\u4fe1\u606f\u91cd\u65b0\u5b9e\u4f8b\u5316\u6a21\u578b\uff1a config = model.get_config() model = Model.from_config(config) # \u6216\u8005\uff0c\u5bf9\u4e8e Sequential: model = Sequential.from_config(config) model.get_weights() \u8fd4\u56de\u6a21\u578b\u4e2d\u6240\u6709\u6743\u91cd\u5f20\u91cf\u7684\u5217\u8868\uff0c\u7c7b\u578b\u4e3a Numpy \u6570\u7ec4\u3002 model.set_weights(weights) \u4ece Numpy \u6570\u7ec4\u4e2d\u4e3a\u6a21\u578b\u8bbe\u7f6e\u6743\u91cd\u3002\u5217\u8868\u4e2d\u7684\u6570\u7ec4\u5fc5\u987b\u4e0e get_weights() \u8fd4\u56de\u7684\u6743\u91cd\u5177\u6709\u76f8\u540c\u7684\u5c3a\u5bf8\u3002 model.to_json() \u4ee5 JSON \u5b57\u7b26\u4e32\u7684\u5f62\u5f0f\u8fd4\u56de\u6a21\u578b\u7684\u8868\u793a\u3002\u8bf7\u6ce8\u610f\uff0c\u8be5\u8868\u793a\u4e0d\u5305\u62ec\u6743\u91cd\uff0c\u4ec5\u5305\u542b\u7ed3\u6784\u3002\u4f60\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u4ece JSON \u5b57\u7b26\u4e32\u91cd\u65b0\u5b9e\u4f8b\u5316\u540c\u4e00\u6a21\u578b\uff08\u4f7f\u7528\u91cd\u65b0\u521d\u59cb\u5316\u7684\u6743\u91cd\uff09\uff1a from keras.models import model_from_json json_string = model.to_json() model = model_from_json(json_string) model.to_yaml() \u4ee5 YAML \u5b57\u7b26\u4e32\u7684\u5f62\u5f0f\u8fd4\u56de\u6a21\u578b\u7684\u8868\u793a\u3002\u8bf7\u6ce8\u610f\uff0c\u8be5\u8868\u793a\u4e0d\u5305\u62ec\u6743\u91cd\uff0c\u53ea\u5305\u542b\u7ed3\u6784\u3002\u4f60\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u4ee3\u7801\uff0c\u4ece YAML \u5b57\u7b26\u4e32\u4e2d\u91cd\u65b0\u5b9e\u4f8b\u5316\u76f8\u540c\u7684\u6a21\u578b\uff08\u4f7f\u7528\u91cd\u65b0\u521d\u59cb\u5316\u7684\u6743\u91cd\uff09\uff1a from keras.models import model_from_yaml yaml_string = model.to_yaml() model = model_from_yaml(yaml_string) model.save_weights(filepath) \u5c06\u6a21\u578b\u6743\u91cd\u5b58\u50a8\u4e3a HDF5 \u6587\u4ef6\u3002 model.load_weights(filepath, by_name=False) : \u4ece HDF5 \u6587\u4ef6\uff08\u7531 save_weights \u521b\u5efa\uff09\u4e2d\u52a0\u8f7d\u6743\u91cd\u3002\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u7684\u7ed3\u6784\u5e94\u8be5\u662f\u4e0d\u53d8\u7684\u3002 \u5982\u679c\u60f3\u5c06\u6743\u91cd\u8f7d\u5165\u4e0d\u540c\u7684\u6a21\u578b\uff08\u90e8\u5206\u5c42\u76f8\u540c\uff09\uff0c \u8bbe\u7f6e by_name=True \u6765\u8f7d\u5165\u90a3\u4e9b\u540d\u5b57\u76f8\u540c\u7684\u5c42\u7684\u6743\u91cd\u3002 \u6ce8\u610f\uff1a\u53e6\u8bf7\u53c2\u9605 \u5982\u4f55\u5b89\u88c5 HDF5 \u6216 h5py \u4ee5\u4fdd\u5b58 Keras \u6a21\u578b \uff0c\u5728\u5e38\u89c1\u95ee\u9898\u4e2d\u4e86\u89e3\u5982\u4f55\u5b89\u88c5 h5py \u7684\u8bf4\u660e\u3002","title":"\u5173\u4e8e Keras \u6a21\u578b"},{"location":"1-Models/0.about-keras-models/#model","text":"\u9664\u4e86\u8fd9\u4e24\u7c7b\u6a21\u578b\u4e4b\u5916\uff0c\u4f60\u8fd8\u53ef\u4ee5\u901a\u8fc7\u7ee7\u627f Model \u7c7b\u5e76\u5728 call \u65b9\u6cd5\u4e2d\u5b9e\u73b0\u4f60\u81ea\u5df1\u7684\u524d\u5411\u4f20\u64ad\uff0c\u4ee5\u521b\u5efa\u4f60\u81ea\u5df1\u7684\u5b8c\u5168\u5b9a\u5236\u5316\u7684\u6a21\u578b\uff0c\uff08 Model \u7c7b\u7ee7\u627f API \u5f15\u5165\u4e8e Keras 2.2.0\uff09\u3002 \u8fd9\u91cc\u662f\u4e00\u4e2a\u7528 Model \u7c7b\u7ee7\u627f\u5199\u7684\u7b80\u5355\u7684\u591a\u5c42\u611f\u77e5\u5668\u7684\u4f8b\u5b50\uff1a import keras class SimpleMLP(keras.Model): def __init__(self, use_bn=False, use_dp=False, num_classes=10): super(SimpleMLP, self).__init__(name='mlp') self.use_bn = use_bn self.use_dp = use_dp self.num_classes = num_classes self.dense1 = keras.layers.Dense(32, activation='relu') self.dense2 = keras.layers.Dense(num_classes, activation='softmax') if self.use_dp: self.dp = keras.layers.Dropout(0.5) if self.use_bn: self.bn = keras.layers.BatchNormalization(axis=-1) def call(self, inputs): x = self.dense1(inputs) if self.use_dp: x = self.dp(x) if self.use_bn: x = self.bn(x) return self.dense2(x) model = SimpleMLP() model.compile(...) model.fit(...) \u7f51\u7edc\u5c42\u5b9a\u4e49\u5728 __init__(self, ...) \u4e2d\uff0c\u524d\u5411\u4f20\u64ad\u5728 call(self, inputs) \u4e2d\u6307\u5b9a\u3002\u5728 call \u4e2d\uff0c\u4f60\u53ef\u4ee5\u6307\u5b9a\u81ea\u5b9a\u4e49\u7684\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u8c03\u7528 self.add_loss(loss_tensor) \uff08\u5c31\u50cf\u4f60\u5728\u81ea\u5b9a\u4e49\u5c42\u4e2d\u4e00\u6837\uff09\u3002 \u5728\u7c7b\u7ee7\u627f\u6a21\u578b\u4e2d\uff0c\u6a21\u578b\u7684\u62d3\u6251\u7ed3\u6784\u662f\u7531 Python \u4ee3\u7801\u5b9a\u4e49\u7684\uff08\u800c\u4e0d\u662f\u7f51\u7edc\u5c42\u7684\u9759\u6001\u56fe\uff09\u3002\u8fd9\u610f\u5473\u7740\u8be5\u6a21\u578b\u7684\u62d3\u6251\u7ed3\u6784\u4e0d\u80fd\u88ab\u68c0\u67e5\u6216\u5e8f\u5217\u5316\u3002\u56e0\u6b64\uff0c\u4ee5\u4e0b\u65b9\u6cd5\u548c\u5c5e\u6027 \u4e0d\u9002\u7528\u4e8e\u7c7b\u7ee7\u627f\u6a21\u578b \uff1a model.inputs \u548c model.outputs \u3002 model.to_yaml() \u548c model.to_json() \u3002 model.get_config() \u548c model.save() \u3002 \u5173\u952e\u70b9 \uff1a\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u4f7f\u7528\u6b63\u786e\u7684 API\u3002 Model \u7c7b\u7ee7\u627f API \u53ef\u4ee5\u4e3a\u5b9e\u73b0\u590d\u6742\u6a21\u578b\u63d0\u4f9b\u66f4\u5927\u7684\u7075\u6d3b\u6027\uff0c\u4f46\u5b83\u9700\u8981\u4ed8\u51fa\u4ee3\u4ef7\uff08\u6bd4\u5982\u7f3a\u5931\u7684\u7279\u6027\uff09\uff1a\u5b83\u66f4\u5197\u957f\uff0c\u66f4\u590d\u6742\uff0c\u5e76\u4e14\u6709\u66f4\u591a\u7684\u7528\u6237\u9519\u8bef\u673a\u4f1a\u3002\u5982\u679c\u53ef\u80fd\u7684\u8bdd\uff0c\u5c3d\u53ef\u80fd\u4f7f\u7528\u51fd\u6570\u5f0f API\uff0c\u8fd9\u5bf9\u7528\u6237\u66f4\u53cb\u597d\u3002","title":"Model \u7c7b\u7ee7\u627f"},{"location":"1-Models/1.sequential/","text":"Sequential \u6a21\u578b API \u5728\u9605\u8bfb\u8fd9\u7247\u6587\u6863\u524d\uff0c\u8bf7\u5148\u9605\u8bfb Keras Sequential \u6a21\u578b\u6307\u5f15 \u3002 Sequential \u6a21\u578b\u65b9\u6cd5 compile compile(optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None) \u7528\u4e8e\u914d\u7f6e\u8bad\u7ec3\u6a21\u578b\u3002 \u53c2\u6570 optimizer : \u5b57\u7b26\u4e32\uff08\u4f18\u5316\u5668\u540d\uff09\u6216\u8005\u4f18\u5316\u5668\u5bf9\u8c61\u3002\u8be6\u89c1 optimizers \u3002 loss : \u5b57\u7b26\u4e32\uff08\u76ee\u6807\u51fd\u6570\u540d\uff09\u6216\u76ee\u6807\u51fd\u6570\u3002\u8be6\u89c1 losses \u3002 \u5982\u679c\u6a21\u578b\u5177\u6709\u591a\u4e2a\u8f93\u51fa\uff0c\u5219\u53ef\u4ee5\u901a\u8fc7\u4f20\u9012\u635f\u5931\u51fd\u6570\u7684\u5b57\u5178\u6216\u5217\u8868\uff0c\u5728\u6bcf\u4e2a\u8f93\u51fa\u4e0a\u4f7f\u7528\u4e0d\u540c\u7684\u635f\u5931\u3002\u6a21\u578b\u5c06\u6700\u5c0f\u5316\u7684\u635f\u5931\u503c\u5c06\u662f\u6240\u6709\u5355\u4e2a\u635f\u5931\u7684\u603b\u548c\u3002 metrics : \u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u671f\u95f4\u7684\u6a21\u578b\u8bc4\u4f30\u6807\u51c6\u3002\u901a\u5e38\u4f60\u4f1a\u4f7f\u7528 metrics = ['accuracy'] \u3002 \u8981\u4e3a\u591a\u8f93\u51fa\u6a21\u578b\u7684\u4e0d\u540c\u8f93\u51fa\u6307\u5b9a\u4e0d\u540c\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u8fd8\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5b57\u5178\uff0c\u5982 metrics = {'output_a'\uff1a'accuracy'} \u3002 loss_weights : \u6307\u5b9a\u6807\u91cf\u7cfb\u6570\uff08Python\u6d6e\u70b9\u6570\uff09\u7684\u53ef\u9009\u5217\u8868\u6216\u5b57\u5178\uff0c\u7528\u4e8e\u52a0\u6743\u4e0d\u540c\u6a21\u578b\u8f93\u51fa\u7684\u635f\u5931\u8d21\u732e\u3002 \u6a21\u578b\u5c06\u8981\u6700\u5c0f\u5316\u7684\u635f\u5931\u503c\u5c06\u662f\u6240\u6709\u5355\u4e2a\u635f\u5931\u7684\u52a0\u6743\u548c\uff0c\u7531 loss_weights \u7cfb\u6570\u52a0\u6743\u3002 \u5982\u679c\u662f\u5217\u8868\uff0c\u5219\u671f\u671b\u4e0e\u6a21\u578b\u7684\u8f93\u51fa\u5177\u6709 1:1 \u6620\u5c04\u3002 \u5982\u679c\u662f\u5f20\u91cf\uff0c\u5219\u671f\u671b\u5c06\u8f93\u51fa\u540d\u79f0\uff08\u5b57\u7b26\u4e32\uff09\u6620\u5c04\u5230\u6807\u91cf\u7cfb\u6570\u3002 sample_weight_mode : \u5982\u679c\u4f60\u9700\u8981\u6267\u884c\u6309\u65f6\u95f4\u6b65\u91c7\u6837\u6743\u91cd\uff082D \u6743\u91cd\uff09\uff0c\u8bf7\u5c06\u5176\u8bbe\u7f6e\u4e3a temporal \u3002 \u9ed8\u8ba4\u4e3a None \uff0c\u4e3a\u91c7\u6837\u6743\u91cd\uff081D\uff09\u3002\u5982\u679c\u6a21\u578b\u6709\u591a\u4e2a\u8f93\u51fa\uff0c\u5219\u53ef\u4ee5\u901a\u8fc7\u4f20\u9012 mode \u7684\u5b57\u5178\u6216\u5217\u8868\uff0c\u4ee5\u5728\u6bcf\u4e2a\u8f93\u51fa\u4e0a\u4f7f\u7528\u4e0d\u540c\u7684 sample_weight_mode \u3002 weighted_metrics : \u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u671f\u95f4\uff0c\u7531 sample_weight \u6216 class_weight \u8bc4\u4f30\u548c\u52a0\u6743\u7684\u5ea6\u91cf\u6807\u51c6\u5217\u8868\u3002 target_tensors : \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cKeras \u5c06\u4e3a\u6a21\u578b\u7684\u76ee\u6807\u521b\u5efa\u4e00\u4e2a\u5360\u4f4d\u7b26\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5c06\u4f7f\u7528\u76ee\u6807\u6570\u636e\u3002\u76f8\u53cd\uff0c\u5982\u679c\u4f60\u60f3\u4f7f\u7528\u81ea\u5df1\u7684\u76ee\u6807\u5f20\u91cf\uff08\u53cd\u8fc7\u6765\u8bf4\uff0cKeras \u5728\u8bad\u7ec3\u671f\u95f4\u4e0d\u4f1a\u8f7d\u5165\u8fd9\u4e9b\u76ee\u6807\u5f20\u91cf\u7684\u5916\u90e8 Numpy \u6570\u636e\uff09\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7 target_tensors \u53c2\u6570\u6307\u5b9a\u5b83\u4eec\u3002\u5b83\u5e94\u8be5\u662f\u5355\u4e2a\u5f20\u91cf\uff08\u5bf9\u4e8e\u5355\u8f93\u51fa Sequential \u6a21\u578b\uff09\u3002 **kwargs : \u5f53\u4f7f\u7528 Theano/CNTK \u540e\u7aef\u65f6\uff0c\u8fd9\u4e9b\u53c2\u6570\u88ab\u4f20\u5165 K.function \u3002\u5f53\u4f7f\u7528 TensorFlow \u540e\u7aef\u65f6\uff0c\u8fd9\u4e9b\u53c2\u6570\u88ab\u4f20\u9012\u5230 tf.Session.run \u3002 \u5f02\u5e38 ValueError : \u5982\u679c optimizer , loss , metrics \u6216 sample_weight_mode \u8fd9\u4e9b\u53c2\u6570\u4e0d\u5408\u6cd5\u3002 fit fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None) \u4ee5\u56fa\u5b9a\u6570\u91cf\u7684\u8f6e\u6b21\uff08\u6570\u636e\u96c6\u4e0a\u7684\u8fed\u4ee3\uff09\u8bad\u7ec3\u6a21\u578b\u3002 \u53c2\u6570 x : \u8bad\u7ec3\u6570\u636e\u7684 Numpy \u6570\u7ec4\u3002 \u5982\u679c\u6a21\u578b\u4e2d\u7684\u8f93\u5165\u5c42\u88ab\u547d\u540d\uff0c\u4f60\u4e5f\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5b57\u5178\uff0c\u5c06\u8f93\u5165\u5c42\u540d\u79f0\u6620\u5c04\u5230 Numpy \u6570\u7ec4\u3002 \u5982\u679c\u4ece\u672c\u5730\u6846\u67b6\u5f20\u91cf\u9988\u9001\uff08\u4f8b\u5982 TensorFlow \u6570\u636e\u5f20\u91cf\uff09\u6570\u636e\uff0cx \u53ef\u4ee5\u662f None \uff08\u9ed8\u8ba4\uff09\u3002 y : \u76ee\u6807\uff08\u6807\u7b7e\uff09\u6570\u636e\u7684 Numpy \u6570\u7ec4\u3002 \u5982\u679c\u6a21\u578b\u4e2d\u7684\u8f93\u51fa\u5c42\u88ab\u547d\u540d\uff0c\u4f60\u4e5f\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5b57\u5178\uff0c\u5c06\u8f93\u51fa\u5c42\u540d\u79f0\u6620\u5c04\u5230 Numpy \u6570\u7ec4\u3002 \u5982\u679c\u4ece\u672c\u5730\u6846\u67b6\u5f20\u91cf\u9988\u9001\uff08\u4f8b\u5982 TensorFlow \u6570\u636e\u5f20\u91cf\uff09\u6570\u636e\uff0cy \u53ef\u4ee5\u662f None \uff08\u9ed8\u8ba4\uff09\u3002 batch_size : \u6574\u6570\u6216 None \u3002\u6bcf\u6b21\u63d0\u5ea6\u66f4\u65b0\u7684\u6837\u672c\u6570\u3002\u5982\u679c\u672a\u6307\u5b9a\uff0c\u9ed8\u8ba4\u4e3a 32. epochs : \u6574\u6570\u3002\u8bad\u7ec3\u6a21\u578b\u8fed\u4ee3\u8f6e\u6b21\u3002\u4e00\u4e2a\u8f6e\u6b21\u662f\u5728\u6574\u4e2a x \u6216 y \u4e0a\u7684\u4e00\u8f6e\u8fed\u4ee3\u3002\u8bf7\u6ce8\u610f\uff0c\u4e0e initial_epoch \u4e00\u8d77\uff0c epochs \u88ab\u7406\u89e3\u4e3a \u300c\u6700\u7ec8\u8f6e\u6b21\u300d\u3002\u6a21\u578b\u5e76\u4e0d\u662f\u8bad\u7ec3\u4e86 epochs \u8f6e\uff0c\u800c\u662f\u5230\u7b2c epochs \u8f6e\u505c\u6b62\u8bad\u7ec3\u3002 verbose : 0, 1 \u6216 2\u3002\u65e5\u5fd7\u663e\u793a\u6a21\u5f0f\u3002 0 = \u5b89\u9759\u6a21\u5f0f, 1 = \u8fdb\u5ea6\u6761, 2 = \u6bcf\u8f6e\u4e00\u884c\u3002 callbacks : \u4e00\u7cfb\u5217\u7684 keras.callbacks.Callback \u5b9e\u4f8b\u3002\u4e00\u7cfb\u5217\u53ef\u4ee5\u5728\u8bad\u7ec3\u65f6\u4f7f\u7528\u7684\u56de\u8c03\u51fd\u6570\u3002\u8be6\u89c1 callbacks \u3002 validation_split : \u5728 0 \u548c 1 \u4e4b\u95f4\u6d6e\u52a8\u3002\u7528\u4f5c\u9a8c\u8bc1\u96c6\u7684\u8bad\u7ec3\u6570\u636e\u7684\u6bd4\u4f8b\u3002\u6a21\u578b\u5c06\u5206\u51fa\u4e00\u90e8\u5206\u4e0d\u4f1a\u88ab\u8bad\u7ec3\u7684\u9a8c\u8bc1\u6570\u636e\uff0c\u5e76\u5c06\u5728\u6bcf\u4e00\u8f6e\u7ed3\u675f\u65f6\u8bc4\u4f30\u8fd9\u4e9b\u9a8c\u8bc1\u6570\u636e\u7684\u8bef\u5dee\u548c\u4efb\u4f55\u5176\u4ed6\u6a21\u578b\u6307\u6807\u3002\u9a8c\u8bc1\u6570\u636e\u662f\u6df7\u6d17\u4e4b\u524d x \u548c y \u6570\u636e\u7684\u6700\u540e\u4e00\u90e8\u5206\u6837\u672c\u4e2d\u3002 validation_data : \u5143\u7ec4 (x_val\uff0cy_val) \u6216\u5143\u7ec4 (x_val\uff0cy_val\uff0cval_sample_weights) \uff0c\u7528\u6765\u8bc4\u4f30\u635f\u5931\uff0c\u4ee5\u53ca\u5728\u6bcf\u8f6e\u7ed3\u675f\u65f6\u7684\u4efb\u4f55\u6a21\u578b\u5ea6\u91cf\u6307\u6807\u3002\u6a21\u578b\u5c06\u4e0d\u4f1a\u5728\u8fd9\u4e2a\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u8fd9\u4e2a\u53c2\u6570\u4f1a\u8986\u76d6 validation_split \u3002 shuffle : \u5e03\u5c14\u503c\uff08\u662f\u5426\u5728\u6bcf\u8f6e\u8fed\u4ee3\u4e4b\u524d\u6df7\u6d17\u6570\u636e\uff09\u6216\u8005 \u5b57\u7b26\u4e32 ( batch )\u3002 batch \u662f\u5904\u7406 HDF5 \u6570\u636e\u9650\u5236\u7684\u7279\u6b8a\u9009\u9879\uff0c\u5b83\u5bf9\u4e00\u4e2a batch \u5185\u90e8\u7684\u6570\u636e\u8fdb\u884c\u6df7\u6d17\u3002\u5f53 steps_per_epoch \u975e None \u65f6\uff0c\u8fd9\u4e2a\u53c2\u6570\u65e0\u6548\u3002 class_weight : \u53ef\u9009\u7684\u5b57\u5178\uff0c\u7528\u6765\u6620\u5c04\u7c7b\u7d22\u5f15\uff08\u6574\u6570\uff09\u5230\u6743\u91cd\uff08\u6d6e\u70b9\uff09\u503c\uff0c\u7528\u4e8e\u52a0\u6743\u635f\u5931\u51fd\u6570\uff08\u4ec5\u5728\u8bad\u7ec3\u671f\u95f4\uff09\u3002\u8fd9\u53ef\u80fd\u6709\u52a9\u4e8e\u544a\u8bc9\u6a21\u578b \u300c\u66f4\u591a\u5173\u6ce8\u300d\u6765\u81ea\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u7c7b\u7684\u6837\u672c\u3002 sample_weight : \u8bad\u7ec3\u6837\u672c\u7684\u53ef\u9009 Numpy \u6743\u91cd\u6570\u7ec4\uff0c\u7528\u4e8e\u5bf9\u635f\u5931\u51fd\u6570\u8fdb\u884c\u52a0\u6743\uff08\u4ec5\u5728\u8bad\u7ec3\u671f\u95f4\uff09\u3002\u60a8\u53ef\u4ee5\u4f20\u9012\u4e0e\u8f93\u5165\u6837\u672c\u957f\u5ea6\u76f8\u540c\u7684\u5e73\u5766\uff081D\uff09Numpy \u6570\u7ec4\uff08\u6743\u91cd\u548c\u6837\u672c\u4e4b\u95f4\u7684 1\uff1a1 \u6620\u5c04\uff09\uff0c\u6216\u8005\u5728\u65f6\u5e8f\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u4f20\u9012\u5c3a\u5bf8\u4e3a (samples, sequence_length) \u7684 2D \u6570\u7ec4\uff0c\u4ee5\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u65f6\u95f4\u6b65\u65bd\u52a0\u4e0d\u540c\u7684\u6743\u91cd\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4f60\u5e94\u8be5\u786e\u4fdd\u5728 compile() \u4e2d\u6307\u5b9a sample_weight_mode=\"temporal\" \u3002 initial_epoch : \u5f00\u59cb\u8bad\u7ec3\u7684\u8f6e\u6b21\uff08\u6709\u52a9\u4e8e\u6062\u590d\u4e4b\u524d\u7684\u8bad\u7ec3\uff09\u3002 steps_per_epoch : \u5728\u58f0\u660e\u4e00\u4e2a\u8f6e\u6b21\u5b8c\u6210\u5e76\u5f00\u59cb\u4e0b\u4e00\u4e2a\u8f6e\u6b21\u4e4b\u524d\u7684\u603b\u6b65\u6570\uff08\u6837\u54c1\u6279\u6b21\uff09\u3002\u4f7f\u7528 TensorFlow \u6570\u636e\u5f20\u91cf\u7b49\u8f93\u5165\u5f20\u91cf\u8fdb\u884c\u8bad\u7ec3\u65f6\uff0c\u9ed8\u8ba4\u503c None \u7b49\u4e8e\u6570\u636e\u96c6\u4e2d\u6837\u672c\u7684\u6570\u91cf\u9664\u4ee5 batch \u7684\u5927\u5c0f\uff0c\u5982\u679c\u65e0\u6cd5\u786e\u5b9a\uff0c\u5219\u4e3a 1\u3002 validation_steps : \u53ea\u6709\u5728\u6307\u5b9a\u4e86 steps_per_epoch \u65f6\u624d\u6709\u7528\u3002\u505c\u6b62\u524d\u8981\u9a8c\u8bc1\u7684\u603b\u6b65\u6570\uff08\u6279\u6b21\u6837\u672c\uff09\u3002 \u8fd4\u56de \u4e00\u4e2a History \u5bf9\u8c61\u3002\u5176 History.history \u5c5e\u6027\u662f\u8fde\u7eed epoch \u8bad\u7ec3\u635f\u5931\u548c\u8bc4\u4f30\u503c\uff0c\u4ee5\u53ca\u9a8c\u8bc1\u96c6\u635f\u5931\u548c\u8bc4\u4f30\u503c\u7684\u8bb0\u5f55\uff08\u5982\u679c\u9002\u7528\uff09\u3002 \u5f02\u5e38 RuntimeError : \u5982\u679c\u6a21\u578b\u4ece\u672a\u7f16\u8bd1\u3002 ValueError : \u5728\u63d0\u4f9b\u7684\u8f93\u5165\u6570\u636e\u4e0e\u6a21\u578b\u671f\u671b\u7684\u4e0d\u5339\u914d\u7684\u60c5\u51b5\u4e0b\u3002 evaluate evaluate(x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None) \u5728\u6d4b\u8bd5\u6a21\u5f0f\uff0c\u8fd4\u56de\u8bef\u5dee\u503c\u548c\u8bc4\u4f30\u6807\u51c6\u503c\u3002 \u8ba1\u7b97\u9010\u6279\u6b21\u8fdb\u884c\u3002 \u53c2\u6570 x : \u8bad\u7ec3\u6570\u636e\u7684 Numpy \u6570\u7ec4\u3002 \u5982\u679c\u6a21\u578b\u4e2d\u7684\u8f93\u5165\u5c42\u88ab\u547d\u540d\uff0c\u4f60\u4e5f\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5b57\u5178\uff0c\u5c06\u8f93\u5165\u5c42\u540d\u79f0\u6620\u5c04\u5230 Numpy \u6570\u7ec4\u3002 \u5982\u679c\u4ece\u672c\u5730\u6846\u67b6\u5f20\u91cf\u9988\u9001\uff08\u4f8b\u5982 TensorFlow \u6570\u636e\u5f20\u91cf\uff09\u6570\u636e\uff0cx \u53ef\u4ee5\u662f None \uff08\u9ed8\u8ba4\uff09\u3002 y : \u76ee\u6807\uff08\u6807\u7b7e\uff09\u6570\u636e\u7684 Numpy \u6570\u7ec4\u3002 \u5982\u679c\u6a21\u578b\u4e2d\u7684\u8f93\u51fa\u5c42\u88ab\u547d\u540d\uff0c\u4f60\u4e5f\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5b57\u5178\uff0c\u5c06\u8f93\u51fa\u5c42\u540d\u79f0\u6620\u5c04\u5230 Numpy \u6570\u7ec4\u3002 \u5982\u679c\u4ece\u672c\u5730\u6846\u67b6\u5f20\u91cf\u9988\u9001\uff08\u4f8b\u5982 TensorFlow \u6570\u636e\u5f20\u91cf\uff09\u6570\u636e\uff0cy \u53ef\u4ee5\u662f None \uff08\u9ed8\u8ba4\uff09\u3002 batch_size : \u6574\u6570\u6216 None \u3002\u6bcf\u6b21\u63d0\u5ea6\u66f4\u65b0\u7684\u6837\u672c\u6570\u3002\u5982\u679c\u672a\u6307\u5b9a\uff0c\u9ed8\u8ba4\u4e3a 32. verbose : 0, 1\u3002\u65e5\u5fd7\u663e\u793a\u6a21\u5f0f\u30020 = \u5b89\u9759\u6a21\u5f0f, 1 = \u8fdb\u5ea6\u6761\u3002 sample_weight : \u8bad\u7ec3\u6837\u672c\u7684\u53ef\u9009 Numpy \u6743\u91cd\u6570\u7ec4\uff0c\u7528\u4e8e\u5bf9\u635f\u5931\u51fd\u6570\u8fdb\u884c\u52a0\u6743\uff08\u4ec5\u5728\u8bad\u7ec3\u671f\u95f4\uff09\u3002 \u60a8\u53ef\u4ee5\u4f20\u9012\u4e0e\u8f93\u5165\u6837\u672c\u957f\u5ea6\u76f8\u540c\u7684\u5e73\u5766\uff081D\uff09Numpy \u6570\u7ec4\uff08\u6743\u91cd\u548c\u6837\u672c\u4e4b\u95f4\u7684 1\uff1a1 \u6620\u5c04\uff09\uff0c\u6216\u8005\u5728\u65f6\u5e8f\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u4f20\u9012\u5c3a\u5bf8\u4e3a (samples, sequence_length) \u7684 2D \u6570\u7ec4\uff0c\u4ee5\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u65f6\u95f4\u6b65\u65bd\u52a0\u4e0d\u540c\u7684\u6743\u91cd\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4f60\u5e94\u8be5\u786e\u4fdd\u5728 compile() \u4e2d\u6307\u5b9a sample_weight_mode=\"temporal\" \u3002 steps : \u6574\u6570\u6216 None \u3002 \u58f0\u660e\u8bc4\u4f30\u7ed3\u675f\u4e4b\u524d\u7684\u603b\u6b65\u6570\uff08\u6279\u6b21\u6837\u672c\uff09\u3002\u9ed8\u8ba4\u503c None \u3002 \u8fd4\u56de \u6807\u91cf\u6d4b\u8bd5\u8bef\u5dee\uff08\u5982\u679c\u6a21\u578b\u53ea\u6709\u5355\u4e2a\u8f93\u51fa\u4e14\u6ca1\u6709\u8bc4\u4f30\u6307\u6807\uff09\u6216\u6807\u91cf\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u5177\u6709\u591a\u4e2a\u8f93\u51fa\u548c/\u6216\u6307\u6807\uff09\u3002 \u5c5e\u6027 model.metrics_names \u5c06\u63d0\u4f9b\u6807\u91cf\u8f93\u51fa\u7684\u663e\u793a\u6807\u7b7e\u3002 predict predict(x, batch_size=None, verbose=0, steps=None) \u4e3a\u8f93\u5165\u6837\u672c\u751f\u6210\u8f93\u51fa\u9884\u6d4b\u3002 \u8ba1\u7b97\u9010\u6279\u6b21\u8fdb\u884c\u3002 \u53c2\u6570 x : \u8f93\u5165\u6570\u636e\uff0cNumpy \u6570\u7ec4\uff08\u6216\u8005\u5982\u679c\u6a21\u578b\u6709\u591a\u4e2a\u8f93\u5165\uff0c\u5219\u4e3a Numpy \u6570\u7ec4\u5217\u8868\uff09\u3002 batch_size : \u6574\u6570\u3002\u5982\u672a\u6307\u5b9a\uff0c\u9ed8\u8ba4\u4e3a 32\u3002 verbose : \u65e5\u5fd7\u663e\u793a\u6a21\u5f0f\uff0c0 \u6216 1\u3002 steps : \u58f0\u660e\u9884\u6d4b\u7ed3\u675f\u4e4b\u524d\u7684\u603b\u6b65\u6570\uff08\u6279\u6b21\u6837\u672c\uff09\u3002\u9ed8\u8ba4\u503c None \u3002 \u8fd4\u56de \u9884\u6d4b\u7684 Numpy \u6570\u7ec4\u3002 \u5f02\u5e38 ValueError : \u5982\u679c\u63d0\u4f9b\u7684\u8f93\u5165\u6570\u636e\u4e0e\u6a21\u578b\u7684\u671f\u671b\u6570\u636e\u4e0d\u5339\u914d\uff0c\u6216\u8005\u6709\u72b6\u6001\u6a21\u578b\u6536\u5230\u7684\u6570\u91cf\u4e0d\u662f\u6279\u91cf\u5927\u5c0f\u7684\u500d\u6570\u3002 train_on_batch train_on_batch(x, y, sample_weight=None, class_weight=None) \u4e00\u6279\u6837\u54c1\u7684\u5355\u6b21\u68af\u5ea6\u66f4\u65b0\u3002 Arguments x : \u8bad\u7ec3\u6570\u636e\u7684 Numpy \u6570\u7ec4\uff0c\u5982\u679c\u6a21\u578b\u5177\u6709\u591a\u4e2a\u8f93\u5165\uff0c\u5219\u4e3a Numpy \u6570\u7ec4\u5217\u8868\u3002\u5982\u679c\u6a21\u578b\u4e2d\u7684\u6240\u6709\u8f93\u5165\u90fd\u5df2\u547d\u540d\uff0c\u4f60\u8fd8\u53ef\u4ee5\u4f20\u5165\u8f93\u5165\u540d\u79f0\u5230 Numpy \u6570\u7ec4\u7684\u6620\u5c04\u5b57\u5178\u3002 y : \u76ee\u6807\u6570\u636e\u7684 Numpy \u6570\u7ec4\uff0c\u5982\u679c\u6a21\u578b\u5177\u6709\u591a\u4e2a\u8f93\u5165\uff0c\u5219\u4e3a Numpy \u6570\u7ec4\u5217\u8868\u3002\u5982\u679c\u6a21\u578b\u4e2d\u7684\u6240\u6709\u8f93\u51fa\u90fd\u5df2\u547d\u540d\uff0c\u4f60\u8fd8\u53ef\u4ee5\u4f20\u5165\u8f93\u51fa\u540d\u79f0\u5230 Numpy \u6570\u7ec4\u7684\u6620\u5c04\u5b57\u5178\u3002 sample_weight : \u8bad\u7ec3\u6837\u672c\u7684\u53ef\u9009 Numpy \u6743\u91cd\u6570\u7ec4\uff0c\u7528\u4e8e\u5bf9\u635f\u5931\u51fd\u6570\u8fdb\u884c\u52a0\u6743\uff08\u4ec5\u5728\u8bad\u7ec3\u671f\u95f4\uff09\u3002 \u60a8\u53ef\u4ee5\u4f20\u9012\u4e0e\u8f93\u5165\u6837\u672c\u957f\u5ea6\u76f8\u540c\u7684\u5e73\u5766\uff081D\uff09Numpy \u6570\u7ec4\uff08\u6743\u91cd\u548c\u6837\u672c\u4e4b\u95f4\u7684 1\uff1a1 \u6620\u5c04\uff09\uff0c\u6216\u8005\u5728\u65f6\u5e8f\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u4f20\u9012\u5c3a\u5bf8\u4e3a (samples, sequence_length) \u7684 2D \u6570\u7ec4\uff0c\u4ee5\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u65f6\u95f4\u6b65\u65bd\u52a0\u4e0d\u540c\u7684\u6743\u91cd\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4f60\u5e94\u8be5\u786e\u4fdd\u5728 compile() \u4e2d\u6307\u5b9a sample_weight_mode=\"temporal\" \u3002 class_weight : \u53ef\u9009\u7684\u5b57\u5178\uff0c\u7528\u6765\u6620\u5c04\u7c7b\u7d22\u5f15\uff08\u6574\u6570\uff09\u5230\u6743\u91cd\uff08\u6d6e\u70b9\uff09\u503c\uff0c\u7528\u4e8e\u52a0\u6743\u635f\u5931\u51fd\u6570\uff08\u4ec5\u5728\u8bad\u7ec3\u671f\u95f4\uff09\u3002\u8fd9\u53ef\u80fd\u6709\u52a9\u4e8e\u544a\u8bc9\u6a21\u578b \u300c\u66f4\u591a\u5173\u6ce8\u300d\u6765\u81ea\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u7c7b\u7684\u6837\u672c\u3002 \u8fd4\u56de \u6807\u91cf\u8bad\u7ec3\u8bef\u5dee\uff08\u5982\u679c\u6a21\u578b\u53ea\u6709\u5355\u4e2a\u8f93\u51fa\u4e14\u6ca1\u6709\u8bc4\u4f30\u6307\u6807\uff09\u6216\u6807\u91cf\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u5177\u6709\u591a\u4e2a\u8f93\u51fa\u548c/\u6216\u6307\u6807\uff09\u3002 \u5c5e\u6027 model.metrics_names \u5c06\u63d0\u4f9b\u6807\u91cf\u8f93\u51fa\u7684\u663e\u793a\u6807\u7b7e\u3002 test_on_batch test_on_batch(x, y, sample_weight=None) \u5728\u4e00\u6279\u6837\u672c\u4e0a\u8bc4\u4f30\u6a21\u578b\u3002 \u53c2\u6570 x : \u8bad\u7ec3\u6570\u636e\u7684 Numpy \u6570\u7ec4\uff0c\u5982\u679c\u6a21\u578b\u5177\u6709\u591a\u4e2a\u8f93\u5165\uff0c\u5219\u4e3a Numpy \u6570\u7ec4\u5217\u8868\u3002\u5982\u679c\u6a21\u578b\u4e2d\u7684\u6240\u6709\u8f93\u5165\u90fd\u5df2\u547d\u540d\uff0c\u4f60\u8fd8\u53ef\u4ee5\u4f20\u5165\u8f93\u5165\u540d\u79f0\u5230 Numpy \u6570\u7ec4\u7684\u6620\u5c04\u5b57\u5178\u3002 y : \u76ee\u6807\u6570\u636e\u7684 Numpy \u6570\u7ec4\uff0c\u5982\u679c\u6a21\u578b\u5177\u6709\u591a\u4e2a\u8f93\u5165\uff0c\u5219\u4e3a Numpy \u6570\u7ec4\u5217\u8868\u3002\u5982\u679c\u6a21\u578b\u4e2d\u7684\u6240\u6709\u8f93\u51fa\u90fd\u5df2\u547d\u540d\uff0c\u4f60\u8fd8\u53ef\u4ee5\u4f20\u5165\u8f93\u51fa\u540d\u79f0\u5230 Numpy \u6570\u7ec4\u7684\u6620\u5c04\u5b57\u5178\u3002 sample_weight : \u8bad\u7ec3\u6837\u672c\u7684\u53ef\u9009 Numpy \u6743\u91cd\u6570\u7ec4\uff0c\u7528\u4e8e\u5bf9\u635f\u5931\u51fd\u6570\u8fdb\u884c\u52a0\u6743\uff08\u4ec5\u5728\u8bad\u7ec3\u671f\u95f4\uff09\u3002 \u60a8\u53ef\u4ee5\u4f20\u9012\u4e0e\u8f93\u5165\u6837\u672c\u957f\u5ea6\u76f8\u540c\u7684\u5e73\u5766\uff081D\uff09Numpy \u6570\u7ec4\uff08\u6743\u91cd\u548c\u6837\u672c\u4e4b\u95f4\u7684 1\uff1a1 \u6620\u5c04\uff09\uff0c\u6216\u8005\u5728\u65f6\u5e8f\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u4f20\u9012\u5c3a\u5bf8\u4e3a (samples, sequence_length) \u7684 2D \u6570\u7ec4\uff0c\u4ee5\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u65f6\u95f4\u6b65\u65bd\u52a0\u4e0d\u540c\u7684\u6743\u91cd\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4f60\u5e94\u8be5\u786e\u4fdd\u5728 compile() \u4e2d\u6307\u5b9a sample_weight_mode=\"temporal\" \u3002 \u8fd4\u56de \u6807\u91cf\u6d4b\u8bd5\u8bef\u5dee\uff08\u5982\u679c\u6a21\u578b\u53ea\u6709\u5355\u4e2a\u8f93\u51fa\u4e14\u6ca1\u6709\u8bc4\u4f30\u6307\u6807\uff09\u6216\u6807\u91cf\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u5177\u6709\u591a\u4e2a\u8f93\u51fa\u548c/\u6216\u6307\u6807\uff09\u3002 \u5c5e\u6027 model.metrics_names \u5c06\u63d0\u4f9b\u6807\u91cf\u8f93\u51fa\u7684\u663e\u793a\u6807\u7b7e\u3002 predict_on_batch predict_on_batch(x) \u8fd4\u56de\u4e00\u6279\u6837\u672c\u7684\u6a21\u578b\u9884\u6d4b\u503c\u3002 \u53c2\u6570 x : \u8f93\u5165\u6570\u636e\uff0cNumpy \u6570\u7ec4\u6216\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u6709\u591a\u8f93\u5165\uff09\u3002 \u8fd4\u56de \u9884\u6d4b\u503c\u7684 Numpy \u6570\u7ec4\u3002 fit_generator fit_generator(generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0) \u4f7f\u7528 Python \u751f\u6210\u5668\u6216 Sequence \u5b9e\u4f8b\u9010\u6279\u751f\u6210\u7684\u6570\u636e\uff0c\u6309\u6279\u6b21\u8bad\u7ec3\u6a21\u578b\u3002 \u751f\u6210\u5668\u4e0e\u6a21\u578b\u5e76\u884c\u8fd0\u884c\uff0c\u4ee5\u63d0\u9ad8\u6548\u7387\u3002 \u4f8b\u5982\uff0c\u8fd9\u53ef\u4ee5\u8ba9\u4f60\u5728 CPU \u4e0a\u5bf9\u56fe\u50cf\u8fdb\u884c\u5b9e\u65f6\u6570\u636e\u589e\u5f3a\uff0c\u4ee5\u5728 GPU \u4e0a\u8bad\u7ec3\u6a21\u578b\u3002 keras.utils.Sequence \u7684\u4f7f\u7528\u53ef\u4ee5\u4fdd\u8bc1\u6570\u636e\u7684\u987a\u5e8f\uff0c \u4ee5\u53ca\u5f53 use_multiprocessing=True \u65f6 \uff0c\u4fdd\u8bc1\u6bcf\u4e2a\u8f93\u5165\u5728\u6bcf\u4e2a epoch \u53ea\u4f7f\u7528\u4e00\u6b21\u3002 \u53c2\u6570 generator : \u4e00\u4e2a\u751f\u6210\u5668\u6216 Sequence ( keras.utils.Sequence ) \u5bf9\u8c61\u7684\u5b9e\u4f8b\uff0c\u4ee5\u907f\u514d\u5728\u4f7f\u7528\u591a\u8fdb\u7a0b\u65f6\u51fa\u73b0\u91cd\u590d\u6570\u636e\u3002 \u751f\u6210\u5668\u7684\u8f93\u51fa\u5e94\u8be5\u4e3a\u4ee5\u4e0b\u4e4b\u4e00\uff1a \u4e00\u4e2a (inputs, targets) \u5143\u7ec4 \u4e00\u4e2a (inputs, targets, sample_weights) \u5143\u7ec4\u3002 \u8fd9\u4e2a\u5143\u7ec4\uff08\u751f\u6210\u5668\u7684\u5355\u4e2a\u8f93\u51fa\uff09\u8868\u793a\u4e00\u4e2a\u72ec\u7acb\u6279\u6b21\u3002\u56e0\u6b64\uff0c\u6b64\u5143\u7ec4\u4e2d\u7684\u6240\u6709\u6570\u7ec4\u5fc5\u987b\u5177\u6709\u76f8\u540c\u7684\u957f\u5ea6\uff08\u7b49\u4e8e\u6b64\u6279\u6b21\u7684\u5927\u5c0f\uff09\u3002\u4e0d\u540c\u7684\u6279\u6b21\u53ef\u80fd\u5177\u6709\u4e0d\u540c\u7684\u5927\u5c0f\u3002\u4f8b\u5982\uff0c\u5982\u679c\u6570\u636e\u96c6\u7684\u5927\u5c0f\u4e0d\u80fd\u88ab\u6279\u91cf\u5927\u5c0f\u6574\u9664\uff0c\u5219\u6700\u540e\u4e00\u6279\u65f6\u671f\u901a\u5e38\u5c0f\u4e8e\u5176\u4ed6\u6279\u6b21\u3002\u751f\u6210\u5668\u5c06\u65e0\u9650\u5730\u5728\u6570\u636e\u96c6\u4e0a\u5faa\u73af\u3002\u5f53\u8fd0\u884c\u5230\u7b2c steps_per_epoch \u65f6\uff0c\u8bb0\u4e00\u4e2a epoch \u7ed3\u675f\u3002 steps_per_epoch : \u6574\u6570\u3002\u5728\u58f0\u660e\u4e00\u4e2a epoch \u5b8c\u6210\u5e76\u5f00\u59cb\u4e0b\u4e00\u4e2a epoch \u4e4b\u524d\u4ece generator \u4ea7\u751f\u7684\u603b\u6b65\u6570\uff08\u6279\u6b21\u6837\u672c\uff09\u3002\u5b83\u901a\u5e38\u5e94\u8be5\u7b49\u4e8e\u4f60\u7684\u6570\u636e\u96c6\u7684\u6837\u672c\u6570\u91cf\u9664\u4ee5\u6279\u91cf\u5927\u5c0f\u3002\u53ef\u9009\u53c2\u6570 Sequence \uff1a\u5982\u679c\u672a\u6307\u5b9a\uff0c\u5c06\u4f7f\u7528 len(generator) \u4f5c\u4e3a\u6b65\u6570\u3002 epochs : \u6574\u6570\uff0c\u6570\u636e\u7684\u8fed\u4ee3\u603b\u8f6e\u6570\u3002\u4e00\u4e2a epoch \u662f\u5bf9\u6240\u63d0\u4f9b\u7684\u6574\u4e2a\u6570\u636e\u7684\u4e00\u8f6e\u8fed\u4ee3\uff0c\u7531 steps_per_epoch \u6240\u5b9a\u4e49\u3002\u8bf7\u6ce8\u610f\uff0c\u4e0e initial_epoch \u4e00\u8d77\uff0c\u53c2\u6570 epochs \u5e94\u88ab\u7406\u89e3\u4e3a \u300c\u6700\u7ec8\u8f6e\u6570\u300d\u3002\u6a21\u578b\u5e76\u4e0d\u662f\u8bad\u7ec3\u4e86 epochs \u8f6e\uff0c\u800c\u662f\u5230\u7b2c epochs \u8f6e\u505c\u6b62\u8bad\u7ec3\u3002 verbose : \u65e5\u5fd7\u663e\u793a\u6a21\u5f0f\u30020\uff0c1 \u6216 2\u30020 = \u5b89\u9759\u6a21\u5f0f\uff0c1 = \u8fdb\u5ea6\u6761\uff0c2 = \u6bcf\u8f6e\u4e00\u884c\u3002 callbacks : keras.callbacks.Callback \u5b9e\u4f8b\u5217\u8868\u3002\u5728\u8bad\u7ec3\u65f6\u8c03\u7528\u7684\u4e00\u7cfb\u5217\u56de\u8c03\u3002\u8be6\u89c1 callbacks \u3002 validation_data : \u5b83\u53ef\u4ee5\u662f\u4ee5\u4e0b\u4e4b\u4e00\uff1a \u9a8c\u8bc1\u6570\u636e\u7684\u751f\u6210\u5668\u6216 Sequence \u5b9e\u4f8b \u4e00\u4e2a (inputs, targets) \u5143\u7ec4 \u4e00\u4e2a (inputs, targets, sample_weights) \u5143\u7ec4\u3002 validation_steps : \u4ec5\u5f53 validation_data \u662f\u4e00\u4e2a\u751f\u6210\u5668\u65f6\u624d\u53ef\u7528\u3002 \u6bcf\u4e2a epoch \u7ed3\u675f\u65f6\u9a8c\u8bc1\u96c6\u751f\u6210\u5668\u4ea7\u751f\u7684\u6b65\u6570\u3002\u5b83\u901a\u5e38\u5e94\u8be5\u7b49\u4e8e\u4f60\u7684\u6570\u636e\u96c6\u7684\u6837\u672c\u6570\u91cf\u9664\u4ee5\u6279\u91cf\u5927\u5c0f\u3002\u53ef\u9009\u53c2\u6570 Sequence \uff1a\u5982\u679c\u672a\u6307\u5b9a\uff0c\u5c06\u4f7f\u7528 len(generator) \u4f5c\u4e3a\u6b65\u6570\u3002 class_weight : \u53ef\u9009\u7684\u5b57\u5178\uff0c\u7528\u6765\u6620\u5c04\u7c7b\u7d22\u5f15\uff08\u6574\u6570\uff09\u5230\u6743\u91cd\uff08\u6d6e\u70b9\uff09\u503c\uff0c\u7528\u4e8e\u52a0\u6743\u635f\u5931\u51fd\u6570\uff08\u4ec5\u5728\u8bad\u7ec3\u671f\u95f4\uff09\u3002\u8fd9\u53ef\u80fd\u6709\u52a9\u4e8e\u544a\u8bc9\u6a21\u578b \u300c\u66f4\u591a\u5173\u6ce8\u300d\u6765\u81ea\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u7c7b\u7684\u6837\u672c\u3002 max_queue_size : \u6574\u6570\u3002\u751f\u6210\u5668\u961f\u5217\u7684\u6700\u5927\u5c3a\u5bf8\u3002\u5982\u679c\u672a\u6307\u5b9a\uff0c max_queue_size \u5c06\u9ed8\u8ba4\u4e3a 10\u3002 workers : \u6574\u6570\u3002\u4f7f\u7528\u57fa\u4e8e\u8fdb\u7a0b\u7684\u591a\u7ebf\u7a0b\u65f6\u542f\u52a8\u7684\u6700\u5927\u8fdb\u7a0b\u6570\u3002\u5982\u679c\u672a\u6307\u5b9a\uff0c worker \u5c06\u9ed8\u8ba4\u4e3a 1\u3002\u5982\u679c\u4e3a 0\uff0c\u5c06\u5728\u4e3b\u7ebf\u7a0b\u4e0a\u6267\u884c\u751f\u6210\u5668\u3002 use_multiprocessing : \u5982\u679c True\uff0c\u5219\u4f7f\u7528\u57fa\u4e8e\u8fdb\u7a0b\u7684\u591a\u7ebf\u7a0b\u3002\u5982\u679c\u672a\u6307\u5b9a\uff0c use_multiprocessing \u5c06\u9ed8\u8ba4\u4e3a False \u3002\u8bf7\u6ce8\u610f\uff0c\u56e0\u4e3a\u6b64\u5b9e\u73b0\u4f9d\u8d56\u4e8e\u591a\u8fdb\u7a0b\uff0c\u6240\u4ee5\u4e0d\u5e94\u5c06\u4e0d\u53ef\u4f20\u9012\u7684\u53c2\u6570\u4f20\u9012\u7ed9\u751f\u6210\u5668\uff0c\u56e0\u4e3a\u5b83\u4eec\u4e0d\u80fd\u88ab\u8f7b\u6613\u5730\u4f20\u9012\u7ed9\u5b50\u8fdb\u7a0b\u3002 shuffle : \u5e03\u5c14\u503c\u3002\u662f\u5426\u5728\u6bcf\u8f6e\u8fed\u4ee3\u4e4b\u524d\u6253\u4e71 batch \u7684\u987a\u5e8f\u3002\u53ea\u80fd\u4e0e Sequence ( keras.utils.Sequence ) \u5b9e\u4f8b\u540c\u7528\u3002\u5728 steps_per_epoch \u4e0d\u4e3a None \u662f\u65e0\u6548\u679c\u3002 initial_epoch : \u6574\u6570\u3002\u5f00\u59cb\u8bad\u7ec3\u7684\u8f6e\u6b21\uff08\u6709\u52a9\u4e8e\u6062\u590d\u4e4b\u524d\u7684\u8bad\u7ec3\uff09\u3002 \u8fd4\u56de \u4e00\u4e2a History \u5bf9\u8c61\u3002\u5176 History.history \u5c5e\u6027\u662f\u8fde\u7eed epoch \u8bad\u7ec3\u635f\u5931\u548c\u8bc4\u4f30\u503c\uff0c\u4ee5\u53ca\u9a8c\u8bc1\u96c6\u635f\u5931\u548c\u8bc4\u4f30\u503c\u7684\u8bb0\u5f55\uff08\u5982\u679c\u9002\u7528\uff09\u3002 \u5f02\u5e38 ValueError : \u5982\u679c\u751f\u6210\u5668\u751f\u6210\u7684\u6570\u636e\u683c\u5f0f\u4e0d\u6b63\u786e\u3002 \u4f8b\u5b50 def generate_arrays_from_file(path): while True: with open(path) as f: for line in f: # \u4ece\u6587\u4ef6\u4e2d\u7684\u6bcf\u4e00\u884c\u751f\u6210\u8f93\u5165\u6570\u636e\u548c\u6807\u7b7e\u7684 numpy \u6570\u7ec4 x1, x2, y = process_line(line) yield ({'input_1': x1, 'input_2': x2}, {'output': y}) model.fit_generator(generate_arrays_from_file('/my_file.txt'), steps_per_epoch=10000, epochs=10) evaluate_generator evaluate_generator(generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0) \u5728\u6570\u636e\u751f\u6210\u5668\u4e0a\u8bc4\u4f30\u6a21\u578b\u3002 \u8fd9\u4e2a\u751f\u6210\u5668\u5e94\u8be5\u8fd4\u56de\u4e0e test_on_batch \u6240\u63a5\u6536\u7684\u540c\u6837\u7684\u6570\u636e\u3002 \u53c2\u6570 generator : \u751f\u6210\u5668\uff0c\u751f\u6210 (inputs, targets) \u6216 (inputs, targets, sample_weights)\uff0c\u6216 Sequence ( keras.utils.Sequence ) \u5bf9\u8c61\u7684\u5b9e\u4f8b\uff0c\u4ee5\u907f\u514d\u5728\u4f7f\u7528\u591a\u8fdb\u7a0b\u65f6\u51fa\u73b0\u91cd\u590d\u6570\u636e\u3002 steps : \u5728\u505c\u6b62\u4e4b\u524d\uff0c\u6765\u81ea generator \u7684\u603b\u6b65\u6570 (\u6837\u672c\u6279\u6b21)\u3002 \u53ef\u9009\u53c2\u6570 Sequence \uff1a\u5982\u679c\u672a\u6307\u5b9a\uff0c\u5c06\u4f7f\u7528 len(generator) \u4f5c\u4e3a\u6b65\u6570\u3002 max_queue_size : \u751f\u6210\u5668\u961f\u5217\u7684\u6700\u5927\u5c3a\u5bf8\u3002 workers : \u6574\u6570\u3002\u4f7f\u7528\u57fa\u4e8e\u8fdb\u7a0b\u7684\u591a\u7ebf\u7a0b\u65f6\u542f\u52a8\u7684\u6700\u5927\u8fdb\u7a0b\u6570\u3002\u5982\u679c\u672a\u6307\u5b9a\uff0c worker \u5c06\u9ed8\u8ba4\u4e3a 1\u3002\u5982\u679c\u4e3a 0\uff0c\u5c06\u5728\u4e3b\u7ebf\u7a0b\u4e0a\u6267\u884c\u751f\u6210\u5668\u3002 use_multiprocessing : \u5982\u679c True\uff0c\u5219\u4f7f\u7528\u57fa\u4e8e\u8fdb\u7a0b\u7684\u591a\u7ebf\u7a0b\u3002 \u8bf7\u6ce8\u610f\uff0c\u56e0\u4e3a\u6b64\u5b9e\u73b0\u4f9d\u8d56\u4e8e\u591a\u8fdb\u7a0b\uff0c\u6240\u4ee5\u4e0d\u5e94\u5c06\u4e0d\u53ef\u4f20\u9012\u7684\u53c2\u6570\u4f20\u9012\u7ed9\u751f\u6210\u5668\uff0c\u56e0\u4e3a\u5b83\u4eec\u4e0d\u80fd\u88ab\u8f7b\u6613\u5730\u4f20\u9012\u7ed9\u5b50\u8fdb\u7a0b\u3002 verbose \uff1a\u65e5\u5fd7\u663e\u793a\u6a21\u5f0f\uff0c0 \u6216 1\u3002 \u8fd4\u56de \u6807\u91cf\u6d4b\u8bd5\u8bef\u5dee\uff08\u5982\u679c\u6a21\u578b\u53ea\u6709\u5355\u4e2a\u8f93\u51fa\u4e14\u6ca1\u6709\u8bc4\u4f30\u6307\u6807\uff09\u6216\u6807\u91cf\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u5177\u6709\u591a\u4e2a\u8f93\u51fa\u548c/\u6216\u6307\u6807\uff09\u3002 \u5c5e\u6027 model.metrics_names \u5c06\u63d0\u4f9b\u6807\u91cf\u8f93\u51fa\u7684\u663e\u793a\u6807\u7b7e\u3002 \u5f02\u5e38 ValueError : \u5982\u679c\u751f\u6210\u5668\u751f\u6210\u7684\u6570\u636e\u683c\u5f0f\u4e0d\u6b63\u786e\u3002 predict_generator predict_generator(generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0) \u4e3a\u6765\u81ea\u6570\u636e\u751f\u6210\u5668\u7684\u8f93\u5165\u6837\u672c\u751f\u6210\u9884\u6d4b\u3002 \u8fd9\u4e2a\u751f\u6210\u5668\u5e94\u8be5\u8fd4\u56de\u4e0e predict_on_batch \u6240\u63a5\u6536\u7684\u540c\u6837\u7684\u6570\u636e\u3002 \u53c2\u6570 generator : \u8fd4\u56de\u6279\u91cf\u8f93\u5165\u6837\u672c\u7684\u751f\u6210\u5668\uff0c\u6216 Sequence ( keras.utils.Sequence ) \u5bf9\u8c61\u7684\u5b9e\u4f8b\uff0c\u4ee5\u907f\u514d\u5728\u4f7f\u7528\u591a\u8fdb\u7a0b\u65f6\u51fa\u73b0\u91cd\u590d\u6570\u636e\u3002 steps : \u5728\u505c\u6b62\u4e4b\u524d\uff0c\u6765\u81ea generator \u7684\u603b\u6b65\u6570 (\u6837\u672c\u6279\u6b21)\u3002 \u53ef\u9009\u53c2\u6570 Sequence \uff1a\u5982\u679c\u672a\u6307\u5b9a\uff0c\u5c06\u4f7f\u7528 len(generator) \u4f5c\u4e3a\u6b65\u6570\u3002 max_queue_size : \u751f\u6210\u5668\u961f\u5217\u7684\u6700\u5927\u5c3a\u5bf8\u3002 workers : \u6574\u6570\u3002\u4f7f\u7528\u57fa\u4e8e\u8fdb\u7a0b\u7684\u591a\u7ebf\u7a0b\u65f6\u542f\u52a8\u7684\u6700\u5927\u8fdb\u7a0b\u6570\u3002\u5982\u679c\u672a\u6307\u5b9a\uff0c worker \u5c06\u9ed8\u8ba4\u4e3a 1\u3002\u5982\u679c\u4e3a 0\uff0c\u5c06\u5728\u4e3b\u7ebf\u7a0b\u4e0a\u6267\u884c\u751f\u6210\u5668\u3002 use_multiprocessing : \u5982\u679c True\uff0c\u5219\u4f7f\u7528\u57fa\u4e8e\u8fdb\u7a0b\u7684\u591a\u7ebf\u7a0b\u3002 \u8bf7\u6ce8\u610f\uff0c\u56e0\u4e3a\u6b64\u5b9e\u73b0\u4f9d\u8d56\u4e8e\u591a\u8fdb\u7a0b\uff0c\u6240\u4ee5\u4e0d\u5e94\u5c06\u4e0d\u53ef\u4f20\u9012\u7684\u53c2\u6570\u4f20\u9012\u7ed9\u751f\u6210\u5668\uff0c\u56e0\u4e3a\u5b83\u4eec\u4e0d\u80fd\u88ab\u8f7b\u6613\u5730\u4f20\u9012\u7ed9\u5b50\u8fdb\u7a0b\u3002 verbose : \u65e5\u5fd7\u663e\u793a\u6a21\u5f0f\uff0c 0 \u6216 1\u3002 \u8fd4\u56de \u9884\u6d4b\u503c\u7684 Numpy \u6570\u7ec4\u3002 \u5f02\u5e38 ValueError : \u5982\u679c\u751f\u6210\u5668\u751f\u6210\u7684\u6570\u636e\u683c\u5f0f\u4e0d\u6b63\u786e\u3002 get_layer get_layer(name=None, index=None) \u6839\u636e\u540d\u79f0\uff08\u552f\u4e00\uff09\u6216\u7d22\u5f15\u503c\u67e5\u627e\u7f51\u7edc\u5c42\u3002 \u5982\u679c\u540c\u65f6\u63d0\u4f9b\u4e86 name \u548c index \uff0c\u5219 index \u5c06\u4f18\u5148\u3002 \u6839\u636e\u7f51\u7edc\u5c42\u7684\u540d\u79f0\uff08\u552f\u4e00\uff09\u6216\u5176\u7d22\u5f15\u8fd4\u56de\u8be5\u5c42\u3002\u7d22\u5f15\u662f\u57fa\u4e8e\u6c34\u5e73\u56fe\u904d\u5386\u7684\u987a\u5e8f\uff08\u81ea\u4e0b\u800c\u4e0a\uff09\u3002 \u53c2\u6570 name : \u5b57\u7b26\u4e32\uff0c\u5c42\u7684\u540d\u5b57\u3002 index : \u6574\u6570\uff0c\u5c42\u7684\u7d22\u5f15\u3002 \u8fd4\u56de \u4e00\u4e2a\u5c42\u5b9e\u4f8b\u3002 \u5f02\u5e38 ValueError : \u5982\u679c\u5c42\u7684\u540d\u79f0\u6216\u7d22\u5f15\u4e0d\u6b63\u786e\u3002","title":"Sequential \u6a21\u578b API"},{"location":"1-Models/1.sequential/#sequential-api","text":"\u5728\u9605\u8bfb\u8fd9\u7247\u6587\u6863\u524d\uff0c\u8bf7\u5148\u9605\u8bfb Keras Sequential \u6a21\u578b\u6307\u5f15 \u3002","title":"Sequential \u6a21\u578b API"},{"location":"1-Models/1.sequential/#sequential","text":"","title":"Sequential \u6a21\u578b\u65b9\u6cd5"},{"location":"1-Models/1.sequential/#compile","text":"compile(optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None) \u7528\u4e8e\u914d\u7f6e\u8bad\u7ec3\u6a21\u578b\u3002 \u53c2\u6570 optimizer : \u5b57\u7b26\u4e32\uff08\u4f18\u5316\u5668\u540d\uff09\u6216\u8005\u4f18\u5316\u5668\u5bf9\u8c61\u3002\u8be6\u89c1 optimizers \u3002 loss : \u5b57\u7b26\u4e32\uff08\u76ee\u6807\u51fd\u6570\u540d\uff09\u6216\u76ee\u6807\u51fd\u6570\u3002\u8be6\u89c1 losses \u3002 \u5982\u679c\u6a21\u578b\u5177\u6709\u591a\u4e2a\u8f93\u51fa\uff0c\u5219\u53ef\u4ee5\u901a\u8fc7\u4f20\u9012\u635f\u5931\u51fd\u6570\u7684\u5b57\u5178\u6216\u5217\u8868\uff0c\u5728\u6bcf\u4e2a\u8f93\u51fa\u4e0a\u4f7f\u7528\u4e0d\u540c\u7684\u635f\u5931\u3002\u6a21\u578b\u5c06\u6700\u5c0f\u5316\u7684\u635f\u5931\u503c\u5c06\u662f\u6240\u6709\u5355\u4e2a\u635f\u5931\u7684\u603b\u548c\u3002 metrics : \u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u671f\u95f4\u7684\u6a21\u578b\u8bc4\u4f30\u6807\u51c6\u3002\u901a\u5e38\u4f60\u4f1a\u4f7f\u7528 metrics = ['accuracy'] \u3002 \u8981\u4e3a\u591a\u8f93\u51fa\u6a21\u578b\u7684\u4e0d\u540c\u8f93\u51fa\u6307\u5b9a\u4e0d\u540c\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u8fd8\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5b57\u5178\uff0c\u5982 metrics = {'output_a'\uff1a'accuracy'} \u3002 loss_weights : \u6307\u5b9a\u6807\u91cf\u7cfb\u6570\uff08Python\u6d6e\u70b9\u6570\uff09\u7684\u53ef\u9009\u5217\u8868\u6216\u5b57\u5178\uff0c\u7528\u4e8e\u52a0\u6743\u4e0d\u540c\u6a21\u578b\u8f93\u51fa\u7684\u635f\u5931\u8d21\u732e\u3002 \u6a21\u578b\u5c06\u8981\u6700\u5c0f\u5316\u7684\u635f\u5931\u503c\u5c06\u662f\u6240\u6709\u5355\u4e2a\u635f\u5931\u7684\u52a0\u6743\u548c\uff0c\u7531 loss_weights \u7cfb\u6570\u52a0\u6743\u3002 \u5982\u679c\u662f\u5217\u8868\uff0c\u5219\u671f\u671b\u4e0e\u6a21\u578b\u7684\u8f93\u51fa\u5177\u6709 1:1 \u6620\u5c04\u3002 \u5982\u679c\u662f\u5f20\u91cf\uff0c\u5219\u671f\u671b\u5c06\u8f93\u51fa\u540d\u79f0\uff08\u5b57\u7b26\u4e32\uff09\u6620\u5c04\u5230\u6807\u91cf\u7cfb\u6570\u3002 sample_weight_mode : \u5982\u679c\u4f60\u9700\u8981\u6267\u884c\u6309\u65f6\u95f4\u6b65\u91c7\u6837\u6743\u91cd\uff082D \u6743\u91cd\uff09\uff0c\u8bf7\u5c06\u5176\u8bbe\u7f6e\u4e3a temporal \u3002 \u9ed8\u8ba4\u4e3a None \uff0c\u4e3a\u91c7\u6837\u6743\u91cd\uff081D\uff09\u3002\u5982\u679c\u6a21\u578b\u6709\u591a\u4e2a\u8f93\u51fa\uff0c\u5219\u53ef\u4ee5\u901a\u8fc7\u4f20\u9012 mode \u7684\u5b57\u5178\u6216\u5217\u8868\uff0c\u4ee5\u5728\u6bcf\u4e2a\u8f93\u51fa\u4e0a\u4f7f\u7528\u4e0d\u540c\u7684 sample_weight_mode \u3002 weighted_metrics : \u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u671f\u95f4\uff0c\u7531 sample_weight \u6216 class_weight \u8bc4\u4f30\u548c\u52a0\u6743\u7684\u5ea6\u91cf\u6807\u51c6\u5217\u8868\u3002 target_tensors : \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cKeras \u5c06\u4e3a\u6a21\u578b\u7684\u76ee\u6807\u521b\u5efa\u4e00\u4e2a\u5360\u4f4d\u7b26\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5c06\u4f7f\u7528\u76ee\u6807\u6570\u636e\u3002\u76f8\u53cd\uff0c\u5982\u679c\u4f60\u60f3\u4f7f\u7528\u81ea\u5df1\u7684\u76ee\u6807\u5f20\u91cf\uff08\u53cd\u8fc7\u6765\u8bf4\uff0cKeras \u5728\u8bad\u7ec3\u671f\u95f4\u4e0d\u4f1a\u8f7d\u5165\u8fd9\u4e9b\u76ee\u6807\u5f20\u91cf\u7684\u5916\u90e8 Numpy \u6570\u636e\uff09\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7 target_tensors \u53c2\u6570\u6307\u5b9a\u5b83\u4eec\u3002\u5b83\u5e94\u8be5\u662f\u5355\u4e2a\u5f20\u91cf\uff08\u5bf9\u4e8e\u5355\u8f93\u51fa Sequential \u6a21\u578b\uff09\u3002 **kwargs : \u5f53\u4f7f\u7528 Theano/CNTK \u540e\u7aef\u65f6\uff0c\u8fd9\u4e9b\u53c2\u6570\u88ab\u4f20\u5165 K.function \u3002\u5f53\u4f7f\u7528 TensorFlow \u540e\u7aef\u65f6\uff0c\u8fd9\u4e9b\u53c2\u6570\u88ab\u4f20\u9012\u5230 tf.Session.run \u3002 \u5f02\u5e38 ValueError : \u5982\u679c optimizer , loss , metrics \u6216 sample_weight_mode \u8fd9\u4e9b\u53c2\u6570\u4e0d\u5408\u6cd5\u3002","title":"compile"},{"location":"1-Models/1.sequential/#fit","text":"fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None) \u4ee5\u56fa\u5b9a\u6570\u91cf\u7684\u8f6e\u6b21\uff08\u6570\u636e\u96c6\u4e0a\u7684\u8fed\u4ee3\uff09\u8bad\u7ec3\u6a21\u578b\u3002 \u53c2\u6570 x : \u8bad\u7ec3\u6570\u636e\u7684 Numpy \u6570\u7ec4\u3002 \u5982\u679c\u6a21\u578b\u4e2d\u7684\u8f93\u5165\u5c42\u88ab\u547d\u540d\uff0c\u4f60\u4e5f\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5b57\u5178\uff0c\u5c06\u8f93\u5165\u5c42\u540d\u79f0\u6620\u5c04\u5230 Numpy \u6570\u7ec4\u3002 \u5982\u679c\u4ece\u672c\u5730\u6846\u67b6\u5f20\u91cf\u9988\u9001\uff08\u4f8b\u5982 TensorFlow \u6570\u636e\u5f20\u91cf\uff09\u6570\u636e\uff0cx \u53ef\u4ee5\u662f None \uff08\u9ed8\u8ba4\uff09\u3002 y : \u76ee\u6807\uff08\u6807\u7b7e\uff09\u6570\u636e\u7684 Numpy \u6570\u7ec4\u3002 \u5982\u679c\u6a21\u578b\u4e2d\u7684\u8f93\u51fa\u5c42\u88ab\u547d\u540d\uff0c\u4f60\u4e5f\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5b57\u5178\uff0c\u5c06\u8f93\u51fa\u5c42\u540d\u79f0\u6620\u5c04\u5230 Numpy \u6570\u7ec4\u3002 \u5982\u679c\u4ece\u672c\u5730\u6846\u67b6\u5f20\u91cf\u9988\u9001\uff08\u4f8b\u5982 TensorFlow \u6570\u636e\u5f20\u91cf\uff09\u6570\u636e\uff0cy \u53ef\u4ee5\u662f None \uff08\u9ed8\u8ba4\uff09\u3002 batch_size : \u6574\u6570\u6216 None \u3002\u6bcf\u6b21\u63d0\u5ea6\u66f4\u65b0\u7684\u6837\u672c\u6570\u3002\u5982\u679c\u672a\u6307\u5b9a\uff0c\u9ed8\u8ba4\u4e3a 32. epochs : \u6574\u6570\u3002\u8bad\u7ec3\u6a21\u578b\u8fed\u4ee3\u8f6e\u6b21\u3002\u4e00\u4e2a\u8f6e\u6b21\u662f\u5728\u6574\u4e2a x \u6216 y \u4e0a\u7684\u4e00\u8f6e\u8fed\u4ee3\u3002\u8bf7\u6ce8\u610f\uff0c\u4e0e initial_epoch \u4e00\u8d77\uff0c epochs \u88ab\u7406\u89e3\u4e3a \u300c\u6700\u7ec8\u8f6e\u6b21\u300d\u3002\u6a21\u578b\u5e76\u4e0d\u662f\u8bad\u7ec3\u4e86 epochs \u8f6e\uff0c\u800c\u662f\u5230\u7b2c epochs \u8f6e\u505c\u6b62\u8bad\u7ec3\u3002 verbose : 0, 1 \u6216 2\u3002\u65e5\u5fd7\u663e\u793a\u6a21\u5f0f\u3002 0 = \u5b89\u9759\u6a21\u5f0f, 1 = \u8fdb\u5ea6\u6761, 2 = \u6bcf\u8f6e\u4e00\u884c\u3002 callbacks : \u4e00\u7cfb\u5217\u7684 keras.callbacks.Callback \u5b9e\u4f8b\u3002\u4e00\u7cfb\u5217\u53ef\u4ee5\u5728\u8bad\u7ec3\u65f6\u4f7f\u7528\u7684\u56de\u8c03\u51fd\u6570\u3002\u8be6\u89c1 callbacks \u3002 validation_split : \u5728 0 \u548c 1 \u4e4b\u95f4\u6d6e\u52a8\u3002\u7528\u4f5c\u9a8c\u8bc1\u96c6\u7684\u8bad\u7ec3\u6570\u636e\u7684\u6bd4\u4f8b\u3002\u6a21\u578b\u5c06\u5206\u51fa\u4e00\u90e8\u5206\u4e0d\u4f1a\u88ab\u8bad\u7ec3\u7684\u9a8c\u8bc1\u6570\u636e\uff0c\u5e76\u5c06\u5728\u6bcf\u4e00\u8f6e\u7ed3\u675f\u65f6\u8bc4\u4f30\u8fd9\u4e9b\u9a8c\u8bc1\u6570\u636e\u7684\u8bef\u5dee\u548c\u4efb\u4f55\u5176\u4ed6\u6a21\u578b\u6307\u6807\u3002\u9a8c\u8bc1\u6570\u636e\u662f\u6df7\u6d17\u4e4b\u524d x \u548c y \u6570\u636e\u7684\u6700\u540e\u4e00\u90e8\u5206\u6837\u672c\u4e2d\u3002 validation_data : \u5143\u7ec4 (x_val\uff0cy_val) \u6216\u5143\u7ec4 (x_val\uff0cy_val\uff0cval_sample_weights) \uff0c\u7528\u6765\u8bc4\u4f30\u635f\u5931\uff0c\u4ee5\u53ca\u5728\u6bcf\u8f6e\u7ed3\u675f\u65f6\u7684\u4efb\u4f55\u6a21\u578b\u5ea6\u91cf\u6307\u6807\u3002\u6a21\u578b\u5c06\u4e0d\u4f1a\u5728\u8fd9\u4e2a\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u8fd9\u4e2a\u53c2\u6570\u4f1a\u8986\u76d6 validation_split \u3002 shuffle : \u5e03\u5c14\u503c\uff08\u662f\u5426\u5728\u6bcf\u8f6e\u8fed\u4ee3\u4e4b\u524d\u6df7\u6d17\u6570\u636e\uff09\u6216\u8005 \u5b57\u7b26\u4e32 ( batch )\u3002 batch \u662f\u5904\u7406 HDF5 \u6570\u636e\u9650\u5236\u7684\u7279\u6b8a\u9009\u9879\uff0c\u5b83\u5bf9\u4e00\u4e2a batch \u5185\u90e8\u7684\u6570\u636e\u8fdb\u884c\u6df7\u6d17\u3002\u5f53 steps_per_epoch \u975e None \u65f6\uff0c\u8fd9\u4e2a\u53c2\u6570\u65e0\u6548\u3002 class_weight : \u53ef\u9009\u7684\u5b57\u5178\uff0c\u7528\u6765\u6620\u5c04\u7c7b\u7d22\u5f15\uff08\u6574\u6570\uff09\u5230\u6743\u91cd\uff08\u6d6e\u70b9\uff09\u503c\uff0c\u7528\u4e8e\u52a0\u6743\u635f\u5931\u51fd\u6570\uff08\u4ec5\u5728\u8bad\u7ec3\u671f\u95f4\uff09\u3002\u8fd9\u53ef\u80fd\u6709\u52a9\u4e8e\u544a\u8bc9\u6a21\u578b \u300c\u66f4\u591a\u5173\u6ce8\u300d\u6765\u81ea\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u7c7b\u7684\u6837\u672c\u3002 sample_weight : \u8bad\u7ec3\u6837\u672c\u7684\u53ef\u9009 Numpy \u6743\u91cd\u6570\u7ec4\uff0c\u7528\u4e8e\u5bf9\u635f\u5931\u51fd\u6570\u8fdb\u884c\u52a0\u6743\uff08\u4ec5\u5728\u8bad\u7ec3\u671f\u95f4\uff09\u3002\u60a8\u53ef\u4ee5\u4f20\u9012\u4e0e\u8f93\u5165\u6837\u672c\u957f\u5ea6\u76f8\u540c\u7684\u5e73\u5766\uff081D\uff09Numpy \u6570\u7ec4\uff08\u6743\u91cd\u548c\u6837\u672c\u4e4b\u95f4\u7684 1\uff1a1 \u6620\u5c04\uff09\uff0c\u6216\u8005\u5728\u65f6\u5e8f\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u4f20\u9012\u5c3a\u5bf8\u4e3a (samples, sequence_length) \u7684 2D \u6570\u7ec4\uff0c\u4ee5\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u65f6\u95f4\u6b65\u65bd\u52a0\u4e0d\u540c\u7684\u6743\u91cd\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4f60\u5e94\u8be5\u786e\u4fdd\u5728 compile() \u4e2d\u6307\u5b9a sample_weight_mode=\"temporal\" \u3002 initial_epoch : \u5f00\u59cb\u8bad\u7ec3\u7684\u8f6e\u6b21\uff08\u6709\u52a9\u4e8e\u6062\u590d\u4e4b\u524d\u7684\u8bad\u7ec3\uff09\u3002 steps_per_epoch : \u5728\u58f0\u660e\u4e00\u4e2a\u8f6e\u6b21\u5b8c\u6210\u5e76\u5f00\u59cb\u4e0b\u4e00\u4e2a\u8f6e\u6b21\u4e4b\u524d\u7684\u603b\u6b65\u6570\uff08\u6837\u54c1\u6279\u6b21\uff09\u3002\u4f7f\u7528 TensorFlow \u6570\u636e\u5f20\u91cf\u7b49\u8f93\u5165\u5f20\u91cf\u8fdb\u884c\u8bad\u7ec3\u65f6\uff0c\u9ed8\u8ba4\u503c None \u7b49\u4e8e\u6570\u636e\u96c6\u4e2d\u6837\u672c\u7684\u6570\u91cf\u9664\u4ee5 batch \u7684\u5927\u5c0f\uff0c\u5982\u679c\u65e0\u6cd5\u786e\u5b9a\uff0c\u5219\u4e3a 1\u3002 validation_steps : \u53ea\u6709\u5728\u6307\u5b9a\u4e86 steps_per_epoch \u65f6\u624d\u6709\u7528\u3002\u505c\u6b62\u524d\u8981\u9a8c\u8bc1\u7684\u603b\u6b65\u6570\uff08\u6279\u6b21\u6837\u672c\uff09\u3002 \u8fd4\u56de \u4e00\u4e2a History \u5bf9\u8c61\u3002\u5176 History.history \u5c5e\u6027\u662f\u8fde\u7eed epoch \u8bad\u7ec3\u635f\u5931\u548c\u8bc4\u4f30\u503c\uff0c\u4ee5\u53ca\u9a8c\u8bc1\u96c6\u635f\u5931\u548c\u8bc4\u4f30\u503c\u7684\u8bb0\u5f55\uff08\u5982\u679c\u9002\u7528\uff09\u3002 \u5f02\u5e38 RuntimeError : \u5982\u679c\u6a21\u578b\u4ece\u672a\u7f16\u8bd1\u3002 ValueError : \u5728\u63d0\u4f9b\u7684\u8f93\u5165\u6570\u636e\u4e0e\u6a21\u578b\u671f\u671b\u7684\u4e0d\u5339\u914d\u7684\u60c5\u51b5\u4e0b\u3002","title":"fit"},{"location":"1-Models/1.sequential/#evaluate","text":"evaluate(x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None) \u5728\u6d4b\u8bd5\u6a21\u5f0f\uff0c\u8fd4\u56de\u8bef\u5dee\u503c\u548c\u8bc4\u4f30\u6807\u51c6\u503c\u3002 \u8ba1\u7b97\u9010\u6279\u6b21\u8fdb\u884c\u3002 \u53c2\u6570 x : \u8bad\u7ec3\u6570\u636e\u7684 Numpy \u6570\u7ec4\u3002 \u5982\u679c\u6a21\u578b\u4e2d\u7684\u8f93\u5165\u5c42\u88ab\u547d\u540d\uff0c\u4f60\u4e5f\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5b57\u5178\uff0c\u5c06\u8f93\u5165\u5c42\u540d\u79f0\u6620\u5c04\u5230 Numpy \u6570\u7ec4\u3002 \u5982\u679c\u4ece\u672c\u5730\u6846\u67b6\u5f20\u91cf\u9988\u9001\uff08\u4f8b\u5982 TensorFlow \u6570\u636e\u5f20\u91cf\uff09\u6570\u636e\uff0cx \u53ef\u4ee5\u662f None \uff08\u9ed8\u8ba4\uff09\u3002 y : \u76ee\u6807\uff08\u6807\u7b7e\uff09\u6570\u636e\u7684 Numpy \u6570\u7ec4\u3002 \u5982\u679c\u6a21\u578b\u4e2d\u7684\u8f93\u51fa\u5c42\u88ab\u547d\u540d\uff0c\u4f60\u4e5f\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5b57\u5178\uff0c\u5c06\u8f93\u51fa\u5c42\u540d\u79f0\u6620\u5c04\u5230 Numpy \u6570\u7ec4\u3002 \u5982\u679c\u4ece\u672c\u5730\u6846\u67b6\u5f20\u91cf\u9988\u9001\uff08\u4f8b\u5982 TensorFlow \u6570\u636e\u5f20\u91cf\uff09\u6570\u636e\uff0cy \u53ef\u4ee5\u662f None \uff08\u9ed8\u8ba4\uff09\u3002 batch_size : \u6574\u6570\u6216 None \u3002\u6bcf\u6b21\u63d0\u5ea6\u66f4\u65b0\u7684\u6837\u672c\u6570\u3002\u5982\u679c\u672a\u6307\u5b9a\uff0c\u9ed8\u8ba4\u4e3a 32. verbose : 0, 1\u3002\u65e5\u5fd7\u663e\u793a\u6a21\u5f0f\u30020 = \u5b89\u9759\u6a21\u5f0f, 1 = \u8fdb\u5ea6\u6761\u3002 sample_weight : \u8bad\u7ec3\u6837\u672c\u7684\u53ef\u9009 Numpy \u6743\u91cd\u6570\u7ec4\uff0c\u7528\u4e8e\u5bf9\u635f\u5931\u51fd\u6570\u8fdb\u884c\u52a0\u6743\uff08\u4ec5\u5728\u8bad\u7ec3\u671f\u95f4\uff09\u3002 \u60a8\u53ef\u4ee5\u4f20\u9012\u4e0e\u8f93\u5165\u6837\u672c\u957f\u5ea6\u76f8\u540c\u7684\u5e73\u5766\uff081D\uff09Numpy \u6570\u7ec4\uff08\u6743\u91cd\u548c\u6837\u672c\u4e4b\u95f4\u7684 1\uff1a1 \u6620\u5c04\uff09\uff0c\u6216\u8005\u5728\u65f6\u5e8f\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u4f20\u9012\u5c3a\u5bf8\u4e3a (samples, sequence_length) \u7684 2D \u6570\u7ec4\uff0c\u4ee5\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u65f6\u95f4\u6b65\u65bd\u52a0\u4e0d\u540c\u7684\u6743\u91cd\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4f60\u5e94\u8be5\u786e\u4fdd\u5728 compile() \u4e2d\u6307\u5b9a sample_weight_mode=\"temporal\" \u3002 steps : \u6574\u6570\u6216 None \u3002 \u58f0\u660e\u8bc4\u4f30\u7ed3\u675f\u4e4b\u524d\u7684\u603b\u6b65\u6570\uff08\u6279\u6b21\u6837\u672c\uff09\u3002\u9ed8\u8ba4\u503c None \u3002 \u8fd4\u56de \u6807\u91cf\u6d4b\u8bd5\u8bef\u5dee\uff08\u5982\u679c\u6a21\u578b\u53ea\u6709\u5355\u4e2a\u8f93\u51fa\u4e14\u6ca1\u6709\u8bc4\u4f30\u6307\u6807\uff09\u6216\u6807\u91cf\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u5177\u6709\u591a\u4e2a\u8f93\u51fa\u548c/\u6216\u6307\u6807\uff09\u3002 \u5c5e\u6027 model.metrics_names \u5c06\u63d0\u4f9b\u6807\u91cf\u8f93\u51fa\u7684\u663e\u793a\u6807\u7b7e\u3002","title":"evaluate"},{"location":"1-Models/1.sequential/#predict","text":"predict(x, batch_size=None, verbose=0, steps=None) \u4e3a\u8f93\u5165\u6837\u672c\u751f\u6210\u8f93\u51fa\u9884\u6d4b\u3002 \u8ba1\u7b97\u9010\u6279\u6b21\u8fdb\u884c\u3002 \u53c2\u6570 x : \u8f93\u5165\u6570\u636e\uff0cNumpy \u6570\u7ec4\uff08\u6216\u8005\u5982\u679c\u6a21\u578b\u6709\u591a\u4e2a\u8f93\u5165\uff0c\u5219\u4e3a Numpy \u6570\u7ec4\u5217\u8868\uff09\u3002 batch_size : \u6574\u6570\u3002\u5982\u672a\u6307\u5b9a\uff0c\u9ed8\u8ba4\u4e3a 32\u3002 verbose : \u65e5\u5fd7\u663e\u793a\u6a21\u5f0f\uff0c0 \u6216 1\u3002 steps : \u58f0\u660e\u9884\u6d4b\u7ed3\u675f\u4e4b\u524d\u7684\u603b\u6b65\u6570\uff08\u6279\u6b21\u6837\u672c\uff09\u3002\u9ed8\u8ba4\u503c None \u3002 \u8fd4\u56de \u9884\u6d4b\u7684 Numpy \u6570\u7ec4\u3002 \u5f02\u5e38 ValueError : \u5982\u679c\u63d0\u4f9b\u7684\u8f93\u5165\u6570\u636e\u4e0e\u6a21\u578b\u7684\u671f\u671b\u6570\u636e\u4e0d\u5339\u914d\uff0c\u6216\u8005\u6709\u72b6\u6001\u6a21\u578b\u6536\u5230\u7684\u6570\u91cf\u4e0d\u662f\u6279\u91cf\u5927\u5c0f\u7684\u500d\u6570\u3002","title":"predict"},{"location":"1-Models/1.sequential/#train_on_batch","text":"train_on_batch(x, y, sample_weight=None, class_weight=None) \u4e00\u6279\u6837\u54c1\u7684\u5355\u6b21\u68af\u5ea6\u66f4\u65b0\u3002 Arguments x : \u8bad\u7ec3\u6570\u636e\u7684 Numpy \u6570\u7ec4\uff0c\u5982\u679c\u6a21\u578b\u5177\u6709\u591a\u4e2a\u8f93\u5165\uff0c\u5219\u4e3a Numpy \u6570\u7ec4\u5217\u8868\u3002\u5982\u679c\u6a21\u578b\u4e2d\u7684\u6240\u6709\u8f93\u5165\u90fd\u5df2\u547d\u540d\uff0c\u4f60\u8fd8\u53ef\u4ee5\u4f20\u5165\u8f93\u5165\u540d\u79f0\u5230 Numpy \u6570\u7ec4\u7684\u6620\u5c04\u5b57\u5178\u3002 y : \u76ee\u6807\u6570\u636e\u7684 Numpy \u6570\u7ec4\uff0c\u5982\u679c\u6a21\u578b\u5177\u6709\u591a\u4e2a\u8f93\u5165\uff0c\u5219\u4e3a Numpy \u6570\u7ec4\u5217\u8868\u3002\u5982\u679c\u6a21\u578b\u4e2d\u7684\u6240\u6709\u8f93\u51fa\u90fd\u5df2\u547d\u540d\uff0c\u4f60\u8fd8\u53ef\u4ee5\u4f20\u5165\u8f93\u51fa\u540d\u79f0\u5230 Numpy \u6570\u7ec4\u7684\u6620\u5c04\u5b57\u5178\u3002 sample_weight : \u8bad\u7ec3\u6837\u672c\u7684\u53ef\u9009 Numpy \u6743\u91cd\u6570\u7ec4\uff0c\u7528\u4e8e\u5bf9\u635f\u5931\u51fd\u6570\u8fdb\u884c\u52a0\u6743\uff08\u4ec5\u5728\u8bad\u7ec3\u671f\u95f4\uff09\u3002 \u60a8\u53ef\u4ee5\u4f20\u9012\u4e0e\u8f93\u5165\u6837\u672c\u957f\u5ea6\u76f8\u540c\u7684\u5e73\u5766\uff081D\uff09Numpy \u6570\u7ec4\uff08\u6743\u91cd\u548c\u6837\u672c\u4e4b\u95f4\u7684 1\uff1a1 \u6620\u5c04\uff09\uff0c\u6216\u8005\u5728\u65f6\u5e8f\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u4f20\u9012\u5c3a\u5bf8\u4e3a (samples, sequence_length) \u7684 2D \u6570\u7ec4\uff0c\u4ee5\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u65f6\u95f4\u6b65\u65bd\u52a0\u4e0d\u540c\u7684\u6743\u91cd\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4f60\u5e94\u8be5\u786e\u4fdd\u5728 compile() \u4e2d\u6307\u5b9a sample_weight_mode=\"temporal\" \u3002 class_weight : \u53ef\u9009\u7684\u5b57\u5178\uff0c\u7528\u6765\u6620\u5c04\u7c7b\u7d22\u5f15\uff08\u6574\u6570\uff09\u5230\u6743\u91cd\uff08\u6d6e\u70b9\uff09\u503c\uff0c\u7528\u4e8e\u52a0\u6743\u635f\u5931\u51fd\u6570\uff08\u4ec5\u5728\u8bad\u7ec3\u671f\u95f4\uff09\u3002\u8fd9\u53ef\u80fd\u6709\u52a9\u4e8e\u544a\u8bc9\u6a21\u578b \u300c\u66f4\u591a\u5173\u6ce8\u300d\u6765\u81ea\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u7c7b\u7684\u6837\u672c\u3002 \u8fd4\u56de \u6807\u91cf\u8bad\u7ec3\u8bef\u5dee\uff08\u5982\u679c\u6a21\u578b\u53ea\u6709\u5355\u4e2a\u8f93\u51fa\u4e14\u6ca1\u6709\u8bc4\u4f30\u6307\u6807\uff09\u6216\u6807\u91cf\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u5177\u6709\u591a\u4e2a\u8f93\u51fa\u548c/\u6216\u6307\u6807\uff09\u3002 \u5c5e\u6027 model.metrics_names \u5c06\u63d0\u4f9b\u6807\u91cf\u8f93\u51fa\u7684\u663e\u793a\u6807\u7b7e\u3002","title":"train_on_batch"},{"location":"1-Models/1.sequential/#test_on_batch","text":"test_on_batch(x, y, sample_weight=None) \u5728\u4e00\u6279\u6837\u672c\u4e0a\u8bc4\u4f30\u6a21\u578b\u3002 \u53c2\u6570 x : \u8bad\u7ec3\u6570\u636e\u7684 Numpy \u6570\u7ec4\uff0c\u5982\u679c\u6a21\u578b\u5177\u6709\u591a\u4e2a\u8f93\u5165\uff0c\u5219\u4e3a Numpy \u6570\u7ec4\u5217\u8868\u3002\u5982\u679c\u6a21\u578b\u4e2d\u7684\u6240\u6709\u8f93\u5165\u90fd\u5df2\u547d\u540d\uff0c\u4f60\u8fd8\u53ef\u4ee5\u4f20\u5165\u8f93\u5165\u540d\u79f0\u5230 Numpy \u6570\u7ec4\u7684\u6620\u5c04\u5b57\u5178\u3002 y : \u76ee\u6807\u6570\u636e\u7684 Numpy \u6570\u7ec4\uff0c\u5982\u679c\u6a21\u578b\u5177\u6709\u591a\u4e2a\u8f93\u5165\uff0c\u5219\u4e3a Numpy \u6570\u7ec4\u5217\u8868\u3002\u5982\u679c\u6a21\u578b\u4e2d\u7684\u6240\u6709\u8f93\u51fa\u90fd\u5df2\u547d\u540d\uff0c\u4f60\u8fd8\u53ef\u4ee5\u4f20\u5165\u8f93\u51fa\u540d\u79f0\u5230 Numpy \u6570\u7ec4\u7684\u6620\u5c04\u5b57\u5178\u3002 sample_weight : \u8bad\u7ec3\u6837\u672c\u7684\u53ef\u9009 Numpy \u6743\u91cd\u6570\u7ec4\uff0c\u7528\u4e8e\u5bf9\u635f\u5931\u51fd\u6570\u8fdb\u884c\u52a0\u6743\uff08\u4ec5\u5728\u8bad\u7ec3\u671f\u95f4\uff09\u3002 \u60a8\u53ef\u4ee5\u4f20\u9012\u4e0e\u8f93\u5165\u6837\u672c\u957f\u5ea6\u76f8\u540c\u7684\u5e73\u5766\uff081D\uff09Numpy \u6570\u7ec4\uff08\u6743\u91cd\u548c\u6837\u672c\u4e4b\u95f4\u7684 1\uff1a1 \u6620\u5c04\uff09\uff0c\u6216\u8005\u5728\u65f6\u5e8f\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u4f20\u9012\u5c3a\u5bf8\u4e3a (samples, sequence_length) \u7684 2D \u6570\u7ec4\uff0c\u4ee5\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u65f6\u95f4\u6b65\u65bd\u52a0\u4e0d\u540c\u7684\u6743\u91cd\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4f60\u5e94\u8be5\u786e\u4fdd\u5728 compile() \u4e2d\u6307\u5b9a sample_weight_mode=\"temporal\" \u3002 \u8fd4\u56de \u6807\u91cf\u6d4b\u8bd5\u8bef\u5dee\uff08\u5982\u679c\u6a21\u578b\u53ea\u6709\u5355\u4e2a\u8f93\u51fa\u4e14\u6ca1\u6709\u8bc4\u4f30\u6307\u6807\uff09\u6216\u6807\u91cf\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u5177\u6709\u591a\u4e2a\u8f93\u51fa\u548c/\u6216\u6307\u6807\uff09\u3002 \u5c5e\u6027 model.metrics_names \u5c06\u63d0\u4f9b\u6807\u91cf\u8f93\u51fa\u7684\u663e\u793a\u6807\u7b7e\u3002","title":"test_on_batch"},{"location":"1-Models/1.sequential/#predict_on_batch","text":"predict_on_batch(x) \u8fd4\u56de\u4e00\u6279\u6837\u672c\u7684\u6a21\u578b\u9884\u6d4b\u503c\u3002 \u53c2\u6570 x : \u8f93\u5165\u6570\u636e\uff0cNumpy \u6570\u7ec4\u6216\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u6709\u591a\u8f93\u5165\uff09\u3002 \u8fd4\u56de \u9884\u6d4b\u503c\u7684 Numpy \u6570\u7ec4\u3002","title":"predict_on_batch"},{"location":"1-Models/1.sequential/#fit_generator","text":"fit_generator(generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0) \u4f7f\u7528 Python \u751f\u6210\u5668\u6216 Sequence \u5b9e\u4f8b\u9010\u6279\u751f\u6210\u7684\u6570\u636e\uff0c\u6309\u6279\u6b21\u8bad\u7ec3\u6a21\u578b\u3002 \u751f\u6210\u5668\u4e0e\u6a21\u578b\u5e76\u884c\u8fd0\u884c\uff0c\u4ee5\u63d0\u9ad8\u6548\u7387\u3002 \u4f8b\u5982\uff0c\u8fd9\u53ef\u4ee5\u8ba9\u4f60\u5728 CPU \u4e0a\u5bf9\u56fe\u50cf\u8fdb\u884c\u5b9e\u65f6\u6570\u636e\u589e\u5f3a\uff0c\u4ee5\u5728 GPU \u4e0a\u8bad\u7ec3\u6a21\u578b\u3002 keras.utils.Sequence \u7684\u4f7f\u7528\u53ef\u4ee5\u4fdd\u8bc1\u6570\u636e\u7684\u987a\u5e8f\uff0c \u4ee5\u53ca\u5f53 use_multiprocessing=True \u65f6 \uff0c\u4fdd\u8bc1\u6bcf\u4e2a\u8f93\u5165\u5728\u6bcf\u4e2a epoch \u53ea\u4f7f\u7528\u4e00\u6b21\u3002 \u53c2\u6570 generator : \u4e00\u4e2a\u751f\u6210\u5668\u6216 Sequence ( keras.utils.Sequence ) \u5bf9\u8c61\u7684\u5b9e\u4f8b\uff0c\u4ee5\u907f\u514d\u5728\u4f7f\u7528\u591a\u8fdb\u7a0b\u65f6\u51fa\u73b0\u91cd\u590d\u6570\u636e\u3002 \u751f\u6210\u5668\u7684\u8f93\u51fa\u5e94\u8be5\u4e3a\u4ee5\u4e0b\u4e4b\u4e00\uff1a \u4e00\u4e2a (inputs, targets) \u5143\u7ec4 \u4e00\u4e2a (inputs, targets, sample_weights) \u5143\u7ec4\u3002 \u8fd9\u4e2a\u5143\u7ec4\uff08\u751f\u6210\u5668\u7684\u5355\u4e2a\u8f93\u51fa\uff09\u8868\u793a\u4e00\u4e2a\u72ec\u7acb\u6279\u6b21\u3002\u56e0\u6b64\uff0c\u6b64\u5143\u7ec4\u4e2d\u7684\u6240\u6709\u6570\u7ec4\u5fc5\u987b\u5177\u6709\u76f8\u540c\u7684\u957f\u5ea6\uff08\u7b49\u4e8e\u6b64\u6279\u6b21\u7684\u5927\u5c0f\uff09\u3002\u4e0d\u540c\u7684\u6279\u6b21\u53ef\u80fd\u5177\u6709\u4e0d\u540c\u7684\u5927\u5c0f\u3002\u4f8b\u5982\uff0c\u5982\u679c\u6570\u636e\u96c6\u7684\u5927\u5c0f\u4e0d\u80fd\u88ab\u6279\u91cf\u5927\u5c0f\u6574\u9664\uff0c\u5219\u6700\u540e\u4e00\u6279\u65f6\u671f\u901a\u5e38\u5c0f\u4e8e\u5176\u4ed6\u6279\u6b21\u3002\u751f\u6210\u5668\u5c06\u65e0\u9650\u5730\u5728\u6570\u636e\u96c6\u4e0a\u5faa\u73af\u3002\u5f53\u8fd0\u884c\u5230\u7b2c steps_per_epoch \u65f6\uff0c\u8bb0\u4e00\u4e2a epoch \u7ed3\u675f\u3002 steps_per_epoch : \u6574\u6570\u3002\u5728\u58f0\u660e\u4e00\u4e2a epoch \u5b8c\u6210\u5e76\u5f00\u59cb\u4e0b\u4e00\u4e2a epoch \u4e4b\u524d\u4ece generator \u4ea7\u751f\u7684\u603b\u6b65\u6570\uff08\u6279\u6b21\u6837\u672c\uff09\u3002\u5b83\u901a\u5e38\u5e94\u8be5\u7b49\u4e8e\u4f60\u7684\u6570\u636e\u96c6\u7684\u6837\u672c\u6570\u91cf\u9664\u4ee5\u6279\u91cf\u5927\u5c0f\u3002\u53ef\u9009\u53c2\u6570 Sequence \uff1a\u5982\u679c\u672a\u6307\u5b9a\uff0c\u5c06\u4f7f\u7528 len(generator) \u4f5c\u4e3a\u6b65\u6570\u3002 epochs : \u6574\u6570\uff0c\u6570\u636e\u7684\u8fed\u4ee3\u603b\u8f6e\u6570\u3002\u4e00\u4e2a epoch \u662f\u5bf9\u6240\u63d0\u4f9b\u7684\u6574\u4e2a\u6570\u636e\u7684\u4e00\u8f6e\u8fed\u4ee3\uff0c\u7531 steps_per_epoch \u6240\u5b9a\u4e49\u3002\u8bf7\u6ce8\u610f\uff0c\u4e0e initial_epoch \u4e00\u8d77\uff0c\u53c2\u6570 epochs \u5e94\u88ab\u7406\u89e3\u4e3a \u300c\u6700\u7ec8\u8f6e\u6570\u300d\u3002\u6a21\u578b\u5e76\u4e0d\u662f\u8bad\u7ec3\u4e86 epochs \u8f6e\uff0c\u800c\u662f\u5230\u7b2c epochs \u8f6e\u505c\u6b62\u8bad\u7ec3\u3002 verbose : \u65e5\u5fd7\u663e\u793a\u6a21\u5f0f\u30020\uff0c1 \u6216 2\u30020 = \u5b89\u9759\u6a21\u5f0f\uff0c1 = \u8fdb\u5ea6\u6761\uff0c2 = \u6bcf\u8f6e\u4e00\u884c\u3002 callbacks : keras.callbacks.Callback \u5b9e\u4f8b\u5217\u8868\u3002\u5728\u8bad\u7ec3\u65f6\u8c03\u7528\u7684\u4e00\u7cfb\u5217\u56de\u8c03\u3002\u8be6\u89c1 callbacks \u3002 validation_data : \u5b83\u53ef\u4ee5\u662f\u4ee5\u4e0b\u4e4b\u4e00\uff1a \u9a8c\u8bc1\u6570\u636e\u7684\u751f\u6210\u5668\u6216 Sequence \u5b9e\u4f8b \u4e00\u4e2a (inputs, targets) \u5143\u7ec4 \u4e00\u4e2a (inputs, targets, sample_weights) \u5143\u7ec4\u3002 validation_steps : \u4ec5\u5f53 validation_data \u662f\u4e00\u4e2a\u751f\u6210\u5668\u65f6\u624d\u53ef\u7528\u3002 \u6bcf\u4e2a epoch \u7ed3\u675f\u65f6\u9a8c\u8bc1\u96c6\u751f\u6210\u5668\u4ea7\u751f\u7684\u6b65\u6570\u3002\u5b83\u901a\u5e38\u5e94\u8be5\u7b49\u4e8e\u4f60\u7684\u6570\u636e\u96c6\u7684\u6837\u672c\u6570\u91cf\u9664\u4ee5\u6279\u91cf\u5927\u5c0f\u3002\u53ef\u9009\u53c2\u6570 Sequence \uff1a\u5982\u679c\u672a\u6307\u5b9a\uff0c\u5c06\u4f7f\u7528 len(generator) \u4f5c\u4e3a\u6b65\u6570\u3002 class_weight : \u53ef\u9009\u7684\u5b57\u5178\uff0c\u7528\u6765\u6620\u5c04\u7c7b\u7d22\u5f15\uff08\u6574\u6570\uff09\u5230\u6743\u91cd\uff08\u6d6e\u70b9\uff09\u503c\uff0c\u7528\u4e8e\u52a0\u6743\u635f\u5931\u51fd\u6570\uff08\u4ec5\u5728\u8bad\u7ec3\u671f\u95f4\uff09\u3002\u8fd9\u53ef\u80fd\u6709\u52a9\u4e8e\u544a\u8bc9\u6a21\u578b \u300c\u66f4\u591a\u5173\u6ce8\u300d\u6765\u81ea\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u7c7b\u7684\u6837\u672c\u3002 max_queue_size : \u6574\u6570\u3002\u751f\u6210\u5668\u961f\u5217\u7684\u6700\u5927\u5c3a\u5bf8\u3002\u5982\u679c\u672a\u6307\u5b9a\uff0c max_queue_size \u5c06\u9ed8\u8ba4\u4e3a 10\u3002 workers : \u6574\u6570\u3002\u4f7f\u7528\u57fa\u4e8e\u8fdb\u7a0b\u7684\u591a\u7ebf\u7a0b\u65f6\u542f\u52a8\u7684\u6700\u5927\u8fdb\u7a0b\u6570\u3002\u5982\u679c\u672a\u6307\u5b9a\uff0c worker \u5c06\u9ed8\u8ba4\u4e3a 1\u3002\u5982\u679c\u4e3a 0\uff0c\u5c06\u5728\u4e3b\u7ebf\u7a0b\u4e0a\u6267\u884c\u751f\u6210\u5668\u3002 use_multiprocessing : \u5982\u679c True\uff0c\u5219\u4f7f\u7528\u57fa\u4e8e\u8fdb\u7a0b\u7684\u591a\u7ebf\u7a0b\u3002\u5982\u679c\u672a\u6307\u5b9a\uff0c use_multiprocessing \u5c06\u9ed8\u8ba4\u4e3a False \u3002\u8bf7\u6ce8\u610f\uff0c\u56e0\u4e3a\u6b64\u5b9e\u73b0\u4f9d\u8d56\u4e8e\u591a\u8fdb\u7a0b\uff0c\u6240\u4ee5\u4e0d\u5e94\u5c06\u4e0d\u53ef\u4f20\u9012\u7684\u53c2\u6570\u4f20\u9012\u7ed9\u751f\u6210\u5668\uff0c\u56e0\u4e3a\u5b83\u4eec\u4e0d\u80fd\u88ab\u8f7b\u6613\u5730\u4f20\u9012\u7ed9\u5b50\u8fdb\u7a0b\u3002 shuffle : \u5e03\u5c14\u503c\u3002\u662f\u5426\u5728\u6bcf\u8f6e\u8fed\u4ee3\u4e4b\u524d\u6253\u4e71 batch \u7684\u987a\u5e8f\u3002\u53ea\u80fd\u4e0e Sequence ( keras.utils.Sequence ) \u5b9e\u4f8b\u540c\u7528\u3002\u5728 steps_per_epoch \u4e0d\u4e3a None \u662f\u65e0\u6548\u679c\u3002 initial_epoch : \u6574\u6570\u3002\u5f00\u59cb\u8bad\u7ec3\u7684\u8f6e\u6b21\uff08\u6709\u52a9\u4e8e\u6062\u590d\u4e4b\u524d\u7684\u8bad\u7ec3\uff09\u3002 \u8fd4\u56de \u4e00\u4e2a History \u5bf9\u8c61\u3002\u5176 History.history \u5c5e\u6027\u662f\u8fde\u7eed epoch \u8bad\u7ec3\u635f\u5931\u548c\u8bc4\u4f30\u503c\uff0c\u4ee5\u53ca\u9a8c\u8bc1\u96c6\u635f\u5931\u548c\u8bc4\u4f30\u503c\u7684\u8bb0\u5f55\uff08\u5982\u679c\u9002\u7528\uff09\u3002 \u5f02\u5e38 ValueError : \u5982\u679c\u751f\u6210\u5668\u751f\u6210\u7684\u6570\u636e\u683c\u5f0f\u4e0d\u6b63\u786e\u3002 \u4f8b\u5b50 def generate_arrays_from_file(path): while True: with open(path) as f: for line in f: # \u4ece\u6587\u4ef6\u4e2d\u7684\u6bcf\u4e00\u884c\u751f\u6210\u8f93\u5165\u6570\u636e\u548c\u6807\u7b7e\u7684 numpy \u6570\u7ec4 x1, x2, y = process_line(line) yield ({'input_1': x1, 'input_2': x2}, {'output': y}) model.fit_generator(generate_arrays_from_file('/my_file.txt'), steps_per_epoch=10000, epochs=10)","title":"fit_generator"},{"location":"1-Models/1.sequential/#evaluate_generator","text":"evaluate_generator(generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0) \u5728\u6570\u636e\u751f\u6210\u5668\u4e0a\u8bc4\u4f30\u6a21\u578b\u3002 \u8fd9\u4e2a\u751f\u6210\u5668\u5e94\u8be5\u8fd4\u56de\u4e0e test_on_batch \u6240\u63a5\u6536\u7684\u540c\u6837\u7684\u6570\u636e\u3002 \u53c2\u6570 generator : \u751f\u6210\u5668\uff0c\u751f\u6210 (inputs, targets) \u6216 (inputs, targets, sample_weights)\uff0c\u6216 Sequence ( keras.utils.Sequence ) \u5bf9\u8c61\u7684\u5b9e\u4f8b\uff0c\u4ee5\u907f\u514d\u5728\u4f7f\u7528\u591a\u8fdb\u7a0b\u65f6\u51fa\u73b0\u91cd\u590d\u6570\u636e\u3002 steps : \u5728\u505c\u6b62\u4e4b\u524d\uff0c\u6765\u81ea generator \u7684\u603b\u6b65\u6570 (\u6837\u672c\u6279\u6b21)\u3002 \u53ef\u9009\u53c2\u6570 Sequence \uff1a\u5982\u679c\u672a\u6307\u5b9a\uff0c\u5c06\u4f7f\u7528 len(generator) \u4f5c\u4e3a\u6b65\u6570\u3002 max_queue_size : \u751f\u6210\u5668\u961f\u5217\u7684\u6700\u5927\u5c3a\u5bf8\u3002 workers : \u6574\u6570\u3002\u4f7f\u7528\u57fa\u4e8e\u8fdb\u7a0b\u7684\u591a\u7ebf\u7a0b\u65f6\u542f\u52a8\u7684\u6700\u5927\u8fdb\u7a0b\u6570\u3002\u5982\u679c\u672a\u6307\u5b9a\uff0c worker \u5c06\u9ed8\u8ba4\u4e3a 1\u3002\u5982\u679c\u4e3a 0\uff0c\u5c06\u5728\u4e3b\u7ebf\u7a0b\u4e0a\u6267\u884c\u751f\u6210\u5668\u3002 use_multiprocessing : \u5982\u679c True\uff0c\u5219\u4f7f\u7528\u57fa\u4e8e\u8fdb\u7a0b\u7684\u591a\u7ebf\u7a0b\u3002 \u8bf7\u6ce8\u610f\uff0c\u56e0\u4e3a\u6b64\u5b9e\u73b0\u4f9d\u8d56\u4e8e\u591a\u8fdb\u7a0b\uff0c\u6240\u4ee5\u4e0d\u5e94\u5c06\u4e0d\u53ef\u4f20\u9012\u7684\u53c2\u6570\u4f20\u9012\u7ed9\u751f\u6210\u5668\uff0c\u56e0\u4e3a\u5b83\u4eec\u4e0d\u80fd\u88ab\u8f7b\u6613\u5730\u4f20\u9012\u7ed9\u5b50\u8fdb\u7a0b\u3002 verbose \uff1a\u65e5\u5fd7\u663e\u793a\u6a21\u5f0f\uff0c0 \u6216 1\u3002 \u8fd4\u56de \u6807\u91cf\u6d4b\u8bd5\u8bef\u5dee\uff08\u5982\u679c\u6a21\u578b\u53ea\u6709\u5355\u4e2a\u8f93\u51fa\u4e14\u6ca1\u6709\u8bc4\u4f30\u6307\u6807\uff09\u6216\u6807\u91cf\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u5177\u6709\u591a\u4e2a\u8f93\u51fa\u548c/\u6216\u6307\u6807\uff09\u3002 \u5c5e\u6027 model.metrics_names \u5c06\u63d0\u4f9b\u6807\u91cf\u8f93\u51fa\u7684\u663e\u793a\u6807\u7b7e\u3002 \u5f02\u5e38 ValueError : \u5982\u679c\u751f\u6210\u5668\u751f\u6210\u7684\u6570\u636e\u683c\u5f0f\u4e0d\u6b63\u786e\u3002","title":"evaluate_generator"},{"location":"1-Models/1.sequential/#predict_generator","text":"predict_generator(generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0) \u4e3a\u6765\u81ea\u6570\u636e\u751f\u6210\u5668\u7684\u8f93\u5165\u6837\u672c\u751f\u6210\u9884\u6d4b\u3002 \u8fd9\u4e2a\u751f\u6210\u5668\u5e94\u8be5\u8fd4\u56de\u4e0e predict_on_batch \u6240\u63a5\u6536\u7684\u540c\u6837\u7684\u6570\u636e\u3002 \u53c2\u6570 generator : \u8fd4\u56de\u6279\u91cf\u8f93\u5165\u6837\u672c\u7684\u751f\u6210\u5668\uff0c\u6216 Sequence ( keras.utils.Sequence ) \u5bf9\u8c61\u7684\u5b9e\u4f8b\uff0c\u4ee5\u907f\u514d\u5728\u4f7f\u7528\u591a\u8fdb\u7a0b\u65f6\u51fa\u73b0\u91cd\u590d\u6570\u636e\u3002 steps : \u5728\u505c\u6b62\u4e4b\u524d\uff0c\u6765\u81ea generator \u7684\u603b\u6b65\u6570 (\u6837\u672c\u6279\u6b21)\u3002 \u53ef\u9009\u53c2\u6570 Sequence \uff1a\u5982\u679c\u672a\u6307\u5b9a\uff0c\u5c06\u4f7f\u7528 len(generator) \u4f5c\u4e3a\u6b65\u6570\u3002 max_queue_size : \u751f\u6210\u5668\u961f\u5217\u7684\u6700\u5927\u5c3a\u5bf8\u3002 workers : \u6574\u6570\u3002\u4f7f\u7528\u57fa\u4e8e\u8fdb\u7a0b\u7684\u591a\u7ebf\u7a0b\u65f6\u542f\u52a8\u7684\u6700\u5927\u8fdb\u7a0b\u6570\u3002\u5982\u679c\u672a\u6307\u5b9a\uff0c worker \u5c06\u9ed8\u8ba4\u4e3a 1\u3002\u5982\u679c\u4e3a 0\uff0c\u5c06\u5728\u4e3b\u7ebf\u7a0b\u4e0a\u6267\u884c\u751f\u6210\u5668\u3002 use_multiprocessing : \u5982\u679c True\uff0c\u5219\u4f7f\u7528\u57fa\u4e8e\u8fdb\u7a0b\u7684\u591a\u7ebf\u7a0b\u3002 \u8bf7\u6ce8\u610f\uff0c\u56e0\u4e3a\u6b64\u5b9e\u73b0\u4f9d\u8d56\u4e8e\u591a\u8fdb\u7a0b\uff0c\u6240\u4ee5\u4e0d\u5e94\u5c06\u4e0d\u53ef\u4f20\u9012\u7684\u53c2\u6570\u4f20\u9012\u7ed9\u751f\u6210\u5668\uff0c\u56e0\u4e3a\u5b83\u4eec\u4e0d\u80fd\u88ab\u8f7b\u6613\u5730\u4f20\u9012\u7ed9\u5b50\u8fdb\u7a0b\u3002 verbose : \u65e5\u5fd7\u663e\u793a\u6a21\u5f0f\uff0c 0 \u6216 1\u3002 \u8fd4\u56de \u9884\u6d4b\u503c\u7684 Numpy \u6570\u7ec4\u3002 \u5f02\u5e38 ValueError : \u5982\u679c\u751f\u6210\u5668\u751f\u6210\u7684\u6570\u636e\u683c\u5f0f\u4e0d\u6b63\u786e\u3002","title":"predict_generator"},{"location":"1-Models/1.sequential/#get_layer","text":"get_layer(name=None, index=None) \u6839\u636e\u540d\u79f0\uff08\u552f\u4e00\uff09\u6216\u7d22\u5f15\u503c\u67e5\u627e\u7f51\u7edc\u5c42\u3002 \u5982\u679c\u540c\u65f6\u63d0\u4f9b\u4e86 name \u548c index \uff0c\u5219 index \u5c06\u4f18\u5148\u3002 \u6839\u636e\u7f51\u7edc\u5c42\u7684\u540d\u79f0\uff08\u552f\u4e00\uff09\u6216\u5176\u7d22\u5f15\u8fd4\u56de\u8be5\u5c42\u3002\u7d22\u5f15\u662f\u57fa\u4e8e\u6c34\u5e73\u56fe\u904d\u5386\u7684\u987a\u5e8f\uff08\u81ea\u4e0b\u800c\u4e0a\uff09\u3002 \u53c2\u6570 name : \u5b57\u7b26\u4e32\uff0c\u5c42\u7684\u540d\u5b57\u3002 index : \u6574\u6570\uff0c\u5c42\u7684\u7d22\u5f15\u3002 \u8fd4\u56de \u4e00\u4e2a\u5c42\u5b9e\u4f8b\u3002 \u5f02\u5e38 ValueError : \u5982\u679c\u5c42\u7684\u540d\u79f0\u6216\u7d22\u5f15\u4e0d\u6b63\u786e\u3002","title":"get_layer"},{"location":"1-Models/2.model/","text":"Model \u7c7b\uff08\u51fd\u6570\u5f0f API\uff09 \u5728\u51fd\u6570\u5f0f API \u4e2d\uff0c\u7ed9\u5b9a\u4e00\u4e9b\u8f93\u5165\u5f20\u91cf\u548c\u8f93\u51fa\u5f20\u91cf\uff0c\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u5b9e\u4f8b\u5316\u4e00\u4e2a Model \uff1a from keras.models import Model from keras.layers import Input, Dense a = Input(shape=(32,)) b = Dense(32)(a) model = Model(inputs=a, outputs=b) \u8fd9\u4e2a\u6a21\u578b\u5c06\u5305\u542b\u4ece a \u5230 b \u7684\u8ba1\u7b97\u7684\u6240\u6709\u7f51\u7edc\u5c42\u3002 \u5728\u591a\u8f93\u5165\u6216\u591a\u8f93\u51fa\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u4f60\u4e5f\u53ef\u4ee5\u4f7f\u7528\u5217\u8868\uff1a model = Model(inputs=[a1, a2], outputs=[b1, b3, b3]) \u6709\u5173 Model \u7684\u8be6\u7ec6\u4ecb\u7ecd\uff0c\u8bf7\u9605\u8bfb Keras \u51fd\u6570\u5f0f API \u6307\u5f15 \u3002 Model \u7c7b\u6a21\u578b\u65b9\u6cd5 compile compile(optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None) \u7528\u4e8e\u914d\u7f6e\u8bad\u7ec3\u6a21\u578b\u3002 \u53c2\u6570 optimizer : \u5b57\u7b26\u4e32\uff08\u4f18\u5316\u5668\u540d\uff09\u6216\u8005\u4f18\u5316\u5668\u5b9e\u4f8b\u3002 \u8be6\u89c1 optimizers \u3002 loss : \u5b57\u7b26\u4e32\uff08\u76ee\u6807\u51fd\u6570\u540d\uff09\u6216\u76ee\u6807\u51fd\u6570\u3002 \u8be6\u89c1 losses \u3002 \u5982\u679c\u6a21\u578b\u5177\u6709\u591a\u4e2a\u8f93\u51fa\uff0c\u5219\u53ef\u4ee5\u901a\u8fc7\u4f20\u9012\u635f\u5931\u51fd\u6570\u7684\u5b57\u5178\u6216\u5217\u8868\uff0c\u5728\u6bcf\u4e2a\u8f93\u51fa\u4e0a\u4f7f\u7528\u4e0d\u540c\u7684\u635f\u5931\u3002 \u6a21\u578b\u5c06\u6700\u5c0f\u5316\u7684\u635f\u5931\u503c\u5c06\u662f\u6240\u6709\u5355\u4e2a\u635f\u5931\u7684\u603b\u548c\u3002 metrics : \u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u671f\u95f4\u7684\u6a21\u578b\u8bc4\u4f30\u6807\u51c6\u3002 \u901a\u5e38\u4f60\u4f1a\u4f7f\u7528 metrics = ['accuracy'] \u3002 \u8981\u4e3a\u591a\u8f93\u51fa\u6a21\u578b\u7684\u4e0d\u540c\u8f93\u51fa\u6307\u5b9a\u4e0d\u540c\u7684\u8bc4\u4f30\u6807\u51c6\uff0c \u8fd8\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5b57\u5178\uff0c\u5982 metrics = {'output_a'\uff1a'accuracy'} \u3002 loss_weights : \u53ef\u9009\u7684\u6307\u5b9a\u6807\u91cf\u7cfb\u6570\uff08Python \u6d6e\u70b9\u6570\uff09\u7684\u5217\u8868\u6216\u5b57\u5178\uff0c \u7528\u4ee5\u8861\u91cf\u635f\u5931\u51fd\u6570\u5bf9\u4e0d\u540c\u7684\u6a21\u578b\u8f93\u51fa\u7684\u8d21\u732e\u3002 \u6a21\u578b\u5c06\u6700\u5c0f\u5316\u7684\u8bef\u5dee\u503c\u662f\u7531 loss_weights \u7cfb\u6570\u52a0\u6743\u7684 \u52a0\u6743\u603b\u548c \u8bef\u5dee\u3002 \u5982\u679c\u662f\u5217\u8868\uff0c\u90a3\u4e48\u5b83\u5e94\u8be5\u662f\u4e0e\u6a21\u578b\u8f93\u51fa\u76f8\u5bf9\u5e94\u7684 1:1 \u6620\u5c04\u3002 \u5982\u679c\u662f\u5f20\u91cf\uff0c\u90a3\u4e48\u5e94\u8be5\u628a\u8f93\u51fa\u7684\u540d\u79f0\uff08\u5b57\u7b26\u4e32\uff09\u6620\u5230\u6807\u91cf\u7cfb\u6570\u3002 sample_weight_mode : \u5982\u679c\u4f60\u9700\u8981\u6267\u884c\u6309\u65f6\u95f4\u6b65\u91c7\u6837\u6743\u91cd\uff082D \u6743\u91cd\uff09\uff0c\u8bf7\u5c06\u5176\u8bbe\u7f6e\u4e3a temporal \u3002 \u9ed8\u8ba4\u4e3a None \uff0c\u4e3a\u91c7\u6837\u6743\u91cd\uff081D\uff09\u3002 \u5982\u679c\u6a21\u578b\u6709\u591a\u4e2a\u8f93\u51fa\uff0c\u5219\u53ef\u4ee5\u901a\u8fc7\u4f20\u9012 mode \u7684\u5b57\u5178\u6216\u5217\u8868\uff0c\u4ee5\u5728\u6bcf\u4e2a\u8f93\u51fa\u4e0a\u4f7f\u7528\u4e0d\u540c\u7684 sample_weight_mode \u3002 weighted_metrics : \u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u671f\u95f4\uff0c\u7531 sample_weight \u6216 class_weight \u8bc4\u4f30\u548c\u52a0\u6743\u7684\u5ea6\u91cf\u6807\u51c6\u5217\u8868\u3002 target_tensors : \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cKeras \u5c06\u4e3a\u6a21\u578b\u7684\u76ee\u6807\u521b\u5efa\u4e00\u4e2a\u5360\u4f4d\u7b26\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5c06\u4f7f\u7528\u76ee\u6807\u6570\u636e\u3002 \u76f8\u53cd\uff0c\u5982\u679c\u4f60\u60f3\u4f7f\u7528\u81ea\u5df1\u7684\u76ee\u6807\u5f20\u91cf\uff08\u53cd\u8fc7\u6765\u8bf4\uff0cKeras \u5728\u8bad\u7ec3\u671f\u95f4\u4e0d\u4f1a\u8f7d\u5165\u8fd9\u4e9b\u76ee\u6807\u5f20\u91cf\u7684\u5916\u90e8 Numpy \u6570\u636e\uff09\uff0c \u60a8\u53ef\u4ee5\u901a\u8fc7 target_tensors \u53c2\u6570\u6307\u5b9a\u5b83\u4eec\u3002 \u5b83\u53ef\u4ee5\u662f\u5355\u4e2a\u5f20\u91cf\uff08\u5355\u8f93\u51fa\u6a21\u578b\uff09\uff0c\u5f20\u91cf\u5217\u8868\uff0c\u6216\u4e00\u4e2a\u6620\u5c04\u8f93\u51fa\u540d\u79f0\u5230\u76ee\u6807\u5f20\u91cf\u7684\u5b57\u5178\u3002 **kwargs : \u5f53\u4f7f\u7528 Theano/CNTK \u540e\u7aef\u65f6\uff0c\u8fd9\u4e9b\u53c2\u6570\u88ab\u4f20\u5165 K.function \u3002 \u5f53\u4f7f\u7528 TensorFlow \u540e\u7aef\u65f6\uff0c\u8fd9\u4e9b\u53c2\u6570\u88ab\u4f20\u9012\u5230 tf.Session.run \u3002 \u5f02\u5e38 ValueError : \u5982\u679c optimizer , loss , metrics \u6216 sample_weight_mode \u8fd9\u4e9b\u53c2\u6570\u4e0d\u5408\u6cd5\u3002 fit fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None) \u4ee5\u7ed9\u5b9a\u6570\u91cf\u7684\u8f6e\u6b21\uff08\u6570\u636e\u96c6\u4e0a\u7684\u8fed\u4ee3\uff09\u8bad\u7ec3\u6a21\u578b\u3002 \u53c2\u6570 x : \u8bad\u7ec3\u6570\u636e\u7684 Numpy \u6570\u7ec4\uff08\u5982\u679c\u6a21\u578b\u53ea\u6709\u4e00\u4e2a\u8f93\u5165\uff09\uff0c \u6216\u8005\u662f Numpy \u6570\u7ec4\u7684\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u6709\u591a\u4e2a\u8f93\u5165\uff09\u3002 \u5982\u679c\u6a21\u578b\u4e2d\u7684\u8f93\u5165\u5c42\u88ab\u547d\u540d\uff0c\u4f60\u4e5f\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5b57\u5178\uff0c\u5c06\u8f93\u5165\u5c42\u540d\u79f0\u6620\u5c04\u5230 Numpy \u6570\u7ec4\u3002 \u5982\u679c\u4ece\u672c\u5730\u6846\u67b6\u5f20\u91cf\u9988\u9001\uff08\u4f8b\u5982 TensorFlow \u6570\u636e\u5f20\u91cf\uff09\u6570\u636e\uff0c x \u53ef\u4ee5\u662f None \uff08\u9ed8\u8ba4\uff09\u3002 y : \u76ee\u6807\uff08\u6807\u7b7e\uff09\u6570\u636e\u7684 Numpy \u6570\u7ec4\uff08\u5982\u679c\u6a21\u578b\u53ea\u6709\u4e00\u4e2a\u8f93\u51fa\uff09\uff0c \u6216\u8005\u662f Numpy \u6570\u7ec4\u7684\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u6709\u591a\u4e2a\u8f93\u51fa\uff09\u3002 \u5982\u679c\u6a21\u578b\u4e2d\u7684\u8f93\u51fa\u5c42\u88ab\u547d\u540d\uff0c\u4f60\u4e5f\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5b57\u5178\uff0c\u5c06\u8f93\u51fa\u5c42\u540d\u79f0\u6620\u5c04\u5230 Numpy \u6570\u7ec4\u3002 \u5982\u679c\u4ece\u672c\u5730\u6846\u67b6\u5f20\u91cf\u9988\u9001\uff08\u4f8b\u5982 TensorFlow \u6570\u636e\u5f20\u91cf\uff09\u6570\u636e\uff0cy \u53ef\u4ee5\u662f None \uff08\u9ed8\u8ba4\uff09\u3002 batch_size : \u6574\u6570\u6216 None \u3002\u6bcf\u6b21\u68af\u5ea6\u66f4\u65b0\u7684\u6837\u672c\u6570\u3002\u5982\u679c\u672a\u6307\u5b9a\uff0c\u9ed8\u8ba4\u4e3a 32\u3002 epochs : \u6574\u6570\u3002\u8bad\u7ec3\u6a21\u578b\u8fed\u4ee3\u8f6e\u6b21\u3002\u4e00\u4e2a\u8f6e\u6b21\u662f\u5728\u6574\u4e2a x \u548c y \u4e0a\u7684\u4e00\u8f6e\u8fed\u4ee3\u3002 \u8bf7\u6ce8\u610f\uff0c\u4e0e initial_epoch \u4e00\u8d77\uff0c epochs \u88ab\u7406\u89e3\u4e3a \u300c\u6700\u7ec8\u8f6e\u6b21\u300d\u3002\u6a21\u578b\u5e76\u4e0d\u662f\u8bad\u7ec3\u4e86 epochs \u8f6e\uff0c\u800c\u662f\u5230\u7b2c epochs \u8f6e\u505c\u6b62\u8bad\u7ec3\u3002 verbose : 0, 1 \u6216 2\u3002\u65e5\u5fd7\u663e\u793a\u6a21\u5f0f\u3002 0 = \u5b89\u9759\u6a21\u5f0f, 1 = \u8fdb\u5ea6\u6761, 2 = \u6bcf\u8f6e\u4e00\u884c\u3002 callbacks : \u4e00\u7cfb\u5217\u7684 keras.callbacks.Callback \u5b9e\u4f8b\u3002\u4e00\u7cfb\u5217\u53ef\u4ee5\u5728\u8bad\u7ec3\u65f6\u4f7f\u7528\u7684\u56de\u8c03\u51fd\u6570\u3002 \u8be6\u89c1 callbacks \u3002 validation_split : 0 \u548c 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002\u7528\u4f5c\u9a8c\u8bc1\u96c6\u7684\u8bad\u7ec3\u6570\u636e\u7684\u6bd4\u4f8b\u3002 \u6a21\u578b\u5c06\u5206\u51fa\u4e00\u90e8\u5206\u4e0d\u4f1a\u88ab\u8bad\u7ec3\u7684\u9a8c\u8bc1\u6570\u636e\uff0c\u5e76\u5c06\u5728\u6bcf\u4e00\u8f6e\u7ed3\u675f\u65f6\u8bc4\u4f30\u8fd9\u4e9b\u9a8c\u8bc1\u6570\u636e\u7684\u8bef\u5dee\u548c\u4efb\u4f55\u5176\u4ed6\u6a21\u578b\u6307\u6807\u3002 \u9a8c\u8bc1\u6570\u636e\u662f\u6df7\u6d17\u4e4b\u524d x \u548c y \u6570\u636e\u7684\u6700\u540e\u4e00\u90e8\u5206\u6837\u672c\u4e2d\u3002 validation_data : \u5143\u7ec4 (x_val\uff0cy_val) \u6216\u5143\u7ec4 (x_val\uff0cy_val\uff0cval_sample_weights) \uff0c \u7528\u6765\u8bc4\u4f30\u635f\u5931\uff0c\u4ee5\u53ca\u5728\u6bcf\u8f6e\u7ed3\u675f\u65f6\u7684\u4efb\u4f55\u6a21\u578b\u5ea6\u91cf\u6307\u6807\u3002 \u6a21\u578b\u5c06\u4e0d\u4f1a\u5728\u8fd9\u4e2a\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u8fd9\u4e2a\u53c2\u6570\u4f1a\u8986\u76d6 validation_split \u3002 shuffle : \u5e03\u5c14\u503c\uff08\u662f\u5426\u5728\u6bcf\u8f6e\u8fed\u4ee3\u4e4b\u524d\u6df7\u6d17\u6570\u636e\uff09\u6216\u8005 \u5b57\u7b26\u4e32 ( batch )\u3002 batch \u662f\u5904\u7406 HDF5 \u6570\u636e\u9650\u5236\u7684\u7279\u6b8a\u9009\u9879\uff0c\u5b83\u5bf9\u4e00\u4e2a batch \u5185\u90e8\u7684\u6570\u636e\u8fdb\u884c\u6df7\u6d17\u3002 \u5f53 steps_per_epoch \u975e None \u65f6\uff0c\u8fd9\u4e2a\u53c2\u6570\u65e0\u6548\u3002 class_weight : \u53ef\u9009\u7684\u5b57\u5178\uff0c\u7528\u6765\u6620\u5c04\u7c7b\u7d22\u5f15\uff08\u6574\u6570\uff09\u5230\u6743\u91cd\uff08\u6d6e\u70b9\uff09\u503c\uff0c\u7528\u4e8e\u52a0\u6743\u635f\u5931\u51fd\u6570\uff08\u4ec5\u5728\u8bad\u7ec3\u671f\u95f4\uff09\u3002 \u8fd9\u53ef\u80fd\u6709\u52a9\u4e8e\u544a\u8bc9\u6a21\u578b \u300c\u66f4\u591a\u5173\u6ce8\u300d\u6765\u81ea\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u7c7b\u7684\u6837\u672c\u3002 sample_weight : \u8bad\u7ec3\u6837\u672c\u7684\u53ef\u9009 Numpy \u6743\u91cd\u6570\u7ec4\uff0c\u7528\u4e8e\u5bf9\u635f\u5931\u51fd\u6570\u8fdb\u884c\u52a0\u6743\uff08\u4ec5\u5728\u8bad\u7ec3\u671f\u95f4\uff09\u3002 \u60a8\u53ef\u4ee5\u4f20\u9012\u4e0e\u8f93\u5165\u6837\u672c\u957f\u5ea6\u76f8\u540c\u7684\u5e73\u5766\uff081D\uff09Numpy \u6570\u7ec4\uff08\u6743\u91cd\u548c\u6837\u672c\u4e4b\u95f4\u7684 1:1 \u6620\u5c04\uff09\uff0c \u6216\u8005\u5728\u65f6\u5e8f\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u4f20\u9012\u5c3a\u5bf8\u4e3a (samples, sequence_length) \u7684 2D \u6570\u7ec4\uff0c\u4ee5\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u65f6\u95f4\u6b65\u65bd\u52a0\u4e0d\u540c\u7684\u6743\u91cd\u3002 \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4f60\u5e94\u8be5\u786e\u4fdd\u5728 compile() \u4e2d\u6307\u5b9a sample_weight_mode=\"temporal\" \u3002 initial_epoch : \u6574\u6570\u3002\u5f00\u59cb\u8bad\u7ec3\u7684\u8f6e\u6b21\uff08\u6709\u52a9\u4e8e\u6062\u590d\u4e4b\u524d\u7684\u8bad\u7ec3\uff09\u3002 steps_per_epoch : \u6574\u6570\u6216 None \u3002 \u5728\u58f0\u660e\u4e00\u4e2a\u8f6e\u6b21\u5b8c\u6210\u5e76\u5f00\u59cb\u4e0b\u4e00\u4e2a\u8f6e\u6b21\u4e4b\u524d\u7684\u603b\u6b65\u6570\uff08\u6837\u54c1\u6279\u6b21\uff09\u3002 \u4f7f\u7528 TensorFlow \u6570\u636e\u5f20\u91cf\u7b49\u8f93\u5165\u5f20\u91cf\u8fdb\u884c\u8bad\u7ec3\u65f6\uff0c\u9ed8\u8ba4\u503c None \u7b49\u4e8e\u6570\u636e\u96c6\u4e2d\u6837\u672c\u7684\u6570\u91cf\u9664\u4ee5 batch \u7684\u5927\u5c0f\uff0c\u5982\u679c\u65e0\u6cd5\u786e\u5b9a\uff0c\u5219\u4e3a 1\u3002 validation_steps : \u53ea\u6709\u5728\u6307\u5b9a\u4e86 steps_per_epoch \u65f6\u624d\u6709\u7528\u3002\u505c\u6b62\u524d\u8981\u9a8c\u8bc1\u7684\u603b\u6b65\u6570\uff08\u6279\u6b21\u6837\u672c\uff09\u3002 \u8fd4\u56de \u4e00\u4e2a History \u5bf9\u8c61\u3002\u5176 History.history \u5c5e\u6027\u662f\u8fde\u7eed epoch \u8bad\u7ec3\u635f\u5931\u548c\u8bc4\u4f30\u503c\uff0c\u4ee5\u53ca\u9a8c\u8bc1\u96c6\u635f\u5931\u548c\u8bc4\u4f30\u503c\u7684\u8bb0\u5f55\uff08\u5982\u679c\u9002\u7528\uff09\u3002 \u5f02\u5e38 RuntimeError : \u5982\u679c\u6a21\u578b\u4ece\u672a\u7f16\u8bd1\u3002 ValueError : \u5728\u63d0\u4f9b\u7684\u8f93\u5165\u6570\u636e\u4e0e\u6a21\u578b\u671f\u671b\u7684\u4e0d\u5339\u914d\u7684\u60c5\u51b5\u4e0b\u3002 evaluate evaluate(x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None) \u5728\u6d4b\u8bd5\u6a21\u5f0f\u4e0b\u8fd4\u56de\u6a21\u578b\u7684\u8bef\u5dee\u503c\u548c\u8bc4\u4f30\u6807\u51c6\u503c\u3002 \u8ba1\u7b97\u662f\u5206\u6279\u8fdb\u884c\u7684\u3002 \u53c2\u6570 x : \u6d4b\u8bd5\u6570\u636e\u7684 Numpy \u6570\u7ec4\uff08\u5982\u679c\u6a21\u578b\u53ea\u6709\u4e00\u4e2a\u8f93\u5165\uff09\uff0c \u6216\u8005\u662f Numpy \u6570\u7ec4\u7684\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u6709\u591a\u4e2a\u8f93\u5165\uff09\u3002 \u5982\u679c\u6a21\u578b\u4e2d\u7684\u8f93\u5165\u5c42\u88ab\u547d\u540d\uff0c\u4f60\u4e5f\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5b57\u5178\uff0c\u5c06\u8f93\u5165\u5c42\u540d\u79f0\u6620\u5c04\u5230 Numpy \u6570\u7ec4\u3002 \u5982\u679c\u4ece\u672c\u5730\u6846\u67b6\u5f20\u91cf\u9988\u9001\uff08\u4f8b\u5982 TensorFlow \u6570\u636e\u5f20\u91cf\uff09\u6570\u636e\uff0cx \u53ef\u4ee5\u662f None \uff08\u9ed8\u8ba4\uff09\u3002 y : \u76ee\u6807\uff08\u6807\u7b7e\uff09\u6570\u636e\u7684 Numpy \u6570\u7ec4\uff0c\u6216 Numpy \u6570\u7ec4\u7684\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u5177\u6709\u591a\u4e2a\u8f93\u51fa\uff09\u3002 \u5982\u679c\u6a21\u578b\u4e2d\u7684\u8f93\u51fa\u5c42\u88ab\u547d\u540d\uff0c\u4f60\u4e5f\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5b57\u5178\uff0c\u5c06\u8f93\u51fa\u5c42\u540d\u79f0\u6620\u5c04\u5230 Numpy \u6570\u7ec4\u3002 \u5982\u679c\u4ece\u672c\u5730\u6846\u67b6\u5f20\u91cf\u9988\u9001\uff08\u4f8b\u5982 TensorFlow \u6570\u636e\u5f20\u91cf\uff09\u6570\u636e\uff0cy \u53ef\u4ee5\u662f None \uff08\u9ed8\u8ba4\uff09\u3002 batch_size : \u6574\u6570\u6216 None \u3002\u6bcf\u6b21\u8bc4\u4f30\u7684\u6837\u672c\u6570\u3002\u5982\u679c\u672a\u6307\u5b9a\uff0c\u9ed8\u8ba4\u4e3a 32\u3002 verbose : 0 \u6216 1\u3002\u65e5\u5fd7\u663e\u793a\u6a21\u5f0f\u3002 0 = \u5b89\u9759\u6a21\u5f0f\uff0c1 = \u8fdb\u5ea6\u6761\u3002 sample_weight : \u6d4b\u8bd5\u6837\u672c\u7684\u53ef\u9009 Numpy \u6743\u91cd\u6570\u7ec4\uff0c\u7528\u4e8e\u5bf9\u635f\u5931\u51fd\u6570\u8fdb\u884c\u52a0\u6743\u3002 \u60a8\u53ef\u4ee5\u4f20\u9012\u4e0e\u8f93\u5165\u6837\u672c\u957f\u5ea6\u76f8\u540c\u7684\u6241\u5e73\uff081D\uff09Numpy \u6570\u7ec4\uff08\u6743\u91cd\u548c\u6837\u672c\u4e4b\u95f4\u7684 1:1 \u6620\u5c04\uff09\uff0c \u6216\u8005\u5728\u65f6\u5e8f\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u4f20\u9012\u5c3a\u5bf8\u4e3a (samples, sequence_length) \u7684 2D \u6570\u7ec4\uff0c\u4ee5\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u65f6\u95f4\u6b65\u65bd\u52a0\u4e0d\u540c\u7684\u6743\u91cd\u3002 \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4f60\u5e94\u8be5\u786e\u4fdd\u5728 compile() \u4e2d\u6307\u5b9a sample_weight_mode=\"temporal\" \u3002 steps : \u6574\u6570\u6216 None \u3002 \u58f0\u660e\u8bc4\u4f30\u7ed3\u675f\u4e4b\u524d\u7684\u603b\u6b65\u6570\uff08\u6279\u6b21\u6837\u672c\uff09\u3002\u9ed8\u8ba4\u503c None \u3002 \u8fd4\u56de \u6807\u91cf\u6d4b\u8bd5\u8bef\u5dee\uff08\u5982\u679c\u6a21\u578b\u53ea\u6709\u4e00\u4e2a\u8f93\u51fa\u4e14\u6ca1\u6709\u8bc4\u4f30\u6807\u51c6\uff09 \u6216\u6807\u91cf\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u5177\u6709\u591a\u4e2a\u8f93\u51fa \u548c/\u6216 \u8bc4\u4f30\u6307\u6807\uff09\u3002 \u5c5e\u6027 model.metrics_names \u5c06\u63d0\u4f9b\u6807\u91cf\u8f93\u51fa\u7684\u663e\u793a\u6807\u7b7e\u3002 predict predict(x, batch_size=None, verbose=0, steps=None) \u4e3a\u8f93\u5165\u6837\u672c\u751f\u6210\u8f93\u51fa\u9884\u6d4b\u3002 \u8ba1\u7b97\u662f\u5206\u6279\u8fdb\u884c\u7684 \u53c2\u6570 x : \u8f93\u5165\u6570\u636e\uff0cNumpy \u6570\u7ec4 \uff08\u6216\u8005 Numpy \u6570\u7ec4\u7684\u5217\u8868\uff0c\u5982\u679c\u6a21\u578b\u6709\u591a\u4e2a\u8f93\u51fa\uff09\u3002 batch_size : \u6574\u6570\u3002\u5982\u672a\u6307\u5b9a\uff0c\u9ed8\u8ba4\u4e3a 32\u3002 verbose : \u65e5\u5fd7\u663e\u793a\u6a21\u5f0f\uff0c0 \u6216 1\u3002 steps : \u58f0\u660e\u9884\u6d4b\u7ed3\u675f\u4e4b\u524d\u7684\u603b\u6b65\u6570\uff08\u6279\u6b21\u6837\u672c\uff09\u3002\u9ed8\u8ba4\u503c None \u3002 \u8fd4\u56de \u9884\u6d4b\u7684 Numpy \u6570\u7ec4\uff08\u6216\u6570\u7ec4\u5217\u8868\uff09\u3002 \u5f02\u5e38 ValueError : \u5728\u63d0\u4f9b\u7684\u8f93\u5165\u6570\u636e\u4e0e\u6a21\u578b\u671f\u671b\u7684\u4e0d\u5339\u914d\u7684\u60c5\u51b5\u4e0b\uff0c \u6216\u8005\u5728\u6709\u72b6\u6001\u7684\u6a21\u578b\u63a5\u6536\u5230\u7684\u6837\u672c\u4e0d\u662f batch size \u7684\u500d\u6570\u7684\u60c5\u51b5\u4e0b\u3002 train_on_batch train_on_batch(x, y, sample_weight=None, class_weight=None) \u8fd0\u884c\u4e00\u6279\u6837\u54c1\u7684\u5355\u6b21\u68af\u5ea6\u66f4\u65b0\u3002 __\u53c2\u6570_ x : \u8bad\u7ec3\u6570\u636e\u7684 Numpy \u6570\u7ec4\uff08\u5982\u679c\u6a21\u578b\u53ea\u6709\u4e00\u4e2a\u8f93\u5165\uff09\uff0c \u6216\u8005\u662f Numpy \u6570\u7ec4\u7684\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u6709\u591a\u4e2a\u8f93\u5165\uff09\u3002 \u5982\u679c\u6a21\u578b\u4e2d\u7684\u8f93\u5165\u5c42\u88ab\u547d\u540d\uff0c\u4f60\u4e5f\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5b57\u5178\uff0c\u5c06\u8f93\u5165\u5c42\u540d\u79f0\u6620\u5c04\u5230 Numpy \u6570\u7ec4\u3002 y : \u76ee\u6807\uff08\u6807\u7b7e\uff09\u6570\u636e\u7684 Numpy \u6570\u7ec4\uff0c\u6216 Numpy \u6570\u7ec4\u7684\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u5177\u6709\u591a\u4e2a\u8f93\u51fa\uff09\u3002 \u5982\u679c\u6a21\u578b\u4e2d\u7684\u8f93\u51fa\u5c42\u88ab\u547d\u540d\uff0c\u4f60\u4e5f\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5b57\u5178\uff0c\u5c06\u8f93\u51fa\u5c42\u540d\u79f0\u6620\u5c04\u5230 Numpy \u6570\u7ec4\u3002 sample_weight : \u53ef\u9009\u6570\u7ec4\uff0c\u4e0e x \u957f\u5ea6\u76f8\u540c\uff0c\u5305\u542b\u5e94\u7528\u5230\u6a21\u578b\u635f\u5931\u51fd\u6570\u7684\u6bcf\u4e2a\u6837\u672c\u7684\u6743\u91cd\u3002 \u5982\u679c\u662f\u65f6\u57df\u6570\u636e\uff0c\u4f60\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5c3a\u5bf8\u4e3a (samples, sequence_length) \u7684 2D \u6570\u7ec4\uff0c \u4e3a\u6bcf\u4e00\u4e2a\u6837\u672c\u7684\u6bcf\u4e00\u4e2a\u65f6\u95f4\u6b65\u5e94\u7528\u4e0d\u540c\u7684\u6743\u91cd\u3002 \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4f60\u5e94\u8be5\u5728 compile() \u4e2d\u6307\u5b9a sample_weight_mode=\"temporal\" \u3002 class_weight : \u53ef\u9009\u7684\u5b57\u5178\uff0c\u7528\u6765\u6620\u5c04\u7c7b\u7d22\u5f15\uff08\u6574\u6570\uff09\u5230\u6743\u91cd\uff08\u6d6e\u70b9\uff09\u503c\uff0c\u4ee5\u5728\u8bad\u7ec3\u65f6\u5bf9\u6a21\u578b\u7684\u635f\u5931\u51fd\u6570\u52a0\u6743\u3002 \u8fd9\u53ef\u80fd\u6709\u52a9\u4e8e\u544a\u8bc9\u6a21\u578b \u300c\u66f4\u591a\u5173\u6ce8\u300d\u6765\u81ea\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u7c7b\u7684\u6837\u672c\u3002 \u8fd4\u56de \u6807\u91cf\u8bad\u7ec3\u8bef\u5dee\uff08\u5982\u679c\u6a21\u578b\u53ea\u6709\u4e00\u4e2a\u8f93\u5165\u4e14\u6ca1\u6709\u8bc4\u4f30\u6807\u51c6\uff09\uff0c \u6216\u8005\u6807\u91cf\u7684\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u6709\u591a\u4e2a\u8f93\u51fa \u548c/\u6216 \u8bc4\u4f30\u6807\u51c6\uff09\u3002 \u5c5e\u6027 model.metrics_names \u5c06\u63d0\u4f9b\u6807\u91cf\u8f93\u51fa\u7684\u663e\u793a\u6807\u7b7e\u3002 test_on_batch test_on_batch(x, y, sample_weight=None) \u5728\u4e00\u6279\u6837\u672c\u4e0a\u6d4b\u8bd5\u6a21\u578b\u3002 \u53c2\u6570 x : \u6d4b\u8bd5\u6570\u636e\u7684 Numpy \u6570\u7ec4\uff08\u5982\u679c\u6a21\u578b\u53ea\u6709\u4e00\u4e2a\u8f93\u5165\uff09\uff0c \u6216\u8005\u662f Numpy \u6570\u7ec4\u7684\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u6709\u591a\u4e2a\u8f93\u5165\uff09\u3002 \u5982\u679c\u6a21\u578b\u4e2d\u7684\u8f93\u5165\u5c42\u88ab\u547d\u540d\uff0c\u4f60\u4e5f\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5b57\u5178\uff0c\u5c06\u8f93\u5165\u5c42\u540d\u79f0\u6620\u5c04\u5230 Numpy \u6570\u7ec4\u3002 y : \u76ee\u6807\uff08\u6807\u7b7e\uff09\u6570\u636e\u7684 Numpy \u6570\u7ec4\uff0c\u6216 Numpy \u6570\u7ec4\u7684\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u5177\u6709\u591a\u4e2a\u8f93\u51fa\uff09\u3002 \u5982\u679c\u6a21\u578b\u4e2d\u7684\u8f93\u51fa\u5c42\u88ab\u547d\u540d\uff0c\u4f60\u4e5f\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5b57\u5178\uff0c\u5c06\u8f93\u51fa\u5c42\u540d\u79f0\u6620\u5c04\u5230 Numpy \u6570\u7ec4\u3002 sample_weight : \u53ef\u9009\u6570\u7ec4\uff0c\u4e0e x \u957f\u5ea6\u76f8\u540c\uff0c\u5305\u542b\u5e94\u7528\u5230\u6a21\u578b\u635f\u5931\u51fd\u6570\u7684\u6bcf\u4e2a\u6837\u672c\u7684\u6743\u91cd\u3002 \u5982\u679c\u662f\u65f6\u57df\u6570\u636e\uff0c\u4f60\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5c3a\u5bf8\u4e3a (samples, sequence_length) \u7684 2D \u6570\u7ec4\uff0c \u4e3a\u6bcf\u4e00\u4e2a\u6837\u672c\u7684\u6bcf\u4e00\u4e2a\u65f6\u95f4\u6b65\u5e94\u7528\u4e0d\u540c\u7684\u6743\u91cd\u3002 \u8fd4\u56de \u6807\u91cf\u6d4b\u8bd5\u8bef\u5dee\uff08\u5982\u679c\u6a21\u578b\u53ea\u6709\u4e00\u4e2a\u8f93\u5165\u4e14\u6ca1\u6709\u8bc4\u4f30\u6807\u51c6\uff09\uff0c \u6216\u8005\u6807\u91cf\u7684\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u6709\u591a\u4e2a\u8f93\u51fa \u548c/\u6216 \u8bc4\u4f30\u6807\u51c6\uff09\u3002 \u5c5e\u6027 model.metrics_names \u5c06\u63d0\u4f9b\u6807\u91cf\u8f93\u51fa\u7684\u663e\u793a\u6807\u7b7e\u3002 predict_on_batch predict_on_batch(x) \u8fd4\u56de\u4e00\u6279\u6837\u672c\u7684\u6a21\u578b\u9884\u6d4b\u503c\u3002 \u53c2\u6570 x : \u8f93\u5165\u6570\u636e\uff0cNumpy \u6570\u7ec4\u3002 \u8fd4\u56de \u9884\u6d4b\u503c\u7684 Numpy \u6570\u7ec4\uff08\u6216\u6570\u7ec4\u5217\u8868\uff09\u3002 fit_generator fit_generator(generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0) \u4f7f\u7528 Python \u751f\u6210\u5668\uff08\u6216 Sequence \u5b9e\u4f8b\uff09\u9010\u6279\u751f\u6210\u7684\u6570\u636e\uff0c\u6309\u6279\u6b21\u8bad\u7ec3\u6a21\u578b\u3002 \u751f\u6210\u5668\u4e0e\u6a21\u578b\u5e76\u884c\u8fd0\u884c\uff0c\u4ee5\u63d0\u9ad8\u6548\u7387\u3002 \u4f8b\u5982\uff0c\u8fd9\u53ef\u4ee5\u8ba9\u4f60\u5728 CPU \u4e0a\u5bf9\u56fe\u50cf\u8fdb\u884c\u5b9e\u65f6\u6570\u636e\u589e\u5f3a\uff0c\u4ee5\u5728 GPU \u4e0a\u8bad\u7ec3\u6a21\u578b\u3002 keras.utils.Sequence \u7684\u4f7f\u7528\u53ef\u4ee5\u4fdd\u8bc1\u6570\u636e\u7684\u987a\u5e8f\uff0c \u4ee5\u53ca\u5f53 use_multiprocessing=True \u65f6 \uff0c\u4fdd\u8bc1\u6bcf\u4e2a\u8f93\u5165\u5728\u6bcf\u4e2a epoch \u53ea\u4f7f\u7528\u4e00\u6b21\u3002 \u53c2\u6570 generator : \u4e00\u4e2a\u751f\u6210\u5668\uff0c\u6216\u8005\u4e00\u4e2a Sequence ( keras.utils.Sequence ) \u5bf9\u8c61\u7684\u5b9e\u4f8b\uff0c \u4ee5\u5728\u4f7f\u7528\u591a\u8fdb\u7a0b\u65f6\u907f\u514d\u6570\u636e\u7684\u91cd\u590d\u3002 \u751f\u6210\u5668\u7684\u8f93\u51fa\u5e94\u8be5\u4e3a\u4ee5\u4e0b\u4e4b\u4e00\uff1a \u4e00\u4e2a (inputs, targets) \u5143\u7ec4 \u4e00\u4e2a (inputs, targets, sample_weights) \u5143\u7ec4\u3002 \u8fd9\u4e2a\u5143\u7ec4\uff08\u751f\u6210\u5668\u7684\u5355\u4e2a\u8f93\u51fa\uff09\u7ec4\u6210\u4e86\u5355\u4e2a\u7684 batch\u3002 \u56e0\u6b64\uff0c\u8fd9\u4e2a\u5143\u7ec4\u4e2d\u7684\u6240\u6709\u6570\u7ec4\u957f\u5ea6\u5fc5\u987b\u76f8\u540c\uff08\u4e0e\u8fd9\u4e00\u4e2a batch \u7684\u5927\u5c0f\u76f8\u7b49\uff09\u3002 \u4e0d\u540c\u7684 batch \u53ef\u80fd\u5927\u5c0f\u4e0d\u540c\u3002 \u4f8b\u5982\uff0c\u4e00\u4e2a epoch \u7684\u6700\u540e\u4e00\u4e2a batch \u5f80\u5f80\u6bd4\u5176\u4ed6 batch \u8981\u5c0f\uff0c \u5982\u679c\u6570\u636e\u96c6\u7684\u5c3a\u5bf8\u4e0d\u80fd\u88ab batch size \u6574\u9664\u3002 \u751f\u6210\u5668\u5c06\u65e0\u9650\u5730\u5728\u6570\u636e\u96c6\u4e0a\u5faa\u73af\u3002\u5f53\u8fd0\u884c\u5230\u7b2c steps_per_epoch \u65f6\uff0c\u8bb0\u4e00\u4e2a epoch \u7ed3\u675f\u3002 steps_per_epoch : \u5728\u58f0\u660e\u4e00\u4e2a epoch \u5b8c\u6210\u5e76\u5f00\u59cb\u4e0b\u4e00\u4e2a epoch \u4e4b\u524d\u4ece generator \u4ea7\u751f\u7684\u603b\u6b65\u6570\uff08\u6279\u6b21\u6837\u672c\uff09\u3002 \u5b83\u901a\u5e38\u5e94\u8be5\u7b49\u4e8e\u4f60\u7684\u6570\u636e\u96c6\u7684\u6837\u672c\u6570\u91cf\u9664\u4ee5\u6279\u91cf\u5927\u5c0f\u3002 \u5bf9\u4e8e Sequence \uff0c\u5b83\u662f\u53ef\u9009\u7684\uff1a\u5982\u679c\u672a\u6307\u5b9a\uff0c\u5c06\u4f7f\u7528 len(generator) \u4f5c\u4e3a\u6b65\u6570\u3002 epochs : \u6574\u6570\u3002\u8bad\u7ec3\u6a21\u578b\u7684\u8fed\u4ee3\u603b\u8f6e\u6570\u3002\u4e00\u4e2a epoch \u662f\u5bf9\u6240\u63d0\u4f9b\u7684\u6574\u4e2a\u6570\u636e\u7684\u4e00\u8f6e\u8fed\u4ee3\uff0c\u5982 steps_per_epoch \u6240\u5b9a\u4e49\u3002\u6ce8\u610f\uff0c\u4e0e initial_epoch \u4e00\u8d77\u4f7f\u7528\uff0cepoch \u5e94\u88ab\u7406\u89e3\u4e3a\u300c\u6700\u540e\u4e00\u8f6e\u300d\u3002\u6a21\u578b\u6ca1\u6709\u7ecf\u5386\u7531 epochs \u7ed9\u51fa\u7684\u591a\u6b21\u8fed\u4ee3\u7684\u8bad\u7ec3\uff0c\u800c\u4ec5\u4ec5\u662f\u76f4\u5230\u8fbe\u5230\u7d22\u5f15 epoch \u7684\u8f6e\u6b21\u3002 verbose : 0, 1 \u6216 2\u3002\u65e5\u5fd7\u663e\u793a\u6a21\u5f0f\u3002 0 = \u5b89\u9759\u6a21\u5f0f, 1 = \u8fdb\u5ea6\u6761, 2 = \u6bcf\u8f6e\u4e00\u884c\u3002 callbacks : keras.callbacks.Callback \u5b9e\u4f8b\u7684\u5217\u8868\u3002\u5728\u8bad\u7ec3\u65f6\u8c03\u7528\u7684\u4e00\u7cfb\u5217\u56de\u8c03\u51fd\u6570\u3002 validation_data : \u5b83\u53ef\u4ee5\u662f\u4ee5\u4e0b\u4e4b\u4e00\uff1a \u9a8c\u8bc1\u6570\u636e\u7684\u751f\u6210\u5668\u6216 Sequence \u5b9e\u4f8b \u4e00\u4e2a (inputs, targets) \u5143\u7ec4 \u4e00\u4e2a (inputs, targets, sample_weights) \u5143\u7ec4\u3002 \u5728\u6bcf\u4e2a epoch \u7ed3\u675f\u65f6\u8bc4\u4f30\u635f\u5931\u548c\u4efb\u4f55\u6a21\u578b\u6307\u6807\u3002\u8be5\u6a21\u578b\u4e0d\u4f1a\u5bf9\u6b64\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002 validation_steps : \u4ec5\u5f53 validation_data \u662f\u4e00\u4e2a\u751f\u6210\u5668\u65f6\u624d\u53ef\u7528\u3002 \u5728\u505c\u6b62\u524d generator \u751f\u6210\u7684\u603b\u6b65\u6570\uff08\u6837\u672c\u6279\u6570\uff09\u3002 \u5bf9\u4e8e Sequence \uff0c\u5b83\u662f\u53ef\u9009\u7684\uff1a\u5982\u679c\u672a\u6307\u5b9a\uff0c\u5c06\u4f7f\u7528 len(generator) \u4f5c\u4e3a\u6b65\u6570\u3002 class_weight : \u53ef\u9009\u7684\u5c06\u7c7b\u7d22\u5f15\uff08\u6574\u6570\uff09\u6620\u5c04\u5230\u6743\u91cd\uff08\u6d6e\u70b9\uff09\u503c\u7684\u5b57\u5178\uff0c\u7528\u4e8e\u52a0\u6743\u635f\u5931\u51fd\u6570\uff08\u4ec5\u5728\u8bad\u7ec3\u671f\u95f4\uff09\u3002 \u8fd9\u53ef\u4ee5\u7528\u6765\u544a\u8bc9\u6a21\u578b\u300c\u66f4\u591a\u5730\u5173\u6ce8\u300d\u6765\u81ea\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u7c7b\u7684\u6837\u672c\u3002 max_queue_size : \u6574\u6570\u3002\u751f\u6210\u5668\u961f\u5217\u7684\u6700\u5927\u5c3a\u5bf8\u3002 \u5982\u672a\u6307\u5b9a\uff0c max_queue_size \u5c06\u9ed8\u8ba4\u4e3a 10\u3002 workers : \u6574\u6570\u3002\u4f7f\u7528\u7684\u6700\u5927\u8fdb\u7a0b\u6570\u91cf\uff0c\u5982\u679c\u4f7f\u7528\u57fa\u4e8e\u8fdb\u7a0b\u7684\u591a\u7ebf\u7a0b\u3002 \u5982\u672a\u6307\u5b9a\uff0c workers \u5c06\u9ed8\u8ba4\u4e3a 1\u3002\u5982\u679c\u4e3a 0\uff0c\u5c06\u5728\u4e3b\u7ebf\u7a0b\u4e0a\u6267\u884c\u751f\u6210\u5668\u3002 use_multiprocessing : \u5e03\u5c14\u503c\u3002\u5982\u679c True\uff0c\u5219\u4f7f\u7528\u57fa\u4e8e\u8fdb\u7a0b\u7684\u591a\u7ebf\u7a0b\u3002 \u5982\u672a\u6307\u5b9a\uff0c use_multiprocessing \u5c06\u9ed8\u8ba4\u4e3a False\u3002 \u8bf7\u6ce8\u610f\uff0c\u7531\u4e8e\u6b64\u5b9e\u73b0\u4f9d\u8d56\u4e8e\u591a\u8fdb\u7a0b\uff0c\u6240\u4ee5\u4e0d\u5e94\u5c06\u4e0d\u53ef\u4f20\u9012\u7684\u53c2\u6570\u4f20\u9012\u7ed9\u751f\u6210\u5668\uff0c\u56e0\u4e3a\u5b83\u4eec\u4e0d\u80fd\u88ab\u8f7b\u6613\u5730\u4f20\u9012\u7ed9\u5b50\u8fdb\u7a0b\u3002 shuffle : \u662f\u5426\u5728\u6bcf\u8f6e\u8fed\u4ee3\u4e4b\u524d\u6253\u4e71 batch \u7684\u987a\u5e8f\u3002 \u53ea\u80fd\u4e0e Sequence (keras.utils.Sequence) \u5b9e\u4f8b\u540c\u7528\u3002 initial_epoch : \u5f00\u59cb\u8bad\u7ec3\u7684\u8f6e\u6b21\uff08\u6709\u52a9\u4e8e\u6062\u590d\u4e4b\u524d\u7684\u8bad\u7ec3\uff09\u3002 \u8fd4\u56de \u4e00\u4e2a History \u5bf9\u8c61\u3002\u5176 History.history \u5c5e\u6027\u662f\u8fde\u7eed epoch \u8bad\u7ec3\u635f\u5931\u548c\u8bc4\u4f30\u503c\uff0c\u4ee5\u53ca\u9a8c\u8bc1\u96c6\u635f\u5931\u548c\u8bc4\u4f30\u503c\u7684\u8bb0\u5f55\uff08\u5982\u679c\u9002\u7528\uff09\u3002 \u5f02\u5e38 ValueError : \u5982\u679c\u751f\u6210\u5668\u751f\u6210\u7684\u6570\u636e\u683c\u5f0f\u4e0d\u6b63\u786e\u3002 \u4f8b def generate_arrays_from_file(path): while True: with open(path) as f: for line in f: # \u4ece\u6587\u4ef6\u4e2d\u7684\u6bcf\u4e00\u884c\u751f\u6210\u8f93\u5165\u6570\u636e\u548c\u6807\u7b7e\u7684 numpy \u6570\u7ec4\uff0c x1, x2, y = process_line(line) yield ({'input_1': x1, 'input_2': x2}, {'output': y}) f.close() model.fit_generator(generate_arrays_from_file('/my_file.txt'), steps_per_epoch=10000, epochs=10) evaluate_generator evaluate_generator(generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0) \u5728\u6570\u636e\u751f\u6210\u5668\u4e0a\u8bc4\u4f30\u6a21\u578b\u3002 \u8fd9\u4e2a\u751f\u6210\u5668\u5e94\u8be5\u8fd4\u56de\u4e0e test_on_batch \u6240\u63a5\u6536\u7684\u540c\u6837\u7684\u6570\u636e\u3002 \u53c2\u6570 generator : \u4e00\u4e2a\u751f\u6210 (inputs, targets) \u6216 (inputs, targets, sample_weights) \u7684\u751f\u6210\u5668\uff0c \u6216\u4e00\u4e2a Sequence ( keras.utils.Sequence ) \u5bf9\u8c61\u7684\u5b9e\u4f8b\uff0c\u4ee5\u907f\u514d\u5728\u4f7f\u7528\u591a\u8fdb\u7a0b\u65f6\u6570\u636e\u7684\u91cd\u590d\u3002 steps : \u5728\u58f0\u660e\u4e00\u4e2a epoch \u5b8c\u6210\u5e76\u5f00\u59cb\u4e0b\u4e00\u4e2a epoch \u4e4b\u524d\u4ece generator \u4ea7\u751f\u7684\u603b\u6b65\u6570\uff08\u6279\u6b21\u6837\u672c\uff09\u3002 \u5b83\u901a\u5e38\u5e94\u8be5\u7b49\u4e8e\u4f60\u7684\u6570\u636e\u96c6\u7684\u6837\u672c\u6570\u91cf\u9664\u4ee5\u6279\u91cf\u5927\u5c0f\u3002 \u5bf9\u4e8e Sequence \uff0c\u5b83\u662f\u53ef\u9009\u7684\uff1a\u5982\u679c\u672a\u6307\u5b9a\uff0c\u5c06\u4f7f\u7528 len(generator) \u4f5c\u4e3a\u6b65\u6570\u3002 max_queue_size : \u751f\u6210\u5668\u961f\u5217\u7684\u6700\u5927\u5c3a\u5bf8\u3002 workers : \u6574\u6570\u3002\u4f7f\u7528\u7684\u6700\u5927\u8fdb\u7a0b\u6570\u91cf\uff0c\u5982\u679c\u4f7f\u7528\u57fa\u4e8e\u8fdb\u7a0b\u7684\u591a\u7ebf\u7a0b\u3002 \u5982\u672a\u6307\u5b9a\uff0c workers \u5c06\u9ed8\u8ba4\u4e3a 1\u3002\u5982\u679c\u4e3a 0\uff0c\u5c06\u5728\u4e3b\u7ebf\u7a0b\u4e0a\u6267\u884c\u751f\u6210\u5668\u3002 use_multiprocessing : \u5e03\u5c14\u503c\u3002\u5982\u679c True\uff0c\u5219\u4f7f\u7528\u57fa\u4e8e\u8fdb\u7a0b\u7684\u591a\u7ebf\u7a0b\u3002 \u8bf7\u6ce8\u610f\uff0c\u7531\u4e8e\u6b64\u5b9e\u73b0\u4f9d\u8d56\u4e8e\u591a\u8fdb\u7a0b\uff0c\u6240\u4ee5\u4e0d\u5e94\u5c06\u4e0d\u53ef\u4f20\u9012\u7684\u53c2\u6570\u4f20\u9012\u7ed9\u751f\u6210\u5668\uff0c\u56e0\u4e3a\u5b83\u4eec\u4e0d\u80fd\u88ab\u8f7b\u6613\u5730\u4f20\u9012\u7ed9\u5b50\u8fdb\u7a0b\u3002 verbose : \u65e5\u5fd7\u663e\u793a\u6a21\u5f0f\uff0c0 \u6216 1\u3002 \u8fd4\u56de \u6807\u91cf\u6d4b\u8bd5\u8bef\u5dee\uff08\u5982\u679c\u6a21\u578b\u53ea\u6709\u4e00\u4e2a\u8f93\u5165\u4e14\u6ca1\u6709\u8bc4\u4f30\u6807\u51c6\uff09\uff0c \u6216\u8005\u6807\u91cf\u7684\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u6709\u591a\u4e2a\u8f93\u51fa \u548c/\u6216 \u8bc4\u4f30\u6807\u51c6\uff09\u3002 \u5c5e\u6027 model.metrics_names \u5c06\u63d0\u4f9b\u6807\u91cf\u8f93\u51fa\u7684\u663e\u793a\u6807\u7b7e\u3002 \u5f02\u5e38 ValueError : \u5982\u679c\u751f\u6210\u5668\u751f\u6210\u7684\u6570\u636e\u683c\u5f0f\u4e0d\u6b63\u786e\u3002 predict_generator predict_generator(generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0) \u4e3a\u6765\u81ea\u6570\u636e\u751f\u6210\u5668\u7684\u8f93\u5165\u6837\u672c\u751f\u6210\u9884\u6d4b\u3002 \u8fd9\u4e2a\u751f\u6210\u5668\u5e94\u8be5\u8fd4\u56de\u4e0e predict_on_batch \u6240\u63a5\u6536\u7684\u540c\u6837\u7684\u6570\u636e\u3002 \u53c2\u6570 generator : \u751f\u6210\u5668\uff0c\u8fd4\u56de\u6279\u91cf\u8f93\u5165\u6837\u672c\uff0c \u6216\u4e00\u4e2a Sequence ( keras.utils.Sequence ) \u5bf9\u8c61\u7684\u5b9e\u4f8b\uff0c\u4ee5\u907f\u514d\u5728\u4f7f\u7528\u591a\u8fdb\u7a0b\u65f6\u6570\u636e\u7684\u91cd\u590d\u3002 steps : \u5728\u58f0\u660e\u4e00\u4e2a epoch \u5b8c\u6210\u5e76\u5f00\u59cb\u4e0b\u4e00\u4e2a epoch \u4e4b\u524d\u4ece generator \u4ea7\u751f\u7684\u603b\u6b65\u6570\uff08\u6279\u6b21\u6837\u672c\uff09\u3002 \u5b83\u901a\u5e38\u5e94\u8be5\u7b49\u4e8e\u4f60\u7684\u6570\u636e\u96c6\u7684\u6837\u672c\u6570\u91cf\u9664\u4ee5\u6279\u91cf\u5927\u5c0f\u3002 \u5bf9\u4e8e Sequence \uff0c\u5b83\u662f\u53ef\u9009\u7684\uff1a\u5982\u679c\u672a\u6307\u5b9a\uff0c\u5c06\u4f7f\u7528 len(generator) \u4f5c\u4e3a\u6b65\u6570\u3002 max_queue_size : \u751f\u6210\u5668\u961f\u5217\u7684\u6700\u5927\u5c3a\u5bf8\u3002 workers : \u6574\u6570\u3002\u4f7f\u7528\u7684\u6700\u5927\u8fdb\u7a0b\u6570\u91cf\uff0c\u5982\u679c\u4f7f\u7528\u57fa\u4e8e\u8fdb\u7a0b\u7684\u591a\u7ebf\u7a0b\u3002 \u5982\u672a\u6307\u5b9a\uff0c workers \u5c06\u9ed8\u8ba4\u4e3a 1\u3002\u5982\u679c\u4e3a 0\uff0c\u5c06\u5728\u4e3b\u7ebf\u7a0b\u4e0a\u6267\u884c\u751f\u6210\u5668\u3002 use_multiprocessing : \u5982\u679c True\uff0c\u5219\u4f7f\u7528\u57fa\u4e8e\u8fdb\u7a0b\u7684\u591a\u7ebf\u7a0b\u3002 \u8bf7\u6ce8\u610f\uff0c\u7531\u4e8e\u6b64\u5b9e\u73b0\u4f9d\u8d56\u4e8e\u591a\u8fdb\u7a0b\uff0c\u6240\u4ee5\u4e0d\u5e94\u5c06\u4e0d\u53ef\u4f20\u9012\u7684\u53c2\u6570\u4f20\u9012\u7ed9\u751f\u6210\u5668\uff0c\u56e0\u4e3a\u5b83\u4eec\u4e0d\u80fd\u88ab\u8f7b\u6613\u5730\u4f20\u9012\u7ed9\u5b50\u8fdb\u7a0b\u3002 verbose : \u65e5\u5fd7\u663e\u793a\u6a21\u5f0f\uff0c0 \u6216 1\u3002 \u8fd4\u56de \u9884\u6d4b\u503c\u7684 Numpy \u6570\u7ec4\uff08\u6216\u6570\u7ec4\u5217\u8868\uff09\u3002 \u5f02\u5e38 ValueError : \u5982\u679c\u751f\u6210\u5668\u751f\u6210\u7684\u6570\u636e\u683c\u5f0f\u4e0d\u6b63\u786e\u3002 get_layer get_layer(self, name=None, index=None) \u6839\u636e\u540d\u79f0\uff08\u552f\u4e00\uff09\u6216\u7d22\u5f15\u503c\u67e5\u627e\u7f51\u7edc\u5c42\u3002 \u5982\u679c\u540c\u65f6\u63d0\u4f9b\u4e86 name \u548c index \uff0c\u5219 index \u5c06\u4f18\u5148\u3002 \u7d22\u5f15\u503c\u6765\u81ea\u4e8e\u6c34\u5e73\u56fe\u904d\u5386\u7684\u987a\u5e8f\uff08\u81ea\u4e0b\u800c\u4e0a\uff09\u3002 \u53c2\u6570 name : \u5b57\u7b26\u4e32\uff0c\u5c42\u7684\u540d\u5b57\u3002 index : \u6574\u6570\uff0c\u5c42\u7684\u7d22\u5f15\u3002 \u8fd4\u56de \u4e00\u4e2a\u5c42\u5b9e\u4f8b\u3002 \u5f02\u5e38 ValueError : \u5982\u679c\u5c42\u7684\u540d\u79f0\u6216\u7d22\u5f15\u4e0d\u6b63\u786e\u3002","title":"Model \u7c7b\uff08\u51fd\u6570\u5f0f API\uff09"},{"location":"1-Models/2.model/#model-api","text":"\u5728\u51fd\u6570\u5f0f API \u4e2d\uff0c\u7ed9\u5b9a\u4e00\u4e9b\u8f93\u5165\u5f20\u91cf\u548c\u8f93\u51fa\u5f20\u91cf\uff0c\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u5b9e\u4f8b\u5316\u4e00\u4e2a Model \uff1a from keras.models import Model from keras.layers import Input, Dense a = Input(shape=(32,)) b = Dense(32)(a) model = Model(inputs=a, outputs=b) \u8fd9\u4e2a\u6a21\u578b\u5c06\u5305\u542b\u4ece a \u5230 b \u7684\u8ba1\u7b97\u7684\u6240\u6709\u7f51\u7edc\u5c42\u3002 \u5728\u591a\u8f93\u5165\u6216\u591a\u8f93\u51fa\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u4f60\u4e5f\u53ef\u4ee5\u4f7f\u7528\u5217\u8868\uff1a model = Model(inputs=[a1, a2], outputs=[b1, b3, b3]) \u6709\u5173 Model \u7684\u8be6\u7ec6\u4ecb\u7ecd\uff0c\u8bf7\u9605\u8bfb Keras \u51fd\u6570\u5f0f API \u6307\u5f15 \u3002","title":"Model \u7c7b\uff08\u51fd\u6570\u5f0f API\uff09"},{"location":"1-Models/2.model/#model","text":"","title":"Model \u7c7b\u6a21\u578b\u65b9\u6cd5"},{"location":"1-Models/2.model/#compile","text":"compile(optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None) \u7528\u4e8e\u914d\u7f6e\u8bad\u7ec3\u6a21\u578b\u3002 \u53c2\u6570 optimizer : \u5b57\u7b26\u4e32\uff08\u4f18\u5316\u5668\u540d\uff09\u6216\u8005\u4f18\u5316\u5668\u5b9e\u4f8b\u3002 \u8be6\u89c1 optimizers \u3002 loss : \u5b57\u7b26\u4e32\uff08\u76ee\u6807\u51fd\u6570\u540d\uff09\u6216\u76ee\u6807\u51fd\u6570\u3002 \u8be6\u89c1 losses \u3002 \u5982\u679c\u6a21\u578b\u5177\u6709\u591a\u4e2a\u8f93\u51fa\uff0c\u5219\u53ef\u4ee5\u901a\u8fc7\u4f20\u9012\u635f\u5931\u51fd\u6570\u7684\u5b57\u5178\u6216\u5217\u8868\uff0c\u5728\u6bcf\u4e2a\u8f93\u51fa\u4e0a\u4f7f\u7528\u4e0d\u540c\u7684\u635f\u5931\u3002 \u6a21\u578b\u5c06\u6700\u5c0f\u5316\u7684\u635f\u5931\u503c\u5c06\u662f\u6240\u6709\u5355\u4e2a\u635f\u5931\u7684\u603b\u548c\u3002 metrics : \u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u671f\u95f4\u7684\u6a21\u578b\u8bc4\u4f30\u6807\u51c6\u3002 \u901a\u5e38\u4f60\u4f1a\u4f7f\u7528 metrics = ['accuracy'] \u3002 \u8981\u4e3a\u591a\u8f93\u51fa\u6a21\u578b\u7684\u4e0d\u540c\u8f93\u51fa\u6307\u5b9a\u4e0d\u540c\u7684\u8bc4\u4f30\u6807\u51c6\uff0c \u8fd8\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5b57\u5178\uff0c\u5982 metrics = {'output_a'\uff1a'accuracy'} \u3002 loss_weights : \u53ef\u9009\u7684\u6307\u5b9a\u6807\u91cf\u7cfb\u6570\uff08Python \u6d6e\u70b9\u6570\uff09\u7684\u5217\u8868\u6216\u5b57\u5178\uff0c \u7528\u4ee5\u8861\u91cf\u635f\u5931\u51fd\u6570\u5bf9\u4e0d\u540c\u7684\u6a21\u578b\u8f93\u51fa\u7684\u8d21\u732e\u3002 \u6a21\u578b\u5c06\u6700\u5c0f\u5316\u7684\u8bef\u5dee\u503c\u662f\u7531 loss_weights \u7cfb\u6570\u52a0\u6743\u7684 \u52a0\u6743\u603b\u548c \u8bef\u5dee\u3002 \u5982\u679c\u662f\u5217\u8868\uff0c\u90a3\u4e48\u5b83\u5e94\u8be5\u662f\u4e0e\u6a21\u578b\u8f93\u51fa\u76f8\u5bf9\u5e94\u7684 1:1 \u6620\u5c04\u3002 \u5982\u679c\u662f\u5f20\u91cf\uff0c\u90a3\u4e48\u5e94\u8be5\u628a\u8f93\u51fa\u7684\u540d\u79f0\uff08\u5b57\u7b26\u4e32\uff09\u6620\u5230\u6807\u91cf\u7cfb\u6570\u3002 sample_weight_mode : \u5982\u679c\u4f60\u9700\u8981\u6267\u884c\u6309\u65f6\u95f4\u6b65\u91c7\u6837\u6743\u91cd\uff082D \u6743\u91cd\uff09\uff0c\u8bf7\u5c06\u5176\u8bbe\u7f6e\u4e3a temporal \u3002 \u9ed8\u8ba4\u4e3a None \uff0c\u4e3a\u91c7\u6837\u6743\u91cd\uff081D\uff09\u3002 \u5982\u679c\u6a21\u578b\u6709\u591a\u4e2a\u8f93\u51fa\uff0c\u5219\u53ef\u4ee5\u901a\u8fc7\u4f20\u9012 mode \u7684\u5b57\u5178\u6216\u5217\u8868\uff0c\u4ee5\u5728\u6bcf\u4e2a\u8f93\u51fa\u4e0a\u4f7f\u7528\u4e0d\u540c\u7684 sample_weight_mode \u3002 weighted_metrics : \u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u671f\u95f4\uff0c\u7531 sample_weight \u6216 class_weight \u8bc4\u4f30\u548c\u52a0\u6743\u7684\u5ea6\u91cf\u6807\u51c6\u5217\u8868\u3002 target_tensors : \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cKeras \u5c06\u4e3a\u6a21\u578b\u7684\u76ee\u6807\u521b\u5efa\u4e00\u4e2a\u5360\u4f4d\u7b26\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5c06\u4f7f\u7528\u76ee\u6807\u6570\u636e\u3002 \u76f8\u53cd\uff0c\u5982\u679c\u4f60\u60f3\u4f7f\u7528\u81ea\u5df1\u7684\u76ee\u6807\u5f20\u91cf\uff08\u53cd\u8fc7\u6765\u8bf4\uff0cKeras \u5728\u8bad\u7ec3\u671f\u95f4\u4e0d\u4f1a\u8f7d\u5165\u8fd9\u4e9b\u76ee\u6807\u5f20\u91cf\u7684\u5916\u90e8 Numpy \u6570\u636e\uff09\uff0c \u60a8\u53ef\u4ee5\u901a\u8fc7 target_tensors \u53c2\u6570\u6307\u5b9a\u5b83\u4eec\u3002 \u5b83\u53ef\u4ee5\u662f\u5355\u4e2a\u5f20\u91cf\uff08\u5355\u8f93\u51fa\u6a21\u578b\uff09\uff0c\u5f20\u91cf\u5217\u8868\uff0c\u6216\u4e00\u4e2a\u6620\u5c04\u8f93\u51fa\u540d\u79f0\u5230\u76ee\u6807\u5f20\u91cf\u7684\u5b57\u5178\u3002 **kwargs : \u5f53\u4f7f\u7528 Theano/CNTK \u540e\u7aef\u65f6\uff0c\u8fd9\u4e9b\u53c2\u6570\u88ab\u4f20\u5165 K.function \u3002 \u5f53\u4f7f\u7528 TensorFlow \u540e\u7aef\u65f6\uff0c\u8fd9\u4e9b\u53c2\u6570\u88ab\u4f20\u9012\u5230 tf.Session.run \u3002 \u5f02\u5e38 ValueError : \u5982\u679c optimizer , loss , metrics \u6216 sample_weight_mode \u8fd9\u4e9b\u53c2\u6570\u4e0d\u5408\u6cd5\u3002","title":"compile"},{"location":"1-Models/2.model/#fit","text":"fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None) \u4ee5\u7ed9\u5b9a\u6570\u91cf\u7684\u8f6e\u6b21\uff08\u6570\u636e\u96c6\u4e0a\u7684\u8fed\u4ee3\uff09\u8bad\u7ec3\u6a21\u578b\u3002 \u53c2\u6570 x : \u8bad\u7ec3\u6570\u636e\u7684 Numpy \u6570\u7ec4\uff08\u5982\u679c\u6a21\u578b\u53ea\u6709\u4e00\u4e2a\u8f93\u5165\uff09\uff0c \u6216\u8005\u662f Numpy \u6570\u7ec4\u7684\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u6709\u591a\u4e2a\u8f93\u5165\uff09\u3002 \u5982\u679c\u6a21\u578b\u4e2d\u7684\u8f93\u5165\u5c42\u88ab\u547d\u540d\uff0c\u4f60\u4e5f\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5b57\u5178\uff0c\u5c06\u8f93\u5165\u5c42\u540d\u79f0\u6620\u5c04\u5230 Numpy \u6570\u7ec4\u3002 \u5982\u679c\u4ece\u672c\u5730\u6846\u67b6\u5f20\u91cf\u9988\u9001\uff08\u4f8b\u5982 TensorFlow \u6570\u636e\u5f20\u91cf\uff09\u6570\u636e\uff0c x \u53ef\u4ee5\u662f None \uff08\u9ed8\u8ba4\uff09\u3002 y : \u76ee\u6807\uff08\u6807\u7b7e\uff09\u6570\u636e\u7684 Numpy \u6570\u7ec4\uff08\u5982\u679c\u6a21\u578b\u53ea\u6709\u4e00\u4e2a\u8f93\u51fa\uff09\uff0c \u6216\u8005\u662f Numpy \u6570\u7ec4\u7684\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u6709\u591a\u4e2a\u8f93\u51fa\uff09\u3002 \u5982\u679c\u6a21\u578b\u4e2d\u7684\u8f93\u51fa\u5c42\u88ab\u547d\u540d\uff0c\u4f60\u4e5f\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5b57\u5178\uff0c\u5c06\u8f93\u51fa\u5c42\u540d\u79f0\u6620\u5c04\u5230 Numpy \u6570\u7ec4\u3002 \u5982\u679c\u4ece\u672c\u5730\u6846\u67b6\u5f20\u91cf\u9988\u9001\uff08\u4f8b\u5982 TensorFlow \u6570\u636e\u5f20\u91cf\uff09\u6570\u636e\uff0cy \u53ef\u4ee5\u662f None \uff08\u9ed8\u8ba4\uff09\u3002 batch_size : \u6574\u6570\u6216 None \u3002\u6bcf\u6b21\u68af\u5ea6\u66f4\u65b0\u7684\u6837\u672c\u6570\u3002\u5982\u679c\u672a\u6307\u5b9a\uff0c\u9ed8\u8ba4\u4e3a 32\u3002 epochs : \u6574\u6570\u3002\u8bad\u7ec3\u6a21\u578b\u8fed\u4ee3\u8f6e\u6b21\u3002\u4e00\u4e2a\u8f6e\u6b21\u662f\u5728\u6574\u4e2a x \u548c y \u4e0a\u7684\u4e00\u8f6e\u8fed\u4ee3\u3002 \u8bf7\u6ce8\u610f\uff0c\u4e0e initial_epoch \u4e00\u8d77\uff0c epochs \u88ab\u7406\u89e3\u4e3a \u300c\u6700\u7ec8\u8f6e\u6b21\u300d\u3002\u6a21\u578b\u5e76\u4e0d\u662f\u8bad\u7ec3\u4e86 epochs \u8f6e\uff0c\u800c\u662f\u5230\u7b2c epochs \u8f6e\u505c\u6b62\u8bad\u7ec3\u3002 verbose : 0, 1 \u6216 2\u3002\u65e5\u5fd7\u663e\u793a\u6a21\u5f0f\u3002 0 = \u5b89\u9759\u6a21\u5f0f, 1 = \u8fdb\u5ea6\u6761, 2 = \u6bcf\u8f6e\u4e00\u884c\u3002 callbacks : \u4e00\u7cfb\u5217\u7684 keras.callbacks.Callback \u5b9e\u4f8b\u3002\u4e00\u7cfb\u5217\u53ef\u4ee5\u5728\u8bad\u7ec3\u65f6\u4f7f\u7528\u7684\u56de\u8c03\u51fd\u6570\u3002 \u8be6\u89c1 callbacks \u3002 validation_split : 0 \u548c 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002\u7528\u4f5c\u9a8c\u8bc1\u96c6\u7684\u8bad\u7ec3\u6570\u636e\u7684\u6bd4\u4f8b\u3002 \u6a21\u578b\u5c06\u5206\u51fa\u4e00\u90e8\u5206\u4e0d\u4f1a\u88ab\u8bad\u7ec3\u7684\u9a8c\u8bc1\u6570\u636e\uff0c\u5e76\u5c06\u5728\u6bcf\u4e00\u8f6e\u7ed3\u675f\u65f6\u8bc4\u4f30\u8fd9\u4e9b\u9a8c\u8bc1\u6570\u636e\u7684\u8bef\u5dee\u548c\u4efb\u4f55\u5176\u4ed6\u6a21\u578b\u6307\u6807\u3002 \u9a8c\u8bc1\u6570\u636e\u662f\u6df7\u6d17\u4e4b\u524d x \u548c y \u6570\u636e\u7684\u6700\u540e\u4e00\u90e8\u5206\u6837\u672c\u4e2d\u3002 validation_data : \u5143\u7ec4 (x_val\uff0cy_val) \u6216\u5143\u7ec4 (x_val\uff0cy_val\uff0cval_sample_weights) \uff0c \u7528\u6765\u8bc4\u4f30\u635f\u5931\uff0c\u4ee5\u53ca\u5728\u6bcf\u8f6e\u7ed3\u675f\u65f6\u7684\u4efb\u4f55\u6a21\u578b\u5ea6\u91cf\u6307\u6807\u3002 \u6a21\u578b\u5c06\u4e0d\u4f1a\u5728\u8fd9\u4e2a\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u8fd9\u4e2a\u53c2\u6570\u4f1a\u8986\u76d6 validation_split \u3002 shuffle : \u5e03\u5c14\u503c\uff08\u662f\u5426\u5728\u6bcf\u8f6e\u8fed\u4ee3\u4e4b\u524d\u6df7\u6d17\u6570\u636e\uff09\u6216\u8005 \u5b57\u7b26\u4e32 ( batch )\u3002 batch \u662f\u5904\u7406 HDF5 \u6570\u636e\u9650\u5236\u7684\u7279\u6b8a\u9009\u9879\uff0c\u5b83\u5bf9\u4e00\u4e2a batch \u5185\u90e8\u7684\u6570\u636e\u8fdb\u884c\u6df7\u6d17\u3002 \u5f53 steps_per_epoch \u975e None \u65f6\uff0c\u8fd9\u4e2a\u53c2\u6570\u65e0\u6548\u3002 class_weight : \u53ef\u9009\u7684\u5b57\u5178\uff0c\u7528\u6765\u6620\u5c04\u7c7b\u7d22\u5f15\uff08\u6574\u6570\uff09\u5230\u6743\u91cd\uff08\u6d6e\u70b9\uff09\u503c\uff0c\u7528\u4e8e\u52a0\u6743\u635f\u5931\u51fd\u6570\uff08\u4ec5\u5728\u8bad\u7ec3\u671f\u95f4\uff09\u3002 \u8fd9\u53ef\u80fd\u6709\u52a9\u4e8e\u544a\u8bc9\u6a21\u578b \u300c\u66f4\u591a\u5173\u6ce8\u300d\u6765\u81ea\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u7c7b\u7684\u6837\u672c\u3002 sample_weight : \u8bad\u7ec3\u6837\u672c\u7684\u53ef\u9009 Numpy \u6743\u91cd\u6570\u7ec4\uff0c\u7528\u4e8e\u5bf9\u635f\u5931\u51fd\u6570\u8fdb\u884c\u52a0\u6743\uff08\u4ec5\u5728\u8bad\u7ec3\u671f\u95f4\uff09\u3002 \u60a8\u53ef\u4ee5\u4f20\u9012\u4e0e\u8f93\u5165\u6837\u672c\u957f\u5ea6\u76f8\u540c\u7684\u5e73\u5766\uff081D\uff09Numpy \u6570\u7ec4\uff08\u6743\u91cd\u548c\u6837\u672c\u4e4b\u95f4\u7684 1:1 \u6620\u5c04\uff09\uff0c \u6216\u8005\u5728\u65f6\u5e8f\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u4f20\u9012\u5c3a\u5bf8\u4e3a (samples, sequence_length) \u7684 2D \u6570\u7ec4\uff0c\u4ee5\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u65f6\u95f4\u6b65\u65bd\u52a0\u4e0d\u540c\u7684\u6743\u91cd\u3002 \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4f60\u5e94\u8be5\u786e\u4fdd\u5728 compile() \u4e2d\u6307\u5b9a sample_weight_mode=\"temporal\" \u3002 initial_epoch : \u6574\u6570\u3002\u5f00\u59cb\u8bad\u7ec3\u7684\u8f6e\u6b21\uff08\u6709\u52a9\u4e8e\u6062\u590d\u4e4b\u524d\u7684\u8bad\u7ec3\uff09\u3002 steps_per_epoch : \u6574\u6570\u6216 None \u3002 \u5728\u58f0\u660e\u4e00\u4e2a\u8f6e\u6b21\u5b8c\u6210\u5e76\u5f00\u59cb\u4e0b\u4e00\u4e2a\u8f6e\u6b21\u4e4b\u524d\u7684\u603b\u6b65\u6570\uff08\u6837\u54c1\u6279\u6b21\uff09\u3002 \u4f7f\u7528 TensorFlow \u6570\u636e\u5f20\u91cf\u7b49\u8f93\u5165\u5f20\u91cf\u8fdb\u884c\u8bad\u7ec3\u65f6\uff0c\u9ed8\u8ba4\u503c None \u7b49\u4e8e\u6570\u636e\u96c6\u4e2d\u6837\u672c\u7684\u6570\u91cf\u9664\u4ee5 batch \u7684\u5927\u5c0f\uff0c\u5982\u679c\u65e0\u6cd5\u786e\u5b9a\uff0c\u5219\u4e3a 1\u3002 validation_steps : \u53ea\u6709\u5728\u6307\u5b9a\u4e86 steps_per_epoch \u65f6\u624d\u6709\u7528\u3002\u505c\u6b62\u524d\u8981\u9a8c\u8bc1\u7684\u603b\u6b65\u6570\uff08\u6279\u6b21\u6837\u672c\uff09\u3002 \u8fd4\u56de \u4e00\u4e2a History \u5bf9\u8c61\u3002\u5176 History.history \u5c5e\u6027\u662f\u8fde\u7eed epoch \u8bad\u7ec3\u635f\u5931\u548c\u8bc4\u4f30\u503c\uff0c\u4ee5\u53ca\u9a8c\u8bc1\u96c6\u635f\u5931\u548c\u8bc4\u4f30\u503c\u7684\u8bb0\u5f55\uff08\u5982\u679c\u9002\u7528\uff09\u3002 \u5f02\u5e38 RuntimeError : \u5982\u679c\u6a21\u578b\u4ece\u672a\u7f16\u8bd1\u3002 ValueError : \u5728\u63d0\u4f9b\u7684\u8f93\u5165\u6570\u636e\u4e0e\u6a21\u578b\u671f\u671b\u7684\u4e0d\u5339\u914d\u7684\u60c5\u51b5\u4e0b\u3002","title":"fit"},{"location":"1-Models/2.model/#evaluate","text":"evaluate(x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None) \u5728\u6d4b\u8bd5\u6a21\u5f0f\u4e0b\u8fd4\u56de\u6a21\u578b\u7684\u8bef\u5dee\u503c\u548c\u8bc4\u4f30\u6807\u51c6\u503c\u3002 \u8ba1\u7b97\u662f\u5206\u6279\u8fdb\u884c\u7684\u3002 \u53c2\u6570 x : \u6d4b\u8bd5\u6570\u636e\u7684 Numpy \u6570\u7ec4\uff08\u5982\u679c\u6a21\u578b\u53ea\u6709\u4e00\u4e2a\u8f93\u5165\uff09\uff0c \u6216\u8005\u662f Numpy \u6570\u7ec4\u7684\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u6709\u591a\u4e2a\u8f93\u5165\uff09\u3002 \u5982\u679c\u6a21\u578b\u4e2d\u7684\u8f93\u5165\u5c42\u88ab\u547d\u540d\uff0c\u4f60\u4e5f\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5b57\u5178\uff0c\u5c06\u8f93\u5165\u5c42\u540d\u79f0\u6620\u5c04\u5230 Numpy \u6570\u7ec4\u3002 \u5982\u679c\u4ece\u672c\u5730\u6846\u67b6\u5f20\u91cf\u9988\u9001\uff08\u4f8b\u5982 TensorFlow \u6570\u636e\u5f20\u91cf\uff09\u6570\u636e\uff0cx \u53ef\u4ee5\u662f None \uff08\u9ed8\u8ba4\uff09\u3002 y : \u76ee\u6807\uff08\u6807\u7b7e\uff09\u6570\u636e\u7684 Numpy \u6570\u7ec4\uff0c\u6216 Numpy \u6570\u7ec4\u7684\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u5177\u6709\u591a\u4e2a\u8f93\u51fa\uff09\u3002 \u5982\u679c\u6a21\u578b\u4e2d\u7684\u8f93\u51fa\u5c42\u88ab\u547d\u540d\uff0c\u4f60\u4e5f\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5b57\u5178\uff0c\u5c06\u8f93\u51fa\u5c42\u540d\u79f0\u6620\u5c04\u5230 Numpy \u6570\u7ec4\u3002 \u5982\u679c\u4ece\u672c\u5730\u6846\u67b6\u5f20\u91cf\u9988\u9001\uff08\u4f8b\u5982 TensorFlow \u6570\u636e\u5f20\u91cf\uff09\u6570\u636e\uff0cy \u53ef\u4ee5\u662f None \uff08\u9ed8\u8ba4\uff09\u3002 batch_size : \u6574\u6570\u6216 None \u3002\u6bcf\u6b21\u8bc4\u4f30\u7684\u6837\u672c\u6570\u3002\u5982\u679c\u672a\u6307\u5b9a\uff0c\u9ed8\u8ba4\u4e3a 32\u3002 verbose : 0 \u6216 1\u3002\u65e5\u5fd7\u663e\u793a\u6a21\u5f0f\u3002 0 = \u5b89\u9759\u6a21\u5f0f\uff0c1 = \u8fdb\u5ea6\u6761\u3002 sample_weight : \u6d4b\u8bd5\u6837\u672c\u7684\u53ef\u9009 Numpy \u6743\u91cd\u6570\u7ec4\uff0c\u7528\u4e8e\u5bf9\u635f\u5931\u51fd\u6570\u8fdb\u884c\u52a0\u6743\u3002 \u60a8\u53ef\u4ee5\u4f20\u9012\u4e0e\u8f93\u5165\u6837\u672c\u957f\u5ea6\u76f8\u540c\u7684\u6241\u5e73\uff081D\uff09Numpy \u6570\u7ec4\uff08\u6743\u91cd\u548c\u6837\u672c\u4e4b\u95f4\u7684 1:1 \u6620\u5c04\uff09\uff0c \u6216\u8005\u5728\u65f6\u5e8f\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u4f20\u9012\u5c3a\u5bf8\u4e3a (samples, sequence_length) \u7684 2D \u6570\u7ec4\uff0c\u4ee5\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u65f6\u95f4\u6b65\u65bd\u52a0\u4e0d\u540c\u7684\u6743\u91cd\u3002 \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4f60\u5e94\u8be5\u786e\u4fdd\u5728 compile() \u4e2d\u6307\u5b9a sample_weight_mode=\"temporal\" \u3002 steps : \u6574\u6570\u6216 None \u3002 \u58f0\u660e\u8bc4\u4f30\u7ed3\u675f\u4e4b\u524d\u7684\u603b\u6b65\u6570\uff08\u6279\u6b21\u6837\u672c\uff09\u3002\u9ed8\u8ba4\u503c None \u3002 \u8fd4\u56de \u6807\u91cf\u6d4b\u8bd5\u8bef\u5dee\uff08\u5982\u679c\u6a21\u578b\u53ea\u6709\u4e00\u4e2a\u8f93\u51fa\u4e14\u6ca1\u6709\u8bc4\u4f30\u6807\u51c6\uff09 \u6216\u6807\u91cf\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u5177\u6709\u591a\u4e2a\u8f93\u51fa \u548c/\u6216 \u8bc4\u4f30\u6307\u6807\uff09\u3002 \u5c5e\u6027 model.metrics_names \u5c06\u63d0\u4f9b\u6807\u91cf\u8f93\u51fa\u7684\u663e\u793a\u6807\u7b7e\u3002","title":"evaluate"},{"location":"1-Models/2.model/#predict","text":"predict(x, batch_size=None, verbose=0, steps=None) \u4e3a\u8f93\u5165\u6837\u672c\u751f\u6210\u8f93\u51fa\u9884\u6d4b\u3002 \u8ba1\u7b97\u662f\u5206\u6279\u8fdb\u884c\u7684 \u53c2\u6570 x : \u8f93\u5165\u6570\u636e\uff0cNumpy \u6570\u7ec4 \uff08\u6216\u8005 Numpy \u6570\u7ec4\u7684\u5217\u8868\uff0c\u5982\u679c\u6a21\u578b\u6709\u591a\u4e2a\u8f93\u51fa\uff09\u3002 batch_size : \u6574\u6570\u3002\u5982\u672a\u6307\u5b9a\uff0c\u9ed8\u8ba4\u4e3a 32\u3002 verbose : \u65e5\u5fd7\u663e\u793a\u6a21\u5f0f\uff0c0 \u6216 1\u3002 steps : \u58f0\u660e\u9884\u6d4b\u7ed3\u675f\u4e4b\u524d\u7684\u603b\u6b65\u6570\uff08\u6279\u6b21\u6837\u672c\uff09\u3002\u9ed8\u8ba4\u503c None \u3002 \u8fd4\u56de \u9884\u6d4b\u7684 Numpy \u6570\u7ec4\uff08\u6216\u6570\u7ec4\u5217\u8868\uff09\u3002 \u5f02\u5e38 ValueError : \u5728\u63d0\u4f9b\u7684\u8f93\u5165\u6570\u636e\u4e0e\u6a21\u578b\u671f\u671b\u7684\u4e0d\u5339\u914d\u7684\u60c5\u51b5\u4e0b\uff0c \u6216\u8005\u5728\u6709\u72b6\u6001\u7684\u6a21\u578b\u63a5\u6536\u5230\u7684\u6837\u672c\u4e0d\u662f batch size \u7684\u500d\u6570\u7684\u60c5\u51b5\u4e0b\u3002","title":"predict"},{"location":"1-Models/2.model/#train_on_batch","text":"train_on_batch(x, y, sample_weight=None, class_weight=None) \u8fd0\u884c\u4e00\u6279\u6837\u54c1\u7684\u5355\u6b21\u68af\u5ea6\u66f4\u65b0\u3002 __\u53c2\u6570_ x : \u8bad\u7ec3\u6570\u636e\u7684 Numpy \u6570\u7ec4\uff08\u5982\u679c\u6a21\u578b\u53ea\u6709\u4e00\u4e2a\u8f93\u5165\uff09\uff0c \u6216\u8005\u662f Numpy \u6570\u7ec4\u7684\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u6709\u591a\u4e2a\u8f93\u5165\uff09\u3002 \u5982\u679c\u6a21\u578b\u4e2d\u7684\u8f93\u5165\u5c42\u88ab\u547d\u540d\uff0c\u4f60\u4e5f\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5b57\u5178\uff0c\u5c06\u8f93\u5165\u5c42\u540d\u79f0\u6620\u5c04\u5230 Numpy \u6570\u7ec4\u3002 y : \u76ee\u6807\uff08\u6807\u7b7e\uff09\u6570\u636e\u7684 Numpy \u6570\u7ec4\uff0c\u6216 Numpy \u6570\u7ec4\u7684\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u5177\u6709\u591a\u4e2a\u8f93\u51fa\uff09\u3002 \u5982\u679c\u6a21\u578b\u4e2d\u7684\u8f93\u51fa\u5c42\u88ab\u547d\u540d\uff0c\u4f60\u4e5f\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5b57\u5178\uff0c\u5c06\u8f93\u51fa\u5c42\u540d\u79f0\u6620\u5c04\u5230 Numpy \u6570\u7ec4\u3002 sample_weight : \u53ef\u9009\u6570\u7ec4\uff0c\u4e0e x \u957f\u5ea6\u76f8\u540c\uff0c\u5305\u542b\u5e94\u7528\u5230\u6a21\u578b\u635f\u5931\u51fd\u6570\u7684\u6bcf\u4e2a\u6837\u672c\u7684\u6743\u91cd\u3002 \u5982\u679c\u662f\u65f6\u57df\u6570\u636e\uff0c\u4f60\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5c3a\u5bf8\u4e3a (samples, sequence_length) \u7684 2D \u6570\u7ec4\uff0c \u4e3a\u6bcf\u4e00\u4e2a\u6837\u672c\u7684\u6bcf\u4e00\u4e2a\u65f6\u95f4\u6b65\u5e94\u7528\u4e0d\u540c\u7684\u6743\u91cd\u3002 \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4f60\u5e94\u8be5\u5728 compile() \u4e2d\u6307\u5b9a sample_weight_mode=\"temporal\" \u3002 class_weight : \u53ef\u9009\u7684\u5b57\u5178\uff0c\u7528\u6765\u6620\u5c04\u7c7b\u7d22\u5f15\uff08\u6574\u6570\uff09\u5230\u6743\u91cd\uff08\u6d6e\u70b9\uff09\u503c\uff0c\u4ee5\u5728\u8bad\u7ec3\u65f6\u5bf9\u6a21\u578b\u7684\u635f\u5931\u51fd\u6570\u52a0\u6743\u3002 \u8fd9\u53ef\u80fd\u6709\u52a9\u4e8e\u544a\u8bc9\u6a21\u578b \u300c\u66f4\u591a\u5173\u6ce8\u300d\u6765\u81ea\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u7c7b\u7684\u6837\u672c\u3002 \u8fd4\u56de \u6807\u91cf\u8bad\u7ec3\u8bef\u5dee\uff08\u5982\u679c\u6a21\u578b\u53ea\u6709\u4e00\u4e2a\u8f93\u5165\u4e14\u6ca1\u6709\u8bc4\u4f30\u6807\u51c6\uff09\uff0c \u6216\u8005\u6807\u91cf\u7684\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u6709\u591a\u4e2a\u8f93\u51fa \u548c/\u6216 \u8bc4\u4f30\u6807\u51c6\uff09\u3002 \u5c5e\u6027 model.metrics_names \u5c06\u63d0\u4f9b\u6807\u91cf\u8f93\u51fa\u7684\u663e\u793a\u6807\u7b7e\u3002","title":"train_on_batch"},{"location":"1-Models/2.model/#test_on_batch","text":"test_on_batch(x, y, sample_weight=None) \u5728\u4e00\u6279\u6837\u672c\u4e0a\u6d4b\u8bd5\u6a21\u578b\u3002 \u53c2\u6570 x : \u6d4b\u8bd5\u6570\u636e\u7684 Numpy \u6570\u7ec4\uff08\u5982\u679c\u6a21\u578b\u53ea\u6709\u4e00\u4e2a\u8f93\u5165\uff09\uff0c \u6216\u8005\u662f Numpy \u6570\u7ec4\u7684\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u6709\u591a\u4e2a\u8f93\u5165\uff09\u3002 \u5982\u679c\u6a21\u578b\u4e2d\u7684\u8f93\u5165\u5c42\u88ab\u547d\u540d\uff0c\u4f60\u4e5f\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5b57\u5178\uff0c\u5c06\u8f93\u5165\u5c42\u540d\u79f0\u6620\u5c04\u5230 Numpy \u6570\u7ec4\u3002 y : \u76ee\u6807\uff08\u6807\u7b7e\uff09\u6570\u636e\u7684 Numpy \u6570\u7ec4\uff0c\u6216 Numpy \u6570\u7ec4\u7684\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u5177\u6709\u591a\u4e2a\u8f93\u51fa\uff09\u3002 \u5982\u679c\u6a21\u578b\u4e2d\u7684\u8f93\u51fa\u5c42\u88ab\u547d\u540d\uff0c\u4f60\u4e5f\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5b57\u5178\uff0c\u5c06\u8f93\u51fa\u5c42\u540d\u79f0\u6620\u5c04\u5230 Numpy \u6570\u7ec4\u3002 sample_weight : \u53ef\u9009\u6570\u7ec4\uff0c\u4e0e x \u957f\u5ea6\u76f8\u540c\uff0c\u5305\u542b\u5e94\u7528\u5230\u6a21\u578b\u635f\u5931\u51fd\u6570\u7684\u6bcf\u4e2a\u6837\u672c\u7684\u6743\u91cd\u3002 \u5982\u679c\u662f\u65f6\u57df\u6570\u636e\uff0c\u4f60\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a\u5c3a\u5bf8\u4e3a (samples, sequence_length) \u7684 2D \u6570\u7ec4\uff0c \u4e3a\u6bcf\u4e00\u4e2a\u6837\u672c\u7684\u6bcf\u4e00\u4e2a\u65f6\u95f4\u6b65\u5e94\u7528\u4e0d\u540c\u7684\u6743\u91cd\u3002 \u8fd4\u56de \u6807\u91cf\u6d4b\u8bd5\u8bef\u5dee\uff08\u5982\u679c\u6a21\u578b\u53ea\u6709\u4e00\u4e2a\u8f93\u5165\u4e14\u6ca1\u6709\u8bc4\u4f30\u6807\u51c6\uff09\uff0c \u6216\u8005\u6807\u91cf\u7684\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u6709\u591a\u4e2a\u8f93\u51fa \u548c/\u6216 \u8bc4\u4f30\u6807\u51c6\uff09\u3002 \u5c5e\u6027 model.metrics_names \u5c06\u63d0\u4f9b\u6807\u91cf\u8f93\u51fa\u7684\u663e\u793a\u6807\u7b7e\u3002","title":"test_on_batch"},{"location":"1-Models/2.model/#predict_on_batch","text":"predict_on_batch(x) \u8fd4\u56de\u4e00\u6279\u6837\u672c\u7684\u6a21\u578b\u9884\u6d4b\u503c\u3002 \u53c2\u6570 x : \u8f93\u5165\u6570\u636e\uff0cNumpy \u6570\u7ec4\u3002 \u8fd4\u56de \u9884\u6d4b\u503c\u7684 Numpy \u6570\u7ec4\uff08\u6216\u6570\u7ec4\u5217\u8868\uff09\u3002","title":"predict_on_batch"},{"location":"1-Models/2.model/#fit_generator","text":"fit_generator(generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0) \u4f7f\u7528 Python \u751f\u6210\u5668\uff08\u6216 Sequence \u5b9e\u4f8b\uff09\u9010\u6279\u751f\u6210\u7684\u6570\u636e\uff0c\u6309\u6279\u6b21\u8bad\u7ec3\u6a21\u578b\u3002 \u751f\u6210\u5668\u4e0e\u6a21\u578b\u5e76\u884c\u8fd0\u884c\uff0c\u4ee5\u63d0\u9ad8\u6548\u7387\u3002 \u4f8b\u5982\uff0c\u8fd9\u53ef\u4ee5\u8ba9\u4f60\u5728 CPU \u4e0a\u5bf9\u56fe\u50cf\u8fdb\u884c\u5b9e\u65f6\u6570\u636e\u589e\u5f3a\uff0c\u4ee5\u5728 GPU \u4e0a\u8bad\u7ec3\u6a21\u578b\u3002 keras.utils.Sequence \u7684\u4f7f\u7528\u53ef\u4ee5\u4fdd\u8bc1\u6570\u636e\u7684\u987a\u5e8f\uff0c \u4ee5\u53ca\u5f53 use_multiprocessing=True \u65f6 \uff0c\u4fdd\u8bc1\u6bcf\u4e2a\u8f93\u5165\u5728\u6bcf\u4e2a epoch \u53ea\u4f7f\u7528\u4e00\u6b21\u3002 \u53c2\u6570 generator : \u4e00\u4e2a\u751f\u6210\u5668\uff0c\u6216\u8005\u4e00\u4e2a Sequence ( keras.utils.Sequence ) \u5bf9\u8c61\u7684\u5b9e\u4f8b\uff0c \u4ee5\u5728\u4f7f\u7528\u591a\u8fdb\u7a0b\u65f6\u907f\u514d\u6570\u636e\u7684\u91cd\u590d\u3002 \u751f\u6210\u5668\u7684\u8f93\u51fa\u5e94\u8be5\u4e3a\u4ee5\u4e0b\u4e4b\u4e00\uff1a \u4e00\u4e2a (inputs, targets) \u5143\u7ec4 \u4e00\u4e2a (inputs, targets, sample_weights) \u5143\u7ec4\u3002 \u8fd9\u4e2a\u5143\u7ec4\uff08\u751f\u6210\u5668\u7684\u5355\u4e2a\u8f93\u51fa\uff09\u7ec4\u6210\u4e86\u5355\u4e2a\u7684 batch\u3002 \u56e0\u6b64\uff0c\u8fd9\u4e2a\u5143\u7ec4\u4e2d\u7684\u6240\u6709\u6570\u7ec4\u957f\u5ea6\u5fc5\u987b\u76f8\u540c\uff08\u4e0e\u8fd9\u4e00\u4e2a batch \u7684\u5927\u5c0f\u76f8\u7b49\uff09\u3002 \u4e0d\u540c\u7684 batch \u53ef\u80fd\u5927\u5c0f\u4e0d\u540c\u3002 \u4f8b\u5982\uff0c\u4e00\u4e2a epoch \u7684\u6700\u540e\u4e00\u4e2a batch \u5f80\u5f80\u6bd4\u5176\u4ed6 batch \u8981\u5c0f\uff0c \u5982\u679c\u6570\u636e\u96c6\u7684\u5c3a\u5bf8\u4e0d\u80fd\u88ab batch size \u6574\u9664\u3002 \u751f\u6210\u5668\u5c06\u65e0\u9650\u5730\u5728\u6570\u636e\u96c6\u4e0a\u5faa\u73af\u3002\u5f53\u8fd0\u884c\u5230\u7b2c steps_per_epoch \u65f6\uff0c\u8bb0\u4e00\u4e2a epoch \u7ed3\u675f\u3002 steps_per_epoch : \u5728\u58f0\u660e\u4e00\u4e2a epoch \u5b8c\u6210\u5e76\u5f00\u59cb\u4e0b\u4e00\u4e2a epoch \u4e4b\u524d\u4ece generator \u4ea7\u751f\u7684\u603b\u6b65\u6570\uff08\u6279\u6b21\u6837\u672c\uff09\u3002 \u5b83\u901a\u5e38\u5e94\u8be5\u7b49\u4e8e\u4f60\u7684\u6570\u636e\u96c6\u7684\u6837\u672c\u6570\u91cf\u9664\u4ee5\u6279\u91cf\u5927\u5c0f\u3002 \u5bf9\u4e8e Sequence \uff0c\u5b83\u662f\u53ef\u9009\u7684\uff1a\u5982\u679c\u672a\u6307\u5b9a\uff0c\u5c06\u4f7f\u7528 len(generator) \u4f5c\u4e3a\u6b65\u6570\u3002 epochs : \u6574\u6570\u3002\u8bad\u7ec3\u6a21\u578b\u7684\u8fed\u4ee3\u603b\u8f6e\u6570\u3002\u4e00\u4e2a epoch \u662f\u5bf9\u6240\u63d0\u4f9b\u7684\u6574\u4e2a\u6570\u636e\u7684\u4e00\u8f6e\u8fed\u4ee3\uff0c\u5982 steps_per_epoch \u6240\u5b9a\u4e49\u3002\u6ce8\u610f\uff0c\u4e0e initial_epoch \u4e00\u8d77\u4f7f\u7528\uff0cepoch \u5e94\u88ab\u7406\u89e3\u4e3a\u300c\u6700\u540e\u4e00\u8f6e\u300d\u3002\u6a21\u578b\u6ca1\u6709\u7ecf\u5386\u7531 epochs \u7ed9\u51fa\u7684\u591a\u6b21\u8fed\u4ee3\u7684\u8bad\u7ec3\uff0c\u800c\u4ec5\u4ec5\u662f\u76f4\u5230\u8fbe\u5230\u7d22\u5f15 epoch \u7684\u8f6e\u6b21\u3002 verbose : 0, 1 \u6216 2\u3002\u65e5\u5fd7\u663e\u793a\u6a21\u5f0f\u3002 0 = \u5b89\u9759\u6a21\u5f0f, 1 = \u8fdb\u5ea6\u6761, 2 = \u6bcf\u8f6e\u4e00\u884c\u3002 callbacks : keras.callbacks.Callback \u5b9e\u4f8b\u7684\u5217\u8868\u3002\u5728\u8bad\u7ec3\u65f6\u8c03\u7528\u7684\u4e00\u7cfb\u5217\u56de\u8c03\u51fd\u6570\u3002 validation_data : \u5b83\u53ef\u4ee5\u662f\u4ee5\u4e0b\u4e4b\u4e00\uff1a \u9a8c\u8bc1\u6570\u636e\u7684\u751f\u6210\u5668\u6216 Sequence \u5b9e\u4f8b \u4e00\u4e2a (inputs, targets) \u5143\u7ec4 \u4e00\u4e2a (inputs, targets, sample_weights) \u5143\u7ec4\u3002 \u5728\u6bcf\u4e2a epoch \u7ed3\u675f\u65f6\u8bc4\u4f30\u635f\u5931\u548c\u4efb\u4f55\u6a21\u578b\u6307\u6807\u3002\u8be5\u6a21\u578b\u4e0d\u4f1a\u5bf9\u6b64\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002 validation_steps : \u4ec5\u5f53 validation_data \u662f\u4e00\u4e2a\u751f\u6210\u5668\u65f6\u624d\u53ef\u7528\u3002 \u5728\u505c\u6b62\u524d generator \u751f\u6210\u7684\u603b\u6b65\u6570\uff08\u6837\u672c\u6279\u6570\uff09\u3002 \u5bf9\u4e8e Sequence \uff0c\u5b83\u662f\u53ef\u9009\u7684\uff1a\u5982\u679c\u672a\u6307\u5b9a\uff0c\u5c06\u4f7f\u7528 len(generator) \u4f5c\u4e3a\u6b65\u6570\u3002 class_weight : \u53ef\u9009\u7684\u5c06\u7c7b\u7d22\u5f15\uff08\u6574\u6570\uff09\u6620\u5c04\u5230\u6743\u91cd\uff08\u6d6e\u70b9\uff09\u503c\u7684\u5b57\u5178\uff0c\u7528\u4e8e\u52a0\u6743\u635f\u5931\u51fd\u6570\uff08\u4ec5\u5728\u8bad\u7ec3\u671f\u95f4\uff09\u3002 \u8fd9\u53ef\u4ee5\u7528\u6765\u544a\u8bc9\u6a21\u578b\u300c\u66f4\u591a\u5730\u5173\u6ce8\u300d\u6765\u81ea\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u7c7b\u7684\u6837\u672c\u3002 max_queue_size : \u6574\u6570\u3002\u751f\u6210\u5668\u961f\u5217\u7684\u6700\u5927\u5c3a\u5bf8\u3002 \u5982\u672a\u6307\u5b9a\uff0c max_queue_size \u5c06\u9ed8\u8ba4\u4e3a 10\u3002 workers : \u6574\u6570\u3002\u4f7f\u7528\u7684\u6700\u5927\u8fdb\u7a0b\u6570\u91cf\uff0c\u5982\u679c\u4f7f\u7528\u57fa\u4e8e\u8fdb\u7a0b\u7684\u591a\u7ebf\u7a0b\u3002 \u5982\u672a\u6307\u5b9a\uff0c workers \u5c06\u9ed8\u8ba4\u4e3a 1\u3002\u5982\u679c\u4e3a 0\uff0c\u5c06\u5728\u4e3b\u7ebf\u7a0b\u4e0a\u6267\u884c\u751f\u6210\u5668\u3002 use_multiprocessing : \u5e03\u5c14\u503c\u3002\u5982\u679c True\uff0c\u5219\u4f7f\u7528\u57fa\u4e8e\u8fdb\u7a0b\u7684\u591a\u7ebf\u7a0b\u3002 \u5982\u672a\u6307\u5b9a\uff0c use_multiprocessing \u5c06\u9ed8\u8ba4\u4e3a False\u3002 \u8bf7\u6ce8\u610f\uff0c\u7531\u4e8e\u6b64\u5b9e\u73b0\u4f9d\u8d56\u4e8e\u591a\u8fdb\u7a0b\uff0c\u6240\u4ee5\u4e0d\u5e94\u5c06\u4e0d\u53ef\u4f20\u9012\u7684\u53c2\u6570\u4f20\u9012\u7ed9\u751f\u6210\u5668\uff0c\u56e0\u4e3a\u5b83\u4eec\u4e0d\u80fd\u88ab\u8f7b\u6613\u5730\u4f20\u9012\u7ed9\u5b50\u8fdb\u7a0b\u3002 shuffle : \u662f\u5426\u5728\u6bcf\u8f6e\u8fed\u4ee3\u4e4b\u524d\u6253\u4e71 batch \u7684\u987a\u5e8f\u3002 \u53ea\u80fd\u4e0e Sequence (keras.utils.Sequence) \u5b9e\u4f8b\u540c\u7528\u3002 initial_epoch : \u5f00\u59cb\u8bad\u7ec3\u7684\u8f6e\u6b21\uff08\u6709\u52a9\u4e8e\u6062\u590d\u4e4b\u524d\u7684\u8bad\u7ec3\uff09\u3002 \u8fd4\u56de \u4e00\u4e2a History \u5bf9\u8c61\u3002\u5176 History.history \u5c5e\u6027\u662f\u8fde\u7eed epoch \u8bad\u7ec3\u635f\u5931\u548c\u8bc4\u4f30\u503c\uff0c\u4ee5\u53ca\u9a8c\u8bc1\u96c6\u635f\u5931\u548c\u8bc4\u4f30\u503c\u7684\u8bb0\u5f55\uff08\u5982\u679c\u9002\u7528\uff09\u3002 \u5f02\u5e38 ValueError : \u5982\u679c\u751f\u6210\u5668\u751f\u6210\u7684\u6570\u636e\u683c\u5f0f\u4e0d\u6b63\u786e\u3002 \u4f8b def generate_arrays_from_file(path): while True: with open(path) as f: for line in f: # \u4ece\u6587\u4ef6\u4e2d\u7684\u6bcf\u4e00\u884c\u751f\u6210\u8f93\u5165\u6570\u636e\u548c\u6807\u7b7e\u7684 numpy \u6570\u7ec4\uff0c x1, x2, y = process_line(line) yield ({'input_1': x1, 'input_2': x2}, {'output': y}) f.close() model.fit_generator(generate_arrays_from_file('/my_file.txt'), steps_per_epoch=10000, epochs=10)","title":"fit_generator"},{"location":"1-Models/2.model/#evaluate_generator","text":"evaluate_generator(generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0) \u5728\u6570\u636e\u751f\u6210\u5668\u4e0a\u8bc4\u4f30\u6a21\u578b\u3002 \u8fd9\u4e2a\u751f\u6210\u5668\u5e94\u8be5\u8fd4\u56de\u4e0e test_on_batch \u6240\u63a5\u6536\u7684\u540c\u6837\u7684\u6570\u636e\u3002 \u53c2\u6570 generator : \u4e00\u4e2a\u751f\u6210 (inputs, targets) \u6216 (inputs, targets, sample_weights) \u7684\u751f\u6210\u5668\uff0c \u6216\u4e00\u4e2a Sequence ( keras.utils.Sequence ) \u5bf9\u8c61\u7684\u5b9e\u4f8b\uff0c\u4ee5\u907f\u514d\u5728\u4f7f\u7528\u591a\u8fdb\u7a0b\u65f6\u6570\u636e\u7684\u91cd\u590d\u3002 steps : \u5728\u58f0\u660e\u4e00\u4e2a epoch \u5b8c\u6210\u5e76\u5f00\u59cb\u4e0b\u4e00\u4e2a epoch \u4e4b\u524d\u4ece generator \u4ea7\u751f\u7684\u603b\u6b65\u6570\uff08\u6279\u6b21\u6837\u672c\uff09\u3002 \u5b83\u901a\u5e38\u5e94\u8be5\u7b49\u4e8e\u4f60\u7684\u6570\u636e\u96c6\u7684\u6837\u672c\u6570\u91cf\u9664\u4ee5\u6279\u91cf\u5927\u5c0f\u3002 \u5bf9\u4e8e Sequence \uff0c\u5b83\u662f\u53ef\u9009\u7684\uff1a\u5982\u679c\u672a\u6307\u5b9a\uff0c\u5c06\u4f7f\u7528 len(generator) \u4f5c\u4e3a\u6b65\u6570\u3002 max_queue_size : \u751f\u6210\u5668\u961f\u5217\u7684\u6700\u5927\u5c3a\u5bf8\u3002 workers : \u6574\u6570\u3002\u4f7f\u7528\u7684\u6700\u5927\u8fdb\u7a0b\u6570\u91cf\uff0c\u5982\u679c\u4f7f\u7528\u57fa\u4e8e\u8fdb\u7a0b\u7684\u591a\u7ebf\u7a0b\u3002 \u5982\u672a\u6307\u5b9a\uff0c workers \u5c06\u9ed8\u8ba4\u4e3a 1\u3002\u5982\u679c\u4e3a 0\uff0c\u5c06\u5728\u4e3b\u7ebf\u7a0b\u4e0a\u6267\u884c\u751f\u6210\u5668\u3002 use_multiprocessing : \u5e03\u5c14\u503c\u3002\u5982\u679c True\uff0c\u5219\u4f7f\u7528\u57fa\u4e8e\u8fdb\u7a0b\u7684\u591a\u7ebf\u7a0b\u3002 \u8bf7\u6ce8\u610f\uff0c\u7531\u4e8e\u6b64\u5b9e\u73b0\u4f9d\u8d56\u4e8e\u591a\u8fdb\u7a0b\uff0c\u6240\u4ee5\u4e0d\u5e94\u5c06\u4e0d\u53ef\u4f20\u9012\u7684\u53c2\u6570\u4f20\u9012\u7ed9\u751f\u6210\u5668\uff0c\u56e0\u4e3a\u5b83\u4eec\u4e0d\u80fd\u88ab\u8f7b\u6613\u5730\u4f20\u9012\u7ed9\u5b50\u8fdb\u7a0b\u3002 verbose : \u65e5\u5fd7\u663e\u793a\u6a21\u5f0f\uff0c0 \u6216 1\u3002 \u8fd4\u56de \u6807\u91cf\u6d4b\u8bd5\u8bef\u5dee\uff08\u5982\u679c\u6a21\u578b\u53ea\u6709\u4e00\u4e2a\u8f93\u5165\u4e14\u6ca1\u6709\u8bc4\u4f30\u6807\u51c6\uff09\uff0c \u6216\u8005\u6807\u91cf\u7684\u5217\u8868\uff08\u5982\u679c\u6a21\u578b\u6709\u591a\u4e2a\u8f93\u51fa \u548c/\u6216 \u8bc4\u4f30\u6807\u51c6\uff09\u3002 \u5c5e\u6027 model.metrics_names \u5c06\u63d0\u4f9b\u6807\u91cf\u8f93\u51fa\u7684\u663e\u793a\u6807\u7b7e\u3002 \u5f02\u5e38 ValueError : \u5982\u679c\u751f\u6210\u5668\u751f\u6210\u7684\u6570\u636e\u683c\u5f0f\u4e0d\u6b63\u786e\u3002","title":"evaluate_generator"},{"location":"1-Models/2.model/#predict_generator","text":"predict_generator(generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0) \u4e3a\u6765\u81ea\u6570\u636e\u751f\u6210\u5668\u7684\u8f93\u5165\u6837\u672c\u751f\u6210\u9884\u6d4b\u3002 \u8fd9\u4e2a\u751f\u6210\u5668\u5e94\u8be5\u8fd4\u56de\u4e0e predict_on_batch \u6240\u63a5\u6536\u7684\u540c\u6837\u7684\u6570\u636e\u3002 \u53c2\u6570 generator : \u751f\u6210\u5668\uff0c\u8fd4\u56de\u6279\u91cf\u8f93\u5165\u6837\u672c\uff0c \u6216\u4e00\u4e2a Sequence ( keras.utils.Sequence ) \u5bf9\u8c61\u7684\u5b9e\u4f8b\uff0c\u4ee5\u907f\u514d\u5728\u4f7f\u7528\u591a\u8fdb\u7a0b\u65f6\u6570\u636e\u7684\u91cd\u590d\u3002 steps : \u5728\u58f0\u660e\u4e00\u4e2a epoch \u5b8c\u6210\u5e76\u5f00\u59cb\u4e0b\u4e00\u4e2a epoch \u4e4b\u524d\u4ece generator \u4ea7\u751f\u7684\u603b\u6b65\u6570\uff08\u6279\u6b21\u6837\u672c\uff09\u3002 \u5b83\u901a\u5e38\u5e94\u8be5\u7b49\u4e8e\u4f60\u7684\u6570\u636e\u96c6\u7684\u6837\u672c\u6570\u91cf\u9664\u4ee5\u6279\u91cf\u5927\u5c0f\u3002 \u5bf9\u4e8e Sequence \uff0c\u5b83\u662f\u53ef\u9009\u7684\uff1a\u5982\u679c\u672a\u6307\u5b9a\uff0c\u5c06\u4f7f\u7528 len(generator) \u4f5c\u4e3a\u6b65\u6570\u3002 max_queue_size : \u751f\u6210\u5668\u961f\u5217\u7684\u6700\u5927\u5c3a\u5bf8\u3002 workers : \u6574\u6570\u3002\u4f7f\u7528\u7684\u6700\u5927\u8fdb\u7a0b\u6570\u91cf\uff0c\u5982\u679c\u4f7f\u7528\u57fa\u4e8e\u8fdb\u7a0b\u7684\u591a\u7ebf\u7a0b\u3002 \u5982\u672a\u6307\u5b9a\uff0c workers \u5c06\u9ed8\u8ba4\u4e3a 1\u3002\u5982\u679c\u4e3a 0\uff0c\u5c06\u5728\u4e3b\u7ebf\u7a0b\u4e0a\u6267\u884c\u751f\u6210\u5668\u3002 use_multiprocessing : \u5982\u679c True\uff0c\u5219\u4f7f\u7528\u57fa\u4e8e\u8fdb\u7a0b\u7684\u591a\u7ebf\u7a0b\u3002 \u8bf7\u6ce8\u610f\uff0c\u7531\u4e8e\u6b64\u5b9e\u73b0\u4f9d\u8d56\u4e8e\u591a\u8fdb\u7a0b\uff0c\u6240\u4ee5\u4e0d\u5e94\u5c06\u4e0d\u53ef\u4f20\u9012\u7684\u53c2\u6570\u4f20\u9012\u7ed9\u751f\u6210\u5668\uff0c\u56e0\u4e3a\u5b83\u4eec\u4e0d\u80fd\u88ab\u8f7b\u6613\u5730\u4f20\u9012\u7ed9\u5b50\u8fdb\u7a0b\u3002 verbose : \u65e5\u5fd7\u663e\u793a\u6a21\u5f0f\uff0c0 \u6216 1\u3002 \u8fd4\u56de \u9884\u6d4b\u503c\u7684 Numpy \u6570\u7ec4\uff08\u6216\u6570\u7ec4\u5217\u8868\uff09\u3002 \u5f02\u5e38 ValueError : \u5982\u679c\u751f\u6210\u5668\u751f\u6210\u7684\u6570\u636e\u683c\u5f0f\u4e0d\u6b63\u786e\u3002","title":"predict_generator"},{"location":"1-Models/2.model/#get_layer","text":"get_layer(self, name=None, index=None) \u6839\u636e\u540d\u79f0\uff08\u552f\u4e00\uff09\u6216\u7d22\u5f15\u503c\u67e5\u627e\u7f51\u7edc\u5c42\u3002 \u5982\u679c\u540c\u65f6\u63d0\u4f9b\u4e86 name \u548c index \uff0c\u5219 index \u5c06\u4f18\u5148\u3002 \u7d22\u5f15\u503c\u6765\u81ea\u4e8e\u6c34\u5e73\u56fe\u904d\u5386\u7684\u987a\u5e8f\uff08\u81ea\u4e0b\u800c\u4e0a\uff09\u3002 \u53c2\u6570 name : \u5b57\u7b26\u4e32\uff0c\u5c42\u7684\u540d\u5b57\u3002 index : \u6574\u6570\uff0c\u5c42\u7684\u7d22\u5f15\u3002 \u8fd4\u56de \u4e00\u4e2a\u5c42\u5b9e\u4f8b\u3002 \u5f02\u5e38 ValueError : \u5982\u679c\u5c42\u7684\u540d\u79f0\u6216\u7d22\u5f15\u4e0d\u6b63\u786e\u3002","title":"get_layer"},{"location":"2-Layers/0.about-keras-layers/","text":"\u5173\u4e8e Keras \u7f51\u7edc\u5c42 \u6240\u6709 Keras \u7f51\u7edc\u5c42\u90fd\u6709\u5f88\u591a\u5171\u540c\u7684\u51fd\u6570\uff1a layer.get_weights() : \u4ee5\u542b\u6709Numpy\u77e9\u9635\u7684\u5217\u8868\u5f62\u5f0f\u8fd4\u56de\u5c42\u7684\u6743\u91cd\u3002 layer.set_weights(weights) : \u4ece\u542b\u6709Numpy\u77e9\u9635\u7684\u5217\u8868\u4e2d\u8bbe\u7f6e\u5c42\u7684\u6743\u91cd\uff08\u4e0e get_weights \u7684\u8f93\u51fa\u5f62\u72b6\u76f8\u540c\uff09\u3002 layer.get_config() : \u8fd4\u56de\u5305\u542b\u5c42\u914d\u7f6e\u7684\u5b57\u5178\u3002\u6b64\u56fe\u5c42\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u91cd\u7f6e\uff1a layer = Dense(32) config = layer.get_config() reconstructed_layer = Dense.from_config(config) \u6216: from keras import layers config = layer.get_config() layer = layers.deserialize({'class_name': layer.__class__.__name__, 'config': config}) \u5982\u679c\u4e00\u4e2a\u5c42\u5177\u6709\u5355\u4e2a\u8282\u70b9 (i.e. \u5982\u679c\u5b83\u4e0d\u662f\u5171\u4eab\u5c42), \u4f60\u53ef\u4ee5\u5f97\u5230\u5b83\u7684\u8f93\u5165\u5f20\u91cf\u3001\u8f93\u51fa\u5f20\u91cf\u3001\u8f93\u5165\u5c3a\u5bf8\u548c\u8f93\u51fa\u5c3a\u5bf8: layer.input layer.output layer.input_shape layer.output_shape \u5982\u679c\u5c42\u6709\u591a\u4e2a\u8282\u70b9 (\u53c2\u89c1: \u5c42\u8282\u70b9\u548c\u5171\u4eab\u5c42\u7684\u6982\u5ff5 ), \u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u51fd\u6570: layer.get_input_at(node_index) layer.get_output_at(node_index) layer.get_input_shape_at(node_index) layer.get_output_shape_at(node_index)","title":"\u5173\u4e8e Keras \u7f51\u7edc\u5c42"},{"location":"2-Layers/0.about-keras-layers/#keras","text":"\u6240\u6709 Keras \u7f51\u7edc\u5c42\u90fd\u6709\u5f88\u591a\u5171\u540c\u7684\u51fd\u6570\uff1a layer.get_weights() : \u4ee5\u542b\u6709Numpy\u77e9\u9635\u7684\u5217\u8868\u5f62\u5f0f\u8fd4\u56de\u5c42\u7684\u6743\u91cd\u3002 layer.set_weights(weights) : \u4ece\u542b\u6709Numpy\u77e9\u9635\u7684\u5217\u8868\u4e2d\u8bbe\u7f6e\u5c42\u7684\u6743\u91cd\uff08\u4e0e get_weights \u7684\u8f93\u51fa\u5f62\u72b6\u76f8\u540c\uff09\u3002 layer.get_config() : \u8fd4\u56de\u5305\u542b\u5c42\u914d\u7f6e\u7684\u5b57\u5178\u3002\u6b64\u56fe\u5c42\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u91cd\u7f6e\uff1a layer = Dense(32) config = layer.get_config() reconstructed_layer = Dense.from_config(config) \u6216: from keras import layers config = layer.get_config() layer = layers.deserialize({'class_name': layer.__class__.__name__, 'config': config}) \u5982\u679c\u4e00\u4e2a\u5c42\u5177\u6709\u5355\u4e2a\u8282\u70b9 (i.e. \u5982\u679c\u5b83\u4e0d\u662f\u5171\u4eab\u5c42), \u4f60\u53ef\u4ee5\u5f97\u5230\u5b83\u7684\u8f93\u5165\u5f20\u91cf\u3001\u8f93\u51fa\u5f20\u91cf\u3001\u8f93\u5165\u5c3a\u5bf8\u548c\u8f93\u51fa\u5c3a\u5bf8: layer.input layer.output layer.input_shape layer.output_shape \u5982\u679c\u5c42\u6709\u591a\u4e2a\u8282\u70b9 (\u53c2\u89c1: \u5c42\u8282\u70b9\u548c\u5171\u4eab\u5c42\u7684\u6982\u5ff5 ), \u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u51fd\u6570: layer.get_input_at(node_index) layer.get_output_at(node_index) layer.get_input_shape_at(node_index) layer.get_output_shape_at(node_index)","title":"\u5173\u4e8e Keras \u7f51\u7edc\u5c42"},{"location":"2-Layers/1.core/","text":"\u6838\u5fc3\u7f51\u7edc\u5c42 [source] Dense keras.layers.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) \u5c31\u662f\u4f60\u5e38\u7528\u7684\u7684\u5168\u8fde\u63a5\u5c42\u3002 Dense \u5b9e\u73b0\u4ee5\u4e0b\u64cd\u4f5c\uff1a output = activation(dot(input, kernel) + bias) \u5176\u4e2d activation \u662f\u6309\u9010\u4e2a\u5143\u7d20\u8ba1\u7b97\u7684\u6fc0\u6d3b\u51fd\u6570\uff0c kernel \u662f\u7531\u7f51\u7edc\u5c42\u521b\u5efa\u7684\u6743\u503c\u77e9\u9635\uff0c\u4ee5\u53ca bias \u662f\u5176\u521b\u5efa\u7684\u504f\u7f6e\u5411\u91cf (\u53ea\u5728 use_bias \u4e3a True \u65f6\u624d\u6709\u7528)\u3002 \u6ce8\u610f : \u5982\u679c\u8be5\u5c42\u7684\u8f93\u5165\u7684\u79e9\u5927\u4e8e 2\uff0c\u90a3\u4e48\u5b83\u9996\u5148\u88ab\u5c55\u5e73\u7136\u540e \u518d\u8ba1\u7b97\u4e0e kernel \u7684\u70b9\u4e58\u3002 \u4f8b # \u4f5c\u4e3a Sequential \u6a21\u578b\u7684\u7b2c\u4e00\u5c42 model = Sequential() model.add(Dense(32, input_shape=(16,))) # \u73b0\u5728\u6a21\u578b\u5c31\u4f1a\u4ee5\u5c3a\u5bf8\u4e3a (*, 16) \u7684\u6570\u7ec4\u4f5c\u4e3a\u8f93\u5165\uff0c # \u5176\u8f93\u51fa\u6570\u7ec4\u7684\u5c3a\u5bf8\u4e3a (*, 32) # \u5728\u7b2c\u4e00\u5c42\u4e4b\u540e\uff0c\u4f60\u5c31\u4e0d\u518d\u9700\u8981\u6307\u5b9a\u8f93\u5165\u7684\u5c3a\u5bf8\u4e86\uff1a model.add(Dense(32)) \u53c2\u6570 units : \u6b63\u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7ef4\u5ea6\u3002 activation : \u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u82e5\u4e0d\u6307\u5b9a\uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373\uff0c\u300c\u7ebf\u6027\u300d\u6fc0\u6d3b: a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 bias_initializer : \u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (see initializers ). kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u7684\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u7684\u8f93\u51fa\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u5b83\u7684 \"activation\")\u3002 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 \u8f93\u5165\u5c3a\u5bf8 nD \u5f20\u91cf\uff0c\u5c3a\u5bf8: (batch_size, ..., input_dim) \u3002 \u6700\u5e38\u89c1\u7684\u60c5\u51b5\u662f\u4e00\u4e2a\u5c3a\u5bf8\u4e3a (batch_size, input_dim) \u7684 2D \u8f93\u5165\u3002 \u8f93\u51fa\u5c3a\u5bf8 nD \u5f20\u91cf\uff0c\u5c3a\u5bf8: (batch_size, ..., units) \u3002 \u4f8b\u5982\uff0c\u5bf9\u4e8e\u5c3a\u5bf8\u4e3a (batch_size, input_dim) \u7684 2D \u8f93\u5165\uff0c \u8f93\u51fa\u7684\u5c3a\u5bf8\u4e3a (batch_size, units) \u3002 [source] Activation keras.layers.Activation(activation) \u5c06\u6fc0\u6d3b\u51fd\u6570\u5e94\u7528\u4e8e\u8f93\u51fa\u3002 \u53c2\u6570 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570\u7684\u540d\u79f0 (\u8be6\u89c1: activations )\uff0c \u6216\u8005\u9009\u62e9\u4e00\u4e2a Theano \u6216 TensorFlow \u64cd\u4f5c\u3002 \u8f93\u5165\u5c3a\u5bf8 \u4efb\u610f\u5c3a\u5bf8\u3002 \u5f53\u4f7f\u7528\u6b64\u5c42\u4f5c\u4e3a\u6a21\u578b\u4e2d\u7684\u7b2c\u4e00\u5c42\u65f6\uff0c \u4f7f\u7528\u53c2\u6570 input_shape \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u62ec\u6837\u672c\u6570\u7684\u8f74\uff09\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u76f8\u540c\u3002 [source] Dropout keras.layers.Dropout(rate, noise_shape=None, seed=None) \u5c06 Dropout \u5e94\u7528\u4e8e\u8f93\u5165\u3002 Dropout \u5305\u62ec\u5728\u8bad\u7ec3\u4e2d\u6bcf\u6b21\u66f4\u65b0\u65f6\uff0c \u5c06\u8f93\u5165\u5355\u5143\u7684\u6309\u6bd4\u7387\u968f\u673a\u8bbe\u7f6e\u4e3a 0\uff0c \u8fd9\u6709\u52a9\u4e8e\u9632\u6b62\u8fc7\u62df\u5408\u3002 \u53c2\u6570 rate : \u5728 0 \u548c 1 \u4e4b\u95f4\u6d6e\u52a8\u3002\u9700\u8981\u4e22\u5f03\u7684\u8f93\u5165\u6bd4\u4f8b\u3002 noise_shape : 1D \u6574\u6570\u5f20\u91cf\uff0c \u8868\u793a\u5c06\u4e0e\u8f93\u5165\u76f8\u4e58\u7684\u4e8c\u8fdb\u5236 dropout \u63a9\u5c42\u7684\u5f62\u72b6\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u4f60\u7684\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch_size, timesteps, features) \uff0c\u7136\u540e \u4f60\u5e0c\u671b dropout \u63a9\u5c42\u5728\u6240\u6709\u65f6\u95f4\u6b65\u90fd\u662f\u4e00\u6837\u7684\uff0c \u4f60\u53ef\u4ee5\u4f7f\u7528 noise_shape=(batch_size, 1, features) \u3002 seed : \u4e00\u4e2a\u4f5c\u4e3a\u968f\u673a\u79cd\u5b50\u7684 Python \u6574\u6570\u3002 \u53c2\u8003\u6587\u732e Dropout: A Simple Way to Prevent Neural Networks from Overfitting [source] Flatten keras.layers.Flatten(data_format=None) \u5c06\u8f93\u5165\u5c55\u5e73\u3002\u4e0d\u5f71\u54cd\u6279\u91cf\u5927\u5c0f\u3002 \u53c2\u6570 data_format \uff1a\u4e00\u4e2a\u5b57\u7b26\u4e32\uff0c\u5176\u503c\u4e3a channels_last \uff08\u9ed8\u8ba4\u503c\uff09\u6216\u8005 channels_first \u3002\u5b83\u8868\u660e\u8f93\u5165\u7684\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002\u6b64\u53c2\u6570\u7684\u76ee\u7684\u662f\u5f53\u6a21\u578b\u4ece\u4e00\u79cd\u6570\u636e\u683c\u5f0f\u5207\u6362\u5230\u53e6\u4e00\u79cd\u6570\u636e\u683c\u5f0f\u65f6\u4fdd\u7559\u6743\u91cd\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u7740\u5c3a\u5bf8\u4e3a (batch, ..., channels) \u7684\u8f93\u5165\uff0c\u800c channels_first \u5bf9\u5e94\u7740\u5c3a\u5bf8\u4e3a (batch, channels, ...) \u7684\u8f93\u5165\u3002\u9ed8\u8ba4\u4e3a image_data_format \u7684\u503c\uff0c\u4f60\u53ef\u4ee5\u5728 Keras \u7684\u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d\u627e\u5230\u5b83\u3002\u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u8fc7\u5b83\uff0c\u90a3\u4e48\u5b83\u5c06\u662f channels_last \u4f8b model = Sequential() model.add(Conv2D(64, (3, 3), input_shape=(3, 32, 32), padding='same',)) # \u73b0\u5728\uff1amodel.output_shape == (None, 64, 32, 32) model.add(Flatten()) # \u73b0\u5728\uff1amodel.output_shape == (None, 65536) [source] Input keras.engine.input_layer.Input() Input() \u7528\u4e8e\u5b9e\u4f8b\u5316 Keras \u5f20\u91cf\u3002 Keras \u5f20\u91cf\u662f\u5e95\u5c42\u540e\u7aef(Theano, TensorFlow \u6216 CNTK) \u7684\u5f20\u91cf\u5bf9\u8c61\uff0c\u6211\u4eec\u589e\u52a0\u4e86\u4e00\u4e9b\u7279\u6027\uff0c\u4f7f\u5f97\u80fd\u591f\u901a\u8fc7\u4e86\u89e3\u6a21\u578b\u7684\u8f93\u5165 \u548c\u8f93\u51fa\u6765\u6784\u5efa Keras \u6a21\u578b\u3002 \u4f8b\u5982\uff0c\u5982\u679c a, b \u548c c \u90fd\u662f Keras \u5f20\u91cf\uff0c \u90a3\u4e48\u4ee5\u4e0b\u64cd\u4f5c\u662f\u53ef\u884c\u7684\uff1a model = Model(input=[a, b], output=c) \u6dfb\u52a0\u7684 Keras \u5c5e\u6027\u662f\uff1a _keras_shape : \u901a\u8fc7 Keras \u7aef\u7684\u5c3a\u5bf8\u63a8\u7406 \u8fdb\u884c\u4f20\u64ad\u7684\u6574\u6570\u5c3a\u5bf8\u5143\u7ec4\u3002 _keras_history : \u5e94\u7528\u4e8e\u5f20\u91cf\u7684\u6700\u540e\u4e00\u5c42\u3002 \u6574\u4e2a\u7f51\u7edc\u5c42\u8ba1\u7b97\u56fe\u53ef\u4ee5\u9012\u5f52\u5730\u4ece\u8be5\u5c42\u4e2d\u68c0\u7d22\u3002 \u53c2\u6570 shape : \u4e00\u4e2a\u5c3a\u5bf8\u5143\u7ec4\uff08\u6574\u6570\uff09\uff0c\u4e0d\u5305\u542b\u6279\u91cf\u5927\u5c0f\u3002 \u4f8b\u5982\uff0c shape=(32,) \u8868\u660e\u671f\u671b\u7684\u8f93\u5165\u662f\u6309\u6279\u6b21\u7684 32 \u7ef4\u5411\u91cf\u3002 batch_shape : \u4e00\u4e2a\u5c3a\u5bf8\u5143\u7ec4\uff08\u6574\u6570\uff09\uff0c\u5305\u542b\u6279\u91cf\u5927\u5c0f\u3002 \u4f8b\u5982\uff0c batch_shape=(10, 32) \u8868\u660e\u671f\u671b\u7684\u8f93\u5165\u662f 10 \u4e2a 32 \u7ef4\u5411\u91cf\u3002 batch_shape=(None, 32) \u8868\u660e\u4efb\u610f\u6279\u6b21\u5927\u5c0f\u7684 32 \u7ef4\u5411\u91cf\u3002 name : \u4e00\u4e2a\u53ef\u9009\u7684\u5c42\u7684\u540d\u79f0\u7684\u5b57\u7b26\u4e32\u3002 \u5728\u4e00\u4e2a\u6a21\u578b\u4e2d\u5e94\u8be5\u662f\u552f\u4e00\u7684\uff08\u4e0d\u53ef\u4ee5\u91cd\u7528\u4e00\u4e2a\u540d\u5b57\u4e24\u6b21\uff09\u3002 \u5982\u672a\u63d0\u4f9b\uff0c\u5c06\u81ea\u52a8\u751f\u6210\u3002 dtype : \u8f93\u5165\u6240\u671f\u671b\u7684\u6570\u636e\u7c7b\u578b\uff0c\u5b57\u7b26\u4e32\u8868\u793a ( float32 , float64 , int32 ...) sparse : \u4e00\u4e2a\u5e03\u5c14\u503c\uff0c\u6307\u660e\u9700\u8981\u521b\u5efa\u7684\u5360\u4f4d\u7b26\u662f\u5426\u662f\u7a00\u758f\u7684\u3002 tensor : \u53ef\u9009\u7684\u53ef\u5c01\u88c5\u5230 Input \u5c42\u7684\u73b0\u6709\u5f20\u91cf\u3002 \u5982\u679c\u8bbe\u5b9a\u4e86\uff0c\u90a3\u4e48\u8fd9\u4e2a\u5c42\u5c06\u4e0d\u4f1a\u521b\u5efa\u5360\u4f4d\u7b26\u5f20\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 \u4f8b # \u8fd9\u662f Keras \u4e2d\u7684\u4e00\u4e2a\u903b\u8f91\u56de\u5f52 x = Input(shape=(32,)) y = Dense(16, activation='softmax')(x) model = Model(x, y) [source] Reshape keras.layers.Reshape(target_shape) \u5c06\u8f93\u5165\u91cd\u65b0\u8c03\u6574\u4e3a\u7279\u5b9a\u7684\u5c3a\u5bf8\u3002 \u53c2\u6570 target_shape : \u76ee\u6807\u5c3a\u5bf8\u3002\u6574\u6570\u5143\u7ec4\u3002 \u4e0d\u5305\u542b\u8868\u793a\u6279\u91cf\u7684\u8f74\u3002 \u8f93\u5165\u5c3a\u5bf8 \u4efb\u610f\uff0c\u5c3d\u7ba1\u8f93\u5165\u5c3a\u5bf8\u4e2d\u7684\u6240\u6709\u7ef4\u5ea6\u5fc5\u987b\u662f\u56fa\u5b9a\u7684\u3002 \u5f53\u4f7f\u7528\u6b64\u5c42\u4f5c\u4e3a\u6a21\u578b\u4e2d\u7684\u7b2c\u4e00\u5c42\u65f6\uff0c \u4f7f\u7528\u53c2\u6570 input_shape \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u62ec\u6837\u672c\u6570\u7684\u8f74\uff09\u3002 \u8f93\u51fa\u5c3a\u5bf8 (batch_size,) + target_shape \u4f8b # \u4f5c\u4e3a Sequential \u6a21\u578b\u7684\u7b2c\u4e00\u5c42 model = Sequential() model.add(Reshape((3, 4), input_shape=(12,))) # \u73b0\u5728\uff1amodel.output_shape == (None, 3, 4) # \u6ce8\u610f\uff1a `None` \u662f\u6279\u8868\u793a\u7684\u7ef4\u5ea6 # \u4f5c\u4e3a Sequential \u6a21\u578b\u7684\u4e2d\u95f4\u5c42 model.add(Reshape((6, 2))) # \u73b0\u5728\uff1a model.output_shape == (None, 6, 2) # \u8fd8\u652f\u6301\u4f7f\u7528 `-1` \u8868\u793a\u7ef4\u5ea6\u7684\u5c3a\u5bf8\u63a8\u65ad model.add(Reshape((-1, 2, 2))) # \u73b0\u5728\uff1a model.output_shape == (None, 3, 2, 2) [source] Permute keras.layers.Permute(dims) \u6839\u636e\u7ed9\u5b9a\u7684\u6a21\u5f0f\u7f6e\u6362\u8f93\u5165\u7684\u7ef4\u5ea6\u3002 \u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u5f88\u6709\u7528\uff0c\u4f8b\u5982\u5c06 RNN \u548c CNN \u8fde\u63a5\u5728\u4e00\u8d77\u3002 \u4f8b model = Sequential() model.add(Permute((2, 1), input_shape=(10, 64))) # \u73b0\u5728\uff1a model.output_shape == (None, 64, 10) # \u6ce8\u610f\uff1a `None` \u662f\u6279\u8868\u793a\u7684\u7ef4\u5ea6 \u53c2\u6570 dims : \u6574\u6570\u5143\u7ec4\u3002\u7f6e\u6362\u6a21\u5f0f\uff0c\u4e0d\u5305\u542b\u6837\u672c\u7ef4\u5ea6\u3002 \u7d22\u5f15\u4ece 1 \u5f00\u59cb\u3002 \u4f8b\u5982, (2, 1) \u7f6e\u6362\u8f93\u5165\u7684\u7b2c\u4e00\u548c\u7b2c\u4e8c\u4e2a\u7ef4\u5ea6\u3002 \u8f93\u5165\u5c3a\u5bf8 \u4efb\u610f\u3002\u5f53\u4f7f\u7528\u6b64\u5c42\u4f5c\u4e3a\u6a21\u578b\u4e2d\u7684\u7b2c\u4e00\u5c42\u65f6\uff0c \u4f7f\u7528\u53c2\u6570 input_shape \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u62ec\u6837\u672c\u6570\u7684\u8f74\uff09\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u5c3a\u5bf8\u76f8\u540c\uff0c\u4f46\u662f\u7ef4\u5ea6\u6839\u636e\u6307\u5b9a\u7684\u6a21\u5f0f\u91cd\u65b0\u6392\u5217\u3002 [source] RepeatVector keras.layers.RepeatVector(n) \u5c06\u8f93\u5165\u91cd\u590d n \u6b21\u3002 \u4f8b model = Sequential() model.add(Dense(32, input_dim=32)) # \u73b0\u5728\uff1a model.output_shape == (None, 32) # \u6ce8\u610f\uff1a `None` \u662f\u6279\u8868\u793a\u7684\u7ef4\u5ea6 model.add(RepeatVector(3)) # \u73b0\u5728\uff1a model.output_shape == (None, 3, 32) \u53c2\u6570 n : \u6574\u6570\uff0c\u91cd\u590d\u6b21\u6570\u3002 \u8f93\u5165\u5c3a\u5bf8 2D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (num_samples, features) \u3002 \u8f93\u51fa\u5c3a\u5bf8 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (num_samples, n, features) \u3002 [source] Lambda keras.layers.Lambda(function, output_shape=None, mask=None, arguments=None) \u5c06\u4efb\u610f\u8868\u8fbe\u5f0f\u5c01\u88c5\u4e3a Layer \u5bf9\u8c61\u3002 \u4f8b # \u6dfb\u52a0\u4e00\u4e2a x -> x^2 \u5c42 model.add(Lambda(lambda x: x ** 2)) # \u6dfb\u52a0\u4e00\u4e2a\u7f51\u7edc\u5c42\uff0c\u8fd4\u56de\u8f93\u5165\u7684\u6b63\u6570\u90e8\u5206 # \u4e0e\u8d1f\u6570\u90e8\u5206\u7684\u53cd\u9762\u7684\u8fde\u63a5 def antirectifier(x): x -= K.mean(x, axis=1, keepdims=True) x = K.l2_normalize(x, axis=1) pos = K.relu(x) neg = K.relu(-x) return K.concatenate([pos, neg], axis=1) def antirectifier_output_shape(input_shape): shape = list(input_shape) assert len(shape) == 2 # only valid for 2D tensors shape[-1] *= 2 return tuple(shape) model.add(Lambda(antirectifier, output_shape=antirectifier_output_shape)) \u53c2\u6570 function : \u9700\u8981\u5c01\u88c5\u7684\u51fd\u6570\u3002 \u5c06\u8f93\u5165\u5f20\u91cf\u4f5c\u4e3a\u7b2c\u4e00\u4e2a\u53c2\u6570\u3002 output_shape : \u9884\u671f\u7684\u51fd\u6570\u8f93\u51fa\u5c3a\u5bf8\u3002 \u53ea\u5728\u4f7f\u7528 Theano \u65f6\u6709\u610f\u4e49\u3002 \u53ef\u4ee5\u662f\u5143\u7ec4\u6216\u8005\u51fd\u6570\u3002 \u5982\u679c\u662f\u5143\u7ec4\uff0c\u5b83\u53ea\u6307\u5b9a\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\uff1b \u6837\u672c\u7ef4\u5ea6\u5047\u8bbe\u4e0e\u8f93\u5165\u76f8\u540c\uff1a output_shape = (input_shape[0], ) + output_shape \u6216\u8005\uff0c\u8f93\u5165\u662f None \u4e14\u6837\u672c\u7ef4\u5ea6\u4e5f\u662f None \uff1a output_shape = (None, ) + output_shape \u5982\u679c\u662f\u51fd\u6570\uff0c\u5b83\u6307\u5b9a\u6574\u4e2a\u5c3a\u5bf8\u4e3a\u8f93\u5165\u5c3a\u5bf8\u7684\u4e00\u4e2a\u51fd\u6570\uff1a output_shape = f(input_shape) arguments : \u53ef\u9009\u7684\u9700\u8981\u4f20\u9012\u7ed9\u51fd\u6570\u7684\u5173\u952e\u5b57\u53c2\u6570\u3002 \u8f93\u5165\u5c3a\u5bf8 \u4efb\u610f\u3002\u5f53\u4f7f\u7528\u6b64\u5c42\u4f5c\u4e3a\u6a21\u578b\u4e2d\u7684\u7b2c\u4e00\u5c42\u65f6\uff0c \u4f7f\u7528\u53c2\u6570 input_shape \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u62ec\u6837\u672c\u6570\u7684\u8f74\uff09\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u7531 output_shape \u53c2\u6570\u6307\u5b9a (\u6216\u8005\u5728\u4f7f\u7528 TensorFlow \u65f6\uff0c\u81ea\u52a8\u63a8\u7406\u5f97\u5230)\u3002 [source] ActivityRegularization keras.layers.ActivityRegularization(l1=0.0, l2=0.0) \u7f51\u7edc\u5c42\uff0c\u5bf9\u57fa\u4e8e\u4ee3\u4ef7\u51fd\u6570\u7684\u8f93\u5165\u6d3b\u52a8\u5e94\u7528\u4e00\u4e2a\u66f4\u65b0\u3002 \u53c2\u6570 l1 : L1 \u6b63\u5219\u5316\u56e0\u5b50 (\u6b63\u6570\u6d6e\u70b9\u578b)\u3002 l2 : L2 \u6b63\u5219\u5316\u56e0\u5b50 (\u6b63\u6570\u6d6e\u70b9\u578b)\u3002 \u8f93\u5165\u5c3a\u5bf8 \u4efb\u610f\u3002\u5f53\u4f7f\u7528\u6b64\u5c42\u4f5c\u4e3a\u6a21\u578b\u4e2d\u7684\u7b2c\u4e00\u5c42\u65f6\uff0c \u4f7f\u7528\u53c2\u6570 input_shape \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u62ec\u6837\u672c\u6570\u7684\u8f74\uff09\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u76f8\u540c\u3002 [source] Masking keras.layers.Masking(mask_value=0.0) \u4f7f\u7528\u8986\u76d6\u503c\u8986\u76d6\u5e8f\u5217\uff0c\u4ee5\u8df3\u8fc7\u65f6\u95f4\u6b65\u3002 \u5bf9\u4e8e\u8f93\u5165\u5f20\u91cf\u7684\u6bcf\u4e00\u4e2a\u65f6\u95f4\u6b65\uff08\u5f20\u91cf\u7684\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\uff09\uff0c \u5982\u679c\u6240\u6709\u65f6\u95f4\u6b65\u4e2d\u8f93\u5165\u5f20\u91cf\u7684\u503c\u4e0e mask_value \u76f8\u7b49\uff0c \u90a3\u4e48\u8fd9\u4e2a\u65f6\u95f4\u6b65\u5c06\u5728\u6240\u6709\u4e0b\u6e38\u5c42\u88ab\u8986\u76d6 (\u8df3\u8fc7) \uff08\u53ea\u8981\u5b83\u4eec\u652f\u6301\u8986\u76d6\uff09\u3002 \u5982\u679c\u4efb\u4f55\u4e0b\u6e38\u5c42\u4e0d\u652f\u6301\u8986\u76d6\u4f46\u4ecd\u7136\u6536\u5230\u6b64\u7c7b\u8f93\u5165\u8986\u76d6\u4fe1\u606f\uff0c\u4f1a\u5f15\u53d1\u5f02\u5e38\u3002 \u4f8b \u8003\u8651\u5c06\u8981\u5582\u5165\u4e00\u4e2a LSTM \u5c42\u7684 Numpy \u77e9\u9635 x \uff0c \u5c3a\u5bf8\u4e3a (samples, timesteps, features) \u3002 \u4f60\u60f3\u8981\u8986\u76d6\u65f6\u95f4\u6b65 #3 \u548c #5\uff0c\u56e0\u4e3a\u4f60\u7f3a\u4e4f\u8fd9\u51e0\u4e2a \u65f6\u95f4\u6b65\u7684\u6570\u636e\u3002\u4f60\u53ef\u4ee5\uff1a \u8bbe\u7f6e x[:, 3, :] = 0. \u4ee5\u53ca x[:, 5, :] = 0. \u5728 LSTM \u5c42\u4e4b\u524d\uff0c\u63d2\u5165\u4e00\u4e2a mask_value=0 \u7684 Masking \u5c42\uff1a model = Sequential() model.add(Masking(mask_value=0., input_shape=(timesteps, features))) model.add(LSTM(32)) [source] SpatialDropout1D keras.layers.SpatialDropout1D(rate) Dropout \u7684 Spatial 1D \u7248\u672c \u6b64\u7248\u672c\u7684\u529f\u80fd\u4e0e Dropout \u76f8\u540c\uff0c\u4f46\u5b83\u4f1a\u4e22\u5f03\u6574\u4e2a 1D \u7684\u7279\u5f81\u56fe\u800c\u4e0d\u662f\u4e22\u5f03\u5355\u4e2a\u5143\u7d20\u3002\u5982\u679c\u7279\u5f81\u56fe\u4e2d\u76f8\u90bb\u7684\u5e27\u662f\u5f3a\u76f8\u5173\u7684\uff08\u901a\u5e38\u662f\u9760\u524d\u7684\u5377\u79ef\u5c42\u4e2d\u7684\u60c5\u51b5\uff09\uff0c\u90a3\u4e48\u5e38\u89c4\u7684 dropout \u5c06\u65e0\u6cd5\u4f7f\u6fc0\u6d3b\u6b63\u5219\u5316\uff0c\u4e14\u5bfc\u81f4\u6709\u6548\u7684\u5b66\u4e60\u901f\u7387\u964d\u4f4e\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0cSpatialDropout1D \u5c06\u6709\u52a9\u4e8e\u63d0\u9ad8\u7279\u5f81\u56fe\u4e4b\u95f4\u7684\u72ec\u7acb\u6027\uff0c\u5e94\u8be5\u4f7f\u7528\u5b83\u6765\u4ee3\u66ff Dropout\u3002 \u53c2\u6570 rate : 0 \u5230 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002\u9700\u8981\u4e22\u5f03\u7684\u8f93\u5165\u6bd4\u4f8b\u3002 \u8f93\u5165\u5c3a\u5bf8 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (samples, timesteps, channels) \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u76f8\u540c\u3002 \u53c2\u8003\u6587\u732e Efficient Object Localization Using Convolutional Networks [source] SpatialDropout2D keras.layers.SpatialDropout2D(rate, data_format=None) Dropout \u7684 Spatial 2D \u7248\u672c \u6b64\u7248\u672c\u7684\u529f\u80fd\u4e0e Dropout \u76f8\u540c\uff0c\u4f46\u5b83\u4f1a\u4e22\u5f03\u6574\u4e2a 2D \u7684\u7279\u5f81\u56fe\u800c\u4e0d\u662f\u4e22\u5f03\u5355\u4e2a\u5143\u7d20\u3002\u5982\u679c\u7279\u5f81\u56fe\u4e2d\u76f8\u90bb\u7684\u50cf\u7d20\u662f\u5f3a\u76f8\u5173\u7684\uff08\u901a\u5e38\u662f\u9760\u524d\u7684\u5377\u79ef\u5c42\u4e2d\u7684\u60c5\u51b5\uff09\uff0c\u90a3\u4e48\u5e38\u89c4\u7684 dropout \u5c06\u65e0\u6cd5\u4f7f\u6fc0\u6d3b\u6b63\u5219\u5316\uff0c\u4e14\u5bfc\u81f4\u6709\u6548\u7684\u5b66\u4e60\u901f\u7387\u964d\u4f4e\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0cSpatialDropout2D \u5c06\u6709\u52a9\u4e8e\u63d0\u9ad8\u7279\u5f81\u56fe\u4e4b\u95f4\u7684\u72ec\u7acb\u6027\uff0c\u5e94\u8be5\u4f7f\u7528\u5b83\u6765\u4ee3\u66ff dropout\u3002 \u53c2\u6570 rate : 0 \u5230 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002\u9700\u8981\u4e22\u5f03\u7684\u8f93\u5165\u6bd4\u4f8b\u3002 data_format \uff1a channels_first \u6216\u8005 channels_last \u3002\u5728 channels_first \u6a21\u5f0f\u4e2d\uff0c\u901a\u9053\u7ef4\u5ea6\uff08\u5373\u6df1\u5ea6\uff09\u4f4d\u4e8e\u7d22\u5f15 1\uff0c\u5728 channels_last \u6a21\u5f0f\u4e2d\uff0c\u901a\u9053\u7ef4\u5ea6\u4f4d\u4e8e\u7d22\u5f15 3\u3002\u9ed8\u8ba4\u4e3a image_data_format \u7684\u503c\uff0c\u4f60\u53ef\u4ee5\u5728 Keras \u7684\u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d\u627e\u5230\u5b83\u3002\u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u8fc7\u5b83\uff0c\u90a3\u4e48\u5b83\u5c06\u662f channels_last \u8f93\u5165\u5c3a\u5bf8 4D \u5f20\u91cf\uff0c\u5982\u679c data_format \uff1d channels_first \uff0c\u5c3a\u5bf8\u4e3a (samples, channels, rows, cols) \uff0c\u5982\u679c data_format \uff1d channels_last \uff0c\u5c3a\u5bf8\u4e3a (samples, rows, cols, channels) \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u76f8\u540c\u3002 \u53c2\u8003\u6587\u732e Efficient Object Localization Using Convolutional Networks [source] SpatialDropout3D keras.layers.SpatialDropout3D(rate, data_format=None) Dropout \u7684 Spatial 3D \u7248\u672c \u6b64\u7248\u672c\u7684\u529f\u80fd\u4e0e Dropout \u76f8\u540c\uff0c\u4f46\u5b83\u4f1a\u4e22\u5f03\u6574\u4e2a 3D \u7684\u7279\u5f81\u56fe\u800c\u4e0d\u662f\u4e22\u5f03\u5355\u4e2a\u5143\u7d20\u3002\u5982\u679c\u7279\u5f81\u56fe\u4e2d\u76f8\u90bb\u7684\u4f53\u7d20\u662f\u5f3a\u76f8\u5173\u7684\uff08\u901a\u5e38\u662f\u9760\u524d\u7684\u5377\u79ef\u5c42\u4e2d\u7684\u60c5\u51b5\uff09\uff0c\u90a3\u4e48\u5e38\u89c4\u7684 dropout \u5c06\u65e0\u6cd5\u4f7f\u6fc0\u6d3b\u6b63\u5219\u5316\uff0c\u4e14\u5bfc\u81f4\u6709\u6548\u7684\u5b66\u4e60\u901f\u7387\u964d\u4f4e\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0cSpatialDropout3D \u5c06\u6709\u52a9\u4e8e\u63d0\u9ad8\u7279\u5f81\u56fe\u4e4b\u95f4\u7684\u72ec\u7acb\u6027\uff0c\u5e94\u8be5\u4f7f\u7528\u5b83\u6765\u4ee3\u66ff dropout\u3002 \u53c2\u6570 rate : 0 \u5230 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002\u9700\u8981\u4e22\u5f03\u7684\u8f93\u5165\u6bd4\u4f8b\u3002 data_format \uff1a channels_first \u6216\u8005 channels_last \u3002\u5728 channels_first \u6a21\u5f0f\u4e2d\uff0c\u901a\u9053\u7ef4\u5ea6\uff08\u5373\u6df1\u5ea6\uff09\u4f4d\u4e8e\u7d22\u5f15 1\uff0c\u5728 channels_last \u6a21\u5f0f\u4e2d\uff0c\u901a\u9053\u7ef4\u5ea6\u4f4d\u4e8e\u7d22\u5f15 4\u3002\u9ed8\u8ba4\u4e3a image_data_format \u7684\u503c\uff0c\u4f60\u53ef\u4ee5\u5728 Keras \u7684\u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d\u627e\u5230\u5b83\u3002\u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u8fc7\u5b83\uff0c\u90a3\u4e48\u5b83\u5c06\u662f channels_last \u8f93\u5165\u5c3a\u5bf8 5D \u5f20\u91cf\uff0c\u5982\u679c data_format \uff1d channels_first \uff0c\u5c3a\u5bf8\u4e3a (samples, channels, dim1, dim2, dim3) \uff0c\u5982\u679c data_format \uff1d channels_last \uff0c\u5c3a\u5bf8\u4e3a (samples, dim1, dim2, dim3, channels) \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u76f8\u540c\u3002 \u53c2\u8003\u6587\u732e Efficient Object Localization Using Convolutional Networks","title":"\u6838\u5fc3\u7f51\u7edc\u5c42"},{"location":"2-Layers/1.core/#_1","text":"[source]","title":"\u6838\u5fc3\u7f51\u7edc\u5c42"},{"location":"2-Layers/1.core/#dense","text":"keras.layers.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) \u5c31\u662f\u4f60\u5e38\u7528\u7684\u7684\u5168\u8fde\u63a5\u5c42\u3002 Dense \u5b9e\u73b0\u4ee5\u4e0b\u64cd\u4f5c\uff1a output = activation(dot(input, kernel) + bias) \u5176\u4e2d activation \u662f\u6309\u9010\u4e2a\u5143\u7d20\u8ba1\u7b97\u7684\u6fc0\u6d3b\u51fd\u6570\uff0c kernel \u662f\u7531\u7f51\u7edc\u5c42\u521b\u5efa\u7684\u6743\u503c\u77e9\u9635\uff0c\u4ee5\u53ca bias \u662f\u5176\u521b\u5efa\u7684\u504f\u7f6e\u5411\u91cf (\u53ea\u5728 use_bias \u4e3a True \u65f6\u624d\u6709\u7528)\u3002 \u6ce8\u610f : \u5982\u679c\u8be5\u5c42\u7684\u8f93\u5165\u7684\u79e9\u5927\u4e8e 2\uff0c\u90a3\u4e48\u5b83\u9996\u5148\u88ab\u5c55\u5e73\u7136\u540e \u518d\u8ba1\u7b97\u4e0e kernel \u7684\u70b9\u4e58\u3002 \u4f8b # \u4f5c\u4e3a Sequential \u6a21\u578b\u7684\u7b2c\u4e00\u5c42 model = Sequential() model.add(Dense(32, input_shape=(16,))) # \u73b0\u5728\u6a21\u578b\u5c31\u4f1a\u4ee5\u5c3a\u5bf8\u4e3a (*, 16) \u7684\u6570\u7ec4\u4f5c\u4e3a\u8f93\u5165\uff0c # \u5176\u8f93\u51fa\u6570\u7ec4\u7684\u5c3a\u5bf8\u4e3a (*, 32) # \u5728\u7b2c\u4e00\u5c42\u4e4b\u540e\uff0c\u4f60\u5c31\u4e0d\u518d\u9700\u8981\u6307\u5b9a\u8f93\u5165\u7684\u5c3a\u5bf8\u4e86\uff1a model.add(Dense(32)) \u53c2\u6570 units : \u6b63\u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7ef4\u5ea6\u3002 activation : \u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u82e5\u4e0d\u6307\u5b9a\uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373\uff0c\u300c\u7ebf\u6027\u300d\u6fc0\u6d3b: a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 bias_initializer : \u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (see initializers ). kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u7684\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u7684\u8f93\u51fa\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u5b83\u7684 \"activation\")\u3002 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 \u8f93\u5165\u5c3a\u5bf8 nD \u5f20\u91cf\uff0c\u5c3a\u5bf8: (batch_size, ..., input_dim) \u3002 \u6700\u5e38\u89c1\u7684\u60c5\u51b5\u662f\u4e00\u4e2a\u5c3a\u5bf8\u4e3a (batch_size, input_dim) \u7684 2D \u8f93\u5165\u3002 \u8f93\u51fa\u5c3a\u5bf8 nD \u5f20\u91cf\uff0c\u5c3a\u5bf8: (batch_size, ..., units) \u3002 \u4f8b\u5982\uff0c\u5bf9\u4e8e\u5c3a\u5bf8\u4e3a (batch_size, input_dim) \u7684 2D \u8f93\u5165\uff0c \u8f93\u51fa\u7684\u5c3a\u5bf8\u4e3a (batch_size, units) \u3002 [source]","title":"Dense"},{"location":"2-Layers/1.core/#activation","text":"keras.layers.Activation(activation) \u5c06\u6fc0\u6d3b\u51fd\u6570\u5e94\u7528\u4e8e\u8f93\u51fa\u3002 \u53c2\u6570 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570\u7684\u540d\u79f0 (\u8be6\u89c1: activations )\uff0c \u6216\u8005\u9009\u62e9\u4e00\u4e2a Theano \u6216 TensorFlow \u64cd\u4f5c\u3002 \u8f93\u5165\u5c3a\u5bf8 \u4efb\u610f\u5c3a\u5bf8\u3002 \u5f53\u4f7f\u7528\u6b64\u5c42\u4f5c\u4e3a\u6a21\u578b\u4e2d\u7684\u7b2c\u4e00\u5c42\u65f6\uff0c \u4f7f\u7528\u53c2\u6570 input_shape \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u62ec\u6837\u672c\u6570\u7684\u8f74\uff09\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u76f8\u540c\u3002 [source]","title":"Activation"},{"location":"2-Layers/1.core/#dropout","text":"keras.layers.Dropout(rate, noise_shape=None, seed=None) \u5c06 Dropout \u5e94\u7528\u4e8e\u8f93\u5165\u3002 Dropout \u5305\u62ec\u5728\u8bad\u7ec3\u4e2d\u6bcf\u6b21\u66f4\u65b0\u65f6\uff0c \u5c06\u8f93\u5165\u5355\u5143\u7684\u6309\u6bd4\u7387\u968f\u673a\u8bbe\u7f6e\u4e3a 0\uff0c \u8fd9\u6709\u52a9\u4e8e\u9632\u6b62\u8fc7\u62df\u5408\u3002 \u53c2\u6570 rate : \u5728 0 \u548c 1 \u4e4b\u95f4\u6d6e\u52a8\u3002\u9700\u8981\u4e22\u5f03\u7684\u8f93\u5165\u6bd4\u4f8b\u3002 noise_shape : 1D \u6574\u6570\u5f20\u91cf\uff0c \u8868\u793a\u5c06\u4e0e\u8f93\u5165\u76f8\u4e58\u7684\u4e8c\u8fdb\u5236 dropout \u63a9\u5c42\u7684\u5f62\u72b6\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u4f60\u7684\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch_size, timesteps, features) \uff0c\u7136\u540e \u4f60\u5e0c\u671b dropout \u63a9\u5c42\u5728\u6240\u6709\u65f6\u95f4\u6b65\u90fd\u662f\u4e00\u6837\u7684\uff0c \u4f60\u53ef\u4ee5\u4f7f\u7528 noise_shape=(batch_size, 1, features) \u3002 seed : \u4e00\u4e2a\u4f5c\u4e3a\u968f\u673a\u79cd\u5b50\u7684 Python \u6574\u6570\u3002 \u53c2\u8003\u6587\u732e Dropout: A Simple Way to Prevent Neural Networks from Overfitting [source]","title":"Dropout"},{"location":"2-Layers/1.core/#flatten","text":"keras.layers.Flatten(data_format=None) \u5c06\u8f93\u5165\u5c55\u5e73\u3002\u4e0d\u5f71\u54cd\u6279\u91cf\u5927\u5c0f\u3002 \u53c2\u6570 data_format \uff1a\u4e00\u4e2a\u5b57\u7b26\u4e32\uff0c\u5176\u503c\u4e3a channels_last \uff08\u9ed8\u8ba4\u503c\uff09\u6216\u8005 channels_first \u3002\u5b83\u8868\u660e\u8f93\u5165\u7684\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002\u6b64\u53c2\u6570\u7684\u76ee\u7684\u662f\u5f53\u6a21\u578b\u4ece\u4e00\u79cd\u6570\u636e\u683c\u5f0f\u5207\u6362\u5230\u53e6\u4e00\u79cd\u6570\u636e\u683c\u5f0f\u65f6\u4fdd\u7559\u6743\u91cd\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u7740\u5c3a\u5bf8\u4e3a (batch, ..., channels) \u7684\u8f93\u5165\uff0c\u800c channels_first \u5bf9\u5e94\u7740\u5c3a\u5bf8\u4e3a (batch, channels, ...) \u7684\u8f93\u5165\u3002\u9ed8\u8ba4\u4e3a image_data_format \u7684\u503c\uff0c\u4f60\u53ef\u4ee5\u5728 Keras \u7684\u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d\u627e\u5230\u5b83\u3002\u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u8fc7\u5b83\uff0c\u90a3\u4e48\u5b83\u5c06\u662f channels_last \u4f8b model = Sequential() model.add(Conv2D(64, (3, 3), input_shape=(3, 32, 32), padding='same',)) # \u73b0\u5728\uff1amodel.output_shape == (None, 64, 32, 32) model.add(Flatten()) # \u73b0\u5728\uff1amodel.output_shape == (None, 65536) [source]","title":"Flatten"},{"location":"2-Layers/1.core/#input","text":"keras.engine.input_layer.Input() Input() \u7528\u4e8e\u5b9e\u4f8b\u5316 Keras \u5f20\u91cf\u3002 Keras \u5f20\u91cf\u662f\u5e95\u5c42\u540e\u7aef(Theano, TensorFlow \u6216 CNTK) \u7684\u5f20\u91cf\u5bf9\u8c61\uff0c\u6211\u4eec\u589e\u52a0\u4e86\u4e00\u4e9b\u7279\u6027\uff0c\u4f7f\u5f97\u80fd\u591f\u901a\u8fc7\u4e86\u89e3\u6a21\u578b\u7684\u8f93\u5165 \u548c\u8f93\u51fa\u6765\u6784\u5efa Keras \u6a21\u578b\u3002 \u4f8b\u5982\uff0c\u5982\u679c a, b \u548c c \u90fd\u662f Keras \u5f20\u91cf\uff0c \u90a3\u4e48\u4ee5\u4e0b\u64cd\u4f5c\u662f\u53ef\u884c\u7684\uff1a model = Model(input=[a, b], output=c) \u6dfb\u52a0\u7684 Keras \u5c5e\u6027\u662f\uff1a _keras_shape : \u901a\u8fc7 Keras \u7aef\u7684\u5c3a\u5bf8\u63a8\u7406 \u8fdb\u884c\u4f20\u64ad\u7684\u6574\u6570\u5c3a\u5bf8\u5143\u7ec4\u3002 _keras_history : \u5e94\u7528\u4e8e\u5f20\u91cf\u7684\u6700\u540e\u4e00\u5c42\u3002 \u6574\u4e2a\u7f51\u7edc\u5c42\u8ba1\u7b97\u56fe\u53ef\u4ee5\u9012\u5f52\u5730\u4ece\u8be5\u5c42\u4e2d\u68c0\u7d22\u3002 \u53c2\u6570 shape : \u4e00\u4e2a\u5c3a\u5bf8\u5143\u7ec4\uff08\u6574\u6570\uff09\uff0c\u4e0d\u5305\u542b\u6279\u91cf\u5927\u5c0f\u3002 \u4f8b\u5982\uff0c shape=(32,) \u8868\u660e\u671f\u671b\u7684\u8f93\u5165\u662f\u6309\u6279\u6b21\u7684 32 \u7ef4\u5411\u91cf\u3002 batch_shape : \u4e00\u4e2a\u5c3a\u5bf8\u5143\u7ec4\uff08\u6574\u6570\uff09\uff0c\u5305\u542b\u6279\u91cf\u5927\u5c0f\u3002 \u4f8b\u5982\uff0c batch_shape=(10, 32) \u8868\u660e\u671f\u671b\u7684\u8f93\u5165\u662f 10 \u4e2a 32 \u7ef4\u5411\u91cf\u3002 batch_shape=(None, 32) \u8868\u660e\u4efb\u610f\u6279\u6b21\u5927\u5c0f\u7684 32 \u7ef4\u5411\u91cf\u3002 name : \u4e00\u4e2a\u53ef\u9009\u7684\u5c42\u7684\u540d\u79f0\u7684\u5b57\u7b26\u4e32\u3002 \u5728\u4e00\u4e2a\u6a21\u578b\u4e2d\u5e94\u8be5\u662f\u552f\u4e00\u7684\uff08\u4e0d\u53ef\u4ee5\u91cd\u7528\u4e00\u4e2a\u540d\u5b57\u4e24\u6b21\uff09\u3002 \u5982\u672a\u63d0\u4f9b\uff0c\u5c06\u81ea\u52a8\u751f\u6210\u3002 dtype : \u8f93\u5165\u6240\u671f\u671b\u7684\u6570\u636e\u7c7b\u578b\uff0c\u5b57\u7b26\u4e32\u8868\u793a ( float32 , float64 , int32 ...) sparse : \u4e00\u4e2a\u5e03\u5c14\u503c\uff0c\u6307\u660e\u9700\u8981\u521b\u5efa\u7684\u5360\u4f4d\u7b26\u662f\u5426\u662f\u7a00\u758f\u7684\u3002 tensor : \u53ef\u9009\u7684\u53ef\u5c01\u88c5\u5230 Input \u5c42\u7684\u73b0\u6709\u5f20\u91cf\u3002 \u5982\u679c\u8bbe\u5b9a\u4e86\uff0c\u90a3\u4e48\u8fd9\u4e2a\u5c42\u5c06\u4e0d\u4f1a\u521b\u5efa\u5360\u4f4d\u7b26\u5f20\u91cf\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\u3002 \u4f8b # \u8fd9\u662f Keras \u4e2d\u7684\u4e00\u4e2a\u903b\u8f91\u56de\u5f52 x = Input(shape=(32,)) y = Dense(16, activation='softmax')(x) model = Model(x, y) [source]","title":"Input"},{"location":"2-Layers/1.core/#reshape","text":"keras.layers.Reshape(target_shape) \u5c06\u8f93\u5165\u91cd\u65b0\u8c03\u6574\u4e3a\u7279\u5b9a\u7684\u5c3a\u5bf8\u3002 \u53c2\u6570 target_shape : \u76ee\u6807\u5c3a\u5bf8\u3002\u6574\u6570\u5143\u7ec4\u3002 \u4e0d\u5305\u542b\u8868\u793a\u6279\u91cf\u7684\u8f74\u3002 \u8f93\u5165\u5c3a\u5bf8 \u4efb\u610f\uff0c\u5c3d\u7ba1\u8f93\u5165\u5c3a\u5bf8\u4e2d\u7684\u6240\u6709\u7ef4\u5ea6\u5fc5\u987b\u662f\u56fa\u5b9a\u7684\u3002 \u5f53\u4f7f\u7528\u6b64\u5c42\u4f5c\u4e3a\u6a21\u578b\u4e2d\u7684\u7b2c\u4e00\u5c42\u65f6\uff0c \u4f7f\u7528\u53c2\u6570 input_shape \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u62ec\u6837\u672c\u6570\u7684\u8f74\uff09\u3002 \u8f93\u51fa\u5c3a\u5bf8 (batch_size,) + target_shape \u4f8b # \u4f5c\u4e3a Sequential \u6a21\u578b\u7684\u7b2c\u4e00\u5c42 model = Sequential() model.add(Reshape((3, 4), input_shape=(12,))) # \u73b0\u5728\uff1amodel.output_shape == (None, 3, 4) # \u6ce8\u610f\uff1a `None` \u662f\u6279\u8868\u793a\u7684\u7ef4\u5ea6 # \u4f5c\u4e3a Sequential \u6a21\u578b\u7684\u4e2d\u95f4\u5c42 model.add(Reshape((6, 2))) # \u73b0\u5728\uff1a model.output_shape == (None, 6, 2) # \u8fd8\u652f\u6301\u4f7f\u7528 `-1` \u8868\u793a\u7ef4\u5ea6\u7684\u5c3a\u5bf8\u63a8\u65ad model.add(Reshape((-1, 2, 2))) # \u73b0\u5728\uff1a model.output_shape == (None, 3, 2, 2) [source]","title":"Reshape"},{"location":"2-Layers/1.core/#permute","text":"keras.layers.Permute(dims) \u6839\u636e\u7ed9\u5b9a\u7684\u6a21\u5f0f\u7f6e\u6362\u8f93\u5165\u7684\u7ef4\u5ea6\u3002 \u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u5f88\u6709\u7528\uff0c\u4f8b\u5982\u5c06 RNN \u548c CNN \u8fde\u63a5\u5728\u4e00\u8d77\u3002 \u4f8b model = Sequential() model.add(Permute((2, 1), input_shape=(10, 64))) # \u73b0\u5728\uff1a model.output_shape == (None, 64, 10) # \u6ce8\u610f\uff1a `None` \u662f\u6279\u8868\u793a\u7684\u7ef4\u5ea6 \u53c2\u6570 dims : \u6574\u6570\u5143\u7ec4\u3002\u7f6e\u6362\u6a21\u5f0f\uff0c\u4e0d\u5305\u542b\u6837\u672c\u7ef4\u5ea6\u3002 \u7d22\u5f15\u4ece 1 \u5f00\u59cb\u3002 \u4f8b\u5982, (2, 1) \u7f6e\u6362\u8f93\u5165\u7684\u7b2c\u4e00\u548c\u7b2c\u4e8c\u4e2a\u7ef4\u5ea6\u3002 \u8f93\u5165\u5c3a\u5bf8 \u4efb\u610f\u3002\u5f53\u4f7f\u7528\u6b64\u5c42\u4f5c\u4e3a\u6a21\u578b\u4e2d\u7684\u7b2c\u4e00\u5c42\u65f6\uff0c \u4f7f\u7528\u53c2\u6570 input_shape \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u62ec\u6837\u672c\u6570\u7684\u8f74\uff09\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u5c3a\u5bf8\u76f8\u540c\uff0c\u4f46\u662f\u7ef4\u5ea6\u6839\u636e\u6307\u5b9a\u7684\u6a21\u5f0f\u91cd\u65b0\u6392\u5217\u3002 [source]","title":"Permute"},{"location":"2-Layers/1.core/#repeatvector","text":"keras.layers.RepeatVector(n) \u5c06\u8f93\u5165\u91cd\u590d n \u6b21\u3002 \u4f8b model = Sequential() model.add(Dense(32, input_dim=32)) # \u73b0\u5728\uff1a model.output_shape == (None, 32) # \u6ce8\u610f\uff1a `None` \u662f\u6279\u8868\u793a\u7684\u7ef4\u5ea6 model.add(RepeatVector(3)) # \u73b0\u5728\uff1a model.output_shape == (None, 3, 32) \u53c2\u6570 n : \u6574\u6570\uff0c\u91cd\u590d\u6b21\u6570\u3002 \u8f93\u5165\u5c3a\u5bf8 2D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (num_samples, features) \u3002 \u8f93\u51fa\u5c3a\u5bf8 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (num_samples, n, features) \u3002 [source]","title":"RepeatVector"},{"location":"2-Layers/1.core/#lambda","text":"keras.layers.Lambda(function, output_shape=None, mask=None, arguments=None) \u5c06\u4efb\u610f\u8868\u8fbe\u5f0f\u5c01\u88c5\u4e3a Layer \u5bf9\u8c61\u3002 \u4f8b # \u6dfb\u52a0\u4e00\u4e2a x -> x^2 \u5c42 model.add(Lambda(lambda x: x ** 2)) # \u6dfb\u52a0\u4e00\u4e2a\u7f51\u7edc\u5c42\uff0c\u8fd4\u56de\u8f93\u5165\u7684\u6b63\u6570\u90e8\u5206 # \u4e0e\u8d1f\u6570\u90e8\u5206\u7684\u53cd\u9762\u7684\u8fde\u63a5 def antirectifier(x): x -= K.mean(x, axis=1, keepdims=True) x = K.l2_normalize(x, axis=1) pos = K.relu(x) neg = K.relu(-x) return K.concatenate([pos, neg], axis=1) def antirectifier_output_shape(input_shape): shape = list(input_shape) assert len(shape) == 2 # only valid for 2D tensors shape[-1] *= 2 return tuple(shape) model.add(Lambda(antirectifier, output_shape=antirectifier_output_shape)) \u53c2\u6570 function : \u9700\u8981\u5c01\u88c5\u7684\u51fd\u6570\u3002 \u5c06\u8f93\u5165\u5f20\u91cf\u4f5c\u4e3a\u7b2c\u4e00\u4e2a\u53c2\u6570\u3002 output_shape : \u9884\u671f\u7684\u51fd\u6570\u8f93\u51fa\u5c3a\u5bf8\u3002 \u53ea\u5728\u4f7f\u7528 Theano \u65f6\u6709\u610f\u4e49\u3002 \u53ef\u4ee5\u662f\u5143\u7ec4\u6216\u8005\u51fd\u6570\u3002 \u5982\u679c\u662f\u5143\u7ec4\uff0c\u5b83\u53ea\u6307\u5b9a\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\uff1b \u6837\u672c\u7ef4\u5ea6\u5047\u8bbe\u4e0e\u8f93\u5165\u76f8\u540c\uff1a output_shape = (input_shape[0], ) + output_shape \u6216\u8005\uff0c\u8f93\u5165\u662f None \u4e14\u6837\u672c\u7ef4\u5ea6\u4e5f\u662f None \uff1a output_shape = (None, ) + output_shape \u5982\u679c\u662f\u51fd\u6570\uff0c\u5b83\u6307\u5b9a\u6574\u4e2a\u5c3a\u5bf8\u4e3a\u8f93\u5165\u5c3a\u5bf8\u7684\u4e00\u4e2a\u51fd\u6570\uff1a output_shape = f(input_shape) arguments : \u53ef\u9009\u7684\u9700\u8981\u4f20\u9012\u7ed9\u51fd\u6570\u7684\u5173\u952e\u5b57\u53c2\u6570\u3002 \u8f93\u5165\u5c3a\u5bf8 \u4efb\u610f\u3002\u5f53\u4f7f\u7528\u6b64\u5c42\u4f5c\u4e3a\u6a21\u578b\u4e2d\u7684\u7b2c\u4e00\u5c42\u65f6\uff0c \u4f7f\u7528\u53c2\u6570 input_shape \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u62ec\u6837\u672c\u6570\u7684\u8f74\uff09\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u7531 output_shape \u53c2\u6570\u6307\u5b9a (\u6216\u8005\u5728\u4f7f\u7528 TensorFlow \u65f6\uff0c\u81ea\u52a8\u63a8\u7406\u5f97\u5230)\u3002 [source]","title":"Lambda"},{"location":"2-Layers/1.core/#activityregularization","text":"keras.layers.ActivityRegularization(l1=0.0, l2=0.0) \u7f51\u7edc\u5c42\uff0c\u5bf9\u57fa\u4e8e\u4ee3\u4ef7\u51fd\u6570\u7684\u8f93\u5165\u6d3b\u52a8\u5e94\u7528\u4e00\u4e2a\u66f4\u65b0\u3002 \u53c2\u6570 l1 : L1 \u6b63\u5219\u5316\u56e0\u5b50 (\u6b63\u6570\u6d6e\u70b9\u578b)\u3002 l2 : L2 \u6b63\u5219\u5316\u56e0\u5b50 (\u6b63\u6570\u6d6e\u70b9\u578b)\u3002 \u8f93\u5165\u5c3a\u5bf8 \u4efb\u610f\u3002\u5f53\u4f7f\u7528\u6b64\u5c42\u4f5c\u4e3a\u6a21\u578b\u4e2d\u7684\u7b2c\u4e00\u5c42\u65f6\uff0c \u4f7f\u7528\u53c2\u6570 input_shape \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u62ec\u6837\u672c\u6570\u7684\u8f74\uff09\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u76f8\u540c\u3002 [source]","title":"ActivityRegularization"},{"location":"2-Layers/1.core/#masking","text":"keras.layers.Masking(mask_value=0.0) \u4f7f\u7528\u8986\u76d6\u503c\u8986\u76d6\u5e8f\u5217\uff0c\u4ee5\u8df3\u8fc7\u65f6\u95f4\u6b65\u3002 \u5bf9\u4e8e\u8f93\u5165\u5f20\u91cf\u7684\u6bcf\u4e00\u4e2a\u65f6\u95f4\u6b65\uff08\u5f20\u91cf\u7684\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\uff09\uff0c \u5982\u679c\u6240\u6709\u65f6\u95f4\u6b65\u4e2d\u8f93\u5165\u5f20\u91cf\u7684\u503c\u4e0e mask_value \u76f8\u7b49\uff0c \u90a3\u4e48\u8fd9\u4e2a\u65f6\u95f4\u6b65\u5c06\u5728\u6240\u6709\u4e0b\u6e38\u5c42\u88ab\u8986\u76d6 (\u8df3\u8fc7) \uff08\u53ea\u8981\u5b83\u4eec\u652f\u6301\u8986\u76d6\uff09\u3002 \u5982\u679c\u4efb\u4f55\u4e0b\u6e38\u5c42\u4e0d\u652f\u6301\u8986\u76d6\u4f46\u4ecd\u7136\u6536\u5230\u6b64\u7c7b\u8f93\u5165\u8986\u76d6\u4fe1\u606f\uff0c\u4f1a\u5f15\u53d1\u5f02\u5e38\u3002 \u4f8b \u8003\u8651\u5c06\u8981\u5582\u5165\u4e00\u4e2a LSTM \u5c42\u7684 Numpy \u77e9\u9635 x \uff0c \u5c3a\u5bf8\u4e3a (samples, timesteps, features) \u3002 \u4f60\u60f3\u8981\u8986\u76d6\u65f6\u95f4\u6b65 #3 \u548c #5\uff0c\u56e0\u4e3a\u4f60\u7f3a\u4e4f\u8fd9\u51e0\u4e2a \u65f6\u95f4\u6b65\u7684\u6570\u636e\u3002\u4f60\u53ef\u4ee5\uff1a \u8bbe\u7f6e x[:, 3, :] = 0. \u4ee5\u53ca x[:, 5, :] = 0. \u5728 LSTM \u5c42\u4e4b\u524d\uff0c\u63d2\u5165\u4e00\u4e2a mask_value=0 \u7684 Masking \u5c42\uff1a model = Sequential() model.add(Masking(mask_value=0., input_shape=(timesteps, features))) model.add(LSTM(32)) [source]","title":"Masking"},{"location":"2-Layers/1.core/#spatialdropout1d","text":"keras.layers.SpatialDropout1D(rate) Dropout \u7684 Spatial 1D \u7248\u672c \u6b64\u7248\u672c\u7684\u529f\u80fd\u4e0e Dropout \u76f8\u540c\uff0c\u4f46\u5b83\u4f1a\u4e22\u5f03\u6574\u4e2a 1D \u7684\u7279\u5f81\u56fe\u800c\u4e0d\u662f\u4e22\u5f03\u5355\u4e2a\u5143\u7d20\u3002\u5982\u679c\u7279\u5f81\u56fe\u4e2d\u76f8\u90bb\u7684\u5e27\u662f\u5f3a\u76f8\u5173\u7684\uff08\u901a\u5e38\u662f\u9760\u524d\u7684\u5377\u79ef\u5c42\u4e2d\u7684\u60c5\u51b5\uff09\uff0c\u90a3\u4e48\u5e38\u89c4\u7684 dropout \u5c06\u65e0\u6cd5\u4f7f\u6fc0\u6d3b\u6b63\u5219\u5316\uff0c\u4e14\u5bfc\u81f4\u6709\u6548\u7684\u5b66\u4e60\u901f\u7387\u964d\u4f4e\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0cSpatialDropout1D \u5c06\u6709\u52a9\u4e8e\u63d0\u9ad8\u7279\u5f81\u56fe\u4e4b\u95f4\u7684\u72ec\u7acb\u6027\uff0c\u5e94\u8be5\u4f7f\u7528\u5b83\u6765\u4ee3\u66ff Dropout\u3002 \u53c2\u6570 rate : 0 \u5230 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002\u9700\u8981\u4e22\u5f03\u7684\u8f93\u5165\u6bd4\u4f8b\u3002 \u8f93\u5165\u5c3a\u5bf8 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (samples, timesteps, channels) \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u76f8\u540c\u3002 \u53c2\u8003\u6587\u732e Efficient Object Localization Using Convolutional Networks [source]","title":"SpatialDropout1D"},{"location":"2-Layers/1.core/#spatialdropout2d","text":"keras.layers.SpatialDropout2D(rate, data_format=None) Dropout \u7684 Spatial 2D \u7248\u672c \u6b64\u7248\u672c\u7684\u529f\u80fd\u4e0e Dropout \u76f8\u540c\uff0c\u4f46\u5b83\u4f1a\u4e22\u5f03\u6574\u4e2a 2D \u7684\u7279\u5f81\u56fe\u800c\u4e0d\u662f\u4e22\u5f03\u5355\u4e2a\u5143\u7d20\u3002\u5982\u679c\u7279\u5f81\u56fe\u4e2d\u76f8\u90bb\u7684\u50cf\u7d20\u662f\u5f3a\u76f8\u5173\u7684\uff08\u901a\u5e38\u662f\u9760\u524d\u7684\u5377\u79ef\u5c42\u4e2d\u7684\u60c5\u51b5\uff09\uff0c\u90a3\u4e48\u5e38\u89c4\u7684 dropout \u5c06\u65e0\u6cd5\u4f7f\u6fc0\u6d3b\u6b63\u5219\u5316\uff0c\u4e14\u5bfc\u81f4\u6709\u6548\u7684\u5b66\u4e60\u901f\u7387\u964d\u4f4e\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0cSpatialDropout2D \u5c06\u6709\u52a9\u4e8e\u63d0\u9ad8\u7279\u5f81\u56fe\u4e4b\u95f4\u7684\u72ec\u7acb\u6027\uff0c\u5e94\u8be5\u4f7f\u7528\u5b83\u6765\u4ee3\u66ff dropout\u3002 \u53c2\u6570 rate : 0 \u5230 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002\u9700\u8981\u4e22\u5f03\u7684\u8f93\u5165\u6bd4\u4f8b\u3002 data_format \uff1a channels_first \u6216\u8005 channels_last \u3002\u5728 channels_first \u6a21\u5f0f\u4e2d\uff0c\u901a\u9053\u7ef4\u5ea6\uff08\u5373\u6df1\u5ea6\uff09\u4f4d\u4e8e\u7d22\u5f15 1\uff0c\u5728 channels_last \u6a21\u5f0f\u4e2d\uff0c\u901a\u9053\u7ef4\u5ea6\u4f4d\u4e8e\u7d22\u5f15 3\u3002\u9ed8\u8ba4\u4e3a image_data_format \u7684\u503c\uff0c\u4f60\u53ef\u4ee5\u5728 Keras \u7684\u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d\u627e\u5230\u5b83\u3002\u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u8fc7\u5b83\uff0c\u90a3\u4e48\u5b83\u5c06\u662f channels_last \u8f93\u5165\u5c3a\u5bf8 4D \u5f20\u91cf\uff0c\u5982\u679c data_format \uff1d channels_first \uff0c\u5c3a\u5bf8\u4e3a (samples, channels, rows, cols) \uff0c\u5982\u679c data_format \uff1d channels_last \uff0c\u5c3a\u5bf8\u4e3a (samples, rows, cols, channels) \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u76f8\u540c\u3002 \u53c2\u8003\u6587\u732e Efficient Object Localization Using Convolutional Networks [source]","title":"SpatialDropout2D"},{"location":"2-Layers/1.core/#spatialdropout3d","text":"keras.layers.SpatialDropout3D(rate, data_format=None) Dropout \u7684 Spatial 3D \u7248\u672c \u6b64\u7248\u672c\u7684\u529f\u80fd\u4e0e Dropout \u76f8\u540c\uff0c\u4f46\u5b83\u4f1a\u4e22\u5f03\u6574\u4e2a 3D \u7684\u7279\u5f81\u56fe\u800c\u4e0d\u662f\u4e22\u5f03\u5355\u4e2a\u5143\u7d20\u3002\u5982\u679c\u7279\u5f81\u56fe\u4e2d\u76f8\u90bb\u7684\u4f53\u7d20\u662f\u5f3a\u76f8\u5173\u7684\uff08\u901a\u5e38\u662f\u9760\u524d\u7684\u5377\u79ef\u5c42\u4e2d\u7684\u60c5\u51b5\uff09\uff0c\u90a3\u4e48\u5e38\u89c4\u7684 dropout \u5c06\u65e0\u6cd5\u4f7f\u6fc0\u6d3b\u6b63\u5219\u5316\uff0c\u4e14\u5bfc\u81f4\u6709\u6548\u7684\u5b66\u4e60\u901f\u7387\u964d\u4f4e\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0cSpatialDropout3D \u5c06\u6709\u52a9\u4e8e\u63d0\u9ad8\u7279\u5f81\u56fe\u4e4b\u95f4\u7684\u72ec\u7acb\u6027\uff0c\u5e94\u8be5\u4f7f\u7528\u5b83\u6765\u4ee3\u66ff dropout\u3002 \u53c2\u6570 rate : 0 \u5230 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002\u9700\u8981\u4e22\u5f03\u7684\u8f93\u5165\u6bd4\u4f8b\u3002 data_format \uff1a channels_first \u6216\u8005 channels_last \u3002\u5728 channels_first \u6a21\u5f0f\u4e2d\uff0c\u901a\u9053\u7ef4\u5ea6\uff08\u5373\u6df1\u5ea6\uff09\u4f4d\u4e8e\u7d22\u5f15 1\uff0c\u5728 channels_last \u6a21\u5f0f\u4e2d\uff0c\u901a\u9053\u7ef4\u5ea6\u4f4d\u4e8e\u7d22\u5f15 4\u3002\u9ed8\u8ba4\u4e3a image_data_format \u7684\u503c\uff0c\u4f60\u53ef\u4ee5\u5728 Keras \u7684\u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d\u627e\u5230\u5b83\u3002\u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u8fc7\u5b83\uff0c\u90a3\u4e48\u5b83\u5c06\u662f channels_last \u8f93\u5165\u5c3a\u5bf8 5D \u5f20\u91cf\uff0c\u5982\u679c data_format \uff1d channels_first \uff0c\u5c3a\u5bf8\u4e3a (samples, channels, dim1, dim2, dim3) \uff0c\u5982\u679c data_format \uff1d channels_last \uff0c\u5c3a\u5bf8\u4e3a (samples, dim1, dim2, dim3, channels) \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u76f8\u540c\u3002 \u53c2\u8003\u6587\u732e Efficient Object Localization Using Convolutional Networks","title":"SpatialDropout3D"},{"location":"2-Layers/2.convolutional/","text":"\u5377\u79ef\u5c42 [source] Conv1D keras.layers.Conv1D(filters, kernel_size, strides=1, padding='valid', data_format='channels_last', dilation_rate=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) 1D \u5377\u79ef\u5c42 (\u4f8b\u5982\u65f6\u5e8f\u5377\u79ef)\u3002 \u8be5\u5c42\u521b\u5efa\u4e86\u4e00\u4e2a\u5377\u79ef\u6838\uff0c\u8be5\u5377\u79ef\u6838\u4ee5 \u5355\u4e2a\u7a7a\u95f4\uff08\u6216\u65f6\u95f4\uff09\u7ef4\u4e0a\u7684\u5c42\u8f93\u5165\u8fdb\u884c\u5377\u79ef\uff0c \u4ee5\u751f\u6210\u8f93\u51fa\u5f20\u91cf\u3002 \u5982\u679c use_bias \u4e3a True\uff0c \u5219\u4f1a\u521b\u5efa\u4e00\u4e2a\u504f\u7f6e\u5411\u91cf\u5e76\u5c06\u5176\u6dfb\u52a0\u5230\u8f93\u51fa\u4e2d\u3002 \u6700\u540e\uff0c\u5982\u679c activation \u4e0d\u662f None \uff0c\u5b83\u4e5f\u4f1a\u5e94\u7528\u4e8e\u8f93\u51fa\u3002 \u5f53\u4f7f\u7528\u8be5\u5c42\u4f5c\u4e3a\u6a21\u578b\u7b2c\u4e00\u5c42\u65f6\uff0c\u9700\u8981\u63d0\u4f9b input_shape \u53c2\u6570\uff08\u6574\u6570\u5143\u7ec4\u6216 None \uff09\uff0c\u4f8b\u5982\uff0c (10, 128) \u8868\u793a 10 \u4e2a 128 \u7ef4\u7684\u5411\u91cf\u7ec4\u6210\u7684\u5411\u91cf\u5e8f\u5217\uff0c (None, 128) \u8868\u793a 128 \u7ef4\u7684\u5411\u91cf\u7ec4\u6210\u7684\u53d8\u957f\u5e8f\u5217\u3002 \u53c2\u6570 filters : \u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6 \uff08\u5373\u5377\u79ef\u4e2d\u6ee4\u6ce2\u5668\u7684\u8f93\u51fa\u6570\u91cf\uff09\u3002 kernel_size : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005\u5355\u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e 1D \u5377\u79ef\u7a97\u53e3\u7684\u957f\u5ea6\u3002 strides : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005\u5355\u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e\u5377\u79ef\u7684\u6b65\u957f\u3002 \u6307\u5b9a\u4efb\u4f55 stride \u503c != 1 \u4e0e\u6307\u5b9a dilation_rate \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 padding : \"valid\" , \"causal\" \u6216 \"same\" \u4e4b\u4e00 (\u5927\u5c0f\u5199\u654f\u611f) \"valid\" \u8868\u793a\u300c\u4e0d\u586b\u5145\u300d\u3002 \"same\" \u8868\u793a\u586b\u5145\u8f93\u5165\u4ee5\u4f7f\u8f93\u51fa\u5177\u6709\u4e0e\u539f\u59cb\u8f93\u5165\u76f8\u540c\u7684\u957f\u5ea6\u3002 \"causal\" \u8868\u793a\u56e0\u679c\uff08\u81a8\u80c0\uff09\u5377\u79ef\uff0c \u4f8b\u5982\uff0c output[t] \u4e0d\u4f9d\u8d56\u4e8e input[t+1:] \uff0c \u5728\u6a21\u578b\u4e0d\u5e94\u8fdd\u53cd\u65f6\u95f4\u987a\u5e8f\u7684\u65f6\u95f4\u6570\u636e\u5efa\u6a21\u65f6\u975e\u5e38\u6709\u7528\u3002 \u8be6\u89c1 WaveNet: A Generative Model for Raw Audio, section 2.1 \u3002 data_format : \u5b57\u7b26\u4e32, \"channels_last\" (\u9ed8\u8ba4) \u6216 \"channels_first\" \u4e4b\u4e00\u3002\u8f93\u5165\u7684\u5404\u4e2a\u7ef4\u5ea6\u987a\u5e8f\u3002 \"channels_last\" \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, steps, channels) (Keras \u4e2d\u65f6\u5e8f\u6570\u636e\u7684\u9ed8\u8ba4\u683c\u5f0f) \u800c \"channels_first\" \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, channels, steps) \u3002 dilation_rate : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005\u5355\u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c\u6307\u5b9a\u7528\u4e8e\u81a8\u80c0\u5377\u79ef\u7684\u81a8\u80c0\u7387\u3002 \u5f53\u524d\uff0c\u6307\u5b9a\u4efb\u4f55 dilation_rate \u503c != 1 \u4e0e\u6307\u5b9a stride \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u5982\u672a\u6307\u5b9a\uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373\u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 bias_initializer : \u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u8f93\u51fa\uff08\u5b83\u7684\u6fc0\u6d3b\u503c\uff09\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 \u8f93\u5165\u5c3a\u5bf8 3D \u5f20\u91cf \uff0c\u5c3a\u5bf8\u4e3a (batch_size, steps, input_dim) \u3002 \u8f93\u51fa\u5c3a\u5bf8 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch_size, new_steps, filters) \u3002 \u7531\u4e8e\u586b\u5145\u6216\u7a97\u53e3\u6309\u6b65\u957f\u6ed1\u52a8\uff0c steps \u503c\u53ef\u80fd\u5df2\u66f4\u6539\u3002 [source] Conv2D keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) 2D \u5377\u79ef\u5c42 (\u4f8b\u5982\u5bf9\u56fe\u50cf\u7684\u7a7a\u95f4\u5377\u79ef)\u3002 \u8be5\u5c42\u521b\u5efa\u4e86\u4e00\u4e2a\u5377\u79ef\u6838\uff0c \u8be5\u5377\u79ef\u6838\u5bf9\u5c42\u8f93\u5165\u8fdb\u884c\u5377\u79ef\uff0c \u4ee5\u751f\u6210\u8f93\u51fa\u5f20\u91cf\u3002 \u5982\u679c use_bias \u4e3a True\uff0c \u5219\u4f1a\u521b\u5efa\u4e00\u4e2a\u504f\u7f6e\u5411\u91cf\u5e76\u5c06\u5176\u6dfb\u52a0\u5230\u8f93\u51fa\u4e2d\u3002 \u6700\u540e\uff0c\u5982\u679c activation \u4e0d\u662f None \uff0c\u5b83\u4e5f\u4f1a\u5e94\u7528\u4e8e\u8f93\u51fa\u3002 \u5f53\u4f7f\u7528\u8be5\u5c42\u4f5c\u4e3a\u6a21\u578b\u7b2c\u4e00\u5c42\u65f6\uff0c\u9700\u8981\u63d0\u4f9b input_shape \u53c2\u6570 \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u542b\u6837\u672c\u8868\u793a\u7684\u8f74\uff09\uff0c\u4f8b\u5982\uff0c input_shape=(128, 128, 3) \u8868\u793a 128x128 RGB \u56fe\u50cf\uff0c \u5728 data_format=\"channels_last\" \u65f6\u3002 \u53c2\u6570 filters : \u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6 \uff08\u5373\u5377\u79ef\u4e2d\u6ee4\u6ce2\u5668\u7684\u8f93\u51fa\u6570\u91cf\uff09\u3002 kernel_size : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e 2D \u5377\u79ef\u7a97\u53e3\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 strides : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e\u5377\u79ef\u6cbf\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u65b9\u5411\u7684\u6b65\u957f\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 \u6307\u5b9a\u4efb\u4f55 stride \u503c != 1 \u4e0e\u6307\u5b9a dilation_rate \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 padding : \"valid\" \u6216 \"same\" (\u5927\u5c0f\u5199\u654f\u611f)\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4) \u6216 channels_first \u4e4b\u4e00\uff0c\u8868\u793a\u8f93\u5165\u4e2d\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, height, width, channels) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, channels, height, width) \u3002 \u5b83\u9ed8\u8ba4\u4e3a\u4ece Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d \u627e\u5230\u7684 image_data_format \u503c\u3002 \u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u5c06\u4f7f\u7528 channels_last \u3002 dilation_rate : \u4e00\u4e2a\u6574\u6570\u6216 2 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u5b9a\u81a8\u80c0\u5377\u79ef\u7684\u81a8\u80c0\u7387\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 \u5f53\u524d\uff0c\u6307\u5b9a\u4efb\u4f55 dilation_rate \u503c != 1 \u4e0e \u6307\u5b9a stride \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u5982\u679c\u4f60\u4e0d\u6307\u5b9a\uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373\u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 bias_initializer : \u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u8f93\u51fa\uff08\u5b83\u7684\u6fc0\u6d3b\u503c\uff09\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_first'\uff0c \u8f93\u5165 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (samples, channels, rows, cols) \u3002 \u5982\u679c data_format='channels_last'\uff0c \u8f93\u5165 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (samples, rows, cols, channels) \u3002 \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format='channels_first'\uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (samples, filters, new_rows, new_cols) \u3002 \u5982\u679c data_format='channels_last'\uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (samples, new_rows, new_cols, filters) \u3002 \u7531\u4e8e\u586b\u5145\u7684\u539f\u56e0\uff0c rows \u548c cols \u503c\u53ef\u80fd\u5df2\u66f4\u6539\u3002 [source] SeparableConv1D keras.layers.SeparableConv1D(filters, kernel_size, strides=1, padding='valid', data_format='channels_last', dilation_rate=1, depth_multiplier=1, activation=None, use_bias=True, depthwise_initializer='glorot_uniform', pointwise_initializer='glorot_uniform', bias_initializer='zeros', depthwise_regularizer=None, pointwise_regularizer=None, bias_regularizer=None, activity_regularizer=None, depthwise_constraint=None, pointwise_constraint=None, bias_constraint=None) \u6df1\u5ea6\u65b9\u5411\u7684\u53ef\u5206\u79bb 1D \u5377\u79ef\u3002 \u53ef\u5206\u79bb\u7684\u5377\u79ef\u7684\u64cd\u4f5c\u5305\u62ec\uff0c\u9996\u5148\u6267\u884c\u6df1\u5ea6\u65b9\u5411\u7684\u7a7a\u95f4\u5377\u79ef \uff08\u5206\u522b\u4f5c\u7528\u4e8e\u6bcf\u4e2a\u8f93\u5165\u901a\u9053\uff09\uff0c\u7d27\u63a5\u4e00\u4e2a\u5c06\u6240\u5f97\u8f93\u51fa\u901a\u9053 \u6df7\u5408\u5728\u4e00\u8d77\u7684\u9010\u70b9\u5377\u79ef\u3002 depth_multiplier \u53c2\u6570\u63a7 \u5236\u6df1\u5ea6\u6b65\u9aa4\u4e2d\u6bcf\u4e2a\u8f93\u5165\u901a\u9053\u751f\u6210\u591a\u5c11\u4e2a\u8f93\u51fa\u901a\u9053\u3002 \u76f4\u89c2\u5730\u8bf4\uff0c\u53ef\u5206\u79bb\u7684\u5377\u79ef\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u79cd\u5c06\u5377\u79ef\u6838\u5206\u89e3\u6210 \u4e24\u4e2a\u8f83\u5c0f\u7684\u5377\u79ef\u6838\u7684\u65b9\u6cd5\uff0c\u6216\u8005\u4f5c\u4e3a Inception \u5757\u7684 \u4e00\u4e2a\u6781\u7aef\u7248\u672c\u3002 \u53c2\u6570 filters : \u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6 \uff08\u5373\u5377\u79ef\u4e2d\u6ee4\u6ce2\u5668\u7684\u8f93\u51fa\u6570\u91cf\uff09\u3002 kernel_size : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005\u5355\u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e 1D \u5377\u79ef\u7a97\u53e3\u7684\u957f\u5ea6\u3002 strides : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005\u5355\u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e\u5377\u79ef\u7684\u6b65\u957f\u3002 \u6307\u5b9a\u4efb\u4f55 stride \u503c != 1 \u4e0e\u6307\u5b9a dilation_rate \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 padding : \"valid\" \u6216 \"same\" (\u5927\u5c0f\u5199\u654f\u611f)\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4) \u6216 channels_first \u4e4b\u4e00\uff0c\u8868\u793a\u8f93\u5165\u4e2d\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, height, width, channels) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, channels, height, width) \u3002 \u5b83\u9ed8\u8ba4\u4e3a\u4ece Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d \u627e\u5230\u7684 image_data_format \u503c\u3002 \u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u5c06\u4f7f\u7528\u300cchannels_last\u300d\u3002 dilation_rate : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005\u5355\u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u4e3a\u4f7f\u7528\u6269\u5f20\uff08\u7a7a\u6d1e\uff09\u5377\u79ef\u6307\u660e\u6269\u5f20\u7387\u3002 \u76ee\u524d\uff0c\u6307\u5b9a\u4efb\u4f55 dilation_rate \u503c != 1 \u4e0e\u6307\u5b9a\u4efb\u4f55 stride \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 depth_multiplier : \u6bcf\u4e2a\u8f93\u5165\u901a\u9053\u7684\u6df1\u5ea6\u65b9\u5411\u5377\u79ef\u8f93\u51fa\u901a\u9053\u7684\u6570\u91cf\u3002 \u6df1\u5ea6\u65b9\u5411\u5377\u79ef\u8f93\u51fa\u901a\u9053\u7684\u603b\u6570\u5c06\u7b49\u4e8e filterss_in * depth_multiplier \u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u5982\u679c\u4f60\u4e0d\u6307\u5b9a\uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373\u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 depthwise_initializer : \u8fd0\u7528\u5230\u6df1\u5ea6\u65b9\u5411\u7684\u6838\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 pointwise_initializer : \u8fd0\u7528\u5230\u9010\u70b9\u6838\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 bias_initializer : \u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 depthwise_regularizer : \u8fd0\u7528\u5230\u6df1\u5ea6\u65b9\u5411\u7684\u6838\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 pointwise_regularizer : \u8fd0\u7528\u5230\u9010\u70b9\u6838\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u8f93\u51fa\uff08\u5b83\u7684\u6fc0\u6d3b\u503c\uff09\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 depthwise_constraint : \u8fd0\u7528\u5230\u6df1\u5ea6\u65b9\u5411\u7684\u6838\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 pointwise_constraint : \u8fd0\u7528\u5230\u9010\u70b9\u6838\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_first'\uff0c \u8f93\u5165 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, channels, steps) \u3002 \u5982\u679c data_format='channels_last'\uff0c \u8f93\u5165 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, steps, channels) \u3002 \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format='channels_first'\uff0c \u8f93\u51fa 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, filters, new_steps) \u3002 \u5982\u679c data_format='channels_last'\uff0c \u8f93\u51fa 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, new_steps, filters) \u3002 \u7531\u4e8e\u586b\u5145\u7684\u539f\u56e0\uff0c new_steps \u503c\u53ef\u80fd\u5df2\u66f4\u6539\u3002 [source] SeparableConv2D keras.layers.SeparableConv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), depth_multiplier=1, activation=None, use_bias=True, depthwise_initializer='glorot_uniform', pointwise_initializer='glorot_uniform', bias_initializer='zeros', depthwise_regularizer=None, pointwise_regularizer=None, bias_regularizer=None, activity_regularizer=None, depthwise_constraint=None, pointwise_constraint=None, bias_constraint=None) \u6df1\u5ea6\u65b9\u5411\u7684\u53ef\u5206\u79bb 2D \u5377\u79ef\u3002 \u53ef\u5206\u79bb\u7684\u5377\u79ef\u7684\u64cd\u4f5c\u5305\u62ec\uff0c\u9996\u5148\u6267\u884c\u6df1\u5ea6\u65b9\u5411\u7684\u7a7a\u95f4\u5377\u79ef \uff08\u5206\u522b\u4f5c\u7528\u4e8e\u6bcf\u4e2a\u8f93\u5165\u901a\u9053\uff09\uff0c\u7d27\u63a5\u4e00\u4e2a\u5c06\u6240\u5f97\u8f93\u51fa\u901a\u9053 \u6df7\u5408\u5728\u4e00\u8d77\u7684\u9010\u70b9\u5377\u79ef\u3002 depth_multiplier \u53c2\u6570\u63a7 \u5236\u6df1\u5ea6\u6b65\u9aa4\u4e2d\u6bcf\u4e2a\u8f93\u5165\u901a\u9053\u751f\u6210\u591a\u5c11\u4e2a\u8f93\u51fa\u901a\u9053\u3002 \u76f4\u89c2\u5730\u8bf4\uff0c\u53ef\u5206\u79bb\u7684\u5377\u79ef\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u79cd\u5c06\u5377\u79ef\u6838\u5206\u89e3\u6210 \u4e24\u4e2a\u8f83\u5c0f\u7684\u5377\u79ef\u6838\u7684\u65b9\u6cd5\uff0c\u6216\u8005\u4f5c\u4e3a Inception \u5757\u7684 \u4e00\u4e2a\u6781\u7aef\u7248\u672c\u3002 \u53c2\u6570 filters : \u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6 \uff08\u5373\u5377\u79ef\u4e2d\u6ee4\u6ce2\u5668\u7684\u8f93\u51fa\u6570\u91cf\uff09\u3002 kernel_size : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e 2D \u5377\u79ef\u7a97\u53e3\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 strides : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e\u5377\u79ef\u6cbf\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u65b9\u5411\u7684\u6b65\u957f\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 \u6307\u5b9a\u4efb\u4f55 stride \u503c != 1 \u4e0e\u6307\u5b9a dilation_rate \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 padding : \"valid\" \u6216 \"same\" (\u5927\u5c0f\u5199\u654f\u611f)\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4) \u6216 channels_first \u4e4b\u4e00\uff0c\u8868\u793a\u8f93\u5165\u4e2d\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, height, width, channels) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, channels, height, width) \u3002 \u5b83\u9ed8\u8ba4\u4e3a\u4ece Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d \u627e\u5230\u7684 image_data_format \u503c\u3002 \u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u5c06\u4f7f\u7528\u300cchannels_last\u300d\u3002 dilation_rate : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u4e3a\u4f7f\u7528\u6269\u5f20\uff08\u7a7a\u6d1e\uff09\u5377\u79ef\u6307\u660e\u6269\u5f20\u7387\u3002 \u76ee\u524d\uff0c\u6307\u5b9a\u4efb\u4f55 dilation_rate \u503c != 1 \u4e0e\u6307\u5b9a\u4efb\u4f55 stride \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 depth_multiplier : \u6bcf\u4e2a\u8f93\u5165\u901a\u9053\u7684\u6df1\u5ea6\u65b9\u5411\u5377\u79ef\u8f93\u51fa\u901a\u9053\u7684\u6570\u91cf\u3002 \u6df1\u5ea6\u65b9\u5411\u5377\u79ef\u8f93\u51fa\u901a\u9053\u7684\u603b\u6570\u5c06\u7b49\u4e8e filterss_in * depth_multiplier \u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u5982\u679c\u4f60\u4e0d\u6307\u5b9a\uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373\u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 depthwise_initializer : \u8fd0\u7528\u5230\u6df1\u5ea6\u65b9\u5411\u7684\u6838\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668 \u8be6\u89c1 initializers )\u3002 pointwise_initializer : \u8fd0\u7528\u5230\u9010\u70b9\u6838\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 bias_initializer : \u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 depthwise_regularizer : \u8fd0\u7528\u5230\u6df1\u5ea6\u65b9\u5411\u7684\u6838\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 pointwise_regularizer : \u8fd0\u7528\u5230\u9010\u70b9\u6838\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u8f93\u51fa\uff08\u5b83\u7684\u6fc0\u6d3b\u503c\uff09\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 depthwise_constraint : \u8fd0\u7528\u5230\u6df1\u5ea6\u65b9\u5411\u7684\u6838\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 pointwise_constraint : \u8fd0\u7528\u5230\u9010\u70b9\u6838\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_first'\uff0c \u8f93\u5165 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, channels, rows, cols) \u3002 \u5982\u679c data_format='channels_last'\uff0c \u8f93\u5165 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, rows, cols, channels) \u3002 \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format='channels_first'\uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, filters, new_rows, new_cols) \u3002 \u5982\u679c data_format='channels_last'\uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, new_rows, new_cols, filters) \u3002 \u7531\u4e8e\u586b\u5145\u7684\u539f\u56e0\uff0c rows \u548c cols \u503c\u53ef\u80fd\u5df2\u66f4\u6539\u3002 [source] DepthwiseConv2D keras.layers.DepthwiseConv2D(kernel_size, strides=(1, 1), padding='valid', depth_multiplier=1, data_format=None, activation=None, use_bias=True, depthwise_initializer='glorot_uniform', bias_initializer='zeros', depthwise_regularizer=None, bias_regularizer=None, activity_regularizer=None, depthwise_constraint=None, bias_constraint=None) \u6df1\u5ea6\u53ef\u5206\u79bb 2D \u5377\u79ef\u3002 \u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u5305\u62ec\u4ec5\u6267\u884c\u6df1\u5ea6\u7a7a\u95f4\u5377\u79ef\u4e2d\u7684\u7b2c\u4e00\u6b65\uff08\u5176\u5206\u522b\u4f5c\u7528\u4e8e\u6bcf\u4e2a\u8f93\u5165\u901a\u9053\uff09\u3002 depth_multiplier \u53c2\u6570\u63a7\u5236\u6df1\u5ea6\u6b65\u9aa4\u4e2d\u6bcf\u4e2a\u8f93\u5165\u901a\u9053\u751f\u6210\u591a\u5c11\u4e2a\u8f93\u51fa\u901a\u9053\u3002 Arguments kernel_size : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e 2D \u5377\u79ef\u7a97\u53e3\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 strides : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e\u5377\u79ef\u6cbf\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u65b9\u5411\u7684\u6b65\u957f\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 \u6307\u5b9a\u4efb\u4f55 stride \u503c != 1 \u4e0e\u6307\u5b9a dilation_rate \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 padding : \"valid\" \u6216 \"same\" (\u5927\u5c0f\u5199\u654f\u611f)\u3002 depth_multiplier : \u6bcf\u4e2a\u8f93\u5165\u901a\u9053\u7684\u6df1\u5ea6\u65b9\u5411\u5377\u79ef\u8f93\u51fa\u901a\u9053\u7684\u6570\u91cf\u3002 \u6df1\u5ea6\u65b9\u5411\u5377\u79ef\u8f93\u51fa\u901a\u9053\u7684\u603b\u6570\u5c06\u7b49\u4e8e filterss_in * depth_multiplier \u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4) \u6216 channels_first \u4e4b\u4e00\uff0c\u8868\u793a\u8f93\u5165\u4e2d\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, height, width, channels) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, channels, height, width) \u3002 \u5b83\u9ed8\u8ba4\u4e3a\u4ece Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d \u627e\u5230\u7684 image_data_format \u503c\u3002 \u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u5c06\u4f7f\u7528\u300cchannels_last\u300d\u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u5982\u679c\u4f60\u4e0d\u6307\u5b9a\uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373\u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 depthwise_initializer : \u8fd0\u7528\u5230\u6df1\u5ea6\u65b9\u5411\u7684\u6838\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668 \u8be6\u89c1 initializers )\u3002 bias_initializer : \u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 depthwise_regularizer : \u8fd0\u7528\u5230\u6df1\u5ea6\u65b9\u5411\u7684\u6838\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u8f93\u51fa\uff08\u5b83\u7684\u6fc0\u6d3b\u503c\uff09\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 depthwise_constraint : \u8fd0\u7528\u5230\u6df1\u5ea6\u65b9\u5411\u7684\u6838\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_first'\uff0c \u8f93\u5165 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, channels, rows, cols) \u3002 \u5982\u679c data_format='channels_last'\uff0c \u8f93\u5165 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, rows, cols, channels) \u3002 \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format='channels_first'\uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, filters, new_rows, new_cols) \u3002 \u5982\u679c data_format='channels_last'\uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, new_rows, new_cols, filters) \u3002 \u7531\u4e8e\u586b\u5145\u7684\u539f\u56e0\uff0c rows \u548c cols \u503c\u53ef\u80fd\u5df2\u66f4\u6539\u3002 [source] Conv2DTranspose keras.layers.Conv2DTranspose(filters, kernel_size, strides=(1, 1), padding='valid', output_padding=None, data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) \u8f6c\u7f6e\u5377\u79ef\u5c42 (\u6709\u65f6\u88ab\u6210\u4e3a\u53cd\u5377\u79ef)\u3002 \u5bf9\u8f6c\u7f6e\u5377\u79ef\u7684\u9700\u6c42\u4e00\u822c\u6765\u81ea\u5e0c\u671b\u4f7f\u7528 \u4e0e\u6b63\u5e38\u5377\u79ef\u76f8\u53cd\u65b9\u5411\u7684\u53d8\u6362\uff0c \u5373\uff0c\u5c06\u5177\u6709\u5377\u79ef\u8f93\u51fa\u5c3a\u5bf8\u7684\u4e1c\u897f \u8f6c\u6362\u4e3a\u5177\u6709\u5377\u79ef\u8f93\u5165\u5c3a\u5bf8\u7684\u4e1c\u897f\uff0c \u540c\u65f6\u4fdd\u6301\u4e0e\u6240\u8ff0\u5377\u79ef\u76f8\u5bb9\u7684\u8fde\u901a\u6027\u6a21\u5f0f\u3002 \u5f53\u4f7f\u7528\u8be5\u5c42\u4f5c\u4e3a\u6a21\u578b\u7b2c\u4e00\u5c42\u65f6\uff0c\u9700\u8981\u63d0\u4f9b input_shape \u53c2\u6570 \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u542b\u6837\u672c\u8868\u793a\u7684\u8f74\uff09\uff0c\u4f8b\u5982\uff0c input_shape=(128, 128, 3) \u8868\u793a 128x128 RGB \u56fe\u50cf\uff0c \u5728 data_format=\"channels_last\" \u65f6\u3002 \u53c2\u6570 filters : \u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6 \uff08\u5373\u5377\u79ef\u4e2d\u6ee4\u6ce2\u5668\u7684\u8f93\u51fa\u6570\u91cf\uff09\u3002 kernel_size : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e 2D \u5377\u79ef\u7a97\u53e3\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 strides : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e\u5377\u79ef\u6cbf\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u65b9\u5411\u7684\u6b65\u957f\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 \u6307\u5b9a\u4efb\u4f55 stride \u503c != 1 \u4e0e\u6307\u5b9a dilation_rate \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 padding : \"valid\" \u6216 \"same\" (\u5927\u5c0f\u5199\u654f\u611f)\u3002 output_padding : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u5b9a\u6cbf\u8f93\u51fa\u5f20\u91cf\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u7684\u586b\u5145\u91cf\u3002 \u53ef\u4ee5\u662f\u5355\u4e2a\u6574\u6570\uff0c\u4ee5\u6307\u5b9a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u7684\u76f8\u540c\u503c\u3002 \u6cbf\u7ed9\u5b9a\u7ef4\u5ea6\u7684\u8f93\u51fa\u586b\u5145\u91cf\u5fc5\u987b\u4f4e\u4e8e\u6cbf\u540c\u4e00\u7ef4\u5ea6\u7684\u6b65\u957f\u3002 \u5982\u679c\u8bbe\u7f6e\u4e3a None (\u9ed8\u8ba4), \u8f93\u51fa\u5c3a\u5bf8\u5c06\u81ea\u52a8\u63a8\u7406\u51fa\u6765\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4) \u6216 channels_first \u4e4b\u4e00\uff0c\u8868\u793a\u8f93\u5165\u4e2d\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, height, width, channels) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, channels, height, width) \u3002 \u5b83\u9ed8\u8ba4\u4e3a\u4ece Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d \u627e\u5230\u7684 image_data_format \u503c\u3002 \u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u5c06\u4f7f\u7528 \"channels_last\"\u3002 dilation_rate : \u4e00\u4e2a\u6574\u6570\u6216 2 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u5b9a\u81a8\u80c0\u5377\u79ef\u7684\u81a8\u80c0\u7387\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 \u5f53\u524d\uff0c\u6307\u5b9a\u4efb\u4f55 dilation_rate \u503c != 1 \u4e0e \u6307\u5b9a stride \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u5982\u679c\u4f60\u4e0d\u6307\u5b9a\uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373\u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 bias_initializer : \u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u8f93\u51fa\uff08\u5b83\u7684\u6fc0\u6d3b\u503c\uff09\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_first'\uff0c \u8f93\u5165 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, channels, rows, cols) \u3002 \u5982\u679c data_format='channels_last'\uff0c \u8f93\u5165 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, rows, cols, channels) \u3002 \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format='channels_first'\uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, filters, new_rows, new_cols) \u3002 \u5982\u679c data_format='channels_last'\uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, new_rows, new_cols, filters) \u3002 \u7531\u4e8e\u586b\u5145\u7684\u539f\u56e0\uff0c rows \u548c cols \u503c\u53ef\u80fd\u5df2\u66f4\u6539\u3002 \u5982\u679c\u6307\u5b9a\u4e86 output_padding : new_rows = ((rows - 1) * strides[0] + kernel_size[0] - 2 * padding[0] + output_padding[0]) new_cols = ((cols - 1) * strides[1] + kernel_size[1] - 2 * padding[1] + output_padding[1]) \u53c2\u8003\u6587\u732e A guide to convolution arithmetic for deep learning Deconvolutional Networks [source] Conv3D keras.layers.Conv3D(filters, kernel_size, strides=(1, 1, 1), padding='valid', data_format=None, dilation_rate=(1, 1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) 3D \u5377\u79ef\u5c42 (\u4f8b\u5982\u7acb\u4f53\u7a7a\u95f4\u5377\u79ef)\u3002 \u8be5\u5c42\u521b\u5efa\u4e86\u4e00\u4e2a\u5377\u79ef\u6838\uff0c \u8be5\u5377\u79ef\u6838\u5bf9\u5c42\u8f93\u5165\u8fdb\u884c\u5377\u79ef\uff0c \u4ee5\u751f\u6210\u8f93\u51fa\u5f20\u91cf\u3002 \u5982\u679c use_bias \u4e3a True\uff0c \u5219\u4f1a\u521b\u5efa\u4e00\u4e2a\u504f\u7f6e\u5411\u91cf\u5e76\u5c06\u5176\u6dfb\u52a0\u5230\u8f93\u51fa\u4e2d\u3002 \u6700\u540e\uff0c\u5982\u679c activation \u4e0d\u662f None \uff0c\u5b83\u4e5f\u4f1a\u5e94\u7528\u4e8e\u8f93\u51fa\u3002 \u5f53\u4f7f\u7528\u8be5\u5c42\u4f5c\u4e3a\u6a21\u578b\u7b2c\u4e00\u5c42\u65f6\uff0c\u9700\u8981\u63d0\u4f9b input_shape \u53c2\u6570 \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u542b\u6837\u672c\u8868\u793a\u7684\u8f74\uff09\uff0c\u4f8b\u5982\uff0c input_shape=(128, 128, 128, 1) \u8868\u793a 128x128x128 \u7684\u5355\u901a\u9053\u7acb\u4f53\uff0c \u5728 data_format=\"channels_last\" \u65f6\u3002 \u53c2\u6570 filters : \u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6 \uff08\u5373\u5377\u79ef\u4e2d\u6ee4\u6ce2\u5668\u7684\u8f93\u51fa\u6570\u91cf\uff09\u3002 kernel_size : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 3 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e 3D \u5377\u79ef\u7a97\u53e3\u7684\u6df1\u5ea6\u3001\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 strides : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 3 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e\u5377\u79ef\u6cbf\u6bcf\u4e00\u4e2a\u7a7a\u95f4\u7ef4\u5ea6\u7684\u6b65\u957f\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u6b65\u957f\u503c\u3002 \u6307\u5b9a\u4efb\u4f55 stride \u503c != 1 \u4e0e\u6307\u5b9a dilation_rate \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 padding : \"valid\" \u6216 \"same\" (\u5927\u5c0f\u5199\u654f\u611f)\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4) \u6216 channels_first \u4e4b\u4e00\uff0c \u8868\u793a\u8f93\u5165\u4e2d\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) \u3002 \u5b83\u9ed8\u8ba4\u4e3a\u4ece Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d \u627e\u5230\u7684 image_data_format \u503c\u3002 \u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u5c06\u4f7f\u7528 \"channels_last\"\u3002 dilation_rate : \u4e00\u4e2a\u6574\u6570\u6216 3 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u5b9a\u81a8\u80c0\u5377\u79ef\u7684\u81a8\u80c0\u7387\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 \u5f53\u524d\uff0c\u6307\u5b9a\u4efb\u4f55 dilation_rate \u503c != 1 \u4e0e \u6307\u5b9a stride \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u5982\u679c\u4f60\u4e0d\u6307\u5b9a\uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373\u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 bias_initializer : \u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u8f93\u51fa\uff08\u5b83\u7684\u6fc0\u6d3b\u503c\uff09\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_first'\uff0c \u8f93\u5165 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (samples, channels, conv_dim1, conv_dim2, conv_dim3) \u3002 \u5982\u679c data_format='channels_last'\uff0c \u8f93\u5165 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (samples, conv_dim1, conv_dim2, conv_dim3, channels) \u3002 \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format='channels_first'\uff0c \u8f93\u51fa 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (samples, filters, new_conv_dim1, new_conv_dim2, new_conv_dim3) \u3002 \u5982\u679c data_format='channels_last'\uff0c \u8f93\u51fa 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (samples, new_conv_dim1, new_conv_dim2, new_conv_dim3, filters) \u3002 \u7531\u4e8e\u586b\u5145\u7684\u539f\u56e0\uff0c new_conv_dim1 , new_conv_dim2 \u548c new_conv_dim3 \u503c\u53ef\u80fd\u5df2\u66f4\u6539\u3002 [source] Conv3DTranspose keras.layers.Conv3DTranspose(filters, kernel_size, strides=(1, 1, 1), padding='valid', output_padding=None, data_format=None, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) \u8f6c\u7f6e\u5377\u79ef\u5c42 (\u6709\u65f6\u88ab\u6210\u4e3a\u53cd\u5377\u79ef)\u3002 \u5bf9\u8f6c\u7f6e\u5377\u79ef\u7684\u9700\u6c42\u4e00\u822c\u6765\u81ea\u5e0c\u671b\u4f7f\u7528 \u4e0e\u6b63\u5e38\u5377\u79ef\u76f8\u53cd\u65b9\u5411\u7684\u53d8\u6362\uff0c \u5373\uff0c\u5c06\u5177\u6709\u5377\u79ef\u8f93\u51fa\u5c3a\u5bf8\u7684\u4e1c\u897f \u8f6c\u6362\u4e3a\u5177\u6709\u5377\u79ef\u8f93\u5165\u5c3a\u5bf8\u7684\u4e1c\u897f\uff0c \u540c\u65f6\u4fdd\u6301\u4e0e\u6240\u8ff0\u5377\u79ef\u76f8\u5bb9\u7684\u8fde\u901a\u6027\u6a21\u5f0f\u3002 \u5f53\u4f7f\u7528\u8be5\u5c42\u4f5c\u4e3a\u6a21\u578b\u7b2c\u4e00\u5c42\u65f6\uff0c\u9700\u8981\u63d0\u4f9b input_shape \u53c2\u6570 \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u542b\u6837\u672c\u8868\u793a\u7684\u8f74\uff09\uff0c\u4f8b\u5982\uff0c input_shape=(128, 128, 128, 3) \u8868\u793a\u5c3a\u5bf8 128x128x128 \u7684 3 \u901a\u9053\u7acb\u4f53\uff0c \u5728 data_format=\"channels_last\" \u65f6\u3002 \u53c2\u6570 filters : \u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6 \uff08\u5373\u5377\u79ef\u4e2d\u6ee4\u6ce2\u5668\u7684\u8f93\u51fa\u6570\u91cf\uff09\u3002 kernel_size : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 3 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e 3D \u5377\u79ef\u7a97\u53e3\u7684\u6df1\u5ea6\u3001\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 strides : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 3 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e\u6cbf\u6df1\u5ea6\u3001\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u65b9\u5411\u7684\u6b65\u957f\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 \u6307\u5b9a\u4efb\u4f55 stride \u503c != 1 \u4e0e\u6307\u5b9a dilation_rate \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 padding : \"valid\" \u6216 \"same\" (\u5927\u5c0f\u5199\u654f\u611f)\u3002 output_padding : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 3 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u5b9a\u6cbf\u8f93\u51fa\u5f20\u91cf\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u7684\u586b\u5145\u91cf\u3002 \u53ef\u4ee5\u662f\u5355\u4e2a\u6574\u6570\uff0c\u4ee5\u6307\u5b9a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u7684\u76f8\u540c\u503c\u3002 \u6cbf\u7ed9\u5b9a\u7ef4\u5ea6\u7684\u8f93\u51fa\u586b\u5145\u91cf\u5fc5\u987b\u4f4e\u4e8e\u6cbf\u540c\u4e00\u7ef4\u5ea6\u7684\u6b65\u957f\u3002 \u5982\u679c\u8bbe\u7f6e\u4e3a None (\u9ed8\u8ba4), \u8f93\u51fa\u5c3a\u5bf8\u5c06\u81ea\u52a8\u63a8\u7406\u51fa\u6765\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4) \u6216 channels_first \u4e4b\u4e00\uff0c\u8868\u793a\u8f93\u5165\u4e2d\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, depth, height, width, channels) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, channels, depth, height, width) \u3002 \u5b83\u9ed8\u8ba4\u4e3a\u4ece Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d \u627e\u5230\u7684 image_data_format \u503c\u3002 \u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u5c06\u4f7f\u7528\u300cchannels_last\u300d\u3002 dilation_rate : \u4e00\u4e2a\u6574\u6570\u6216 3 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u5b9a\u81a8\u80c0\u5377\u79ef\u7684\u81a8\u80c0\u7387\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 \u5f53\u524d\uff0c\u6307\u5b9a\u4efb\u4f55 dilation_rate \u503c != 1 \u4e0e \u6307\u5b9a stride \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u5982\u679c\u4f60\u4e0d\u6307\u5b9a\uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373\u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 bias_initializer : \u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u8f93\u51fa\uff08\u5b83\u7684\u6fc0\u6d3b\u503c\uff09\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_first'\uff0c \u8f93\u5165 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, channels, depth, rows, cols) \uff0c \u5982\u679c data_format='channels_last'\uff0c \u8f93\u5165 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, depth, rows, cols, channels) \u3002 Output shape \u5982\u679c data_format='channels_first'\uff0c \u8f93\u51fa 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, filters, new_depth, new_rows, new_cols) \uff0c \u5982\u679c data_format='channels_last'\uff0c \u8f93\u51fa 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, new_depth, new_rows, new_cols, filters) \u3002 depth \u548c rows \u548c cols \u53ef\u80fd\u56e0\u4e3a\u586b\u5145\u800c\u6539\u53d8\u3002 \u5982\u679c\u6307\u5b9a\u4e86 output_padding \uff1a new_depth = ((depth - 1) * strides[0] + kernel_size[0] - 2 * padding[0] + output_padding[0]) new_rows = ((rows - 1) * strides[1] + kernel_size[1] - 2 * padding[1] + output_padding[1]) new_cols = ((cols - 1) * strides[2] + kernel_size[2] - 2 * padding[2] + output_padding[2]) \u53c2\u8003\u6587\u732e [A guide to convolution arithmetic for deep learning] (https://arxiv.org/abs/1603.07285v1) [Deconvolutional Networks] (http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf) [source] Cropping1D keras.layers.Cropping1D(cropping=(1, 1)) 1D \u8f93\u5165\u7684\u88c1\u526a\u5c42\uff08\u4f8b\u5982\u65f6\u95f4\u5e8f\u5217\uff09\u3002 \u5b83\u6cbf\u7740\u65f6\u95f4\u7ef4\u5ea6\uff08\u7b2c 1 \u4e2a\u8f74\uff09\u88c1\u526a\u3002 \u53c2\u6570 cropping : \u6574\u6570\u6216\u6574\u6570\u5143\u7ec4\uff08\u957f\u5ea6\u4e3a 2\uff09\u3002 \u5728\u88c1\u526a\u7ef4\u5ea6\uff08\u7b2c 1 \u4e2a\u8f74\uff09\u7684\u5f00\u59cb\u548c\u7ed3\u675f\u4f4d\u7f6e \u5e94\u8be5\u88c1\u526a\u591a\u5c11\u4e2a\u5355\u4f4d\u3002 \u5982\u679c\u53ea\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6574\u6570\uff0c\u90a3\u4e48\u8fd9\u4e24\u4e2a\u4f4d\u7f6e\u5c06\u4f7f\u7528 \u76f8\u540c\u7684\u503c\u3002 \u8f93\u5165\u5c3a\u5bf8 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, axis_to_crop, features) \u3002 \u8f93\u51fa\u5c3a\u5bf8 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, cropped_axis, features) \u3002 [source] Cropping2D keras.layers.Cropping2D(cropping=((0, 0), (0, 0)), data_format=None) 2D \u8f93\u5165\u7684\u88c1\u526a\u5c42\uff08\u4f8b\u5982\u56fe\u50cf\uff09\u3002 \u5b83\u6cbf\u7740\u7a7a\u95f4\u7ef4\u5ea6\u88c1\u526a\uff0c\u5373\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u3002 \u53c2\u6570 cropping : \u6574\u6570\uff0c\u6216 2 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\uff0c\u6216 2 \u4e2a\u6574\u6570\u7684 2 \u4e2a\u5143\u7ec4\u3002 \u5982\u679c\u4e3a\u6574\u6570\uff1a \u5c06\u5bf9\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u5e94\u7528\u76f8\u540c\u7684\u5bf9\u79f0\u88c1\u526a\u3002 \u5982\u679c\u4e3a 2 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\uff1a \u89e3\u91ca\u4e3a\u5bf9\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u7684\u4e24\u4e2a\u4e0d\u540c\u7684\u5bf9\u79f0\u88c1\u526a\u503c\uff1a (symmetric_height_crop, symmetric_width_crop) \u3002 \u5982\u679c\u4e3a 2 \u4e2a\u6574\u6570\u7684 2 \u4e2a\u5143\u7ec4\uff1a \u89e3\u91ca\u4e3a ((top_crop, bottom_crop), (left_crop, right_crop)) \u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4) \u6216 channels_first \u4e4b\u4e00\uff0c \u8868\u793a\u8f93\u5165\u4e2d\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, height, width, channels) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, channels, height, width) \u3002 \u5b83\u9ed8\u8ba4\u4e3a\u4ece Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d \u627e\u5230\u7684 image_data_format \u503c\u3002 \u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u5c06\u4f7f\u7528 \"channels_last\"\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format='channels_last'\uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, rows, cols, channels) \u3002 \u5982\u679c data_format='channels_first'\uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, channels, rows, cols) \u3002 \u7531\u4e8e\u586b\u5145\u7684\u539f\u56e0\uff0c rows \u548c cols \u503c\u53ef\u80fd\u5df2\u66f4\u6539\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format \u4e3a \"channels_last\" \uff0c \u8f93\u5165 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, cropped_rows, cropped_cols, channels) \u3002 \u5982\u679c data_format \u4e3a \"channels_first\" \uff0c \u8f93\u5165 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, channels, cropped_rows, cropped_cols) \u3002 \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format \u4e3a \"channels_last\" \uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, cropped_rows, cropped_cols, channels) \u5982\u679c data_format \u4e3a \"channels_first\" \uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, channels, cropped_rows, cropped_cols) \u3002 \u4f8b\u5b50 # \u88c1\u526a\u8f93\u5165\u7684 2D \u56fe\u50cf\u6216\u7279\u5f81\u56fe model = Sequential() model.add(Cropping2D(cropping=((2, 2), (4, 4)), input_shape=(28, 28, 3))) # \u73b0\u5728 model.output_shape == (None, 24, 20, 3) model.add(Conv2D(64, (3, 3), padding='same')) model.add(Cropping2D(cropping=((2, 2), (2, 2)))) # \u73b0\u5728 model.output_shape == (None, 20, 16. 64) [source] Cropping3D keras.layers.Cropping3D(cropping=((1, 1), (1, 1), (1, 1)), data_format=None) 3D \u6570\u636e\u7684\u88c1\u526a\u5c42\uff08\u4f8b\u5982\u7a7a\u95f4\u6216\u65f6\u7a7a\uff09\u3002 \u53c2\u6570 cropping : \u6574\u6570\uff0c\u6216 3 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\uff0c\u6216 2 \u4e2a\u6574\u6570\u7684 3 \u4e2a\u5143\u7ec4\u3002 \u5982\u679c\u4e3a\u6574\u6570\uff1a \u5c06\u5bf9\u6df1\u5ea6\u3001\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u5e94\u7528\u76f8\u540c\u7684\u5bf9\u79f0\u88c1\u526a\u3002 \u5982\u679c\u4e3a 3 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\uff1a \u89e3\u91ca\u4e3a\u5bf9\u6df1\u5ea6\u3001\u9ad8\u5ea6\u548c\u9ad8\u5ea6\u7684 3 \u4e2a\u4e0d\u540c\u7684\u5bf9\u79f0\u88c1\u526a\u503c\uff1a (symmetric_dim1_crop, symmetric_dim2_crop, symmetric_dim3_crop) \u3002 \u5982\u679c\u4e3a 2 \u4e2a\u6574\u6570\u7684 3 \u4e2a\u5143\u7ec4\uff1a \u89e3\u91ca\u4e3a ((left_dim1_crop, right_dim1_crop), (left_dim2_crop, right_dim2_crop), (left_dim3_crop, right_dim3_crop)) \u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4) \u6216 channels_first \u4e4b\u4e00\uff0c \u8868\u793a\u8f93\u5165\u4e2d\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) \u3002 \u5b83\u9ed8\u8ba4\u4e3a\u4ece Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d \u627e\u5230\u7684 image_data_format \u503c\u3002 \u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u5c06\u4f7f\u7528 \"channels_last\"\u3002 \u8f93\u5165\u5c3a\u5bf8 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a \u5982\u679c data_format \u4e3a \"channels_last\" : (batch, first_cropped_axis, second_cropped_axis, third_cropped_axis, depth) \u5982\u679c data_format \u4e3a \"channels_first\" : (batch, depth, first_cropped_axis, second_cropped_axis, third_cropped_axis) \u8f93\u51fa\u5c3a\u5bf8 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a \u5982\u679c data_format \u4e3a \"channels_last\" : (batch, first_cropped_axis, second_cropped_axis, third_cropped_axis, depth) \u5982\u679c data_format \u4e3a \"channels_first\" : (batch, depth, first_cropped_axis, second_cropped_axis, third_cropped_axis) \u3002 [source] UpSampling1D keras.layers.UpSampling1D(size=2) 1D \u8f93\u5165\u7684\u4e0a\u91c7\u6837\u5c42\u3002 \u6cbf\u7740\u65f6\u95f4\u8f74\u91cd\u590d\u6bcf\u4e2a\u65f6\u95f4\u6b65 size \u6b21\u3002 \u53c2\u6570 size : \u6574\u6570\u3002\u4e0a\u91c7\u6837\u56e0\u5b50\u3002 \u8f93\u5165\u5c3a\u5bf8 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, steps, features) \u3002 \u8f93\u51fa\u5c3a\u5bf8 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, upsampled_steps, features) \u3002 [source] UpSampling2D keras.layers.UpSampling2D(size=(2, 2), data_format=None, interpolation='nearest') 2D \u8f93\u5165\u7684\u4e0a\u91c7\u6837\u5c42\u3002 \u6cbf\u7740\u6570\u636e\u7684\u884c\u548c\u5217\u5206\u522b\u91cd\u590d size[0] \u548c size[1] \u6b21\u3002 \u53c2\u6570 size : \u6574\u6570\uff0c\u6216 2 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\u3002 \u884c\u548c\u5217\u7684\u4e0a\u91c7\u6837\u56e0\u5b50\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4) \u6216 channels_first \u4e4b\u4e00\uff0c \u8868\u793a\u8f93\u5165\u4e2d\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, height, width, channels) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, channels, height, width) \u3002 \u5b83\u9ed8\u8ba4\u4e3a\u4ece Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d \u627e\u5230\u7684 image_data_format \u503c\u3002 \u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u5c06\u4f7f\u7528 \"channels_last\"\u3002 interpolation : \u5b57\u7b26\u4e32\uff0c nearest \u6216 bilinear \u4e4b\u4e00\u3002 \u6ce8\u610f CNTK \u6682\u4e0d\u652f\u6301 bilinear upscaling\uff0c \u4ee5\u53ca\u5bf9\u4e8e Theano\uff0c\u53ea\u53ef\u4ee5\u4f7f\u7528 size=(2, 2) \u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format \u4e3a \"channels_last\" \uff0c \u8f93\u5165 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, rows, cols, channels) \u3002 \u5982\u679c data_format \u4e3a \"channels_first\" \uff0c \u8f93\u5165 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, channels, rows, cols) \u3002 \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format \u4e3a \"channels_last\" \uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, upsampled_rows, upsampled_cols, channels) \u3002 \u5982\u679c data_format \u4e3a \"channels_first\" \uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, channels, upsampled_rows, upsampled_cols) \u3002 [source] UpSampling3D keras.layers.UpSampling3D(size=(2, 2, 2), data_format=None) 3D \u8f93\u5165\u7684\u4e0a\u91c7\u6837\u5c42\u3002 \u6cbf\u7740\u6570\u636e\u7684\u7b2c 1\u30012\u30013 \u7ef4\u5ea6\u5206\u522b\u91cd\u590d size[0] \u3001 size[1] \u548c size[2] \u6b21\u3002 \u53c2\u6570 size : \u6574\u6570\uff0c\u6216 3 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\u3002 dim1, dim2 \u548c dim3 \u7684\u4e0a\u91c7\u6837\u56e0\u5b50\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4) \u6216 channels_first \u4e4b\u4e00\uff0c \u8868\u793a\u8f93\u5165\u4e2d\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) \u3002 \u5b83\u9ed8\u8ba4\u4e3a\u4ece Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d \u627e\u5230\u7684 image_data_format \u503c\u3002 \u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u5c06\u4f7f\u7528 \"channels_last\"\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format \u4e3a \"channels_last\" \uff0c \u8f93\u5165 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, dim1, dim2, dim3, channels) \u3002 \u5982\u679c data_format \u4e3a \"channels_first\" \uff0c \u8f93\u5165 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, channels, dim1, dim2, dim3) \u3002 \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format \u4e3a \"channels_last\" \uff0c \u8f93\u51fa 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, upsampled_dim1, upsampled_dim2, upsampled_dim3, channels) \u3002 \u5982\u679c data_format \u4e3a \"channels_first\" \uff0c \u8f93\u51fa 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, channels, upsampled_dim1, upsampled_dim2, upsampled_dim3) \u3002 [source] ZeroPadding1D keras.layers.ZeroPadding1D(padding=1) 1D \u8f93\u5165\u7684\u96f6\u586b\u5145\u5c42\uff08\u4f8b\u5982\uff0c\u65f6\u95f4\u5e8f\u5217\uff09\u3002 \u53c2\u6570 padding : \u6574\u6570\uff0c\u6216\u957f\u5ea6\u4e3a 2 \u7684\u6574\u6570\u5143\u7ec4\uff0c\u6216\u5b57\u5178\u3002 \u5982\u679c\u4e3a\u6574\u6570\uff1a \u5728\u586b\u5145\u7ef4\u5ea6\uff08\u7b2c\u4e00\u4e2a\u8f74\uff09\u7684\u5f00\u59cb\u548c\u7ed3\u675f\u5904\u6dfb\u52a0\u591a\u5c11\u4e2a\u96f6\u3002 \u5982\u679c\u662f\u957f\u5ea6\u4e3a 2 \u7684\u6574\u6570\u5143\u7ec4\uff1a \u5728\u586b\u5145\u7ef4\u5ea6\u7684\u5f00\u59cb\u548c\u7ed3\u5c3e\u5904\u6dfb\u52a0\u591a\u5c11\u4e2a\u96f6 ( (left_pad, right_pad) )\u3002 \u8f93\u5165\u5c3a\u5bf8 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, axis_to_pad, features) \u3002 \u8f93\u51fa\u5c3a\u5bf8 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, padded_axis, features) \u3002 [source] ZeroPadding2D keras.layers.ZeroPadding2D(padding=(1, 1), data_format=None) 2D \u8f93\u5165\u7684\u96f6\u586b\u5145\u5c42\uff08\u4f8b\u5982\u56fe\u50cf\uff09\u3002 \u8be5\u56fe\u5c42\u53ef\u4ee5\u5728\u56fe\u50cf\u5f20\u91cf\u7684\u9876\u90e8\u3001\u5e95\u90e8\u3001\u5de6\u4fa7\u548c\u53f3\u4fa7\u6dfb\u52a0\u96f6\u8868\u793a\u7684\u884c\u548c\u5217\u3002 \u53c2\u6570 padding : \u6574\u6570\uff0c\u6216 2 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\uff0c\u6216 2 \u4e2a\u6574\u6570\u7684 2 \u4e2a\u5143\u7ec4\u3002 \u5982\u679c\u4e3a\u6574\u6570\uff1a\u5c06\u5bf9\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u8fd0\u7528\u76f8\u540c\u7684\u5bf9\u79f0\u586b\u5145\u3002 \u5982\u679c\u4e3a 2 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\uff1a \u5982\u679c\u4e3a\u6574\u6570\uff1a: \u89e3\u91ca\u4e3a\u9ad8\u5ea6\u548c\u9ad8\u5ea6\u7684 2 \u4e2a\u4e0d\u540c\u7684\u5bf9\u79f0\u88c1\u526a\u503c\uff1a (symmetric_height_pad, symmetric_width_pad) \u3002 \u5982\u679c\u4e3a 2 \u4e2a\u6574\u6570\u7684 2 \u4e2a\u5143\u7ec4\uff1a \u89e3\u91ca\u4e3a ((top_pad, bottom_pad), (left_pad, right_pad)) \u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4) \u6216 channels_first \u4e4b\u4e00\uff0c \u8868\u793a\u8f93\u5165\u4e2d\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, height, width, channels) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, channels, height, width) \u3002 \u5b83\u9ed8\u8ba4\u4e3a\u4ece Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d \u627e\u5230\u7684 image_data_format \u503c\u3002 \u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u5c06\u4f7f\u7528 \"channels_last\"\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format \u4e3a \"channels_last\" \uff0c \u8f93\u5165 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, rows, cols, channels) \u3002 \u5982\u679c data_format \u4e3a \"channels_first\" \uff0c \u8f93\u5165 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, channels, rows, cols) \u3002 \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format \u4e3a \"channels_last\" \uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, padded_rows, padded_cols, channels) \u3002 \u5982\u679c data_format \u4e3a \"channels_first\" \uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, channels, padded_rows, padded_cols) \u3002 [source] ZeroPadding3D keras.layers.ZeroPadding3D(padding=(1, 1, 1), data_format=None) 3D \u6570\u636e\u7684\u96f6\u586b\u5145\u5c42(\u7a7a\u95f4\u6216\u65f6\u7a7a)\u3002 \u53c2\u6570 padding : \u6574\u6570\uff0c\u6216 3 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\uff0c\u6216 2 \u4e2a\u6574\u6570\u7684 3 \u4e2a\u5143\u7ec4\u3002 \u5982\u679c\u4e3a\u6574\u6570\uff1a\u5c06\u5bf9\u6df1\u5ea6\u3001\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u8fd0\u7528\u76f8\u540c\u7684\u5bf9\u79f0\u586b\u5145\u3002 \u5982\u679c\u4e3a 3 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\uff1a \u89e3\u91ca\u4e3a\u6df1\u5ea6\u3001\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u7684\u4e09\u4e2a\u4e0d\u540c\u7684\u5bf9\u79f0\u586b\u5145\u503c\uff1a (symmetric_dim1_pad, symmetric_dim2_pad, symmetric_dim3_pad) . \u5982\u679c\u4e3a 2 \u4e2a\u6574\u6570\u7684 3 \u4e2a\u5143\u7ec4\uff1a\u89e3\u91ca\u4e3a ((left_dim1_pad, right_dim1_pad), (left_dim2_pad, right_dim2_pad), (left_dim3_pad, right_dim3_pad)) data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4) \u6216 channels_first \u4e4b\u4e00\uff0c \u8868\u793a\u8f93\u5165\u4e2d\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) \u3002 \u5b83\u9ed8\u8ba4\u4e3a\u4ece Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d \u627e\u5230\u7684 image_data_format \u503c\u3002 \u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u5c06\u4f7f\u7528 \"channels_last\"\u3002 \u8f93\u5165\u5c3a\u5bf8 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a \u5982\u679c data_format \u4e3a \"channels_last\" : (batch, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad, depth) \u3002 \u5982\u679c data_format \u4e3a \"channels_first\" : (batch, depth, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad) \u3002 \u8f93\u51fa\u5c3a\u5bf8 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a \u5982\u679c data_format \u4e3a \"channels_last\" : (batch, first_padded_axis, second_padded_axis, third_axis_to_pad, depth) \u3002 \u5982\u679c data_format \u4e3a \"channels_first\" : (batch, depth, first_padded_axis, second_padded_axis, third_axis_to_pad) \u3002","title":"\u5377\u79ef\u5c42"},{"location":"2-Layers/2.convolutional/#_1","text":"[source]","title":"\u5377\u79ef\u5c42"},{"location":"2-Layers/2.convolutional/#conv1d","text":"keras.layers.Conv1D(filters, kernel_size, strides=1, padding='valid', data_format='channels_last', dilation_rate=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) 1D \u5377\u79ef\u5c42 (\u4f8b\u5982\u65f6\u5e8f\u5377\u79ef)\u3002 \u8be5\u5c42\u521b\u5efa\u4e86\u4e00\u4e2a\u5377\u79ef\u6838\uff0c\u8be5\u5377\u79ef\u6838\u4ee5 \u5355\u4e2a\u7a7a\u95f4\uff08\u6216\u65f6\u95f4\uff09\u7ef4\u4e0a\u7684\u5c42\u8f93\u5165\u8fdb\u884c\u5377\u79ef\uff0c \u4ee5\u751f\u6210\u8f93\u51fa\u5f20\u91cf\u3002 \u5982\u679c use_bias \u4e3a True\uff0c \u5219\u4f1a\u521b\u5efa\u4e00\u4e2a\u504f\u7f6e\u5411\u91cf\u5e76\u5c06\u5176\u6dfb\u52a0\u5230\u8f93\u51fa\u4e2d\u3002 \u6700\u540e\uff0c\u5982\u679c activation \u4e0d\u662f None \uff0c\u5b83\u4e5f\u4f1a\u5e94\u7528\u4e8e\u8f93\u51fa\u3002 \u5f53\u4f7f\u7528\u8be5\u5c42\u4f5c\u4e3a\u6a21\u578b\u7b2c\u4e00\u5c42\u65f6\uff0c\u9700\u8981\u63d0\u4f9b input_shape \u53c2\u6570\uff08\u6574\u6570\u5143\u7ec4\u6216 None \uff09\uff0c\u4f8b\u5982\uff0c (10, 128) \u8868\u793a 10 \u4e2a 128 \u7ef4\u7684\u5411\u91cf\u7ec4\u6210\u7684\u5411\u91cf\u5e8f\u5217\uff0c (None, 128) \u8868\u793a 128 \u7ef4\u7684\u5411\u91cf\u7ec4\u6210\u7684\u53d8\u957f\u5e8f\u5217\u3002 \u53c2\u6570 filters : \u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6 \uff08\u5373\u5377\u79ef\u4e2d\u6ee4\u6ce2\u5668\u7684\u8f93\u51fa\u6570\u91cf\uff09\u3002 kernel_size : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005\u5355\u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e 1D \u5377\u79ef\u7a97\u53e3\u7684\u957f\u5ea6\u3002 strides : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005\u5355\u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e\u5377\u79ef\u7684\u6b65\u957f\u3002 \u6307\u5b9a\u4efb\u4f55 stride \u503c != 1 \u4e0e\u6307\u5b9a dilation_rate \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 padding : \"valid\" , \"causal\" \u6216 \"same\" \u4e4b\u4e00 (\u5927\u5c0f\u5199\u654f\u611f) \"valid\" \u8868\u793a\u300c\u4e0d\u586b\u5145\u300d\u3002 \"same\" \u8868\u793a\u586b\u5145\u8f93\u5165\u4ee5\u4f7f\u8f93\u51fa\u5177\u6709\u4e0e\u539f\u59cb\u8f93\u5165\u76f8\u540c\u7684\u957f\u5ea6\u3002 \"causal\" \u8868\u793a\u56e0\u679c\uff08\u81a8\u80c0\uff09\u5377\u79ef\uff0c \u4f8b\u5982\uff0c output[t] \u4e0d\u4f9d\u8d56\u4e8e input[t+1:] \uff0c \u5728\u6a21\u578b\u4e0d\u5e94\u8fdd\u53cd\u65f6\u95f4\u987a\u5e8f\u7684\u65f6\u95f4\u6570\u636e\u5efa\u6a21\u65f6\u975e\u5e38\u6709\u7528\u3002 \u8be6\u89c1 WaveNet: A Generative Model for Raw Audio, section 2.1 \u3002 data_format : \u5b57\u7b26\u4e32, \"channels_last\" (\u9ed8\u8ba4) \u6216 \"channels_first\" \u4e4b\u4e00\u3002\u8f93\u5165\u7684\u5404\u4e2a\u7ef4\u5ea6\u987a\u5e8f\u3002 \"channels_last\" \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, steps, channels) (Keras \u4e2d\u65f6\u5e8f\u6570\u636e\u7684\u9ed8\u8ba4\u683c\u5f0f) \u800c \"channels_first\" \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, channels, steps) \u3002 dilation_rate : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005\u5355\u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c\u6307\u5b9a\u7528\u4e8e\u81a8\u80c0\u5377\u79ef\u7684\u81a8\u80c0\u7387\u3002 \u5f53\u524d\uff0c\u6307\u5b9a\u4efb\u4f55 dilation_rate \u503c != 1 \u4e0e\u6307\u5b9a stride \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u5982\u672a\u6307\u5b9a\uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373\u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 bias_initializer : \u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u8f93\u51fa\uff08\u5b83\u7684\u6fc0\u6d3b\u503c\uff09\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 \u8f93\u5165\u5c3a\u5bf8 3D \u5f20\u91cf \uff0c\u5c3a\u5bf8\u4e3a (batch_size, steps, input_dim) \u3002 \u8f93\u51fa\u5c3a\u5bf8 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch_size, new_steps, filters) \u3002 \u7531\u4e8e\u586b\u5145\u6216\u7a97\u53e3\u6309\u6b65\u957f\u6ed1\u52a8\uff0c steps \u503c\u53ef\u80fd\u5df2\u66f4\u6539\u3002 [source]","title":"Conv1D"},{"location":"2-Layers/2.convolutional/#conv2d","text":"keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) 2D \u5377\u79ef\u5c42 (\u4f8b\u5982\u5bf9\u56fe\u50cf\u7684\u7a7a\u95f4\u5377\u79ef)\u3002 \u8be5\u5c42\u521b\u5efa\u4e86\u4e00\u4e2a\u5377\u79ef\u6838\uff0c \u8be5\u5377\u79ef\u6838\u5bf9\u5c42\u8f93\u5165\u8fdb\u884c\u5377\u79ef\uff0c \u4ee5\u751f\u6210\u8f93\u51fa\u5f20\u91cf\u3002 \u5982\u679c use_bias \u4e3a True\uff0c \u5219\u4f1a\u521b\u5efa\u4e00\u4e2a\u504f\u7f6e\u5411\u91cf\u5e76\u5c06\u5176\u6dfb\u52a0\u5230\u8f93\u51fa\u4e2d\u3002 \u6700\u540e\uff0c\u5982\u679c activation \u4e0d\u662f None \uff0c\u5b83\u4e5f\u4f1a\u5e94\u7528\u4e8e\u8f93\u51fa\u3002 \u5f53\u4f7f\u7528\u8be5\u5c42\u4f5c\u4e3a\u6a21\u578b\u7b2c\u4e00\u5c42\u65f6\uff0c\u9700\u8981\u63d0\u4f9b input_shape \u53c2\u6570 \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u542b\u6837\u672c\u8868\u793a\u7684\u8f74\uff09\uff0c\u4f8b\u5982\uff0c input_shape=(128, 128, 3) \u8868\u793a 128x128 RGB \u56fe\u50cf\uff0c \u5728 data_format=\"channels_last\" \u65f6\u3002 \u53c2\u6570 filters : \u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6 \uff08\u5373\u5377\u79ef\u4e2d\u6ee4\u6ce2\u5668\u7684\u8f93\u51fa\u6570\u91cf\uff09\u3002 kernel_size : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e 2D \u5377\u79ef\u7a97\u53e3\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 strides : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e\u5377\u79ef\u6cbf\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u65b9\u5411\u7684\u6b65\u957f\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 \u6307\u5b9a\u4efb\u4f55 stride \u503c != 1 \u4e0e\u6307\u5b9a dilation_rate \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 padding : \"valid\" \u6216 \"same\" (\u5927\u5c0f\u5199\u654f\u611f)\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4) \u6216 channels_first \u4e4b\u4e00\uff0c\u8868\u793a\u8f93\u5165\u4e2d\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, height, width, channels) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, channels, height, width) \u3002 \u5b83\u9ed8\u8ba4\u4e3a\u4ece Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d \u627e\u5230\u7684 image_data_format \u503c\u3002 \u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u5c06\u4f7f\u7528 channels_last \u3002 dilation_rate : \u4e00\u4e2a\u6574\u6570\u6216 2 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u5b9a\u81a8\u80c0\u5377\u79ef\u7684\u81a8\u80c0\u7387\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 \u5f53\u524d\uff0c\u6307\u5b9a\u4efb\u4f55 dilation_rate \u503c != 1 \u4e0e \u6307\u5b9a stride \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u5982\u679c\u4f60\u4e0d\u6307\u5b9a\uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373\u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 bias_initializer : \u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u8f93\u51fa\uff08\u5b83\u7684\u6fc0\u6d3b\u503c\uff09\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_first'\uff0c \u8f93\u5165 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (samples, channels, rows, cols) \u3002 \u5982\u679c data_format='channels_last'\uff0c \u8f93\u5165 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (samples, rows, cols, channels) \u3002 \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format='channels_first'\uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (samples, filters, new_rows, new_cols) \u3002 \u5982\u679c data_format='channels_last'\uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (samples, new_rows, new_cols, filters) \u3002 \u7531\u4e8e\u586b\u5145\u7684\u539f\u56e0\uff0c rows \u548c cols \u503c\u53ef\u80fd\u5df2\u66f4\u6539\u3002 [source]","title":"Conv2D"},{"location":"2-Layers/2.convolutional/#separableconv1d","text":"keras.layers.SeparableConv1D(filters, kernel_size, strides=1, padding='valid', data_format='channels_last', dilation_rate=1, depth_multiplier=1, activation=None, use_bias=True, depthwise_initializer='glorot_uniform', pointwise_initializer='glorot_uniform', bias_initializer='zeros', depthwise_regularizer=None, pointwise_regularizer=None, bias_regularizer=None, activity_regularizer=None, depthwise_constraint=None, pointwise_constraint=None, bias_constraint=None) \u6df1\u5ea6\u65b9\u5411\u7684\u53ef\u5206\u79bb 1D \u5377\u79ef\u3002 \u53ef\u5206\u79bb\u7684\u5377\u79ef\u7684\u64cd\u4f5c\u5305\u62ec\uff0c\u9996\u5148\u6267\u884c\u6df1\u5ea6\u65b9\u5411\u7684\u7a7a\u95f4\u5377\u79ef \uff08\u5206\u522b\u4f5c\u7528\u4e8e\u6bcf\u4e2a\u8f93\u5165\u901a\u9053\uff09\uff0c\u7d27\u63a5\u4e00\u4e2a\u5c06\u6240\u5f97\u8f93\u51fa\u901a\u9053 \u6df7\u5408\u5728\u4e00\u8d77\u7684\u9010\u70b9\u5377\u79ef\u3002 depth_multiplier \u53c2\u6570\u63a7 \u5236\u6df1\u5ea6\u6b65\u9aa4\u4e2d\u6bcf\u4e2a\u8f93\u5165\u901a\u9053\u751f\u6210\u591a\u5c11\u4e2a\u8f93\u51fa\u901a\u9053\u3002 \u76f4\u89c2\u5730\u8bf4\uff0c\u53ef\u5206\u79bb\u7684\u5377\u79ef\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u79cd\u5c06\u5377\u79ef\u6838\u5206\u89e3\u6210 \u4e24\u4e2a\u8f83\u5c0f\u7684\u5377\u79ef\u6838\u7684\u65b9\u6cd5\uff0c\u6216\u8005\u4f5c\u4e3a Inception \u5757\u7684 \u4e00\u4e2a\u6781\u7aef\u7248\u672c\u3002 \u53c2\u6570 filters : \u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6 \uff08\u5373\u5377\u79ef\u4e2d\u6ee4\u6ce2\u5668\u7684\u8f93\u51fa\u6570\u91cf\uff09\u3002 kernel_size : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005\u5355\u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e 1D \u5377\u79ef\u7a97\u53e3\u7684\u957f\u5ea6\u3002 strides : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005\u5355\u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e\u5377\u79ef\u7684\u6b65\u957f\u3002 \u6307\u5b9a\u4efb\u4f55 stride \u503c != 1 \u4e0e\u6307\u5b9a dilation_rate \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 padding : \"valid\" \u6216 \"same\" (\u5927\u5c0f\u5199\u654f\u611f)\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4) \u6216 channels_first \u4e4b\u4e00\uff0c\u8868\u793a\u8f93\u5165\u4e2d\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, height, width, channels) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, channels, height, width) \u3002 \u5b83\u9ed8\u8ba4\u4e3a\u4ece Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d \u627e\u5230\u7684 image_data_format \u503c\u3002 \u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u5c06\u4f7f\u7528\u300cchannels_last\u300d\u3002 dilation_rate : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005\u5355\u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u4e3a\u4f7f\u7528\u6269\u5f20\uff08\u7a7a\u6d1e\uff09\u5377\u79ef\u6307\u660e\u6269\u5f20\u7387\u3002 \u76ee\u524d\uff0c\u6307\u5b9a\u4efb\u4f55 dilation_rate \u503c != 1 \u4e0e\u6307\u5b9a\u4efb\u4f55 stride \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 depth_multiplier : \u6bcf\u4e2a\u8f93\u5165\u901a\u9053\u7684\u6df1\u5ea6\u65b9\u5411\u5377\u79ef\u8f93\u51fa\u901a\u9053\u7684\u6570\u91cf\u3002 \u6df1\u5ea6\u65b9\u5411\u5377\u79ef\u8f93\u51fa\u901a\u9053\u7684\u603b\u6570\u5c06\u7b49\u4e8e filterss_in * depth_multiplier \u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u5982\u679c\u4f60\u4e0d\u6307\u5b9a\uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373\u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 depthwise_initializer : \u8fd0\u7528\u5230\u6df1\u5ea6\u65b9\u5411\u7684\u6838\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 pointwise_initializer : \u8fd0\u7528\u5230\u9010\u70b9\u6838\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 bias_initializer : \u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 depthwise_regularizer : \u8fd0\u7528\u5230\u6df1\u5ea6\u65b9\u5411\u7684\u6838\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 pointwise_regularizer : \u8fd0\u7528\u5230\u9010\u70b9\u6838\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u8f93\u51fa\uff08\u5b83\u7684\u6fc0\u6d3b\u503c\uff09\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 depthwise_constraint : \u8fd0\u7528\u5230\u6df1\u5ea6\u65b9\u5411\u7684\u6838\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 pointwise_constraint : \u8fd0\u7528\u5230\u9010\u70b9\u6838\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_first'\uff0c \u8f93\u5165 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, channels, steps) \u3002 \u5982\u679c data_format='channels_last'\uff0c \u8f93\u5165 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, steps, channels) \u3002 \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format='channels_first'\uff0c \u8f93\u51fa 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, filters, new_steps) \u3002 \u5982\u679c data_format='channels_last'\uff0c \u8f93\u51fa 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, new_steps, filters) \u3002 \u7531\u4e8e\u586b\u5145\u7684\u539f\u56e0\uff0c new_steps \u503c\u53ef\u80fd\u5df2\u66f4\u6539\u3002 [source]","title":"SeparableConv1D"},{"location":"2-Layers/2.convolutional/#separableconv2d","text":"keras.layers.SeparableConv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), depth_multiplier=1, activation=None, use_bias=True, depthwise_initializer='glorot_uniform', pointwise_initializer='glorot_uniform', bias_initializer='zeros', depthwise_regularizer=None, pointwise_regularizer=None, bias_regularizer=None, activity_regularizer=None, depthwise_constraint=None, pointwise_constraint=None, bias_constraint=None) \u6df1\u5ea6\u65b9\u5411\u7684\u53ef\u5206\u79bb 2D \u5377\u79ef\u3002 \u53ef\u5206\u79bb\u7684\u5377\u79ef\u7684\u64cd\u4f5c\u5305\u62ec\uff0c\u9996\u5148\u6267\u884c\u6df1\u5ea6\u65b9\u5411\u7684\u7a7a\u95f4\u5377\u79ef \uff08\u5206\u522b\u4f5c\u7528\u4e8e\u6bcf\u4e2a\u8f93\u5165\u901a\u9053\uff09\uff0c\u7d27\u63a5\u4e00\u4e2a\u5c06\u6240\u5f97\u8f93\u51fa\u901a\u9053 \u6df7\u5408\u5728\u4e00\u8d77\u7684\u9010\u70b9\u5377\u79ef\u3002 depth_multiplier \u53c2\u6570\u63a7 \u5236\u6df1\u5ea6\u6b65\u9aa4\u4e2d\u6bcf\u4e2a\u8f93\u5165\u901a\u9053\u751f\u6210\u591a\u5c11\u4e2a\u8f93\u51fa\u901a\u9053\u3002 \u76f4\u89c2\u5730\u8bf4\uff0c\u53ef\u5206\u79bb\u7684\u5377\u79ef\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u79cd\u5c06\u5377\u79ef\u6838\u5206\u89e3\u6210 \u4e24\u4e2a\u8f83\u5c0f\u7684\u5377\u79ef\u6838\u7684\u65b9\u6cd5\uff0c\u6216\u8005\u4f5c\u4e3a Inception \u5757\u7684 \u4e00\u4e2a\u6781\u7aef\u7248\u672c\u3002 \u53c2\u6570 filters : \u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6 \uff08\u5373\u5377\u79ef\u4e2d\u6ee4\u6ce2\u5668\u7684\u8f93\u51fa\u6570\u91cf\uff09\u3002 kernel_size : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e 2D \u5377\u79ef\u7a97\u53e3\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 strides : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e\u5377\u79ef\u6cbf\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u65b9\u5411\u7684\u6b65\u957f\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 \u6307\u5b9a\u4efb\u4f55 stride \u503c != 1 \u4e0e\u6307\u5b9a dilation_rate \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 padding : \"valid\" \u6216 \"same\" (\u5927\u5c0f\u5199\u654f\u611f)\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4) \u6216 channels_first \u4e4b\u4e00\uff0c\u8868\u793a\u8f93\u5165\u4e2d\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, height, width, channels) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, channels, height, width) \u3002 \u5b83\u9ed8\u8ba4\u4e3a\u4ece Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d \u627e\u5230\u7684 image_data_format \u503c\u3002 \u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u5c06\u4f7f\u7528\u300cchannels_last\u300d\u3002 dilation_rate : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u4e3a\u4f7f\u7528\u6269\u5f20\uff08\u7a7a\u6d1e\uff09\u5377\u79ef\u6307\u660e\u6269\u5f20\u7387\u3002 \u76ee\u524d\uff0c\u6307\u5b9a\u4efb\u4f55 dilation_rate \u503c != 1 \u4e0e\u6307\u5b9a\u4efb\u4f55 stride \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 depth_multiplier : \u6bcf\u4e2a\u8f93\u5165\u901a\u9053\u7684\u6df1\u5ea6\u65b9\u5411\u5377\u79ef\u8f93\u51fa\u901a\u9053\u7684\u6570\u91cf\u3002 \u6df1\u5ea6\u65b9\u5411\u5377\u79ef\u8f93\u51fa\u901a\u9053\u7684\u603b\u6570\u5c06\u7b49\u4e8e filterss_in * depth_multiplier \u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u5982\u679c\u4f60\u4e0d\u6307\u5b9a\uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373\u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 depthwise_initializer : \u8fd0\u7528\u5230\u6df1\u5ea6\u65b9\u5411\u7684\u6838\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668 \u8be6\u89c1 initializers )\u3002 pointwise_initializer : \u8fd0\u7528\u5230\u9010\u70b9\u6838\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 bias_initializer : \u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 depthwise_regularizer : \u8fd0\u7528\u5230\u6df1\u5ea6\u65b9\u5411\u7684\u6838\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 pointwise_regularizer : \u8fd0\u7528\u5230\u9010\u70b9\u6838\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u8f93\u51fa\uff08\u5b83\u7684\u6fc0\u6d3b\u503c\uff09\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 depthwise_constraint : \u8fd0\u7528\u5230\u6df1\u5ea6\u65b9\u5411\u7684\u6838\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 pointwise_constraint : \u8fd0\u7528\u5230\u9010\u70b9\u6838\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_first'\uff0c \u8f93\u5165 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, channels, rows, cols) \u3002 \u5982\u679c data_format='channels_last'\uff0c \u8f93\u5165 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, rows, cols, channels) \u3002 \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format='channels_first'\uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, filters, new_rows, new_cols) \u3002 \u5982\u679c data_format='channels_last'\uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, new_rows, new_cols, filters) \u3002 \u7531\u4e8e\u586b\u5145\u7684\u539f\u56e0\uff0c rows \u548c cols \u503c\u53ef\u80fd\u5df2\u66f4\u6539\u3002 [source]","title":"SeparableConv2D"},{"location":"2-Layers/2.convolutional/#depthwiseconv2d","text":"keras.layers.DepthwiseConv2D(kernel_size, strides=(1, 1), padding='valid', depth_multiplier=1, data_format=None, activation=None, use_bias=True, depthwise_initializer='glorot_uniform', bias_initializer='zeros', depthwise_regularizer=None, bias_regularizer=None, activity_regularizer=None, depthwise_constraint=None, bias_constraint=None) \u6df1\u5ea6\u53ef\u5206\u79bb 2D \u5377\u79ef\u3002 \u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u5305\u62ec\u4ec5\u6267\u884c\u6df1\u5ea6\u7a7a\u95f4\u5377\u79ef\u4e2d\u7684\u7b2c\u4e00\u6b65\uff08\u5176\u5206\u522b\u4f5c\u7528\u4e8e\u6bcf\u4e2a\u8f93\u5165\u901a\u9053\uff09\u3002 depth_multiplier \u53c2\u6570\u63a7\u5236\u6df1\u5ea6\u6b65\u9aa4\u4e2d\u6bcf\u4e2a\u8f93\u5165\u901a\u9053\u751f\u6210\u591a\u5c11\u4e2a\u8f93\u51fa\u901a\u9053\u3002 Arguments kernel_size : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e 2D \u5377\u79ef\u7a97\u53e3\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 strides : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e\u5377\u79ef\u6cbf\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u65b9\u5411\u7684\u6b65\u957f\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 \u6307\u5b9a\u4efb\u4f55 stride \u503c != 1 \u4e0e\u6307\u5b9a dilation_rate \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 padding : \"valid\" \u6216 \"same\" (\u5927\u5c0f\u5199\u654f\u611f)\u3002 depth_multiplier : \u6bcf\u4e2a\u8f93\u5165\u901a\u9053\u7684\u6df1\u5ea6\u65b9\u5411\u5377\u79ef\u8f93\u51fa\u901a\u9053\u7684\u6570\u91cf\u3002 \u6df1\u5ea6\u65b9\u5411\u5377\u79ef\u8f93\u51fa\u901a\u9053\u7684\u603b\u6570\u5c06\u7b49\u4e8e filterss_in * depth_multiplier \u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4) \u6216 channels_first \u4e4b\u4e00\uff0c\u8868\u793a\u8f93\u5165\u4e2d\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, height, width, channels) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, channels, height, width) \u3002 \u5b83\u9ed8\u8ba4\u4e3a\u4ece Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d \u627e\u5230\u7684 image_data_format \u503c\u3002 \u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u5c06\u4f7f\u7528\u300cchannels_last\u300d\u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u5982\u679c\u4f60\u4e0d\u6307\u5b9a\uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373\u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 depthwise_initializer : \u8fd0\u7528\u5230\u6df1\u5ea6\u65b9\u5411\u7684\u6838\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668 \u8be6\u89c1 initializers )\u3002 bias_initializer : \u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 depthwise_regularizer : \u8fd0\u7528\u5230\u6df1\u5ea6\u65b9\u5411\u7684\u6838\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u8f93\u51fa\uff08\u5b83\u7684\u6fc0\u6d3b\u503c\uff09\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 depthwise_constraint : \u8fd0\u7528\u5230\u6df1\u5ea6\u65b9\u5411\u7684\u6838\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_first'\uff0c \u8f93\u5165 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, channels, rows, cols) \u3002 \u5982\u679c data_format='channels_last'\uff0c \u8f93\u5165 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, rows, cols, channels) \u3002 \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format='channels_first'\uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, filters, new_rows, new_cols) \u3002 \u5982\u679c data_format='channels_last'\uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, new_rows, new_cols, filters) \u3002 \u7531\u4e8e\u586b\u5145\u7684\u539f\u56e0\uff0c rows \u548c cols \u503c\u53ef\u80fd\u5df2\u66f4\u6539\u3002 [source]","title":"DepthwiseConv2D"},{"location":"2-Layers/2.convolutional/#conv2dtranspose","text":"keras.layers.Conv2DTranspose(filters, kernel_size, strides=(1, 1), padding='valid', output_padding=None, data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) \u8f6c\u7f6e\u5377\u79ef\u5c42 (\u6709\u65f6\u88ab\u6210\u4e3a\u53cd\u5377\u79ef)\u3002 \u5bf9\u8f6c\u7f6e\u5377\u79ef\u7684\u9700\u6c42\u4e00\u822c\u6765\u81ea\u5e0c\u671b\u4f7f\u7528 \u4e0e\u6b63\u5e38\u5377\u79ef\u76f8\u53cd\u65b9\u5411\u7684\u53d8\u6362\uff0c \u5373\uff0c\u5c06\u5177\u6709\u5377\u79ef\u8f93\u51fa\u5c3a\u5bf8\u7684\u4e1c\u897f \u8f6c\u6362\u4e3a\u5177\u6709\u5377\u79ef\u8f93\u5165\u5c3a\u5bf8\u7684\u4e1c\u897f\uff0c \u540c\u65f6\u4fdd\u6301\u4e0e\u6240\u8ff0\u5377\u79ef\u76f8\u5bb9\u7684\u8fde\u901a\u6027\u6a21\u5f0f\u3002 \u5f53\u4f7f\u7528\u8be5\u5c42\u4f5c\u4e3a\u6a21\u578b\u7b2c\u4e00\u5c42\u65f6\uff0c\u9700\u8981\u63d0\u4f9b input_shape \u53c2\u6570 \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u542b\u6837\u672c\u8868\u793a\u7684\u8f74\uff09\uff0c\u4f8b\u5982\uff0c input_shape=(128, 128, 3) \u8868\u793a 128x128 RGB \u56fe\u50cf\uff0c \u5728 data_format=\"channels_last\" \u65f6\u3002 \u53c2\u6570 filters : \u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6 \uff08\u5373\u5377\u79ef\u4e2d\u6ee4\u6ce2\u5668\u7684\u8f93\u51fa\u6570\u91cf\uff09\u3002 kernel_size : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e 2D \u5377\u79ef\u7a97\u53e3\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 strides : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e\u5377\u79ef\u6cbf\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u65b9\u5411\u7684\u6b65\u957f\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 \u6307\u5b9a\u4efb\u4f55 stride \u503c != 1 \u4e0e\u6307\u5b9a dilation_rate \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 padding : \"valid\" \u6216 \"same\" (\u5927\u5c0f\u5199\u654f\u611f)\u3002 output_padding : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u5b9a\u6cbf\u8f93\u51fa\u5f20\u91cf\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u7684\u586b\u5145\u91cf\u3002 \u53ef\u4ee5\u662f\u5355\u4e2a\u6574\u6570\uff0c\u4ee5\u6307\u5b9a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u7684\u76f8\u540c\u503c\u3002 \u6cbf\u7ed9\u5b9a\u7ef4\u5ea6\u7684\u8f93\u51fa\u586b\u5145\u91cf\u5fc5\u987b\u4f4e\u4e8e\u6cbf\u540c\u4e00\u7ef4\u5ea6\u7684\u6b65\u957f\u3002 \u5982\u679c\u8bbe\u7f6e\u4e3a None (\u9ed8\u8ba4), \u8f93\u51fa\u5c3a\u5bf8\u5c06\u81ea\u52a8\u63a8\u7406\u51fa\u6765\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4) \u6216 channels_first \u4e4b\u4e00\uff0c\u8868\u793a\u8f93\u5165\u4e2d\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, height, width, channels) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, channels, height, width) \u3002 \u5b83\u9ed8\u8ba4\u4e3a\u4ece Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d \u627e\u5230\u7684 image_data_format \u503c\u3002 \u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u5c06\u4f7f\u7528 \"channels_last\"\u3002 dilation_rate : \u4e00\u4e2a\u6574\u6570\u6216 2 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u5b9a\u81a8\u80c0\u5377\u79ef\u7684\u81a8\u80c0\u7387\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 \u5f53\u524d\uff0c\u6307\u5b9a\u4efb\u4f55 dilation_rate \u503c != 1 \u4e0e \u6307\u5b9a stride \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u5982\u679c\u4f60\u4e0d\u6307\u5b9a\uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373\u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 bias_initializer : \u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u8f93\u51fa\uff08\u5b83\u7684\u6fc0\u6d3b\u503c\uff09\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_first'\uff0c \u8f93\u5165 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, channels, rows, cols) \u3002 \u5982\u679c data_format='channels_last'\uff0c \u8f93\u5165 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, rows, cols, channels) \u3002 \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format='channels_first'\uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, filters, new_rows, new_cols) \u3002 \u5982\u679c data_format='channels_last'\uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, new_rows, new_cols, filters) \u3002 \u7531\u4e8e\u586b\u5145\u7684\u539f\u56e0\uff0c rows \u548c cols \u503c\u53ef\u80fd\u5df2\u66f4\u6539\u3002 \u5982\u679c\u6307\u5b9a\u4e86 output_padding : new_rows = ((rows - 1) * strides[0] + kernel_size[0] - 2 * padding[0] + output_padding[0]) new_cols = ((cols - 1) * strides[1] + kernel_size[1] - 2 * padding[1] + output_padding[1]) \u53c2\u8003\u6587\u732e A guide to convolution arithmetic for deep learning Deconvolutional Networks [source]","title":"Conv2DTranspose"},{"location":"2-Layers/2.convolutional/#conv3d","text":"keras.layers.Conv3D(filters, kernel_size, strides=(1, 1, 1), padding='valid', data_format=None, dilation_rate=(1, 1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) 3D \u5377\u79ef\u5c42 (\u4f8b\u5982\u7acb\u4f53\u7a7a\u95f4\u5377\u79ef)\u3002 \u8be5\u5c42\u521b\u5efa\u4e86\u4e00\u4e2a\u5377\u79ef\u6838\uff0c \u8be5\u5377\u79ef\u6838\u5bf9\u5c42\u8f93\u5165\u8fdb\u884c\u5377\u79ef\uff0c \u4ee5\u751f\u6210\u8f93\u51fa\u5f20\u91cf\u3002 \u5982\u679c use_bias \u4e3a True\uff0c \u5219\u4f1a\u521b\u5efa\u4e00\u4e2a\u504f\u7f6e\u5411\u91cf\u5e76\u5c06\u5176\u6dfb\u52a0\u5230\u8f93\u51fa\u4e2d\u3002 \u6700\u540e\uff0c\u5982\u679c activation \u4e0d\u662f None \uff0c\u5b83\u4e5f\u4f1a\u5e94\u7528\u4e8e\u8f93\u51fa\u3002 \u5f53\u4f7f\u7528\u8be5\u5c42\u4f5c\u4e3a\u6a21\u578b\u7b2c\u4e00\u5c42\u65f6\uff0c\u9700\u8981\u63d0\u4f9b input_shape \u53c2\u6570 \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u542b\u6837\u672c\u8868\u793a\u7684\u8f74\uff09\uff0c\u4f8b\u5982\uff0c input_shape=(128, 128, 128, 1) \u8868\u793a 128x128x128 \u7684\u5355\u901a\u9053\u7acb\u4f53\uff0c \u5728 data_format=\"channels_last\" \u65f6\u3002 \u53c2\u6570 filters : \u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6 \uff08\u5373\u5377\u79ef\u4e2d\u6ee4\u6ce2\u5668\u7684\u8f93\u51fa\u6570\u91cf\uff09\u3002 kernel_size : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 3 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e 3D \u5377\u79ef\u7a97\u53e3\u7684\u6df1\u5ea6\u3001\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 strides : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 3 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e\u5377\u79ef\u6cbf\u6bcf\u4e00\u4e2a\u7a7a\u95f4\u7ef4\u5ea6\u7684\u6b65\u957f\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u6b65\u957f\u503c\u3002 \u6307\u5b9a\u4efb\u4f55 stride \u503c != 1 \u4e0e\u6307\u5b9a dilation_rate \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 padding : \"valid\" \u6216 \"same\" (\u5927\u5c0f\u5199\u654f\u611f)\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4) \u6216 channels_first \u4e4b\u4e00\uff0c \u8868\u793a\u8f93\u5165\u4e2d\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) \u3002 \u5b83\u9ed8\u8ba4\u4e3a\u4ece Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d \u627e\u5230\u7684 image_data_format \u503c\u3002 \u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u5c06\u4f7f\u7528 \"channels_last\"\u3002 dilation_rate : \u4e00\u4e2a\u6574\u6570\u6216 3 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u5b9a\u81a8\u80c0\u5377\u79ef\u7684\u81a8\u80c0\u7387\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 \u5f53\u524d\uff0c\u6307\u5b9a\u4efb\u4f55 dilation_rate \u503c != 1 \u4e0e \u6307\u5b9a stride \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u5982\u679c\u4f60\u4e0d\u6307\u5b9a\uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373\u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 bias_initializer : \u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u8f93\u51fa\uff08\u5b83\u7684\u6fc0\u6d3b\u503c\uff09\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_first'\uff0c \u8f93\u5165 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (samples, channels, conv_dim1, conv_dim2, conv_dim3) \u3002 \u5982\u679c data_format='channels_last'\uff0c \u8f93\u5165 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (samples, conv_dim1, conv_dim2, conv_dim3, channels) \u3002 \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format='channels_first'\uff0c \u8f93\u51fa 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (samples, filters, new_conv_dim1, new_conv_dim2, new_conv_dim3) \u3002 \u5982\u679c data_format='channels_last'\uff0c \u8f93\u51fa 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (samples, new_conv_dim1, new_conv_dim2, new_conv_dim3, filters) \u3002 \u7531\u4e8e\u586b\u5145\u7684\u539f\u56e0\uff0c new_conv_dim1 , new_conv_dim2 \u548c new_conv_dim3 \u503c\u53ef\u80fd\u5df2\u66f4\u6539\u3002 [source]","title":"Conv3D"},{"location":"2-Layers/2.convolutional/#conv3dtranspose","text":"keras.layers.Conv3DTranspose(filters, kernel_size, strides=(1, 1, 1), padding='valid', output_padding=None, data_format=None, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) \u8f6c\u7f6e\u5377\u79ef\u5c42 (\u6709\u65f6\u88ab\u6210\u4e3a\u53cd\u5377\u79ef)\u3002 \u5bf9\u8f6c\u7f6e\u5377\u79ef\u7684\u9700\u6c42\u4e00\u822c\u6765\u81ea\u5e0c\u671b\u4f7f\u7528 \u4e0e\u6b63\u5e38\u5377\u79ef\u76f8\u53cd\u65b9\u5411\u7684\u53d8\u6362\uff0c \u5373\uff0c\u5c06\u5177\u6709\u5377\u79ef\u8f93\u51fa\u5c3a\u5bf8\u7684\u4e1c\u897f \u8f6c\u6362\u4e3a\u5177\u6709\u5377\u79ef\u8f93\u5165\u5c3a\u5bf8\u7684\u4e1c\u897f\uff0c \u540c\u65f6\u4fdd\u6301\u4e0e\u6240\u8ff0\u5377\u79ef\u76f8\u5bb9\u7684\u8fde\u901a\u6027\u6a21\u5f0f\u3002 \u5f53\u4f7f\u7528\u8be5\u5c42\u4f5c\u4e3a\u6a21\u578b\u7b2c\u4e00\u5c42\u65f6\uff0c\u9700\u8981\u63d0\u4f9b input_shape \u53c2\u6570 \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u542b\u6837\u672c\u8868\u793a\u7684\u8f74\uff09\uff0c\u4f8b\u5982\uff0c input_shape=(128, 128, 128, 3) \u8868\u793a\u5c3a\u5bf8 128x128x128 \u7684 3 \u901a\u9053\u7acb\u4f53\uff0c \u5728 data_format=\"channels_last\" \u65f6\u3002 \u53c2\u6570 filters : \u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6 \uff08\u5373\u5377\u79ef\u4e2d\u6ee4\u6ce2\u5668\u7684\u8f93\u51fa\u6570\u91cf\uff09\u3002 kernel_size : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 3 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e 3D \u5377\u79ef\u7a97\u53e3\u7684\u6df1\u5ea6\u3001\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 strides : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 3 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e\u6cbf\u6df1\u5ea6\u3001\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u65b9\u5411\u7684\u6b65\u957f\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 \u6307\u5b9a\u4efb\u4f55 stride \u503c != 1 \u4e0e\u6307\u5b9a dilation_rate \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 padding : \"valid\" \u6216 \"same\" (\u5927\u5c0f\u5199\u654f\u611f)\u3002 output_padding : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 3 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u5b9a\u6cbf\u8f93\u51fa\u5f20\u91cf\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u7684\u586b\u5145\u91cf\u3002 \u53ef\u4ee5\u662f\u5355\u4e2a\u6574\u6570\uff0c\u4ee5\u6307\u5b9a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u7684\u76f8\u540c\u503c\u3002 \u6cbf\u7ed9\u5b9a\u7ef4\u5ea6\u7684\u8f93\u51fa\u586b\u5145\u91cf\u5fc5\u987b\u4f4e\u4e8e\u6cbf\u540c\u4e00\u7ef4\u5ea6\u7684\u6b65\u957f\u3002 \u5982\u679c\u8bbe\u7f6e\u4e3a None (\u9ed8\u8ba4), \u8f93\u51fa\u5c3a\u5bf8\u5c06\u81ea\u52a8\u63a8\u7406\u51fa\u6765\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4) \u6216 channels_first \u4e4b\u4e00\uff0c\u8868\u793a\u8f93\u5165\u4e2d\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, depth, height, width, channels) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, channels, depth, height, width) \u3002 \u5b83\u9ed8\u8ba4\u4e3a\u4ece Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d \u627e\u5230\u7684 image_data_format \u503c\u3002 \u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u5c06\u4f7f\u7528\u300cchannels_last\u300d\u3002 dilation_rate : \u4e00\u4e2a\u6574\u6570\u6216 3 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u5b9a\u81a8\u80c0\u5377\u79ef\u7684\u81a8\u80c0\u7387\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 \u5f53\u524d\uff0c\u6307\u5b9a\u4efb\u4f55 dilation_rate \u503c != 1 \u4e0e \u6307\u5b9a stride \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u5982\u679c\u4f60\u4e0d\u6307\u5b9a\uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373\u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 bias_initializer : \u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u8f93\u51fa\uff08\u5b83\u7684\u6fc0\u6d3b\u503c\uff09\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_first'\uff0c \u8f93\u5165 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, channels, depth, rows, cols) \uff0c \u5982\u679c data_format='channels_last'\uff0c \u8f93\u5165 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, depth, rows, cols, channels) \u3002 Output shape \u5982\u679c data_format='channels_first'\uff0c \u8f93\u51fa 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, filters, new_depth, new_rows, new_cols) \uff0c \u5982\u679c data_format='channels_last'\uff0c \u8f93\u51fa 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, new_depth, new_rows, new_cols, filters) \u3002 depth \u548c rows \u548c cols \u53ef\u80fd\u56e0\u4e3a\u586b\u5145\u800c\u6539\u53d8\u3002 \u5982\u679c\u6307\u5b9a\u4e86 output_padding \uff1a new_depth = ((depth - 1) * strides[0] + kernel_size[0] - 2 * padding[0] + output_padding[0]) new_rows = ((rows - 1) * strides[1] + kernel_size[1] - 2 * padding[1] + output_padding[1]) new_cols = ((cols - 1) * strides[2] + kernel_size[2] - 2 * padding[2] + output_padding[2]) \u53c2\u8003\u6587\u732e [A guide to convolution arithmetic for deep learning] (https://arxiv.org/abs/1603.07285v1) [Deconvolutional Networks] (http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf) [source]","title":"Conv3DTranspose"},{"location":"2-Layers/2.convolutional/#cropping1d","text":"keras.layers.Cropping1D(cropping=(1, 1)) 1D \u8f93\u5165\u7684\u88c1\u526a\u5c42\uff08\u4f8b\u5982\u65f6\u95f4\u5e8f\u5217\uff09\u3002 \u5b83\u6cbf\u7740\u65f6\u95f4\u7ef4\u5ea6\uff08\u7b2c 1 \u4e2a\u8f74\uff09\u88c1\u526a\u3002 \u53c2\u6570 cropping : \u6574\u6570\u6216\u6574\u6570\u5143\u7ec4\uff08\u957f\u5ea6\u4e3a 2\uff09\u3002 \u5728\u88c1\u526a\u7ef4\u5ea6\uff08\u7b2c 1 \u4e2a\u8f74\uff09\u7684\u5f00\u59cb\u548c\u7ed3\u675f\u4f4d\u7f6e \u5e94\u8be5\u88c1\u526a\u591a\u5c11\u4e2a\u5355\u4f4d\u3002 \u5982\u679c\u53ea\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6574\u6570\uff0c\u90a3\u4e48\u8fd9\u4e24\u4e2a\u4f4d\u7f6e\u5c06\u4f7f\u7528 \u76f8\u540c\u7684\u503c\u3002 \u8f93\u5165\u5c3a\u5bf8 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, axis_to_crop, features) \u3002 \u8f93\u51fa\u5c3a\u5bf8 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, cropped_axis, features) \u3002 [source]","title":"Cropping1D"},{"location":"2-Layers/2.convolutional/#cropping2d","text":"keras.layers.Cropping2D(cropping=((0, 0), (0, 0)), data_format=None) 2D \u8f93\u5165\u7684\u88c1\u526a\u5c42\uff08\u4f8b\u5982\u56fe\u50cf\uff09\u3002 \u5b83\u6cbf\u7740\u7a7a\u95f4\u7ef4\u5ea6\u88c1\u526a\uff0c\u5373\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u3002 \u53c2\u6570 cropping : \u6574\u6570\uff0c\u6216 2 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\uff0c\u6216 2 \u4e2a\u6574\u6570\u7684 2 \u4e2a\u5143\u7ec4\u3002 \u5982\u679c\u4e3a\u6574\u6570\uff1a \u5c06\u5bf9\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u5e94\u7528\u76f8\u540c\u7684\u5bf9\u79f0\u88c1\u526a\u3002 \u5982\u679c\u4e3a 2 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\uff1a \u89e3\u91ca\u4e3a\u5bf9\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u7684\u4e24\u4e2a\u4e0d\u540c\u7684\u5bf9\u79f0\u88c1\u526a\u503c\uff1a (symmetric_height_crop, symmetric_width_crop) \u3002 \u5982\u679c\u4e3a 2 \u4e2a\u6574\u6570\u7684 2 \u4e2a\u5143\u7ec4\uff1a \u89e3\u91ca\u4e3a ((top_crop, bottom_crop), (left_crop, right_crop)) \u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4) \u6216 channels_first \u4e4b\u4e00\uff0c \u8868\u793a\u8f93\u5165\u4e2d\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, height, width, channels) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, channels, height, width) \u3002 \u5b83\u9ed8\u8ba4\u4e3a\u4ece Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d \u627e\u5230\u7684 image_data_format \u503c\u3002 \u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u5c06\u4f7f\u7528 \"channels_last\"\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format='channels_last'\uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, rows, cols, channels) \u3002 \u5982\u679c data_format='channels_first'\uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, channels, rows, cols) \u3002 \u7531\u4e8e\u586b\u5145\u7684\u539f\u56e0\uff0c rows \u548c cols \u503c\u53ef\u80fd\u5df2\u66f4\u6539\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format \u4e3a \"channels_last\" \uff0c \u8f93\u5165 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, cropped_rows, cropped_cols, channels) \u3002 \u5982\u679c data_format \u4e3a \"channels_first\" \uff0c \u8f93\u5165 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, channels, cropped_rows, cropped_cols) \u3002 \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format \u4e3a \"channels_last\" \uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, cropped_rows, cropped_cols, channels) \u5982\u679c data_format \u4e3a \"channels_first\" \uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, channels, cropped_rows, cropped_cols) \u3002 \u4f8b\u5b50 # \u88c1\u526a\u8f93\u5165\u7684 2D \u56fe\u50cf\u6216\u7279\u5f81\u56fe model = Sequential() model.add(Cropping2D(cropping=((2, 2), (4, 4)), input_shape=(28, 28, 3))) # \u73b0\u5728 model.output_shape == (None, 24, 20, 3) model.add(Conv2D(64, (3, 3), padding='same')) model.add(Cropping2D(cropping=((2, 2), (2, 2)))) # \u73b0\u5728 model.output_shape == (None, 20, 16. 64) [source]","title":"Cropping2D"},{"location":"2-Layers/2.convolutional/#cropping3d","text":"keras.layers.Cropping3D(cropping=((1, 1), (1, 1), (1, 1)), data_format=None) 3D \u6570\u636e\u7684\u88c1\u526a\u5c42\uff08\u4f8b\u5982\u7a7a\u95f4\u6216\u65f6\u7a7a\uff09\u3002 \u53c2\u6570 cropping : \u6574\u6570\uff0c\u6216 3 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\uff0c\u6216 2 \u4e2a\u6574\u6570\u7684 3 \u4e2a\u5143\u7ec4\u3002 \u5982\u679c\u4e3a\u6574\u6570\uff1a \u5c06\u5bf9\u6df1\u5ea6\u3001\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u5e94\u7528\u76f8\u540c\u7684\u5bf9\u79f0\u88c1\u526a\u3002 \u5982\u679c\u4e3a 3 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\uff1a \u89e3\u91ca\u4e3a\u5bf9\u6df1\u5ea6\u3001\u9ad8\u5ea6\u548c\u9ad8\u5ea6\u7684 3 \u4e2a\u4e0d\u540c\u7684\u5bf9\u79f0\u88c1\u526a\u503c\uff1a (symmetric_dim1_crop, symmetric_dim2_crop, symmetric_dim3_crop) \u3002 \u5982\u679c\u4e3a 2 \u4e2a\u6574\u6570\u7684 3 \u4e2a\u5143\u7ec4\uff1a \u89e3\u91ca\u4e3a ((left_dim1_crop, right_dim1_crop), (left_dim2_crop, right_dim2_crop), (left_dim3_crop, right_dim3_crop)) \u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4) \u6216 channels_first \u4e4b\u4e00\uff0c \u8868\u793a\u8f93\u5165\u4e2d\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) \u3002 \u5b83\u9ed8\u8ba4\u4e3a\u4ece Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d \u627e\u5230\u7684 image_data_format \u503c\u3002 \u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u5c06\u4f7f\u7528 \"channels_last\"\u3002 \u8f93\u5165\u5c3a\u5bf8 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a \u5982\u679c data_format \u4e3a \"channels_last\" : (batch, first_cropped_axis, second_cropped_axis, third_cropped_axis, depth) \u5982\u679c data_format \u4e3a \"channels_first\" : (batch, depth, first_cropped_axis, second_cropped_axis, third_cropped_axis) \u8f93\u51fa\u5c3a\u5bf8 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a \u5982\u679c data_format \u4e3a \"channels_last\" : (batch, first_cropped_axis, second_cropped_axis, third_cropped_axis, depth) \u5982\u679c data_format \u4e3a \"channels_first\" : (batch, depth, first_cropped_axis, second_cropped_axis, third_cropped_axis) \u3002 [source]","title":"Cropping3D"},{"location":"2-Layers/2.convolutional/#upsampling1d","text":"keras.layers.UpSampling1D(size=2) 1D \u8f93\u5165\u7684\u4e0a\u91c7\u6837\u5c42\u3002 \u6cbf\u7740\u65f6\u95f4\u8f74\u91cd\u590d\u6bcf\u4e2a\u65f6\u95f4\u6b65 size \u6b21\u3002 \u53c2\u6570 size : \u6574\u6570\u3002\u4e0a\u91c7\u6837\u56e0\u5b50\u3002 \u8f93\u5165\u5c3a\u5bf8 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, steps, features) \u3002 \u8f93\u51fa\u5c3a\u5bf8 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, upsampled_steps, features) \u3002 [source]","title":"UpSampling1D"},{"location":"2-Layers/2.convolutional/#upsampling2d","text":"keras.layers.UpSampling2D(size=(2, 2), data_format=None, interpolation='nearest') 2D \u8f93\u5165\u7684\u4e0a\u91c7\u6837\u5c42\u3002 \u6cbf\u7740\u6570\u636e\u7684\u884c\u548c\u5217\u5206\u522b\u91cd\u590d size[0] \u548c size[1] \u6b21\u3002 \u53c2\u6570 size : \u6574\u6570\uff0c\u6216 2 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\u3002 \u884c\u548c\u5217\u7684\u4e0a\u91c7\u6837\u56e0\u5b50\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4) \u6216 channels_first \u4e4b\u4e00\uff0c \u8868\u793a\u8f93\u5165\u4e2d\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, height, width, channels) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, channels, height, width) \u3002 \u5b83\u9ed8\u8ba4\u4e3a\u4ece Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d \u627e\u5230\u7684 image_data_format \u503c\u3002 \u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u5c06\u4f7f\u7528 \"channels_last\"\u3002 interpolation : \u5b57\u7b26\u4e32\uff0c nearest \u6216 bilinear \u4e4b\u4e00\u3002 \u6ce8\u610f CNTK \u6682\u4e0d\u652f\u6301 bilinear upscaling\uff0c \u4ee5\u53ca\u5bf9\u4e8e Theano\uff0c\u53ea\u53ef\u4ee5\u4f7f\u7528 size=(2, 2) \u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format \u4e3a \"channels_last\" \uff0c \u8f93\u5165 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, rows, cols, channels) \u3002 \u5982\u679c data_format \u4e3a \"channels_first\" \uff0c \u8f93\u5165 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, channels, rows, cols) \u3002 \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format \u4e3a \"channels_last\" \uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, upsampled_rows, upsampled_cols, channels) \u3002 \u5982\u679c data_format \u4e3a \"channels_first\" \uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, channels, upsampled_rows, upsampled_cols) \u3002 [source]","title":"UpSampling2D"},{"location":"2-Layers/2.convolutional/#upsampling3d","text":"keras.layers.UpSampling3D(size=(2, 2, 2), data_format=None) 3D \u8f93\u5165\u7684\u4e0a\u91c7\u6837\u5c42\u3002 \u6cbf\u7740\u6570\u636e\u7684\u7b2c 1\u30012\u30013 \u7ef4\u5ea6\u5206\u522b\u91cd\u590d size[0] \u3001 size[1] \u548c size[2] \u6b21\u3002 \u53c2\u6570 size : \u6574\u6570\uff0c\u6216 3 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\u3002 dim1, dim2 \u548c dim3 \u7684\u4e0a\u91c7\u6837\u56e0\u5b50\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4) \u6216 channels_first \u4e4b\u4e00\uff0c \u8868\u793a\u8f93\u5165\u4e2d\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) \u3002 \u5b83\u9ed8\u8ba4\u4e3a\u4ece Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d \u627e\u5230\u7684 image_data_format \u503c\u3002 \u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u5c06\u4f7f\u7528 \"channels_last\"\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format \u4e3a \"channels_last\" \uff0c \u8f93\u5165 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, dim1, dim2, dim3, channels) \u3002 \u5982\u679c data_format \u4e3a \"channels_first\" \uff0c \u8f93\u5165 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, channels, dim1, dim2, dim3) \u3002 \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format \u4e3a \"channels_last\" \uff0c \u8f93\u51fa 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, upsampled_dim1, upsampled_dim2, upsampled_dim3, channels) \u3002 \u5982\u679c data_format \u4e3a \"channels_first\" \uff0c \u8f93\u51fa 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, channels, upsampled_dim1, upsampled_dim2, upsampled_dim3) \u3002 [source]","title":"UpSampling3D"},{"location":"2-Layers/2.convolutional/#zeropadding1d","text":"keras.layers.ZeroPadding1D(padding=1) 1D \u8f93\u5165\u7684\u96f6\u586b\u5145\u5c42\uff08\u4f8b\u5982\uff0c\u65f6\u95f4\u5e8f\u5217\uff09\u3002 \u53c2\u6570 padding : \u6574\u6570\uff0c\u6216\u957f\u5ea6\u4e3a 2 \u7684\u6574\u6570\u5143\u7ec4\uff0c\u6216\u5b57\u5178\u3002 \u5982\u679c\u4e3a\u6574\u6570\uff1a \u5728\u586b\u5145\u7ef4\u5ea6\uff08\u7b2c\u4e00\u4e2a\u8f74\uff09\u7684\u5f00\u59cb\u548c\u7ed3\u675f\u5904\u6dfb\u52a0\u591a\u5c11\u4e2a\u96f6\u3002 \u5982\u679c\u662f\u957f\u5ea6\u4e3a 2 \u7684\u6574\u6570\u5143\u7ec4\uff1a \u5728\u586b\u5145\u7ef4\u5ea6\u7684\u5f00\u59cb\u548c\u7ed3\u5c3e\u5904\u6dfb\u52a0\u591a\u5c11\u4e2a\u96f6 ( (left_pad, right_pad) )\u3002 \u8f93\u5165\u5c3a\u5bf8 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, axis_to_pad, features) \u3002 \u8f93\u51fa\u5c3a\u5bf8 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, padded_axis, features) \u3002 [source]","title":"ZeroPadding1D"},{"location":"2-Layers/2.convolutional/#zeropadding2d","text":"keras.layers.ZeroPadding2D(padding=(1, 1), data_format=None) 2D \u8f93\u5165\u7684\u96f6\u586b\u5145\u5c42\uff08\u4f8b\u5982\u56fe\u50cf\uff09\u3002 \u8be5\u56fe\u5c42\u53ef\u4ee5\u5728\u56fe\u50cf\u5f20\u91cf\u7684\u9876\u90e8\u3001\u5e95\u90e8\u3001\u5de6\u4fa7\u548c\u53f3\u4fa7\u6dfb\u52a0\u96f6\u8868\u793a\u7684\u884c\u548c\u5217\u3002 \u53c2\u6570 padding : \u6574\u6570\uff0c\u6216 2 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\uff0c\u6216 2 \u4e2a\u6574\u6570\u7684 2 \u4e2a\u5143\u7ec4\u3002 \u5982\u679c\u4e3a\u6574\u6570\uff1a\u5c06\u5bf9\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u8fd0\u7528\u76f8\u540c\u7684\u5bf9\u79f0\u586b\u5145\u3002 \u5982\u679c\u4e3a 2 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\uff1a \u5982\u679c\u4e3a\u6574\u6570\uff1a: \u89e3\u91ca\u4e3a\u9ad8\u5ea6\u548c\u9ad8\u5ea6\u7684 2 \u4e2a\u4e0d\u540c\u7684\u5bf9\u79f0\u88c1\u526a\u503c\uff1a (symmetric_height_pad, symmetric_width_pad) \u3002 \u5982\u679c\u4e3a 2 \u4e2a\u6574\u6570\u7684 2 \u4e2a\u5143\u7ec4\uff1a \u89e3\u91ca\u4e3a ((top_pad, bottom_pad), (left_pad, right_pad)) \u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4) \u6216 channels_first \u4e4b\u4e00\uff0c \u8868\u793a\u8f93\u5165\u4e2d\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, height, width, channels) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, channels, height, width) \u3002 \u5b83\u9ed8\u8ba4\u4e3a\u4ece Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d \u627e\u5230\u7684 image_data_format \u503c\u3002 \u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u5c06\u4f7f\u7528 \"channels_last\"\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format \u4e3a \"channels_last\" \uff0c \u8f93\u5165 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, rows, cols, channels) \u3002 \u5982\u679c data_format \u4e3a \"channels_first\" \uff0c \u8f93\u5165 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, channels, rows, cols) \u3002 \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format \u4e3a \"channels_last\" \uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, padded_rows, padded_cols, channels) \u3002 \u5982\u679c data_format \u4e3a \"channels_first\" \uff0c \u8f93\u51fa 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch, channels, padded_rows, padded_cols) \u3002 [source]","title":"ZeroPadding2D"},{"location":"2-Layers/2.convolutional/#zeropadding3d","text":"keras.layers.ZeroPadding3D(padding=(1, 1, 1), data_format=None) 3D \u6570\u636e\u7684\u96f6\u586b\u5145\u5c42(\u7a7a\u95f4\u6216\u65f6\u7a7a)\u3002 \u53c2\u6570 padding : \u6574\u6570\uff0c\u6216 3 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\uff0c\u6216 2 \u4e2a\u6574\u6570\u7684 3 \u4e2a\u5143\u7ec4\u3002 \u5982\u679c\u4e3a\u6574\u6570\uff1a\u5c06\u5bf9\u6df1\u5ea6\u3001\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u8fd0\u7528\u76f8\u540c\u7684\u5bf9\u79f0\u586b\u5145\u3002 \u5982\u679c\u4e3a 3 \u4e2a\u6574\u6570\u7684\u5143\u7ec4\uff1a \u89e3\u91ca\u4e3a\u6df1\u5ea6\u3001\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u7684\u4e09\u4e2a\u4e0d\u540c\u7684\u5bf9\u79f0\u586b\u5145\u503c\uff1a (symmetric_dim1_pad, symmetric_dim2_pad, symmetric_dim3_pad) . \u5982\u679c\u4e3a 2 \u4e2a\u6574\u6570\u7684 3 \u4e2a\u5143\u7ec4\uff1a\u89e3\u91ca\u4e3a ((left_dim1_pad, right_dim1_pad), (left_dim2_pad, right_dim2_pad), (left_dim3_pad, right_dim3_pad)) data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4) \u6216 channels_first \u4e4b\u4e00\uff0c \u8868\u793a\u8f93\u5165\u4e2d\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) \u3002 \u5b83\u9ed8\u8ba4\u4e3a\u4ece Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d \u627e\u5230\u7684 image_data_format \u503c\u3002 \u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u5c06\u4f7f\u7528 \"channels_last\"\u3002 \u8f93\u5165\u5c3a\u5bf8 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a \u5982\u679c data_format \u4e3a \"channels_last\" : (batch, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad, depth) \u3002 \u5982\u679c data_format \u4e3a \"channels_first\" : (batch, depth, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad) \u3002 \u8f93\u51fa\u5c3a\u5bf8 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a \u5982\u679c data_format \u4e3a \"channels_last\" : (batch, first_padded_axis, second_padded_axis, third_axis_to_pad, depth) \u3002 \u5982\u679c data_format \u4e3a \"channels_first\" : (batch, depth, first_padded_axis, second_padded_axis, third_axis_to_pad) \u3002","title":"ZeroPadding3D"},{"location":"2-Layers/3.pooling/","text":"\u6c60\u5316\u5c42 [source] MaxPooling1D keras.layers.MaxPooling1D(pool_size=2, strides=None, padding='valid', data_format='channels_last') \u5bf9\u4e8e\u65f6\u5e8f\u6570\u636e\u7684\u6700\u5927\u6c60\u5316\u3002 \u53c2\u6570 pool_size : \u6574\u6570\uff0c\u6700\u5927\u6c60\u5316\u7684\u7a97\u53e3\u5927\u5c0f\u3002 strides : \u6574\u6570\uff0c\u6216\u8005\u662f None \u3002\u4f5c\u4e3a\u7f29\u5c0f\u6bd4\u4f8b\u7684\u56e0\u6570\u3002 \u4f8b\u5982\uff0c2 \u4f1a\u4f7f\u5f97\u8f93\u5165\u5f20\u91cf\u7f29\u5c0f\u4e00\u534a\u3002 \u5982\u679c\u662f None \uff0c\u90a3\u4e48\u9ed8\u8ba4\u503c\u662f pool_size \u3002 padding : \"valid\" \u6216\u8005 \"same\" \uff08\u533a\u5206\u5927\u5c0f\u5199\uff09\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4)\u6216 channels_first \u4e4b\u4e00\u3002 \u8868\u793a\u8f93\u5165\u5404\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, steps, features) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, features, steps) \u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' \uff0c \u8f93\u5165\u4e3a 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (batch_size, steps, features) \u5982\u679c data_format='channels_first' \uff0c \u8f93\u5165\u4e3a 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (batch_size, features, steps) \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' \uff0c \u8f93\u51fa\u4e3a 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (batch_size, downsampled_steps, features) \u5982\u679c data_format='channels_first' \uff0c \u8f93\u51fa\u4e3a 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (batch_size, features, downsampled_steps) [source] MaxPooling2D keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None) \u5bf9\u4e8e\u7a7a\u95f4\u6570\u636e\u7684\u6700\u5927\u6c60\u5316\u3002 \u53c2\u6570 pool_size : \u6574\u6570\uff0c\u6216\u8005 2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\uff0c \u6cbf\uff08\u5782\u76f4\uff0c\u6c34\u5e73\uff09\u65b9\u5411\u7f29\u5c0f\u6bd4\u4f8b\u7684\u56e0\u6570\u3002 \uff082\uff0c2\uff09\u4f1a\u628a\u8f93\u5165\u5f20\u91cf\u7684\u4e24\u4e2a\u7ef4\u5ea6\u90fd\u7f29\u5c0f\u4e00\u534a\u3002 \u5982\u679c\u53ea\u4f7f\u7528\u4e00\u4e2a\u6574\u6570\uff0c\u90a3\u4e48\u4e24\u4e2a\u7ef4\u5ea6\u90fd\u4f1a\u4f7f\u7528\u540c\u6837\u7684\u7a97\u53e3\u957f\u5ea6\u3002 strides : \u6574\u6570\uff0c2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\uff0c\u6216\u8005\u662f None \u3002 \u8868\u793a\u6b65\u957f\u503c\u3002 \u5982\u679c\u662f None \uff0c\u90a3\u4e48\u9ed8\u8ba4\u503c\u662f pool_size \u3002 padding : \"valid\" \u6216\u8005 \"same\" \uff08\u533a\u5206\u5927\u5c0f\u5199\uff09\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4)\u6216 channels_first \u4e4b\u4e00\u3002 \u8868\u793a\u8f93\u5165\u5404\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, height, width, channels) \u7684\u8f93\u5165\u5f20\u91cf\uff0c \u800c channels_first \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, channels, height, width) \u7684\u8f93\u5165\u5f20\u91cf\u3002 \u9ed8\u8ba4\u503c\u6839\u636e Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d\u7684 image_data_format \u503c\u6765\u8bbe\u7f6e\u3002 \u5982\u679c\u8fd8\u6ca1\u6709\u8bbe\u7f6e\u8fc7\uff0c\u90a3\u4e48\u9ed8\u8ba4\u503c\u5c31\u662f \"channels_last\"\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' : \u5c3a\u5bf8\u662f (batch_size, rows, cols, channels) \u7684 4D \u5f20\u91cf \u5982\u679c data_format='channels_first' : \u5c3a\u5bf8\u662f (batch_size, channels, rows, cols) \u7684 4D \u5f20\u91cf \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' : \u5c3a\u5bf8\u662f (batch_size, pooled_rows, pooled_cols, channels) \u7684 4D \u5f20\u91cf \u5982\u679c data_format='channels_first' : \u5c3a\u5bf8\u662f (batch_size, channels, pooled_rows, pooled_cols) \u7684 4D \u5f20\u91cf [source] MaxPooling3D keras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None) \u5bf9\u4e8e 3D\uff08\u7a7a\u95f4\uff0c\u6216\u65f6\u7a7a\u95f4\uff09\u6570\u636e\u7684\u6700\u5927\u6c60\u5316\u3002 \u53c2\u6570 pool_size : 3 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\uff0c\u7f29\u5c0f\uff08dim1\uff0cdim2\uff0cdim3\uff09\u6bd4\u4f8b\u7684\u56e0\u6570\u3002 (2, 2, 2) \u4f1a\u628a 3D \u8f93\u5165\u5f20\u91cf\u7684\u6bcf\u4e2a\u7ef4\u5ea6\u7f29\u5c0f\u4e00\u534a\u3002 strides : 3 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\uff0c\u6216\u8005\u662f None \u3002\u6b65\u957f\u503c\u3002 padding : \"valid\" \u6216\u8005 \"same\" \uff08\u533a\u5206\u5927\u5c0f\u5199\uff09\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4)\u6216 channels_first \u4e4b\u4e00\u3002 \u8868\u793a\u8f93\u5165\u5404\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) \u7684\u8f93\u5165\u5f20\u91cf\uff0c \u800c channels_first \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) \u7684\u8f93\u5165\u5f20\u91cf\u3002 \u9ed8\u8ba4\u503c\u6839\u636e Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d\u7684 image_data_format \u503c\u6765\u8bbe\u7f6e\u3002 \u5982\u679c\u8fd8\u6ca1\u6709\u8bbe\u7f6e\u8fc7\uff0c\u90a3\u4e48\u9ed8\u8ba4\u503c\u5c31\u662f \"channels_last\"\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' : \u5c3a\u5bf8\u662f (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels) \u7684 5D \u5f20\u91cf \u5982\u679c data_format='channels_first' : \u5c3a\u5bf8\u662f (batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3) \u7684 5D \u5f20\u91cf \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' : \u5c3a\u5bf8\u662f (batch_size, pooled_dim1, pooled_dim2, pooled_dim3, channels) \u7684 5D \u5f20\u91cf \u5982\u679c data_format='channels_first' : \u5c3a\u5bf8\u662f (batch_size, channels, pooled_dim1, pooled_dim2, pooled_dim3) \u7684 5D \u5f20\u91cf [source] AveragePooling1D keras.layers.AveragePooling1D(pool_size=2, strides=None, padding='valid', data_format='channels_last') \u5bf9\u4e8e\u65f6\u5e8f\u6570\u636e\u7684\u5e73\u5747\u6c60\u5316\u3002 \u53c2\u6570 pool_size : \u6574\u6570\uff0c\u5e73\u5747\u6c60\u5316\u7684\u7a97\u53e3\u5927\u5c0f\u3002 strides : \u6574\u6570\uff0c\u6216\u8005\u662f None \u3002\u4f5c\u4e3a\u7f29\u5c0f\u6bd4\u4f8b\u7684\u56e0\u6570\u3002 \u4f8b\u5982\uff0c2 \u4f1a\u4f7f\u5f97\u8f93\u5165\u5f20\u91cf\u7f29\u5c0f\u4e00\u534a\u3002 \u5982\u679c\u662f None \uff0c\u90a3\u4e48\u9ed8\u8ba4\u503c\u662f pool_size \u3002 padding : \"valid\" \u6216\u8005 \"same\" \uff08\u533a\u5206\u5927\u5c0f\u5199\uff09\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4)\u6216 channels_first \u4e4b\u4e00\u3002 \u8868\u793a\u8f93\u5165\u5404\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, steps, features) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, features, steps) \u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' \uff0c \u8f93\u5165\u4e3a 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (batch_size, steps, features) \u5982\u679c data_format='channels_first' \uff0c \u8f93\u5165\u4e3a 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (batch_size, features, steps) \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' \uff0c \u8f93\u51fa\u4e3a 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (batch_size, downsampled_steps, features) \u5982\u679c data_format='channels_first' \uff0c \u8f93\u51fa\u4e3a 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (batch_size, features, downsampled_steps) [source] AveragePooling2D keras.layers.AveragePooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None) \u5bf9\u4e8e\u7a7a\u95f4\u6570\u636e\u7684\u5e73\u5747\u6c60\u5316\u3002 \u53c2\u6570 pool_size : \u6574\u6570\uff0c\u6216\u8005 2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\uff0c \u6cbf\uff08\u5782\u76f4\uff0c\u6c34\u5e73\uff09\u65b9\u5411\u7f29\u5c0f\u6bd4\u4f8b\u7684\u56e0\u6570\u3002 \uff082\uff0c2\uff09\u4f1a\u628a\u8f93\u5165\u5f20\u91cf\u7684\u4e24\u4e2a\u7ef4\u5ea6\u90fd\u7f29\u5c0f\u4e00\u534a\u3002 \u5982\u679c\u53ea\u4f7f\u7528\u4e00\u4e2a\u6574\u6570\uff0c\u90a3\u4e48\u4e24\u4e2a\u7ef4\u5ea6\u90fd\u4f1a\u4f7f\u7528\u540c\u6837\u7684\u7a97\u53e3\u957f\u5ea6\u3002 strides : \u6574\u6570\uff0c2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\uff0c\u6216\u8005\u662f None \u3002 \u8868\u793a\u6b65\u957f\u503c\u3002 \u5982\u679c\u662f None \uff0c\u90a3\u4e48\u9ed8\u8ba4\u503c\u662f pool_size \u3002 padding : \"valid\" \u6216\u8005 \"same\" \uff08\u533a\u5206\u5927\u5c0f\u5199\uff09\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4)\u6216 channels_first \u4e4b\u4e00\u3002 \u8868\u793a\u8f93\u5165\u5404\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, height, width, channels) \u7684\u8f93\u5165\u5f20\u91cf\uff0c \u800c channels_first \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, channels, height, width) \u7684\u8f93\u5165\u5f20\u91cf\u3002 \u9ed8\u8ba4\u503c\u6839\u636e Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d\u7684 image_data_format \u503c\u6765\u8bbe\u7f6e\u3002 \u5982\u679c\u8fd8\u6ca1\u6709\u8bbe\u7f6e\u8fc7\uff0c\u90a3\u4e48\u9ed8\u8ba4\u503c\u5c31\u662f \"channels_last\"\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' : \u5c3a\u5bf8\u662f (batch_size, rows, cols, channels) \u7684 4D \u5f20\u91cf \u5982\u679c data_format='channels_first' : \u5c3a\u5bf8\u662f (batch_size, channels, rows, cols) \u7684 4D \u5f20\u91cf \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' : \u5c3a\u5bf8\u662f (batch_size, pooled_rows, pooled_cols, channels) \u7684 4D \u5f20\u91cf \u5982\u679c data_format='channels_first' : \u5c3a\u5bf8\u662f (batch_size, channels, pooled_rows, pooled_cols) \u7684 4D \u5f20\u91cf [source] AveragePooling3D keras.layers.AveragePooling3D(pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None) \u5bf9\u4e8e 3D \uff08\u7a7a\u95f4\uff0c\u6216\u8005\u65f6\u7a7a\u95f4\uff09\u6570\u636e\u7684\u5e73\u5747\u6c60\u5316\u3002 \u53c2\u6570 pool_size : 3 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\uff0c\u7f29\u5c0f\uff08dim1\uff0cdim2\uff0cdim3\uff09\u6bd4\u4f8b\u7684\u56e0\u6570\u3002 (2, 2, 2) \u4f1a\u628a 3D \u8f93\u5165\u5f20\u91cf\u7684\u6bcf\u4e2a\u7ef4\u5ea6\u7f29\u5c0f\u4e00\u534a\u3002 strides : 3 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\uff0c\u6216\u8005\u662f None \u3002\u6b65\u957f\u503c\u3002 padding : \"valid\" \u6216\u8005 \"same\" \uff08\u533a\u5206\u5927\u5c0f\u5199\uff09\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4)\u6216 channels_first \u4e4b\u4e00\u3002 \u8868\u793a\u8f93\u5165\u5404\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) \u7684\u8f93\u5165\u5f20\u91cf\uff0c \u800c channels_first \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) \u7684\u8f93\u5165\u5f20\u91cf\u3002 \u9ed8\u8ba4\u503c\u6839\u636e Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d\u7684 image_data_format \u503c\u6765\u8bbe\u7f6e\u3002 \u5982\u679c\u8fd8\u6ca1\u6709\u8bbe\u7f6e\u8fc7\uff0c\u90a3\u4e48\u9ed8\u8ba4\u503c\u5c31\u662f \"channels_last\"\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' : \u5c3a\u5bf8\u662f (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels) \u7684 5D \u5f20\u91cf \u5982\u679c data_format='channels_first' : \u5c3a\u5bf8\u662f (batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3) \u7684 5D \u5f20\u91cf \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' : \u5c3a\u5bf8\u662f (batch_size, pooled_dim1, pooled_dim2, pooled_dim3, channels) \u7684 5D \u5f20\u91cf \u5982\u679c data_format='channels_first' : \u5c3a\u5bf8\u662f (batch_size, channels, pooled_dim1, pooled_dim2, pooled_dim3) \u7684 5D \u5f20\u91cf [source] GlobalMaxPooling1D keras.layers.GlobalMaxPooling1D(data_format='channels_last') \u5bf9\u4e8e\u65f6\u5e8f\u6570\u636e\u7684\u5168\u5c40\u6700\u5927\u6c60\u5316\u3002 \u53c2\u6570 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4)\u6216 channels_first \u4e4b\u4e00\u3002 \u8868\u793a\u8f93\u5165\u5404\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, steps, features) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, features, steps) \u3002 \u8f93\u5165\u5c3a\u5bf8 \u5c3a\u5bf8\u662f (batch_size, steps, features) \u7684 3D \u5f20\u91cf\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u5c3a\u5bf8\u662f (batch_size, features) \u7684 2D \u5f20\u91cf\u3002 [source] GlobalAveragePooling1D keras.layers.GlobalAveragePooling1D() \u5bf9\u4e8e\u65f6\u5e8f\u6570\u636e\u7684\u5168\u5c40\u5e73\u5747\u6c60\u5316\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' \uff0c \u8f93\u5165\u4e3a 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (batch_size, steps, features) \u5982\u679c data_format='channels_first' \uff0c \u8f93\u5165\u4e3a 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (batch_size, features, steps) \u8f93\u51fa\u5c3a\u5bf8 \u5c3a\u5bf8\u662f (batch_size, features) \u7684 2D \u5f20\u91cf\u3002 [source] GlobalMaxPooling2D keras.layers.GlobalMaxPooling2D(data_format=None) \u5bf9\u4e8e\u7a7a\u57df\u6570\u636e\u7684\u5168\u5c40\u6700\u5927\u6c60\u5316\u3002 \u53c2\u6570 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4)\u6216 channels_first \u4e4b\u4e00\u3002 \u8868\u793a\u8f93\u5165\u5404\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, height, width, channels) \u7684\u8f93\u5165\u5f20\u91cf\uff0c \u800c channels_first \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, channels, height, width) \u7684\u8f93\u5165\u5f20\u91cf\u3002 \u9ed8\u8ba4\u503c\u6839\u636e Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d\u7684 image_data_format \u503c\u6765\u8bbe\u7f6e\u3002 \u5982\u679c\u8fd8\u6ca1\u6709\u8bbe\u7f6e\u8fc7\uff0c\u90a3\u4e48\u9ed8\u8ba4\u503c\u5c31\u662f \"channels_last\"\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' : \u5c3a\u5bf8\u662f (batch_size, rows, cols, channels) \u7684 4D \u5f20\u91cf \u5982\u679c data_format='channels_first' : \u5c3a\u5bf8\u662f (batch_size, channels, rows, cols) \u7684 4D \u5f20\u91cf \u8f93\u51fa\u5c3a\u5bf8 \u5c3a\u5bf8\u662f (batch_size, channels) \u7684 2D \u5f20\u91cf [source] GlobalAveragePooling2D keras.layers.GlobalAveragePooling2D(data_format=None) \u5bf9\u4e8e\u7a7a\u57df\u6570\u636e\u7684\u5168\u5c40\u5e73\u5747\u6c60\u5316\u3002 \u53c2\u6570 data_format : \u4e00\u4e2a\u5b57\u7b26\u4e32\uff0c channels_last \uff08\u9ed8\u8ba4\u503c\uff09\u6216\u8005 channels_first \u3002 \u8f93\u5165\u5f20\u91cf\u4e2d\u7684\u7ef4\u5ea6\u987a\u5e8f\u3002 channels_last \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, height, width, channels) \u7684\u8f93\u5165\u5f20\u91cf\uff0c\u800c channels_first \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, channels, height, width) \u7684\u8f93\u5165\u5f20\u91cf\u3002 \u9ed8\u8ba4\u503c\u6839\u636e Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d\u7684 image_data_format \u503c\u6765\u8bbe\u7f6e\u3002 \u5982\u679c\u8fd8\u6ca1\u6709\u8bbe\u7f6e\u8fc7\uff0c\u90a3\u4e48\u9ed8\u8ba4\u503c\u5c31\u662f \"channels_last\"\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' : \u5c3a\u5bf8\u662f (batch_size, rows, cols, channels) \u7684 4D \u5f20\u91cf \u5982\u679c data_format='channels_first' : \u5c3a\u5bf8\u662f (batch_size, channels, rows, cols) \u7684 4D \u5f20\u91cf \u8f93\u51fa\u5c3a\u5bf8 \u5c3a\u5bf8\u662f (batch_size, channels) \u7684 2D \u5f20\u91cf [source] GlobalMaxPooling3D keras.layers.GlobalMaxPooling3D(data_format=None) \u5bf9\u4e8e 3D \u6570\u636e\u7684\u5168\u5c40\u6700\u5927\u6c60\u5316\u3002 \u53c2\u6570 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4)\u6216 channels_first \u4e4b\u4e00\u3002 \u8868\u793a\u8f93\u5165\u5404\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) \u7684\u8f93\u5165\u5f20\u91cf\uff0c \u800c channels_first \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) \u7684\u8f93\u5165\u5f20\u91cf\u3002 \u9ed8\u8ba4\u503c\u6839\u636e Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d\u7684 image_data_format \u503c\u6765\u8bbe\u7f6e\u3002 \u5982\u679c\u8fd8\u6ca1\u6709\u8bbe\u7f6e\u8fc7\uff0c\u90a3\u4e48\u9ed8\u8ba4\u503c\u5c31\u662f \"channels_last\"\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' : \u5c3a\u5bf8\u662f (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels) \u7684 5D \u5f20\u91cf \u5982\u679c data_format='channels_first' : \u5c3a\u5bf8\u662f (batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3) \u7684 5D \u5f20\u91cf \u8f93\u51fa\u5c3a\u5bf8 \u5c3a\u5bf8\u662f (batch_size, channels) \u7684 2D \u5f20\u91cf [source] GlobalAveragePooling3D keras.layers.GlobalAveragePooling3D(data_format=None) \u5bf9\u4e8e 3D \u6570\u636e\u7684\u5168\u5c40\u5e73\u5747\u6c60\u5316\u3002 \u53c2\u6570 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4)\u6216 channels_first \u4e4b\u4e00\u3002 \u8868\u793a\u8f93\u5165\u5404\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) \u7684\u8f93\u5165\u5f20\u91cf\uff0c \u800c channels_first \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) \u7684\u8f93\u5165\u5f20\u91cf\u3002 \u9ed8\u8ba4\u503c\u6839\u636e Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d\u7684 image_data_format \u503c\u6765\u8bbe\u7f6e\u3002 \u5982\u679c\u8fd8\u6ca1\u6709\u8bbe\u7f6e\u8fc7\uff0c\u90a3\u4e48\u9ed8\u8ba4\u503c\u5c31\u662f \"channels_last\"\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' : \u5c3a\u5bf8\u662f (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels) \u7684 5D \u5f20\u91cf \u5982\u679c data_format='channels_first' : \u5c3a\u5bf8\u662f (batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3) \u7684 5D \u5f20\u91cf \u8f93\u51fa\u5c3a\u5bf8 \u5c3a\u5bf8\u662f (batch_size, channels) \u7684 2D \u5f20\u91cf","title":"\u6c60\u5316\u5c42"},{"location":"2-Layers/3.pooling/#_1","text":"[source]","title":"\u6c60\u5316\u5c42"},{"location":"2-Layers/3.pooling/#maxpooling1d","text":"keras.layers.MaxPooling1D(pool_size=2, strides=None, padding='valid', data_format='channels_last') \u5bf9\u4e8e\u65f6\u5e8f\u6570\u636e\u7684\u6700\u5927\u6c60\u5316\u3002 \u53c2\u6570 pool_size : \u6574\u6570\uff0c\u6700\u5927\u6c60\u5316\u7684\u7a97\u53e3\u5927\u5c0f\u3002 strides : \u6574\u6570\uff0c\u6216\u8005\u662f None \u3002\u4f5c\u4e3a\u7f29\u5c0f\u6bd4\u4f8b\u7684\u56e0\u6570\u3002 \u4f8b\u5982\uff0c2 \u4f1a\u4f7f\u5f97\u8f93\u5165\u5f20\u91cf\u7f29\u5c0f\u4e00\u534a\u3002 \u5982\u679c\u662f None \uff0c\u90a3\u4e48\u9ed8\u8ba4\u503c\u662f pool_size \u3002 padding : \"valid\" \u6216\u8005 \"same\" \uff08\u533a\u5206\u5927\u5c0f\u5199\uff09\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4)\u6216 channels_first \u4e4b\u4e00\u3002 \u8868\u793a\u8f93\u5165\u5404\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, steps, features) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, features, steps) \u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' \uff0c \u8f93\u5165\u4e3a 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (batch_size, steps, features) \u5982\u679c data_format='channels_first' \uff0c \u8f93\u5165\u4e3a 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (batch_size, features, steps) \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' \uff0c \u8f93\u51fa\u4e3a 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (batch_size, downsampled_steps, features) \u5982\u679c data_format='channels_first' \uff0c \u8f93\u51fa\u4e3a 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (batch_size, features, downsampled_steps) [source]","title":"MaxPooling1D"},{"location":"2-Layers/3.pooling/#maxpooling2d","text":"keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None) \u5bf9\u4e8e\u7a7a\u95f4\u6570\u636e\u7684\u6700\u5927\u6c60\u5316\u3002 \u53c2\u6570 pool_size : \u6574\u6570\uff0c\u6216\u8005 2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\uff0c \u6cbf\uff08\u5782\u76f4\uff0c\u6c34\u5e73\uff09\u65b9\u5411\u7f29\u5c0f\u6bd4\u4f8b\u7684\u56e0\u6570\u3002 \uff082\uff0c2\uff09\u4f1a\u628a\u8f93\u5165\u5f20\u91cf\u7684\u4e24\u4e2a\u7ef4\u5ea6\u90fd\u7f29\u5c0f\u4e00\u534a\u3002 \u5982\u679c\u53ea\u4f7f\u7528\u4e00\u4e2a\u6574\u6570\uff0c\u90a3\u4e48\u4e24\u4e2a\u7ef4\u5ea6\u90fd\u4f1a\u4f7f\u7528\u540c\u6837\u7684\u7a97\u53e3\u957f\u5ea6\u3002 strides : \u6574\u6570\uff0c2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\uff0c\u6216\u8005\u662f None \u3002 \u8868\u793a\u6b65\u957f\u503c\u3002 \u5982\u679c\u662f None \uff0c\u90a3\u4e48\u9ed8\u8ba4\u503c\u662f pool_size \u3002 padding : \"valid\" \u6216\u8005 \"same\" \uff08\u533a\u5206\u5927\u5c0f\u5199\uff09\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4)\u6216 channels_first \u4e4b\u4e00\u3002 \u8868\u793a\u8f93\u5165\u5404\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, height, width, channels) \u7684\u8f93\u5165\u5f20\u91cf\uff0c \u800c channels_first \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, channels, height, width) \u7684\u8f93\u5165\u5f20\u91cf\u3002 \u9ed8\u8ba4\u503c\u6839\u636e Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d\u7684 image_data_format \u503c\u6765\u8bbe\u7f6e\u3002 \u5982\u679c\u8fd8\u6ca1\u6709\u8bbe\u7f6e\u8fc7\uff0c\u90a3\u4e48\u9ed8\u8ba4\u503c\u5c31\u662f \"channels_last\"\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' : \u5c3a\u5bf8\u662f (batch_size, rows, cols, channels) \u7684 4D \u5f20\u91cf \u5982\u679c data_format='channels_first' : \u5c3a\u5bf8\u662f (batch_size, channels, rows, cols) \u7684 4D \u5f20\u91cf \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' : \u5c3a\u5bf8\u662f (batch_size, pooled_rows, pooled_cols, channels) \u7684 4D \u5f20\u91cf \u5982\u679c data_format='channels_first' : \u5c3a\u5bf8\u662f (batch_size, channels, pooled_rows, pooled_cols) \u7684 4D \u5f20\u91cf [source]","title":"MaxPooling2D"},{"location":"2-Layers/3.pooling/#maxpooling3d","text":"keras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None) \u5bf9\u4e8e 3D\uff08\u7a7a\u95f4\uff0c\u6216\u65f6\u7a7a\u95f4\uff09\u6570\u636e\u7684\u6700\u5927\u6c60\u5316\u3002 \u53c2\u6570 pool_size : 3 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\uff0c\u7f29\u5c0f\uff08dim1\uff0cdim2\uff0cdim3\uff09\u6bd4\u4f8b\u7684\u56e0\u6570\u3002 (2, 2, 2) \u4f1a\u628a 3D \u8f93\u5165\u5f20\u91cf\u7684\u6bcf\u4e2a\u7ef4\u5ea6\u7f29\u5c0f\u4e00\u534a\u3002 strides : 3 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\uff0c\u6216\u8005\u662f None \u3002\u6b65\u957f\u503c\u3002 padding : \"valid\" \u6216\u8005 \"same\" \uff08\u533a\u5206\u5927\u5c0f\u5199\uff09\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4)\u6216 channels_first \u4e4b\u4e00\u3002 \u8868\u793a\u8f93\u5165\u5404\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) \u7684\u8f93\u5165\u5f20\u91cf\uff0c \u800c channels_first \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) \u7684\u8f93\u5165\u5f20\u91cf\u3002 \u9ed8\u8ba4\u503c\u6839\u636e Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d\u7684 image_data_format \u503c\u6765\u8bbe\u7f6e\u3002 \u5982\u679c\u8fd8\u6ca1\u6709\u8bbe\u7f6e\u8fc7\uff0c\u90a3\u4e48\u9ed8\u8ba4\u503c\u5c31\u662f \"channels_last\"\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' : \u5c3a\u5bf8\u662f (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels) \u7684 5D \u5f20\u91cf \u5982\u679c data_format='channels_first' : \u5c3a\u5bf8\u662f (batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3) \u7684 5D \u5f20\u91cf \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' : \u5c3a\u5bf8\u662f (batch_size, pooled_dim1, pooled_dim2, pooled_dim3, channels) \u7684 5D \u5f20\u91cf \u5982\u679c data_format='channels_first' : \u5c3a\u5bf8\u662f (batch_size, channels, pooled_dim1, pooled_dim2, pooled_dim3) \u7684 5D \u5f20\u91cf [source]","title":"MaxPooling3D"},{"location":"2-Layers/3.pooling/#averagepooling1d","text":"keras.layers.AveragePooling1D(pool_size=2, strides=None, padding='valid', data_format='channels_last') \u5bf9\u4e8e\u65f6\u5e8f\u6570\u636e\u7684\u5e73\u5747\u6c60\u5316\u3002 \u53c2\u6570 pool_size : \u6574\u6570\uff0c\u5e73\u5747\u6c60\u5316\u7684\u7a97\u53e3\u5927\u5c0f\u3002 strides : \u6574\u6570\uff0c\u6216\u8005\u662f None \u3002\u4f5c\u4e3a\u7f29\u5c0f\u6bd4\u4f8b\u7684\u56e0\u6570\u3002 \u4f8b\u5982\uff0c2 \u4f1a\u4f7f\u5f97\u8f93\u5165\u5f20\u91cf\u7f29\u5c0f\u4e00\u534a\u3002 \u5982\u679c\u662f None \uff0c\u90a3\u4e48\u9ed8\u8ba4\u503c\u662f pool_size \u3002 padding : \"valid\" \u6216\u8005 \"same\" \uff08\u533a\u5206\u5927\u5c0f\u5199\uff09\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4)\u6216 channels_first \u4e4b\u4e00\u3002 \u8868\u793a\u8f93\u5165\u5404\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, steps, features) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, features, steps) \u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' \uff0c \u8f93\u5165\u4e3a 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (batch_size, steps, features) \u5982\u679c data_format='channels_first' \uff0c \u8f93\u5165\u4e3a 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (batch_size, features, steps) \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' \uff0c \u8f93\u51fa\u4e3a 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (batch_size, downsampled_steps, features) \u5982\u679c data_format='channels_first' \uff0c \u8f93\u51fa\u4e3a 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (batch_size, features, downsampled_steps) [source]","title":"AveragePooling1D"},{"location":"2-Layers/3.pooling/#averagepooling2d","text":"keras.layers.AveragePooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None) \u5bf9\u4e8e\u7a7a\u95f4\u6570\u636e\u7684\u5e73\u5747\u6c60\u5316\u3002 \u53c2\u6570 pool_size : \u6574\u6570\uff0c\u6216\u8005 2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\uff0c \u6cbf\uff08\u5782\u76f4\uff0c\u6c34\u5e73\uff09\u65b9\u5411\u7f29\u5c0f\u6bd4\u4f8b\u7684\u56e0\u6570\u3002 \uff082\uff0c2\uff09\u4f1a\u628a\u8f93\u5165\u5f20\u91cf\u7684\u4e24\u4e2a\u7ef4\u5ea6\u90fd\u7f29\u5c0f\u4e00\u534a\u3002 \u5982\u679c\u53ea\u4f7f\u7528\u4e00\u4e2a\u6574\u6570\uff0c\u90a3\u4e48\u4e24\u4e2a\u7ef4\u5ea6\u90fd\u4f1a\u4f7f\u7528\u540c\u6837\u7684\u7a97\u53e3\u957f\u5ea6\u3002 strides : \u6574\u6570\uff0c2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\uff0c\u6216\u8005\u662f None \u3002 \u8868\u793a\u6b65\u957f\u503c\u3002 \u5982\u679c\u662f None \uff0c\u90a3\u4e48\u9ed8\u8ba4\u503c\u662f pool_size \u3002 padding : \"valid\" \u6216\u8005 \"same\" \uff08\u533a\u5206\u5927\u5c0f\u5199\uff09\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4)\u6216 channels_first \u4e4b\u4e00\u3002 \u8868\u793a\u8f93\u5165\u5404\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, height, width, channels) \u7684\u8f93\u5165\u5f20\u91cf\uff0c \u800c channels_first \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, channels, height, width) \u7684\u8f93\u5165\u5f20\u91cf\u3002 \u9ed8\u8ba4\u503c\u6839\u636e Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d\u7684 image_data_format \u503c\u6765\u8bbe\u7f6e\u3002 \u5982\u679c\u8fd8\u6ca1\u6709\u8bbe\u7f6e\u8fc7\uff0c\u90a3\u4e48\u9ed8\u8ba4\u503c\u5c31\u662f \"channels_last\"\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' : \u5c3a\u5bf8\u662f (batch_size, rows, cols, channels) \u7684 4D \u5f20\u91cf \u5982\u679c data_format='channels_first' : \u5c3a\u5bf8\u662f (batch_size, channels, rows, cols) \u7684 4D \u5f20\u91cf \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' : \u5c3a\u5bf8\u662f (batch_size, pooled_rows, pooled_cols, channels) \u7684 4D \u5f20\u91cf \u5982\u679c data_format='channels_first' : \u5c3a\u5bf8\u662f (batch_size, channels, pooled_rows, pooled_cols) \u7684 4D \u5f20\u91cf [source]","title":"AveragePooling2D"},{"location":"2-Layers/3.pooling/#averagepooling3d","text":"keras.layers.AveragePooling3D(pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None) \u5bf9\u4e8e 3D \uff08\u7a7a\u95f4\uff0c\u6216\u8005\u65f6\u7a7a\u95f4\uff09\u6570\u636e\u7684\u5e73\u5747\u6c60\u5316\u3002 \u53c2\u6570 pool_size : 3 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\uff0c\u7f29\u5c0f\uff08dim1\uff0cdim2\uff0cdim3\uff09\u6bd4\u4f8b\u7684\u56e0\u6570\u3002 (2, 2, 2) \u4f1a\u628a 3D \u8f93\u5165\u5f20\u91cf\u7684\u6bcf\u4e2a\u7ef4\u5ea6\u7f29\u5c0f\u4e00\u534a\u3002 strides : 3 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\uff0c\u6216\u8005\u662f None \u3002\u6b65\u957f\u503c\u3002 padding : \"valid\" \u6216\u8005 \"same\" \uff08\u533a\u5206\u5927\u5c0f\u5199\uff09\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4)\u6216 channels_first \u4e4b\u4e00\u3002 \u8868\u793a\u8f93\u5165\u5404\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) \u7684\u8f93\u5165\u5f20\u91cf\uff0c \u800c channels_first \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) \u7684\u8f93\u5165\u5f20\u91cf\u3002 \u9ed8\u8ba4\u503c\u6839\u636e Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d\u7684 image_data_format \u503c\u6765\u8bbe\u7f6e\u3002 \u5982\u679c\u8fd8\u6ca1\u6709\u8bbe\u7f6e\u8fc7\uff0c\u90a3\u4e48\u9ed8\u8ba4\u503c\u5c31\u662f \"channels_last\"\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' : \u5c3a\u5bf8\u662f (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels) \u7684 5D \u5f20\u91cf \u5982\u679c data_format='channels_first' : \u5c3a\u5bf8\u662f (batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3) \u7684 5D \u5f20\u91cf \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' : \u5c3a\u5bf8\u662f (batch_size, pooled_dim1, pooled_dim2, pooled_dim3, channels) \u7684 5D \u5f20\u91cf \u5982\u679c data_format='channels_first' : \u5c3a\u5bf8\u662f (batch_size, channels, pooled_dim1, pooled_dim2, pooled_dim3) \u7684 5D \u5f20\u91cf [source]","title":"AveragePooling3D"},{"location":"2-Layers/3.pooling/#globalmaxpooling1d","text":"keras.layers.GlobalMaxPooling1D(data_format='channels_last') \u5bf9\u4e8e\u65f6\u5e8f\u6570\u636e\u7684\u5168\u5c40\u6700\u5927\u6c60\u5316\u3002 \u53c2\u6570 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4)\u6216 channels_first \u4e4b\u4e00\u3002 \u8868\u793a\u8f93\u5165\u5404\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, steps, features) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, features, steps) \u3002 \u8f93\u5165\u5c3a\u5bf8 \u5c3a\u5bf8\u662f (batch_size, steps, features) \u7684 3D \u5f20\u91cf\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u5c3a\u5bf8\u662f (batch_size, features) \u7684 2D \u5f20\u91cf\u3002 [source]","title":"GlobalMaxPooling1D"},{"location":"2-Layers/3.pooling/#globalaveragepooling1d","text":"keras.layers.GlobalAveragePooling1D() \u5bf9\u4e8e\u65f6\u5e8f\u6570\u636e\u7684\u5168\u5c40\u5e73\u5747\u6c60\u5316\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' \uff0c \u8f93\u5165\u4e3a 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (batch_size, steps, features) \u5982\u679c data_format='channels_first' \uff0c \u8f93\u5165\u4e3a 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (batch_size, features, steps) \u8f93\u51fa\u5c3a\u5bf8 \u5c3a\u5bf8\u662f (batch_size, features) \u7684 2D \u5f20\u91cf\u3002 [source]","title":"GlobalAveragePooling1D"},{"location":"2-Layers/3.pooling/#globalmaxpooling2d","text":"keras.layers.GlobalMaxPooling2D(data_format=None) \u5bf9\u4e8e\u7a7a\u57df\u6570\u636e\u7684\u5168\u5c40\u6700\u5927\u6c60\u5316\u3002 \u53c2\u6570 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4)\u6216 channels_first \u4e4b\u4e00\u3002 \u8868\u793a\u8f93\u5165\u5404\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, height, width, channels) \u7684\u8f93\u5165\u5f20\u91cf\uff0c \u800c channels_first \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, channels, height, width) \u7684\u8f93\u5165\u5f20\u91cf\u3002 \u9ed8\u8ba4\u503c\u6839\u636e Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d\u7684 image_data_format \u503c\u6765\u8bbe\u7f6e\u3002 \u5982\u679c\u8fd8\u6ca1\u6709\u8bbe\u7f6e\u8fc7\uff0c\u90a3\u4e48\u9ed8\u8ba4\u503c\u5c31\u662f \"channels_last\"\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' : \u5c3a\u5bf8\u662f (batch_size, rows, cols, channels) \u7684 4D \u5f20\u91cf \u5982\u679c data_format='channels_first' : \u5c3a\u5bf8\u662f (batch_size, channels, rows, cols) \u7684 4D \u5f20\u91cf \u8f93\u51fa\u5c3a\u5bf8 \u5c3a\u5bf8\u662f (batch_size, channels) \u7684 2D \u5f20\u91cf [source]","title":"GlobalMaxPooling2D"},{"location":"2-Layers/3.pooling/#globalaveragepooling2d","text":"keras.layers.GlobalAveragePooling2D(data_format=None) \u5bf9\u4e8e\u7a7a\u57df\u6570\u636e\u7684\u5168\u5c40\u5e73\u5747\u6c60\u5316\u3002 \u53c2\u6570 data_format : \u4e00\u4e2a\u5b57\u7b26\u4e32\uff0c channels_last \uff08\u9ed8\u8ba4\u503c\uff09\u6216\u8005 channels_first \u3002 \u8f93\u5165\u5f20\u91cf\u4e2d\u7684\u7ef4\u5ea6\u987a\u5e8f\u3002 channels_last \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, height, width, channels) \u7684\u8f93\u5165\u5f20\u91cf\uff0c\u800c channels_first \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, channels, height, width) \u7684\u8f93\u5165\u5f20\u91cf\u3002 \u9ed8\u8ba4\u503c\u6839\u636e Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d\u7684 image_data_format \u503c\u6765\u8bbe\u7f6e\u3002 \u5982\u679c\u8fd8\u6ca1\u6709\u8bbe\u7f6e\u8fc7\uff0c\u90a3\u4e48\u9ed8\u8ba4\u503c\u5c31\u662f \"channels_last\"\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' : \u5c3a\u5bf8\u662f (batch_size, rows, cols, channels) \u7684 4D \u5f20\u91cf \u5982\u679c data_format='channels_first' : \u5c3a\u5bf8\u662f (batch_size, channels, rows, cols) \u7684 4D \u5f20\u91cf \u8f93\u51fa\u5c3a\u5bf8 \u5c3a\u5bf8\u662f (batch_size, channels) \u7684 2D \u5f20\u91cf [source]","title":"GlobalAveragePooling2D"},{"location":"2-Layers/3.pooling/#globalmaxpooling3d","text":"keras.layers.GlobalMaxPooling3D(data_format=None) \u5bf9\u4e8e 3D \u6570\u636e\u7684\u5168\u5c40\u6700\u5927\u6c60\u5316\u3002 \u53c2\u6570 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4)\u6216 channels_first \u4e4b\u4e00\u3002 \u8868\u793a\u8f93\u5165\u5404\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) \u7684\u8f93\u5165\u5f20\u91cf\uff0c \u800c channels_first \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) \u7684\u8f93\u5165\u5f20\u91cf\u3002 \u9ed8\u8ba4\u503c\u6839\u636e Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d\u7684 image_data_format \u503c\u6765\u8bbe\u7f6e\u3002 \u5982\u679c\u8fd8\u6ca1\u6709\u8bbe\u7f6e\u8fc7\uff0c\u90a3\u4e48\u9ed8\u8ba4\u503c\u5c31\u662f \"channels_last\"\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' : \u5c3a\u5bf8\u662f (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels) \u7684 5D \u5f20\u91cf \u5982\u679c data_format='channels_first' : \u5c3a\u5bf8\u662f (batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3) \u7684 5D \u5f20\u91cf \u8f93\u51fa\u5c3a\u5bf8 \u5c3a\u5bf8\u662f (batch_size, channels) \u7684 2D \u5f20\u91cf [source]","title":"GlobalMaxPooling3D"},{"location":"2-Layers/3.pooling/#globalaveragepooling3d","text":"keras.layers.GlobalAveragePooling3D(data_format=None) \u5bf9\u4e8e 3D \u6570\u636e\u7684\u5168\u5c40\u5e73\u5747\u6c60\u5316\u3002 \u53c2\u6570 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4)\u6216 channels_first \u4e4b\u4e00\u3002 \u8868\u793a\u8f93\u5165\u5404\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) \u7684\u8f93\u5165\u5f20\u91cf\uff0c \u800c channels_first \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) \u7684\u8f93\u5165\u5f20\u91cf\u3002 \u9ed8\u8ba4\u503c\u6839\u636e Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d\u7684 image_data_format \u503c\u6765\u8bbe\u7f6e\u3002 \u5982\u679c\u8fd8\u6ca1\u6709\u8bbe\u7f6e\u8fc7\uff0c\u90a3\u4e48\u9ed8\u8ba4\u503c\u5c31\u662f \"channels_last\"\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_last' : \u5c3a\u5bf8\u662f (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels) \u7684 5D \u5f20\u91cf \u5982\u679c data_format='channels_first' : \u5c3a\u5bf8\u662f (batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3) \u7684 5D \u5f20\u91cf \u8f93\u51fa\u5c3a\u5bf8 \u5c3a\u5bf8\u662f (batch_size, channels) \u7684 2D \u5f20\u91cf","title":"GlobalAveragePooling3D"},{"location":"2-Layers/4.local/","text":"\u5c40\u90e8\u8fde\u63a5\u5c42 [source] LocallyConnected1D keras.layers.LocallyConnected1D(filters, kernel_size, strides=1, padding='valid', data_format=None, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) 1D \u8f93\u5165\u7684\u5c40\u90e8\u8fde\u63a5\u5c42\u3002 LocallyConnected1D \u5c42\u4e0e Conv1D \u5c42\u7684\u5de5\u4f5c\u65b9\u5f0f\u76f8\u540c\uff0c\u9664\u4e86\u6743\u503c\u4e0d\u5171\u4eab\u5916\uff0c \u4e5f\u5c31\u662f\u8bf4\uff0c\u5728\u8f93\u5165\u7684\u6bcf\u4e2a\u4e0d\u540c\u90e8\u5206\u5e94\u7528\u4e0d\u540c\u7684\u4e00\u7ec4\u8fc7\u6ee4\u5668\u3002 \u4f8b\u5b50 # \u5c06\u957f\u5ea6\u4e3a 3 \u7684\u975e\u5171\u4eab\u6743\u91cd 1D \u5377\u79ef\u5e94\u7528\u4e8e # \u5177\u6709 10 \u4e2a\u65f6\u95f4\u6b65\u957f\u7684\u5e8f\u5217\uff0c\u5e76\u4f7f\u7528 64\u4e2a \u8f93\u51fa\u6ee4\u6ce2\u5668 model = Sequential() model.add(LocallyConnected1D(64, 3, input_shape=(10, 32))) # \u73b0\u5728 model.output_shape == (None, 8, 64) # \u5728\u4e0a\u9762\u518d\u6dfb\u52a0\u4e00\u4e2a\u65b0\u7684 conv1d model.add(LocallyConnected1D(32, 3)) # \u73b0\u5728 model.output_shape == (None, 6, 32) \u53c2\u6570 filters : \u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6 \uff08\u5373\u5377\u79ef\u4e2d\u6ee4\u6ce2\u5668\u7684\u8f93\u51fa\u6570\u91cf\uff09\u3002 kernel_size : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005\u5355\u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e 1D \u5377\u79ef\u7a97\u53e3\u7684\u957f\u5ea6\u3002 strides : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005\u5355\u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e\u5377\u79ef\u7684\u6b65\u957f\u3002 \u6307\u5b9a\u4efb\u4f55 stride \u503c != 1 \u4e0e\u6307\u5b9a dilation_rate \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 padding : \u5f53\u524d\u4ec5\u652f\u6301 \"valid\" (\u5927\u5c0f\u5199\u654f\u611f)\u3002 \"same\" \u53ef\u80fd\u4f1a\u5728\u672a\u6765\u652f\u6301\u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u5982\u679c\u4f60\u4e0d\u6307\u5b9a\uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373\u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 bias_initializer : \u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u8f93\u51fa\uff08\u5b83\u7684\u6fc0\u6d3b\u503c\uff09\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 \u8f93\u5165\u5c3a\u5bf8 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (batch_size, steps, input_dim) \u3002 \u8f93\u51fa\u5c3a\u5bf8 3D \u5f20\u91cf \uff0c\u5c3a\u5bf8\u4e3a\uff1a (batch_size, new_steps, filters) \uff0c steps \u503c\u53ef\u80fd\u56e0\u586b\u5145\u6216\u6b65\u957f\u800c\u6539\u53d8\u3002 [source] LocallyConnected2D keras.layers.LocallyConnected2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) 2D \u8f93\u5165\u7684\u5c40\u90e8\u8fde\u63a5\u5c42\u3002 LocallyConnected2D \u5c42\u4e0e Conv2D \u5c42\u7684\u5de5\u4f5c\u65b9\u5f0f\u76f8\u540c\uff0c\u9664\u4e86\u6743\u503c\u4e0d\u5171\u4eab\u5916\uff0c \u4e5f\u5c31\u662f\u8bf4\uff0c\u5728\u8f93\u5165\u7684\u6bcf\u4e2a\u4e0d\u540c\u90e8\u5206\u5e94\u7528\u4e0d\u540c\u7684\u4e00\u7ec4\u8fc7\u6ee4\u5668\u3002 \u4f8b\u5b50 # \u5728 32x32 \u56fe\u50cf\u4e0a\u5e94\u7528 3x3 \u975e\u5171\u4eab\u6743\u503c\u548c64\u4e2a\u8f93\u51fa\u8fc7\u6ee4\u5668\u7684\u5377\u79ef # \u6570\u636e\u683c\u5f0f `data_format=\"channels_last\"`\uff1a model = Sequential() model.add(LocallyConnected2D(64, (3, 3), input_shape=(32, 32, 3))) # \u73b0\u5728 model.output_shape == (None, 30, 30, 64) # \u6ce8\u610f\u8fd9\u4e00\u5c42\u7684\u53c2\u6570\u6570\u91cf\u4e3a (30*30)*(3*3*3*64) + (30*30)*64 # \u5728\u4e0a\u9762\u518d\u52a0\u4e00\u4e2a 3x3 \u975e\u5171\u4eab\u6743\u503c\u548c 32 \u4e2a\u8f93\u51fa\u6ee4\u6ce2\u5668\u7684\u5377\u79ef\uff1a model.add(LocallyConnected2D(32, (3, 3))) # \u73b0\u5728 model.output_shape == (None, 28, 28, 32) \u53c2\u6570 filters : \u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6 \uff08\u5373\u5377\u79ef\u4e2d\u6ee4\u6ce2\u5668\u7684\u8f93\u51fa\u6570\u91cf\uff09\u3002 kernel_size : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e 2D \u5377\u79ef\u7a97\u53e3\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 strides : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e\u5377\u79ef\u6cbf\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u65b9\u5411\u7684\u6b65\u957f\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 padding : \u5f53\u524d\u4ec5\u652f\u6301 \"valid\" (\u5927\u5c0f\u5199\u654f\u611f)\u3002 \"same\" \u53ef\u80fd\u4f1a\u5728\u672a\u6765\u652f\u6301\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4) \u6216 channels_first \u4e4b\u4e00\u3002 \u8f93\u5165\u4e2d\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, height, width, channels) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, channels, height, width) \u3002 \u5b83\u9ed8\u8ba4\u4e3a\u4ece Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d \u627e\u5230\u7684 image_data_format \u503c\u3002 \u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u5c06\u4f7f\u7528 \"channels_last\"\u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u5982\u679c\u4f60\u4e0d\u6307\u5b9a\uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373\u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 bias_initializer : \u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u8f93\u51fa\uff08\u5b83\u7684\u6fc0\u6d3b\u503c\uff09\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 \u8f93\u5165\u5c3a\u5bf8 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (samples, channels, rows, cols) \uff0c\u5982\u679c data_format='channels_first'\uff1b \u6216\u8005 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (samples, rows, cols, channels) \uff0c\u5982\u679c data_format='channels_last'\u3002 \u8f93\u51fa\u5c3a\u5bf8 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (samples, filters, new_rows, new_cols) \uff0c\u5982\u679c data_format='channels_first'\uff1b \u6216\u8005 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (samples, new_rows, new_cols, filters) \uff0c\u5982\u679c data_format='channels_last'\u3002 rows \u548c cols \u7684\u503c\u53ef\u80fd\u56e0\u586b\u5145\u800c\u6539\u53d8\u3002","title":"\u5c40\u90e8\u8fde\u63a5\u5c42"},{"location":"2-Layers/4.local/#_1","text":"[source]","title":"\u5c40\u90e8\u8fde\u63a5\u5c42"},{"location":"2-Layers/4.local/#locallyconnected1d","text":"keras.layers.LocallyConnected1D(filters, kernel_size, strides=1, padding='valid', data_format=None, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) 1D \u8f93\u5165\u7684\u5c40\u90e8\u8fde\u63a5\u5c42\u3002 LocallyConnected1D \u5c42\u4e0e Conv1D \u5c42\u7684\u5de5\u4f5c\u65b9\u5f0f\u76f8\u540c\uff0c\u9664\u4e86\u6743\u503c\u4e0d\u5171\u4eab\u5916\uff0c \u4e5f\u5c31\u662f\u8bf4\uff0c\u5728\u8f93\u5165\u7684\u6bcf\u4e2a\u4e0d\u540c\u90e8\u5206\u5e94\u7528\u4e0d\u540c\u7684\u4e00\u7ec4\u8fc7\u6ee4\u5668\u3002 \u4f8b\u5b50 # \u5c06\u957f\u5ea6\u4e3a 3 \u7684\u975e\u5171\u4eab\u6743\u91cd 1D \u5377\u79ef\u5e94\u7528\u4e8e # \u5177\u6709 10 \u4e2a\u65f6\u95f4\u6b65\u957f\u7684\u5e8f\u5217\uff0c\u5e76\u4f7f\u7528 64\u4e2a \u8f93\u51fa\u6ee4\u6ce2\u5668 model = Sequential() model.add(LocallyConnected1D(64, 3, input_shape=(10, 32))) # \u73b0\u5728 model.output_shape == (None, 8, 64) # \u5728\u4e0a\u9762\u518d\u6dfb\u52a0\u4e00\u4e2a\u65b0\u7684 conv1d model.add(LocallyConnected1D(32, 3)) # \u73b0\u5728 model.output_shape == (None, 6, 32) \u53c2\u6570 filters : \u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6 \uff08\u5373\u5377\u79ef\u4e2d\u6ee4\u6ce2\u5668\u7684\u8f93\u51fa\u6570\u91cf\uff09\u3002 kernel_size : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005\u5355\u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e 1D \u5377\u79ef\u7a97\u53e3\u7684\u957f\u5ea6\u3002 strides : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005\u5355\u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e\u5377\u79ef\u7684\u6b65\u957f\u3002 \u6307\u5b9a\u4efb\u4f55 stride \u503c != 1 \u4e0e\u6307\u5b9a dilation_rate \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 padding : \u5f53\u524d\u4ec5\u652f\u6301 \"valid\" (\u5927\u5c0f\u5199\u654f\u611f)\u3002 \"same\" \u53ef\u80fd\u4f1a\u5728\u672a\u6765\u652f\u6301\u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u5982\u679c\u4f60\u4e0d\u6307\u5b9a\uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373\u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 bias_initializer : \u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u8f93\u51fa\uff08\u5b83\u7684\u6fc0\u6d3b\u503c\uff09\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 \u8f93\u5165\u5c3a\u5bf8 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (batch_size, steps, input_dim) \u3002 \u8f93\u51fa\u5c3a\u5bf8 3D \u5f20\u91cf \uff0c\u5c3a\u5bf8\u4e3a\uff1a (batch_size, new_steps, filters) \uff0c steps \u503c\u53ef\u80fd\u56e0\u586b\u5145\u6216\u6b65\u957f\u800c\u6539\u53d8\u3002 [source]","title":"LocallyConnected1D"},{"location":"2-Layers/4.local/#locallyconnected2d","text":"keras.layers.LocallyConnected2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) 2D \u8f93\u5165\u7684\u5c40\u90e8\u8fde\u63a5\u5c42\u3002 LocallyConnected2D \u5c42\u4e0e Conv2D \u5c42\u7684\u5de5\u4f5c\u65b9\u5f0f\u76f8\u540c\uff0c\u9664\u4e86\u6743\u503c\u4e0d\u5171\u4eab\u5916\uff0c \u4e5f\u5c31\u662f\u8bf4\uff0c\u5728\u8f93\u5165\u7684\u6bcf\u4e2a\u4e0d\u540c\u90e8\u5206\u5e94\u7528\u4e0d\u540c\u7684\u4e00\u7ec4\u8fc7\u6ee4\u5668\u3002 \u4f8b\u5b50 # \u5728 32x32 \u56fe\u50cf\u4e0a\u5e94\u7528 3x3 \u975e\u5171\u4eab\u6743\u503c\u548c64\u4e2a\u8f93\u51fa\u8fc7\u6ee4\u5668\u7684\u5377\u79ef # \u6570\u636e\u683c\u5f0f `data_format=\"channels_last\"`\uff1a model = Sequential() model.add(LocallyConnected2D(64, (3, 3), input_shape=(32, 32, 3))) # \u73b0\u5728 model.output_shape == (None, 30, 30, 64) # \u6ce8\u610f\u8fd9\u4e00\u5c42\u7684\u53c2\u6570\u6570\u91cf\u4e3a (30*30)*(3*3*3*64) + (30*30)*64 # \u5728\u4e0a\u9762\u518d\u52a0\u4e00\u4e2a 3x3 \u975e\u5171\u4eab\u6743\u503c\u548c 32 \u4e2a\u8f93\u51fa\u6ee4\u6ce2\u5668\u7684\u5377\u79ef\uff1a model.add(LocallyConnected2D(32, (3, 3))) # \u73b0\u5728 model.output_shape == (None, 28, 28, 32) \u53c2\u6570 filters : \u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6 \uff08\u5373\u5377\u79ef\u4e2d\u6ee4\u6ce2\u5668\u7684\u8f93\u51fa\u6570\u91cf\uff09\u3002 kernel_size : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e 2D \u5377\u79ef\u7a97\u53e3\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 strides : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 2 \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e\u5377\u79ef\u6cbf\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u65b9\u5411\u7684\u6b65\u957f\u3002 \u53ef\u4ee5\u662f\u4e00\u4e2a\u6574\u6570\uff0c\u4e3a\u6240\u6709\u7a7a\u95f4\u7ef4\u5ea6\u6307\u5b9a\u76f8\u540c\u7684\u503c\u3002 padding : \u5f53\u524d\u4ec5\u652f\u6301 \"valid\" (\u5927\u5c0f\u5199\u654f\u611f)\u3002 \"same\" \u53ef\u80fd\u4f1a\u5728\u672a\u6765\u652f\u6301\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4) \u6216 channels_first \u4e4b\u4e00\u3002 \u8f93\u5165\u4e2d\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, height, width, channels) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, channels, height, width) \u3002 \u5b83\u9ed8\u8ba4\u4e3a\u4ece Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d \u627e\u5230\u7684 image_data_format \u503c\u3002 \u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u5c06\u4f7f\u7528 \"channels_last\"\u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u5982\u679c\u4f60\u4e0d\u6307\u5b9a\uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373\u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 bias_initializer : \u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers )\u3002 kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u8f93\u51fa\uff08\u5b83\u7684\u6fc0\u6d3b\u503c\uff09\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 \u8f93\u5165\u5c3a\u5bf8 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (samples, channels, rows, cols) \uff0c\u5982\u679c data_format='channels_first'\uff1b \u6216\u8005 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (samples, rows, cols, channels) \uff0c\u5982\u679c data_format='channels_last'\u3002 \u8f93\u51fa\u5c3a\u5bf8 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (samples, filters, new_rows, new_cols) \uff0c\u5982\u679c data_format='channels_first'\uff1b \u6216\u8005 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (samples, new_rows, new_cols, filters) \uff0c\u5982\u679c data_format='channels_last'\u3002 rows \u548c cols \u7684\u503c\u53ef\u80fd\u56e0\u586b\u5145\u800c\u6539\u53d8\u3002","title":"LocallyConnected2D"},{"location":"2-Layers/5.recurrent/","text":"\u5faa\u73af\u5c42 [source] RNN keras.layers.RNN(cell, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False) \u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u5c42\u57fa\u7c7b\u3002 \u53c2\u6570 cell : \u4e00\u4e2a RNN \u5355\u5143\u5b9e\u4f8b\u3002RNN \u5355\u5143\u662f\u4e00\u4e2a\u5177\u6709\u4ee5\u4e0b\u51e0\u9879\u7684\u7c7b\uff1a \u4e00\u4e2a call(input_at_t, states_at_t) \u65b9\u6cd5\uff0c \u5b83\u8fd4\u56de (output_at_t, states_at_t_plus_1) \u3002 \u5355\u5143\u7684\u8c03\u7528\u65b9\u6cd5\u4e5f\u53ef\u4ee5\u91c7\u5f15\u5165\u53ef\u9009\u53c2\u6570 constants \uff0c \u8be6\u89c1\u4e0b\u9762\u7684\u5c0f\u8282\u300c\u5173\u4e8e\u7ed9 RNN \u4f20\u9012\u5916\u90e8\u5e38\u91cf\u7684\u8bf4\u660e\u300d\u3002 \u4e00\u4e2a state_size \u5c5e\u6027\u3002\u8fd9\u53ef\u4ee5\u662f\u5355\u4e2a\u6574\u6570\uff08\u5355\u4e2a\u72b6\u6001\uff09\uff0c \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b83\u662f\u5faa\u73af\u5c42\u72b6\u6001\u7684\u5927\u5c0f\uff08\u5e94\u8be5\u4e0e\u5355\u5143\u8f93\u51fa\u7684\u5927\u5c0f\u76f8\u540c\uff09\u3002 \u8fd9\u4e5f\u53ef\u4ee5\u662f\u6574\u6570\u8868\u793a\u7684\u5217\u8868/\u5143\u7ec4\uff08\u6bcf\u4e2a\u72b6\u6001\u4e00\u4e2a\u5927\u5c0f\uff09\u3002 \u4e00\u4e2a output_size \u5c5e\u6027\u3002 \u8fd9\u53ef\u4ee5\u662f\u5355\u4e2a\u6574\u6570\u6216\u8005\u662f\u4e00\u4e2a TensorShape\uff0c \u5b83\u8868\u793a\u8f93\u51fa\u7684\u5c3a\u5bf8\u3002\u51fa\u4e8e\u5411\u540e\u517c\u5bb9\u7684\u539f\u56e0\uff0c\u5982\u679c\u6b64\u5c5e\u6027\u5bf9\u4e8e\u5f53\u524d\u5355\u5143\u4e0d\u53ef\u7528\uff0c \u5219\u8be5\u503c\u5c06\u7531 state_size \u7684\u7b2c\u4e00\u4e2a\u5143\u7d20\u63a8\u65ad\u3002 cell \u4e5f\u53ef\u80fd\u662f RNN \u5355\u5143\u5b9e\u4f8b\u7684\u5217\u8868\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0cRNN \u7684\u5355\u5143\u5c06\u5806\u53e0\u5728\u53e6\u4e00\u4e2a\u5355\u5143\u4e0a\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u5806\u53e0 RNN\u3002 return_sequences : \u5e03\u5c14\u503c\u3002\u662f\u8fd4\u56de\u8f93\u51fa\u5e8f\u5217\u4e2d\u7684\u6700\u540e\u4e00\u4e2a\u8f93\u51fa\uff0c\u8fd8\u662f\u5168\u90e8\u5e8f\u5217\u3002 return_state : \u5e03\u5c14\u503c\u3002\u9664\u4e86\u8f93\u51fa\u4e4b\u5916\u662f\u5426\u8fd4\u56de\u6700\u540e\u4e00\u4e2a\u72b6\u6001\u3002 go_backwards : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u5411\u540e\u5904\u7406\u8f93\u5165\u5e8f\u5217\u5e76\u8fd4\u56de\u76f8\u53cd\u7684\u5e8f\u5217\u3002 stateful : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u6279\u6b21\u4e2d\u7d22\u5f15 i \u5904\u7684\u6bcf\u4e2a\u6837\u54c1\u7684\u6700\u540e\u72b6\u6001\u5c06\u7528\u4f5c\u4e0b\u4e00\u6279\u6b21\u4e2d\u7d22\u5f15 i \u6837\u54c1\u7684\u521d\u59cb\u72b6\u6001\u3002 unroll : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u7f51\u7edc\u5c06\u5c55\u5f00\uff0c\u5426\u5219\u5c06\u4f7f\u7528\u7b26\u53f7\u5faa\u73af\u3002 \u5c55\u5f00\u53ef\u4ee5\u52a0\u901f RNN\uff0c\u4f46\u5b83\u5f80\u5f80\u4f1a\u5360\u7528\u66f4\u591a\u7684\u5185\u5b58\u3002 \u5c55\u5f00\u53ea\u9002\u7528\u4e8e\u77ed\u5e8f\u5217\u3002 input_dim : \u8f93\u5165\u7684\u7ef4\u5ea6\uff08\u6574\u6570\uff09\u3002 \u5c06\u6b64\u5c42\u7528\u4f5c\u6a21\u578b\u4e2d\u7684\u7b2c\u4e00\u5c42\u65f6\uff0c\u6b64\u53c2\u6570\uff08\u6216\u8005\uff0c\u5173\u952e\u5b57\u53c2\u6570 input_shape \uff09\u662f\u5fc5\u9700\u7684\u3002 input_length : \u8f93\u5165\u5e8f\u5217\u7684\u957f\u5ea6\uff0c\u5728\u6052\u5b9a\u65f6\u6307\u5b9a\u3002 \u5982\u679c\u4f60\u8981\u5728\u4e0a\u6e38\u8fde\u63a5 Flatten \u548c Dense \u5c42\uff0c \u5219\u9700\u8981\u6b64\u53c2\u6570\uff08\u5982\u679c\u6ca1\u6709\u5b83\uff0c\u65e0\u6cd5\u8ba1\u7b97\u5168\u8fde\u63a5\u8f93\u51fa\u7684\u5c3a\u5bf8\uff09\u3002 \u8bf7\u6ce8\u610f\uff0c\u5982\u679c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u5c42\u4e0d\u662f\u6a21\u578b\u4e2d\u7684\u7b2c\u4e00\u5c42\uff0c \u5219\u9700\u8981\u5728\u7b2c\u4e00\u5c42\u7684\u5c42\u7ea7\u6307\u5b9a\u8f93\u5165\u957f\u5ea6\uff08\u4f8b\u5982\uff0c\u901a\u8fc7 input_shape \u53c2\u6570\uff09\u3002 \u8f93\u5165\u5c3a\u5bf8 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch_size, timesteps, input_dim) \u3002 \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c return_state \uff1a\u8fd4\u56de\u5f20\u91cf\u5217\u8868\u3002 \u7b2c\u4e00\u4e2a\u5f20\u91cf\u4e3a\u8f93\u51fa\u3002\u5269\u4f59\u7684\u5f20\u91cf\u4e3a\u6700\u540e\u7684\u72b6\u6001\uff0c \u6bcf\u4e2a\u5f20\u91cf\u7684\u5c3a\u5bf8\u4e3a (batch_size, units) \u3002 \u5982\u679c return_sequences \uff1a\u8fd4\u56de 3D \u5f20\u91cf\uff0c \u5c3a\u5bf8\u4e3a (batch_size, timesteps, units) \u3002 \u5426\u5219\uff0c\u8fd4\u56de\u5c3a\u5bf8\u4e3a (batch_size, units) \u7684 2D \u5f20\u91cf\u3002 Masking \u8be5\u5c42\u652f\u6301\u4ee5\u53ef\u53d8\u6570\u91cf\u7684\u65f6\u95f4\u6b65\u5bf9\u8f93\u5165\u6570\u636e\u8fdb\u884c masking\u3002 \u8981\u5c06 masking \u5f15\u5165\u4f60\u7684\u6570\u636e\uff0c\u8bf7\u4f7f\u7528 Embedding \u5c42\uff0c\u5e76\u5c06 mask_zero \u53c2\u6570\u8bbe\u7f6e\u4e3a True \u3002 \u5173\u4e8e\u5728 RNN \u4e2d\u4f7f\u7528\u300c\u72b6\u6001\uff08statefulness\uff09\u300d\u7684\u8bf4\u660e \u4f60\u53ef\u4ee5\u5c06 RNN \u5c42\u8bbe\u7f6e\u4e3a stateful \uff08\u6709\u72b6\u6001\u7684\uff09\uff0c \u8fd9\u610f\u5473\u7740\u9488\u5bf9\u4e00\u4e2a\u6279\u6b21\u7684\u6837\u672c\u8ba1\u7b97\u7684\u72b6\u6001\u5c06\u88ab\u91cd\u65b0\u7528\u4f5c\u4e0b\u4e00\u6279\u6837\u672c\u7684\u521d\u59cb\u72b6\u6001\u3002 \u8fd9\u5047\u5b9a\u5728\u4e0d\u540c\u8fde\u7eed\u6279\u6b21\u7684\u6837\u54c1\u4e4b\u95f4\u6709\u4e00\u5bf9\u4e00\u7684\u6620\u5c04\u3002 \u4e3a\u4e86\u4f7f\u72b6\u6001\u6709\u6548\uff1a \u5728\u5c42\u6784\u9020\u5668\u4e2d\u6307\u5b9a stateful=True \u3002 \u4e3a\u4f60\u7684\u6a21\u578b\u6307\u5b9a\u4e00\u4e2a\u56fa\u5b9a\u7684\u6279\u6b21\u5927\u5c0f\uff0c \u5982\u679c\u662f\u987a\u5e8f\u6a21\u578b\uff0c\u4e3a\u4f60\u7684\u6a21\u578b\u7684\u7b2c\u4e00\u5c42\u4f20\u9012\u4e00\u4e2a batch_input_shape=(...) \u53c2\u6570\u3002 \u4e3a\u4f60\u7684\u6a21\u578b\u6307\u5b9a\u4e00\u4e2a\u56fa\u5b9a\u7684\u6279\u6b21\u5927\u5c0f\uff0c \u5982\u679c\u662f\u987a\u5e8f\u6a21\u578b\uff0c\u4e3a\u4f60\u7684\u6a21\u578b\u7684\u7b2c\u4e00\u5c42\u4f20\u9012\u4e00\u4e2a batch_input_shape=(...) \u3002 \u5982\u679c\u662f\u5e26\u6709 1 \u4e2a\u6216\u591a\u4e2a Input \u5c42\u7684\u51fd\u6570\u5f0f\u6a21\u578b\uff0c\u4e3a\u4f60\u7684\u6a21\u578b\u7684\u6240\u6709\u7b2c\u4e00\u5c42\u4f20\u9012\u4e00\u4e2a batch_shape=(...) \u3002 \u8fd9\u662f\u4f60\u7684\u8f93\u5165\u7684\u9884\u671f\u5c3a\u5bf8\uff0c \u5305\u62ec\u6279\u91cf\u7ef4\u5ea6 \u3002 \u5b83\u5e94\u8be5\u662f\u6574\u6570\u7684\u5143\u7ec4\uff0c\u4f8b\u5982 (32, 10, 100) \u3002 \u5728\u8c03\u7528 fit() \u662f\u6307\u5b9a shuffle=False \u3002 \u8981\u91cd\u7f6e\u6a21\u578b\u7684\u72b6\u6001\uff0c\u8bf7\u5728\u7279\u5b9a\u56fe\u5c42\u6216\u6574\u4e2a\u6a21\u578b\u4e0a\u8c03\u7528 .reset_states() \u3002 \u5173\u4e8e\u6307\u5b9a RNN \u521d\u59cb\u72b6\u6001\u7684\u8bf4\u660e \u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u5173\u952e\u5b57\u53c2\u6570 initial_state \u8c03\u7528\u5b83\u4eec\u6765\u7b26\u53f7\u5316\u5730\u6307\u5b9a RNN \u5c42\u7684\u521d\u59cb\u72b6\u6001\u3002 initial_state \u7684\u503c\u5e94\u8be5\u662f\u8868\u793a RNN \u5c42\u521d\u59cb\u72b6\u6001\u7684\u5f20\u91cf\u6216\u5f20\u91cf\u5217\u8868\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u8c03\u7528\u5e26\u6709\u5173\u952e\u5b57\u53c2\u6570 states \u7684 reset_states \u65b9\u6cd5\u6765\u6570\u5b57\u5316\u5730\u6307\u5b9a RNN \u5c42\u7684\u521d\u59cb\u72b6\u6001\u3002 states \u7684\u503c\u5e94\u8be5\u662f\u4e00\u4e2a\u4ee3\u8868 RNN \u5c42\u521d\u59cb\u72b6\u6001\u7684 Numpy \u6570\u7ec4\u6216\u8005 Numpy \u6570\u7ec4\u5217\u8868\u3002 \u5173\u4e8e\u7ed9 RNN \u4f20\u9012\u5916\u90e8\u5e38\u91cf\u7684\u8bf4\u660e \u4f60\u53ef\u4ee5\u4f7f\u7528 RNN.__call__ \uff08\u4ee5\u53ca RNN.call \uff09\u7684 constants \u5173\u952e\u5b57\u53c2\u6570\u5c06\u300c\u5916\u90e8\u300d\u5e38\u91cf\u4f20\u9012\u7ed9\u5355\u5143\u3002 \u8fd9\u8981\u6c42 cell.call \u65b9\u6cd5\u63a5\u53d7\u76f8\u540c\u7684\u5173\u952e\u5b57\u53c2\u6570 constants \u3002 \u8fd9\u4e9b\u5e38\u6570\u53ef\u7528\u4e8e\u8c03\u8282\u9644\u52a0\u9759\u6001\u8f93\u5165\uff08\u4e0d\u968f\u65f6\u95f4\u53d8\u5316\uff09\u4e0a\u7684\u5355\u5143\u8f6c\u6362\uff0c\u4e5f\u53ef\u7528\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u3002 \u4f8b\u5b50 # \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b9a\u4e49\u4e00\u4e2a RNN \u5355\u5143\uff0c\u4f5c\u4e3a\u7f51\u7edc\u5c42\u5b50\u7c7b\u3002 class MinimalRNNCell(keras.layers.Layer): def __init__(self, units, **kwargs): self.units = units self.state_size = units super(MinimalRNNCell, self).__init__(**kwargs) def build(self, input_shape): self.kernel = self.add_weight(shape=(input_shape[-1], self.units), initializer='uniform', name='kernel') self.recurrent_kernel = self.add_weight( shape=(self.units, self.units), initializer='uniform', name='recurrent_kernel') self.built = True def call(self, inputs, states): prev_output = states[0] h = K.dot(inputs, self.kernel) output = h + K.dot(prev_output, self.recurrent_kernel) return output, [output] # \u8ba9\u6211\u4eec\u5728 RNN \u5c42\u4f7f\u7528\u8fd9\u4e2a\u5355\u5143\uff1a cell = MinimalRNNCell(32) x = keras.Input((None, 5)) layer = RNN(cell) y = layer(x) # \u4ee5\u4e0b\u662f\u5982\u4f55\u4f7f\u7528\u5355\u5143\u683c\u6784\u5efa\u5806\u53e0\u7684 RNN\u7684\u65b9\u6cd5\uff1a cells = [MinimalRNNCell(32), MinimalRNNCell(64)] x = keras.Input((None, 5)) layer = RNN(cells) y = layer(x) [source] SimpleRNN keras.layers.SimpleRNN(units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False) \u5168\u8fde\u63a5\u7684 RNN\uff0c\u5176\u8f93\u51fa\u5c06\u88ab\u53cd\u9988\u5230\u8f93\u5165\u3002 \u53c2\u6570 units : \u6b63\u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6\u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u9ed8\u8ba4\uff1a\u53cc\u66f2\u6b63\u5207\uff08 tanh \uff09\u3002 \u5982\u679c\u4f20\u5165 None \uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373 \u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668\uff0c \u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 recurrent_initializer : recurrent_kernel \u6743\u503c\u77e9\u9635 \u7684\u521d\u59cb\u5316\u5668\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 bias_initializer :\u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers ). kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 recurrent_regularizer : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u8f93\u51fa\uff08\u5b83\u7684\u6fc0\u6d3b\u503c\uff09\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 recurrent_constraint : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 dropout : \u5728 0 \u548c 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 \u5355\u5143\u7684\u4e22\u5f03\u6bd4\u4f8b\uff0c\u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362\u3002 recurrent_dropout : \u5728 0 \u548c 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 \u5355\u5143\u7684\u4e22\u5f03\u6bd4\u4f8b\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362\u3002 return_sequences : \u5e03\u5c14\u503c\u3002\u662f\u8fd4\u56de\u8f93\u51fa\u5e8f\u5217\u4e2d\u7684\u6700\u540e\u4e00\u4e2a\u8f93\u51fa\uff0c\u8fd8\u662f\u5168\u90e8\u5e8f\u5217\u3002 return_state : \u5e03\u5c14\u503c\u3002\u9664\u4e86\u8f93\u51fa\u4e4b\u5916\u662f\u5426\u8fd4\u56de\u6700\u540e\u4e00\u4e2a\u72b6\u6001\u3002 go_backwards : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u5411\u540e\u5904\u7406\u8f93\u5165\u5e8f\u5217\u5e76\u8fd4\u56de\u76f8\u53cd\u7684\u5e8f\u5217\u3002 stateful : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u6279\u6b21\u4e2d\u7d22\u5f15 i \u5904\u7684\u6bcf\u4e2a\u6837\u54c1 \u7684\u6700\u540e\u72b6\u6001\u5c06\u7528\u4f5c\u4e0b\u4e00\u6279\u6b21\u4e2d\u7d22\u5f15 i \u6837\u54c1\u7684\u521d\u59cb\u72b6\u6001\u3002 unroll : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u7f51\u7edc\u5c06\u5c55\u5f00\uff0c\u5426\u5219\u5c06\u4f7f\u7528\u7b26\u53f7\u5faa\u73af\u3002 \u5c55\u5f00\u53ef\u4ee5\u52a0\u901f RNN\uff0c\u4f46\u5b83\u5f80\u5f80\u4f1a\u5360\u7528\u66f4\u591a\u7684\u5185\u5b58\u3002 \u5c55\u5f00\u53ea\u9002\u7528\u4e8e\u77ed\u5e8f\u5217\u3002 [source] GRU keras.layers.GRU(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, reset_after=False) \u95e8\u9650\u5faa\u73af\u5355\u5143\u7f51\u7edc\uff08Gated Recurrent Unit\uff09 - Cho et al. 2014. \u6709\u4e24\u79cd\u53d8\u4f53\u3002\u9ed8\u8ba4\u7684\u662f\u57fa\u4e8e 1406.1078v3 \u7684\u5b9e\u73b0\uff0c\u540c\u65f6\u5728\u77e9\u9635\u4e58\u6cd5\u4e4b\u524d\u5c06\u590d\u4f4d\u95e8\u5e94\u7528\u4e8e\u9690\u85cf\u72b6\u6001\u3002 \u53e6\u4e00\u79cd\u5219\u662f\u57fa\u4e8e 1406.1078v1 \u7684\u5b9e\u73b0\uff0c\u5b83\u5305\u62ec\u987a\u5e8f\u5012\u7f6e\u7684\u64cd\u4f5c\u3002 \u7b2c\u4e8c\u79cd\u53d8\u4f53\u4e0e CuDNNGRU(GPU-only) \u517c\u5bb9\u5e76\u4e14\u5141\u8bb8\u5728 CPU \u4e0a\u8fdb\u884c\u63a8\u7406\u3002 \u56e0\u6b64\u5b83\u5bf9\u4e8e kernel \u548c recurrent_kernel \u6709\u53ef\u5206\u79bb\u504f\u7f6e\u3002 \u4f7f\u7528 'reset_after'=True \u548c recurrent_activation='sigmoid' \u3002 \u53c2\u6570 units : \u6b63\u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6\u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u9ed8\u8ba4\uff1a\u53cc\u66f2\u6b63\u5207 ( tanh )\u3002 \u5982\u679c\u4f20\u5165 None \uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373 \u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 recurrent_activation : \u7528\u4e8e\u5faa\u73af\u65f6\u95f4\u6b65\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u9ed8\u8ba4\uff1a\u5206\u6bb5\u7ebf\u6027\u8fd1\u4f3c sigmoid ( hard_sigmoid )\u3002 \u5982\u679c\u4f20\u5165 None\uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373 \u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668\uff0c \u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 recurrent_initializer : recurrent_kernel \u6743\u503c\u77e9\u9635 \u7684\u521d\u59cb\u5316\u5668\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 bias_initializer :\u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers ). kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 recurrent_regularizer : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u8f93\u51fa\uff08\u5b83\u7684\u6fc0\u6d3b\u503c\uff09\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 recurrent_constraint : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 dropout : \u5728 0 \u548c 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 \u5355\u5143\u7684\u4e22\u5f03\u6bd4\u4f8b\uff0c\u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362\u3002 recurrent_dropout : \u5728 0 \u548c 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 \u5355\u5143\u7684\u4e22\u5f03\u6bd4\u4f8b\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362\u3002 implementation : \u5b9e\u73b0\u6a21\u5f0f\uff0c1 \u6216 2\u3002 \u6a21\u5f0f 1 \u5c06\u628a\u5b83\u7684\u64cd\u4f5c\u7ed3\u6784\u5316\u4e3a\u66f4\u591a\u7684\u5c0f\u7684\u70b9\u79ef\u548c\u52a0\u6cd5\u64cd\u4f5c\uff0c \u800c\u6a21\u5f0f 2 \u5c06\u628a\u5b83\u4eec\u5206\u6279\u5230\u66f4\u5c11\uff0c\u66f4\u5927\u7684\u64cd\u4f5c\u4e2d\u3002 \u8fd9\u4e9b\u6a21\u5f0f\u5728\u4e0d\u540c\u7684\u786c\u4ef6\u548c\u4e0d\u540c\u7684\u5e94\u7528\u4e2d\u5177\u6709\u4e0d\u540c\u7684\u6027\u80fd\u914d\u7f6e\u6587\u4ef6\u3002 return_sequences : \u5e03\u5c14\u503c\u3002\u662f\u8fd4\u56de\u8f93\u51fa\u5e8f\u5217\u4e2d\u7684\u6700\u540e\u4e00\u4e2a\u8f93\u51fa\uff0c\u8fd8\u662f\u5168\u90e8\u5e8f\u5217\u3002 return_state : \u5e03\u5c14\u503c\u3002\u9664\u4e86\u8f93\u51fa\u4e4b\u5916\u662f\u5426\u8fd4\u56de\u6700\u540e\u4e00\u4e2a\u72b6\u6001\u3002 go_backwards : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u5411\u540e\u5904\u7406\u8f93\u5165\u5e8f\u5217\u5e76\u8fd4\u56de\u76f8\u53cd\u7684\u5e8f\u5217\u3002 stateful : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u6279\u6b21\u4e2d\u7d22\u5f15 i \u5904\u7684\u6bcf\u4e2a\u6837\u54c1\u7684\u6700\u540e\u72b6\u6001 \u5c06\u7528\u4f5c\u4e0b\u4e00\u6279\u6b21\u4e2d\u7d22\u5f15 i \u6837\u54c1\u7684\u521d\u59cb\u72b6\u6001\u3002 unroll : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u7f51\u7edc\u5c06\u5c55\u5f00\uff0c\u5426\u5219\u5c06\u4f7f\u7528\u7b26\u53f7\u5faa\u73af\u3002 \u5c55\u5f00\u53ef\u4ee5\u52a0\u901f RNN\uff0c\u4f46\u5b83\u5f80\u5f80\u4f1a\u5360\u7528\u66f4\u591a\u7684\u5185\u5b58\u3002 \u5c55\u5f00\u53ea\u9002\u7528\u4e8e\u77ed\u5e8f\u5217\u3002 reset_after : GRU \u516c\u7ea6 (\u662f\u5426\u5728\u77e9\u9635\u4e58\u6cd5\u4e4b\u524d\u6216\u8005\u4e4b\u540e\u4f7f\u7528\u91cd\u7f6e\u95e8)\u3002 False =\u300c\u4e4b\u524d\u300d(\u9ed8\u8ba4)\uff0cTure =\u300c\u4e4b\u540e\u300d( CuDNN \u517c\u5bb9)\u3002 \u53c2\u8003\u6587\u732e Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation On the Properties of Neural Machine Translation: Encoder-Decoder Approaches Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling A Theoretically Grounded Application of Dropout in Recurrent Neural Networks [source] LSTM keras.layers.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False) \u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\u5c42\uff08Long Short-Term Memory\uff09 - Hochreiter 1997. \u53c2\u6570 units : \u6b63\u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6\u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u5982\u679c\u4f20\u5165 None \uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373 \u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 recurrent_activation : \u7528\u4e8e\u5faa\u73af\u65f6\u95f4\u6b65\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u9ed8\u8ba4\uff1a\u5206\u6bb5\u7ebf\u6027\u8fd1\u4f3c sigmoid ( hard_sigmoid )\u3002 \u5982\u679c\u4f20\u5165 None \uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373 \u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668\uff0c \u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 recurrent_initializer : recurrent_kernel \u6743\u503c\u77e9\u9635 \u7684\u521d\u59cb\u5316\u5668\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 bias_initializer :\u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers ). unit_forget_bias : \u5e03\u5c14\u503c\u3002 \u5982\u679c\u4e3a True\uff0c\u521d\u59cb\u5316\u65f6\uff0c\u5c06\u5fd8\u8bb0\u95e8\u7684\u504f\u7f6e\u52a0 1\u3002 \u5c06\u5176\u8bbe\u7f6e\u4e3a True \u540c\u65f6\u8fd8\u4f1a\u5f3a\u5236 bias_initializer=\"zeros\" \u3002 \u8fd9\u4e2a\u5efa\u8bae\u6765\u81ea Jozefowicz et al. \u3002 kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 recurrent_regularizer : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u8f93\u51fa\uff08\u5b83\u7684\u6fc0\u6d3b\u503c\uff09\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 recurrent_constraint : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 dropout : \u5728 0 \u548c 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 \u5355\u5143\u7684\u4e22\u5f03\u6bd4\u4f8b\uff0c\u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362\u3002 recurrent_dropout : \u5728 0 \u548c 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 \u5355\u5143\u7684\u4e22\u5f03\u6bd4\u4f8b\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362\u3002 implementation : \u5b9e\u73b0\u6a21\u5f0f\uff0c1 \u6216 2\u3002 \u6a21\u5f0f 1 \u5c06\u628a\u5b83\u7684\u64cd\u4f5c\u7ed3\u6784\u5316\u4e3a\u66f4\u591a\u7684\u5c0f\u7684\u70b9\u79ef\u548c\u52a0\u6cd5\u64cd\u4f5c\uff0c \u800c\u6a21\u5f0f 2 \u5c06\u628a\u5b83\u4eec\u5206\u6279\u5230\u66f4\u5c11\uff0c\u66f4\u5927\u7684\u64cd\u4f5c\u4e2d\u3002 \u8fd9\u4e9b\u6a21\u5f0f\u5728\u4e0d\u540c\u7684\u786c\u4ef6\u548c\u4e0d\u540c\u7684\u5e94\u7528\u4e2d\u5177\u6709\u4e0d\u540c\u7684\u6027\u80fd\u914d\u7f6e\u6587\u4ef6\u3002 return_sequences : \u5e03\u5c14\u503c\u3002\u662f\u8fd4\u56de\u8f93\u51fa\u5e8f\u5217\u4e2d\u7684\u6700\u540e\u4e00\u4e2a\u8f93\u51fa\uff0c\u8fd8\u662f\u5168\u90e8\u5e8f\u5217\u3002 return_state : \u5e03\u5c14\u503c\u3002\u9664\u4e86\u8f93\u51fa\u4e4b\u5916\u662f\u5426\u8fd4\u56de\u6700\u540e\u4e00\u4e2a\u72b6\u6001\u3002 go_backwards : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u5411\u540e\u5904\u7406\u8f93\u5165\u5e8f\u5217\u5e76\u8fd4\u56de\u76f8\u53cd\u7684\u5e8f\u5217\u3002 stateful : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u6279\u6b21\u4e2d\u7d22\u5f15 i \u5904\u7684\u6bcf\u4e2a\u6837\u54c1\u7684\u6700\u540e\u72b6\u6001 \u5c06\u7528\u4f5c\u4e0b\u4e00\u6279\u6b21\u4e2d\u7d22\u5f15 i \u6837\u54c1\u7684\u521d\u59cb\u72b6\u6001\u3002 unroll : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u7f51\u7edc\u5c06\u5c55\u5f00\uff0c\u5426\u5219\u5c06\u4f7f\u7528\u7b26\u53f7\u5faa\u73af\u3002 \u5c55\u5f00\u53ef\u4ee5\u52a0\u901f RNN\uff0c\u4f46\u5b83\u5f80\u5f80\u4f1a\u5360\u7528\u66f4\u591a\u7684\u5185\u5b58\u3002 \u5c55\u5f00\u53ea\u9002\u7528\u4e8e\u77ed\u5e8f\u5217\u3002 \u53c2\u8003\u6587\u732e Long short-term memory (original 1997 paper) Learning to forget: Continual prediction with LSTM Supervised sequence labeling with recurrent neural networks A Theoretically Grounded Application of Dropout in Recurrent Neural Networks [source] ConvLSTM2D keras.layers.ConvLSTM2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, return_sequences=False, go_backwards=False, stateful=False, dropout=0.0, recurrent_dropout=0.0) \u5377\u79ef LSTM\u3002 \u5b83\u7c7b\u4f3c\u4e8e LSTM \u5c42\uff0c\u4f46\u8f93\u5165\u53d8\u6362\u548c\u5faa\u73af\u53d8\u6362\u90fd\u662f\u5377\u79ef\u7684\u3002 \u53c2\u6570 filters : \u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6 \uff08\u5373\u5377\u79ef\u4e2d\u6ee4\u6ce2\u5668\u7684\u8f93\u51fa\u6570\u91cf\uff09\u3002 kernel_size : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 n \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e\u5377\u79ef\u7a97\u53e3\u7684\u7ef4\u5ea6\u3002 strides : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 n \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e\u5377\u79ef\u7684\u6b65\u957f\u3002 \u6307\u5b9a\u4efb\u4f55 stride \u503c != 1 \u4e0e\u6307\u5b9a dilation_rate \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 padding : \"valid\" \u6216 \"same\" \u4e4b\u4e00 (\u5927\u5c0f\u5199\u654f\u611f)\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4) \u6216 channels_first \u4e4b\u4e00\u3002 \u8f93\u5165\u4e2d\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, time, ..., channels) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, time, channels, ...) \u3002 \u5b83\u9ed8\u8ba4\u4e3a\u4ece Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d \u627e\u5230\u7684 image_data_format \u503c\u3002 \u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u5c06\u4f7f\u7528 \"channels_last\" \u3002 dilation_rate : \u4e00\u4e2a\u6574\u6570\uff0c\u6216 n \u4e2a\u6574\u6570\u7684\u5143\u7ec4/\u5217\u8868\uff0c\u6307\u5b9a\u7528\u4e8e\u81a8\u80c0\u5377\u79ef\u7684\u81a8\u80c0\u7387\u3002 \u76ee\u524d\uff0c\u6307\u5b9a\u4efb\u4f55 dilation_rate \u503c != 1 \u4e0e\u6307\u5b9a stride \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u5982\u679c\u4f20\u5165 None\uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373 \u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 recurrent_activation : \u7528\u4e8e\u5faa\u73af\u65f6\u95f4\u6b65\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668\uff0c \u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 recurrent_initializer : recurrent_kernel \u6743\u503c\u77e9\u9635 \u7684\u521d\u59cb\u5316\u5668\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 bias_initializer :\u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers ). unit_forget_bias : \u5e03\u5c14\u503c\u3002 \u5982\u679c\u4e3a True\uff0c\u521d\u59cb\u5316\u65f6\uff0c\u5c06\u5fd8\u8bb0\u95e8\u7684\u504f\u7f6e\u52a0 1\u3002 \u5c06\u5176\u8bbe\u7f6e\u4e3a True \u540c\u65f6\u8fd8\u4f1a\u5f3a\u5236 bias_initializer=\"zeros\" \u3002 \u8fd9\u4e2a\u5efa\u8bae\u6765\u81ea Jozefowicz et al. \u3002 kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 recurrent_regularizer : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u8f93\u51fa\uff08\u5b83\u7684\u6fc0\u6d3b\u503c\uff09\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 recurrent_constraint : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 return_sequences : \u5e03\u5c14\u503c\u3002\u662f\u8fd4\u56de\u8f93\u51fa\u5e8f\u5217\u4e2d\u7684\u6700\u540e\u4e00\u4e2a\u8f93\u51fa\uff0c\u8fd8\u662f\u5168\u90e8\u5e8f\u5217\u3002 go_backwards : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u5411\u540e\u5904\u7406\u8f93\u5165\u5e8f\u5217\u5e76\u8fd4\u56de\u76f8\u53cd\u7684\u5e8f\u5217\u3002 stateful : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u6279\u6b21\u4e2d\u7d22\u5f15 i \u5904\u7684\u6bcf\u4e2a\u6837\u54c1\u7684\u6700\u540e\u72b6\u6001 \u5c06\u7528\u4f5c\u4e0b\u4e00\u6279\u6b21\u4e2d\u7d22\u5f15 i \u6837\u54c1\u7684\u521d\u59cb\u72b6\u6001\u3002 dropout : \u5728 0 \u548c 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 \u5355\u5143\u7684\u4e22\u5f03\u6bd4\u4f8b\uff0c\u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362\u3002 recurrent_dropout : \u5728 0 \u548c 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 \u5355\u5143\u7684\u4e22\u5f03\u6bd4\u4f8b\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_first'\uff0c \u8f93\u5165 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (samples,time, channels, rows, cols) \u3002 \u5982\u679c data_format='channels_last'\uff0c \u8f93\u5165 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (samples,time, rows, cols, channels) \u3002 \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c return_sequences \uff0c \u5982\u679c data_format='channels_first'\uff0c\u8fd4\u56de 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (samples, time, filters, output_row, output_col) \u3002 \u5982\u679c data_format='channels_last'\uff0c\u8fd4\u56de 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (samples, time, output_row, output_col, filters) \u3002 \u5426\u5219\uff0c \u5982\u679c data_format ='channels_first'\uff0c\u8fd4\u56de 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (samples, filters, output_row, output_col) \u3002 \u5982\u679c data_format='channels_last'\uff0c\u8fd4\u56de 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (samples, output_row, output_col, filters) \u3002 o_row \u548c o_col \u53d6\u51b3\u4e8e filter \u548c padding \u7684\u5c3a\u5bf8\u3002 \u5f02\u5e38 ValueError : \u65e0\u6548\u7684\u6784\u9020\u53c2\u6570\u3002 \u53c2\u8003\u6587\u732e Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting \u3002 \u5f53\u524d\u7684\u5b9e\u73b0\u4e0d\u5305\u62ec\u5355\u5143\u8f93\u51fa\u7684\u53cd\u9988\u56de\u8def\u3002 [source] SimpleRNNCell keras.layers.SimpleRNNCell(units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0) SimpleRNN \u7684\u5355\u5143\u7c7b\u3002 \u53c2\u6570 units : \u6b63\u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6\u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u9ed8\u8ba4\uff1a\u53cc\u66f2\u6b63\u5207 ( tanh )\u3002 \u5982\u679c\u4f20\u5165 None \uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373 \u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668\uff0c \u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 recurrent_initializer : recurrent_kernel \u6743\u503c\u77e9\u9635 \u7684\u521d\u59cb\u5316\u5668\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 bias_initializer :\u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers ). kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 recurrent_regularizer : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 recurrent_constraint : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 dropout : \u5728 0 \u548c 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 \u5355\u5143\u7684\u4e22\u5f03\u6bd4\u4f8b\uff0c\u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362\u3002 recurrent_dropout : \u5728 0 \u548c 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 \u5355\u5143\u7684\u4e22\u5f03\u6bd4\u4f8b\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362\u3002 [source] GRUCell keras.layers.GRUCell(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, reset_after=False) GRU \u5c42\u7684\u5355\u5143\u7c7b\u3002 \u53c2\u6570 units : \u6b63\u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6\u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u9ed8\u8ba4\uff1a\u53cc\u66f2\u6b63\u5207 ( tanh )\u3002 \u5982\u679c\u4f20\u5165 None \uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373 \u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 recurrent_activation : \u7528\u4e8e\u5faa\u73af\u65f6\u95f4\u6b65\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u9ed8\u8ba4\uff1a\u5206\u6bb5\u7ebf\u6027\u8fd1\u4f3c sigmoid ( hard_sigmoid )\u3002 \u5982\u679c\u4f20\u5165 None \uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373 \u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668\uff0c \u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 recurrent_initializer : recurrent_kernel \u6743\u503c\u77e9\u9635 \u7684\u521d\u59cb\u5316\u5668\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 bias_initializer :\u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers ). kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 recurrent_regularizer : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 recurrent_constraint : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 dropout : \u5728 0 \u548c 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 \u5355\u5143\u7684\u4e22\u5f03\u6bd4\u4f8b\uff0c\u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362\u3002 recurrent_dropout : \u5728 0 \u548c 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 \u5355\u5143\u7684\u4e22\u5f03\u6bd4\u4f8b\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362\u3002 implementation : \u5b9e\u73b0\u6a21\u5f0f\uff0c1 \u6216 2\u3002 \u6a21\u5f0f 1 \u5c06\u628a\u5b83\u7684\u64cd\u4f5c\u7ed3\u6784\u5316\u4e3a\u66f4\u591a\u7684\u5c0f\u7684\u70b9\u79ef\u548c\u52a0\u6cd5\u64cd\u4f5c\uff0c \u800c\u6a21\u5f0f 2 \u5c06\u628a\u5b83\u4eec\u5206\u6279\u5230\u66f4\u5c11\uff0c\u66f4\u5927\u7684\u64cd\u4f5c\u4e2d\u3002 \u8fd9\u4e9b\u6a21\u5f0f\u5728\u4e0d\u540c\u7684\u786c\u4ef6\u548c\u4e0d\u540c\u7684\u5e94\u7528\u4e2d\u5177\u6709\u4e0d\u540c\u7684\u6027\u80fd\u914d\u7f6e\u6587\u4ef6\u3002 reset_after : GRU \u516c\u7ea6 (\u662f\u5426\u5728\u77e9\u9635\u4e58\u6cd5\u4e4b\u524d\u6216\u8005\u4e4b\u540e\u4f7f\u7528\u91cd\u7f6e\u95e8)\u3002 False = \"before\" (\u9ed8\u8ba4)\uff0cTure = \"after\" ( CuDNN \u517c\u5bb9)\u3002 reset_after : GRU convention (whether to apply reset gate after or before matrix multiplication). False = \"before\" (default), True = \"after\" (CuDNN compatible). [source] LSTMCell keras.layers.LSTMCell(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1) LSTM \u5c42\u7684\u5355\u5143\u7c7b\u3002 \u53c2\u6570 units : \u6b63\u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6\u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u9ed8\u8ba4\uff1a\u53cc\u66f2\u6b63\u5207\uff08 tanh \uff09\u3002 \u5982\u679c\u4f20\u5165 None \uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373 \u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 recurrent_activation : \u7528\u4e8e\u5faa\u73af\u65f6\u95f4\u6b65\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u9ed8\u8ba4\uff1a\u5206\u6bb5\u7ebf\u6027\u8fd1\u4f3c sigmoid ( hard_sigmoid )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668\uff0c \u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 recurrent_initializer : recurrent_kernel \u6743\u503c\u77e9\u9635 \u7684\u521d\u59cb\u5316\u5668\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 bias_initializer :\u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers ). unit_forget_bias : \u5e03\u5c14\u503c\u3002 \u5982\u679c\u4e3a True\uff0c\u521d\u59cb\u5316\u65f6\uff0c\u5c06\u5fd8\u8bb0\u95e8\u7684\u504f\u7f6e\u52a0 1\u3002 \u5c06\u5176\u8bbe\u7f6e\u4e3a True \u540c\u65f6\u8fd8\u4f1a\u5f3a\u5236 bias_initializer=\"zeros\" \u3002 \u8fd9\u4e2a\u5efa\u8bae\u6765\u81ea Jozefowicz et al. \u3002 kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 recurrent_regularizer : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 recurrent_constraint : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 dropout : \u5728 0 \u548c 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 \u5355\u5143\u7684\u4e22\u5f03\u6bd4\u4f8b\uff0c\u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362\u3002 recurrent_dropout : \u5728 0 \u548c 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 \u5355\u5143\u7684\u4e22\u5f03\u6bd4\u4f8b\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362\u3002 implementation : \u5b9e\u73b0\u6a21\u5f0f\uff0c1 \u6216 2\u3002 \u6a21\u5f0f 1 \u5c06\u628a\u5b83\u7684\u64cd\u4f5c\u7ed3\u6784\u5316\u4e3a\u66f4\u591a\u7684\u5c0f\u7684\u70b9\u79ef\u548c\u52a0\u6cd5\u64cd\u4f5c\uff0c \u800c\u6a21\u5f0f 2 \u5c06\u628a\u5b83\u4eec\u5206\u6279\u5230\u66f4\u5c11\uff0c\u66f4\u5927\u7684\u64cd\u4f5c\u4e2d\u3002 \u8fd9\u4e9b\u6a21\u5f0f\u5728\u4e0d\u540c\u7684\u786c\u4ef6\u548c\u4e0d\u540c\u7684\u5e94\u7528\u4e2d\u5177\u6709\u4e0d\u540c\u7684\u6027\u80fd\u914d\u7f6e\u6587\u4ef6\u3002 [source] CuDNNGRU keras.layers.CuDNNGRU(units, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, return_sequences=False, return_state=False, stateful=False) \u7531 CuDNN \u652f\u6301\u7684\u5feb\u901f GRU \u5b9e\u73b0\u3002 \u53ea\u80fd\u4ee5 TensorFlow \u540e\u7aef\u8fd0\u884c\u5728 GPU \u4e0a\u3002 \u53c2\u6570 units : \u6b63\u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668\uff0c \u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 recurrent_initializer : recurrent_kernel \u6743\u503c\u77e9\u9635 \u7684\u521d\u59cb\u5316\u5668\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 bias_initializer :\u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers ). kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 recurrent_regularizer : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 recurrent_constraint : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 return_sequences : \u5e03\u5c14\u503c\u3002\u662f\u8fd4\u56de\u8f93\u51fa\u5e8f\u5217\u4e2d\u7684\u6700\u540e\u4e00\u4e2a\u8f93\u51fa\uff0c\u8fd8\u662f\u5168\u90e8\u5e8f\u5217\u3002 return_state : \u5e03\u5c14\u503c\u3002\u9664\u4e86\u8f93\u51fa\u4e4b\u5916\u662f\u5426\u8fd4\u56de\u6700\u540e\u4e00\u4e2a\u72b6\u6001\u3002 stateful : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u6279\u6b21\u4e2d\u7d22\u5f15 i \u5904\u7684\u6bcf\u4e2a\u6837\u54c1\u7684\u6700\u540e\u72b6\u6001 \u5c06\u7528\u4f5c\u4e0b\u4e00\u6279\u6b21\u4e2d\u7d22\u5f15 i \u6837\u54c1\u7684\u521d\u59cb\u72b6\u6001\u3002 [source] CuDNNLSTM keras.layers.CuDNNLSTM(units, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, return_sequences=False, return_state=False, stateful=False) \u7531 CuDNN \u652f\u6301\u7684\u5feb\u901f LSTM \u5b9e\u73b0\u3002 \u53ea\u80fd\u4ee5 TensorFlow \u540e\u7aef\u8fd0\u884c\u5728 GPU \u4e0a\u3002 \u53c2\u6570 units : \u6b63\u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668\uff0c\u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362(\u8be6\u89c1 initializers )\u3002 unit_forget_bias : \u5e03\u5c14\u503c\u3002 \u5982\u679c\u4e3a True\uff0c\u521d\u59cb\u5316\u65f6\uff0c\u5c06\u5fd8\u8bb0\u95e8\u7684\u504f\u7f6e\u52a0 1\u3002 \u5c06\u5176\u8bbe\u7f6e\u4e3a True \u540c\u65f6\u8fd8\u4f1a\u5f3a\u5236 bias_initializer=\"zeros\" \u3002 \u8fd9\u4e2a\u5efa\u8bae\u6765\u81ea Jozefowicz et al. \u3002 recurrent_initializer : recurrent_kernel \u6743\u503c\u77e9\u9635 \u7684\u521d\u59cb\u5316\u5668\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 bias_initializer :\u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers ). kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 recurrent_regularizer : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u8f93\u51fa\uff08\u5b83\u7684\u6fc0\u6d3b\u503c\uff09\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 recurrent_constraint : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 return_sequences : \u5e03\u5c14\u503c\u3002\u662f\u8fd4\u56de\u8f93\u51fa\u5e8f\u5217\u4e2d\u7684\u6700\u540e\u4e00\u4e2a\u8f93\u51fa\uff0c\u8fd8\u662f\u5168\u90e8\u5e8f\u5217\u3002 return_state : \u5e03\u5c14\u503c\u3002\u9664\u4e86\u8f93\u51fa\u4e4b\u5916\u662f\u5426\u8fd4\u56de\u6700\u540e\u4e00\u4e2a\u72b6\u6001\u3002 stateful : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u6279\u6b21\u4e2d\u7d22\u5f15 i \u5904\u7684\u6bcf\u4e2a\u6837\u54c1\u7684\u6700\u540e\u72b6\u6001 \u5c06\u7528\u4f5c\u4e0b\u4e00\u6279\u6b21\u4e2d\u7d22\u5f15 i \u6837\u54c1\u7684\u521d\u59cb\u72b6\u6001\u3002","title":"\u5faa\u73af\u5c42"},{"location":"2-Layers/5.recurrent/#_1","text":"[source]","title":"\u5faa\u73af\u5c42"},{"location":"2-Layers/5.recurrent/#rnn","text":"keras.layers.RNN(cell, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False) \u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u5c42\u57fa\u7c7b\u3002 \u53c2\u6570 cell : \u4e00\u4e2a RNN \u5355\u5143\u5b9e\u4f8b\u3002RNN \u5355\u5143\u662f\u4e00\u4e2a\u5177\u6709\u4ee5\u4e0b\u51e0\u9879\u7684\u7c7b\uff1a \u4e00\u4e2a call(input_at_t, states_at_t) \u65b9\u6cd5\uff0c \u5b83\u8fd4\u56de (output_at_t, states_at_t_plus_1) \u3002 \u5355\u5143\u7684\u8c03\u7528\u65b9\u6cd5\u4e5f\u53ef\u4ee5\u91c7\u5f15\u5165\u53ef\u9009\u53c2\u6570 constants \uff0c \u8be6\u89c1\u4e0b\u9762\u7684\u5c0f\u8282\u300c\u5173\u4e8e\u7ed9 RNN \u4f20\u9012\u5916\u90e8\u5e38\u91cf\u7684\u8bf4\u660e\u300d\u3002 \u4e00\u4e2a state_size \u5c5e\u6027\u3002\u8fd9\u53ef\u4ee5\u662f\u5355\u4e2a\u6574\u6570\uff08\u5355\u4e2a\u72b6\u6001\uff09\uff0c \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b83\u662f\u5faa\u73af\u5c42\u72b6\u6001\u7684\u5927\u5c0f\uff08\u5e94\u8be5\u4e0e\u5355\u5143\u8f93\u51fa\u7684\u5927\u5c0f\u76f8\u540c\uff09\u3002 \u8fd9\u4e5f\u53ef\u4ee5\u662f\u6574\u6570\u8868\u793a\u7684\u5217\u8868/\u5143\u7ec4\uff08\u6bcf\u4e2a\u72b6\u6001\u4e00\u4e2a\u5927\u5c0f\uff09\u3002 \u4e00\u4e2a output_size \u5c5e\u6027\u3002 \u8fd9\u53ef\u4ee5\u662f\u5355\u4e2a\u6574\u6570\u6216\u8005\u662f\u4e00\u4e2a TensorShape\uff0c \u5b83\u8868\u793a\u8f93\u51fa\u7684\u5c3a\u5bf8\u3002\u51fa\u4e8e\u5411\u540e\u517c\u5bb9\u7684\u539f\u56e0\uff0c\u5982\u679c\u6b64\u5c5e\u6027\u5bf9\u4e8e\u5f53\u524d\u5355\u5143\u4e0d\u53ef\u7528\uff0c \u5219\u8be5\u503c\u5c06\u7531 state_size \u7684\u7b2c\u4e00\u4e2a\u5143\u7d20\u63a8\u65ad\u3002 cell \u4e5f\u53ef\u80fd\u662f RNN \u5355\u5143\u5b9e\u4f8b\u7684\u5217\u8868\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0cRNN \u7684\u5355\u5143\u5c06\u5806\u53e0\u5728\u53e6\u4e00\u4e2a\u5355\u5143\u4e0a\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u5806\u53e0 RNN\u3002 return_sequences : \u5e03\u5c14\u503c\u3002\u662f\u8fd4\u56de\u8f93\u51fa\u5e8f\u5217\u4e2d\u7684\u6700\u540e\u4e00\u4e2a\u8f93\u51fa\uff0c\u8fd8\u662f\u5168\u90e8\u5e8f\u5217\u3002 return_state : \u5e03\u5c14\u503c\u3002\u9664\u4e86\u8f93\u51fa\u4e4b\u5916\u662f\u5426\u8fd4\u56de\u6700\u540e\u4e00\u4e2a\u72b6\u6001\u3002 go_backwards : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u5411\u540e\u5904\u7406\u8f93\u5165\u5e8f\u5217\u5e76\u8fd4\u56de\u76f8\u53cd\u7684\u5e8f\u5217\u3002 stateful : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u6279\u6b21\u4e2d\u7d22\u5f15 i \u5904\u7684\u6bcf\u4e2a\u6837\u54c1\u7684\u6700\u540e\u72b6\u6001\u5c06\u7528\u4f5c\u4e0b\u4e00\u6279\u6b21\u4e2d\u7d22\u5f15 i \u6837\u54c1\u7684\u521d\u59cb\u72b6\u6001\u3002 unroll : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u7f51\u7edc\u5c06\u5c55\u5f00\uff0c\u5426\u5219\u5c06\u4f7f\u7528\u7b26\u53f7\u5faa\u73af\u3002 \u5c55\u5f00\u53ef\u4ee5\u52a0\u901f RNN\uff0c\u4f46\u5b83\u5f80\u5f80\u4f1a\u5360\u7528\u66f4\u591a\u7684\u5185\u5b58\u3002 \u5c55\u5f00\u53ea\u9002\u7528\u4e8e\u77ed\u5e8f\u5217\u3002 input_dim : \u8f93\u5165\u7684\u7ef4\u5ea6\uff08\u6574\u6570\uff09\u3002 \u5c06\u6b64\u5c42\u7528\u4f5c\u6a21\u578b\u4e2d\u7684\u7b2c\u4e00\u5c42\u65f6\uff0c\u6b64\u53c2\u6570\uff08\u6216\u8005\uff0c\u5173\u952e\u5b57\u53c2\u6570 input_shape \uff09\u662f\u5fc5\u9700\u7684\u3002 input_length : \u8f93\u5165\u5e8f\u5217\u7684\u957f\u5ea6\uff0c\u5728\u6052\u5b9a\u65f6\u6307\u5b9a\u3002 \u5982\u679c\u4f60\u8981\u5728\u4e0a\u6e38\u8fde\u63a5 Flatten \u548c Dense \u5c42\uff0c \u5219\u9700\u8981\u6b64\u53c2\u6570\uff08\u5982\u679c\u6ca1\u6709\u5b83\uff0c\u65e0\u6cd5\u8ba1\u7b97\u5168\u8fde\u63a5\u8f93\u51fa\u7684\u5c3a\u5bf8\uff09\u3002 \u8bf7\u6ce8\u610f\uff0c\u5982\u679c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u5c42\u4e0d\u662f\u6a21\u578b\u4e2d\u7684\u7b2c\u4e00\u5c42\uff0c \u5219\u9700\u8981\u5728\u7b2c\u4e00\u5c42\u7684\u5c42\u7ea7\u6307\u5b9a\u8f93\u5165\u957f\u5ea6\uff08\u4f8b\u5982\uff0c\u901a\u8fc7 input_shape \u53c2\u6570\uff09\u3002 \u8f93\u5165\u5c3a\u5bf8 3D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a (batch_size, timesteps, input_dim) \u3002 \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c return_state \uff1a\u8fd4\u56de\u5f20\u91cf\u5217\u8868\u3002 \u7b2c\u4e00\u4e2a\u5f20\u91cf\u4e3a\u8f93\u51fa\u3002\u5269\u4f59\u7684\u5f20\u91cf\u4e3a\u6700\u540e\u7684\u72b6\u6001\uff0c \u6bcf\u4e2a\u5f20\u91cf\u7684\u5c3a\u5bf8\u4e3a (batch_size, units) \u3002 \u5982\u679c return_sequences \uff1a\u8fd4\u56de 3D \u5f20\u91cf\uff0c \u5c3a\u5bf8\u4e3a (batch_size, timesteps, units) \u3002 \u5426\u5219\uff0c\u8fd4\u56de\u5c3a\u5bf8\u4e3a (batch_size, units) \u7684 2D \u5f20\u91cf\u3002 Masking \u8be5\u5c42\u652f\u6301\u4ee5\u53ef\u53d8\u6570\u91cf\u7684\u65f6\u95f4\u6b65\u5bf9\u8f93\u5165\u6570\u636e\u8fdb\u884c masking\u3002 \u8981\u5c06 masking \u5f15\u5165\u4f60\u7684\u6570\u636e\uff0c\u8bf7\u4f7f\u7528 Embedding \u5c42\uff0c\u5e76\u5c06 mask_zero \u53c2\u6570\u8bbe\u7f6e\u4e3a True \u3002 \u5173\u4e8e\u5728 RNN \u4e2d\u4f7f\u7528\u300c\u72b6\u6001\uff08statefulness\uff09\u300d\u7684\u8bf4\u660e \u4f60\u53ef\u4ee5\u5c06 RNN \u5c42\u8bbe\u7f6e\u4e3a stateful \uff08\u6709\u72b6\u6001\u7684\uff09\uff0c \u8fd9\u610f\u5473\u7740\u9488\u5bf9\u4e00\u4e2a\u6279\u6b21\u7684\u6837\u672c\u8ba1\u7b97\u7684\u72b6\u6001\u5c06\u88ab\u91cd\u65b0\u7528\u4f5c\u4e0b\u4e00\u6279\u6837\u672c\u7684\u521d\u59cb\u72b6\u6001\u3002 \u8fd9\u5047\u5b9a\u5728\u4e0d\u540c\u8fde\u7eed\u6279\u6b21\u7684\u6837\u54c1\u4e4b\u95f4\u6709\u4e00\u5bf9\u4e00\u7684\u6620\u5c04\u3002 \u4e3a\u4e86\u4f7f\u72b6\u6001\u6709\u6548\uff1a \u5728\u5c42\u6784\u9020\u5668\u4e2d\u6307\u5b9a stateful=True \u3002 \u4e3a\u4f60\u7684\u6a21\u578b\u6307\u5b9a\u4e00\u4e2a\u56fa\u5b9a\u7684\u6279\u6b21\u5927\u5c0f\uff0c \u5982\u679c\u662f\u987a\u5e8f\u6a21\u578b\uff0c\u4e3a\u4f60\u7684\u6a21\u578b\u7684\u7b2c\u4e00\u5c42\u4f20\u9012\u4e00\u4e2a batch_input_shape=(...) \u53c2\u6570\u3002 \u4e3a\u4f60\u7684\u6a21\u578b\u6307\u5b9a\u4e00\u4e2a\u56fa\u5b9a\u7684\u6279\u6b21\u5927\u5c0f\uff0c \u5982\u679c\u662f\u987a\u5e8f\u6a21\u578b\uff0c\u4e3a\u4f60\u7684\u6a21\u578b\u7684\u7b2c\u4e00\u5c42\u4f20\u9012\u4e00\u4e2a batch_input_shape=(...) \u3002 \u5982\u679c\u662f\u5e26\u6709 1 \u4e2a\u6216\u591a\u4e2a Input \u5c42\u7684\u51fd\u6570\u5f0f\u6a21\u578b\uff0c\u4e3a\u4f60\u7684\u6a21\u578b\u7684\u6240\u6709\u7b2c\u4e00\u5c42\u4f20\u9012\u4e00\u4e2a batch_shape=(...) \u3002 \u8fd9\u662f\u4f60\u7684\u8f93\u5165\u7684\u9884\u671f\u5c3a\u5bf8\uff0c \u5305\u62ec\u6279\u91cf\u7ef4\u5ea6 \u3002 \u5b83\u5e94\u8be5\u662f\u6574\u6570\u7684\u5143\u7ec4\uff0c\u4f8b\u5982 (32, 10, 100) \u3002 \u5728\u8c03\u7528 fit() \u662f\u6307\u5b9a shuffle=False \u3002 \u8981\u91cd\u7f6e\u6a21\u578b\u7684\u72b6\u6001\uff0c\u8bf7\u5728\u7279\u5b9a\u56fe\u5c42\u6216\u6574\u4e2a\u6a21\u578b\u4e0a\u8c03\u7528 .reset_states() \u3002 \u5173\u4e8e\u6307\u5b9a RNN \u521d\u59cb\u72b6\u6001\u7684\u8bf4\u660e \u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u5173\u952e\u5b57\u53c2\u6570 initial_state \u8c03\u7528\u5b83\u4eec\u6765\u7b26\u53f7\u5316\u5730\u6307\u5b9a RNN \u5c42\u7684\u521d\u59cb\u72b6\u6001\u3002 initial_state \u7684\u503c\u5e94\u8be5\u662f\u8868\u793a RNN \u5c42\u521d\u59cb\u72b6\u6001\u7684\u5f20\u91cf\u6216\u5f20\u91cf\u5217\u8868\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u8c03\u7528\u5e26\u6709\u5173\u952e\u5b57\u53c2\u6570 states \u7684 reset_states \u65b9\u6cd5\u6765\u6570\u5b57\u5316\u5730\u6307\u5b9a RNN \u5c42\u7684\u521d\u59cb\u72b6\u6001\u3002 states \u7684\u503c\u5e94\u8be5\u662f\u4e00\u4e2a\u4ee3\u8868 RNN \u5c42\u521d\u59cb\u72b6\u6001\u7684 Numpy \u6570\u7ec4\u6216\u8005 Numpy \u6570\u7ec4\u5217\u8868\u3002 \u5173\u4e8e\u7ed9 RNN \u4f20\u9012\u5916\u90e8\u5e38\u91cf\u7684\u8bf4\u660e \u4f60\u53ef\u4ee5\u4f7f\u7528 RNN.__call__ \uff08\u4ee5\u53ca RNN.call \uff09\u7684 constants \u5173\u952e\u5b57\u53c2\u6570\u5c06\u300c\u5916\u90e8\u300d\u5e38\u91cf\u4f20\u9012\u7ed9\u5355\u5143\u3002 \u8fd9\u8981\u6c42 cell.call \u65b9\u6cd5\u63a5\u53d7\u76f8\u540c\u7684\u5173\u952e\u5b57\u53c2\u6570 constants \u3002 \u8fd9\u4e9b\u5e38\u6570\u53ef\u7528\u4e8e\u8c03\u8282\u9644\u52a0\u9759\u6001\u8f93\u5165\uff08\u4e0d\u968f\u65f6\u95f4\u53d8\u5316\uff09\u4e0a\u7684\u5355\u5143\u8f6c\u6362\uff0c\u4e5f\u53ef\u7528\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u3002 \u4f8b\u5b50 # \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b9a\u4e49\u4e00\u4e2a RNN \u5355\u5143\uff0c\u4f5c\u4e3a\u7f51\u7edc\u5c42\u5b50\u7c7b\u3002 class MinimalRNNCell(keras.layers.Layer): def __init__(self, units, **kwargs): self.units = units self.state_size = units super(MinimalRNNCell, self).__init__(**kwargs) def build(self, input_shape): self.kernel = self.add_weight(shape=(input_shape[-1], self.units), initializer='uniform', name='kernel') self.recurrent_kernel = self.add_weight( shape=(self.units, self.units), initializer='uniform', name='recurrent_kernel') self.built = True def call(self, inputs, states): prev_output = states[0] h = K.dot(inputs, self.kernel) output = h + K.dot(prev_output, self.recurrent_kernel) return output, [output] # \u8ba9\u6211\u4eec\u5728 RNN \u5c42\u4f7f\u7528\u8fd9\u4e2a\u5355\u5143\uff1a cell = MinimalRNNCell(32) x = keras.Input((None, 5)) layer = RNN(cell) y = layer(x) # \u4ee5\u4e0b\u662f\u5982\u4f55\u4f7f\u7528\u5355\u5143\u683c\u6784\u5efa\u5806\u53e0\u7684 RNN\u7684\u65b9\u6cd5\uff1a cells = [MinimalRNNCell(32), MinimalRNNCell(64)] x = keras.Input((None, 5)) layer = RNN(cells) y = layer(x) [source]","title":"RNN"},{"location":"2-Layers/5.recurrent/#simplernn","text":"keras.layers.SimpleRNN(units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False) \u5168\u8fde\u63a5\u7684 RNN\uff0c\u5176\u8f93\u51fa\u5c06\u88ab\u53cd\u9988\u5230\u8f93\u5165\u3002 \u53c2\u6570 units : \u6b63\u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6\u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u9ed8\u8ba4\uff1a\u53cc\u66f2\u6b63\u5207\uff08 tanh \uff09\u3002 \u5982\u679c\u4f20\u5165 None \uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373 \u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668\uff0c \u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 recurrent_initializer : recurrent_kernel \u6743\u503c\u77e9\u9635 \u7684\u521d\u59cb\u5316\u5668\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 bias_initializer :\u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers ). kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 recurrent_regularizer : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u8f93\u51fa\uff08\u5b83\u7684\u6fc0\u6d3b\u503c\uff09\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 recurrent_constraint : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 dropout : \u5728 0 \u548c 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 \u5355\u5143\u7684\u4e22\u5f03\u6bd4\u4f8b\uff0c\u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362\u3002 recurrent_dropout : \u5728 0 \u548c 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 \u5355\u5143\u7684\u4e22\u5f03\u6bd4\u4f8b\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362\u3002 return_sequences : \u5e03\u5c14\u503c\u3002\u662f\u8fd4\u56de\u8f93\u51fa\u5e8f\u5217\u4e2d\u7684\u6700\u540e\u4e00\u4e2a\u8f93\u51fa\uff0c\u8fd8\u662f\u5168\u90e8\u5e8f\u5217\u3002 return_state : \u5e03\u5c14\u503c\u3002\u9664\u4e86\u8f93\u51fa\u4e4b\u5916\u662f\u5426\u8fd4\u56de\u6700\u540e\u4e00\u4e2a\u72b6\u6001\u3002 go_backwards : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u5411\u540e\u5904\u7406\u8f93\u5165\u5e8f\u5217\u5e76\u8fd4\u56de\u76f8\u53cd\u7684\u5e8f\u5217\u3002 stateful : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u6279\u6b21\u4e2d\u7d22\u5f15 i \u5904\u7684\u6bcf\u4e2a\u6837\u54c1 \u7684\u6700\u540e\u72b6\u6001\u5c06\u7528\u4f5c\u4e0b\u4e00\u6279\u6b21\u4e2d\u7d22\u5f15 i \u6837\u54c1\u7684\u521d\u59cb\u72b6\u6001\u3002 unroll : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u7f51\u7edc\u5c06\u5c55\u5f00\uff0c\u5426\u5219\u5c06\u4f7f\u7528\u7b26\u53f7\u5faa\u73af\u3002 \u5c55\u5f00\u53ef\u4ee5\u52a0\u901f RNN\uff0c\u4f46\u5b83\u5f80\u5f80\u4f1a\u5360\u7528\u66f4\u591a\u7684\u5185\u5b58\u3002 \u5c55\u5f00\u53ea\u9002\u7528\u4e8e\u77ed\u5e8f\u5217\u3002 [source]","title":"SimpleRNN"},{"location":"2-Layers/5.recurrent/#gru","text":"keras.layers.GRU(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, reset_after=False) \u95e8\u9650\u5faa\u73af\u5355\u5143\u7f51\u7edc\uff08Gated Recurrent Unit\uff09 - Cho et al. 2014. \u6709\u4e24\u79cd\u53d8\u4f53\u3002\u9ed8\u8ba4\u7684\u662f\u57fa\u4e8e 1406.1078v3 \u7684\u5b9e\u73b0\uff0c\u540c\u65f6\u5728\u77e9\u9635\u4e58\u6cd5\u4e4b\u524d\u5c06\u590d\u4f4d\u95e8\u5e94\u7528\u4e8e\u9690\u85cf\u72b6\u6001\u3002 \u53e6\u4e00\u79cd\u5219\u662f\u57fa\u4e8e 1406.1078v1 \u7684\u5b9e\u73b0\uff0c\u5b83\u5305\u62ec\u987a\u5e8f\u5012\u7f6e\u7684\u64cd\u4f5c\u3002 \u7b2c\u4e8c\u79cd\u53d8\u4f53\u4e0e CuDNNGRU(GPU-only) \u517c\u5bb9\u5e76\u4e14\u5141\u8bb8\u5728 CPU \u4e0a\u8fdb\u884c\u63a8\u7406\u3002 \u56e0\u6b64\u5b83\u5bf9\u4e8e kernel \u548c recurrent_kernel \u6709\u53ef\u5206\u79bb\u504f\u7f6e\u3002 \u4f7f\u7528 'reset_after'=True \u548c recurrent_activation='sigmoid' \u3002 \u53c2\u6570 units : \u6b63\u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6\u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u9ed8\u8ba4\uff1a\u53cc\u66f2\u6b63\u5207 ( tanh )\u3002 \u5982\u679c\u4f20\u5165 None \uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373 \u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 recurrent_activation : \u7528\u4e8e\u5faa\u73af\u65f6\u95f4\u6b65\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u9ed8\u8ba4\uff1a\u5206\u6bb5\u7ebf\u6027\u8fd1\u4f3c sigmoid ( hard_sigmoid )\u3002 \u5982\u679c\u4f20\u5165 None\uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373 \u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668\uff0c \u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 recurrent_initializer : recurrent_kernel \u6743\u503c\u77e9\u9635 \u7684\u521d\u59cb\u5316\u5668\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 bias_initializer :\u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers ). kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 recurrent_regularizer : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u8f93\u51fa\uff08\u5b83\u7684\u6fc0\u6d3b\u503c\uff09\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 recurrent_constraint : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 dropout : \u5728 0 \u548c 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 \u5355\u5143\u7684\u4e22\u5f03\u6bd4\u4f8b\uff0c\u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362\u3002 recurrent_dropout : \u5728 0 \u548c 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 \u5355\u5143\u7684\u4e22\u5f03\u6bd4\u4f8b\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362\u3002 implementation : \u5b9e\u73b0\u6a21\u5f0f\uff0c1 \u6216 2\u3002 \u6a21\u5f0f 1 \u5c06\u628a\u5b83\u7684\u64cd\u4f5c\u7ed3\u6784\u5316\u4e3a\u66f4\u591a\u7684\u5c0f\u7684\u70b9\u79ef\u548c\u52a0\u6cd5\u64cd\u4f5c\uff0c \u800c\u6a21\u5f0f 2 \u5c06\u628a\u5b83\u4eec\u5206\u6279\u5230\u66f4\u5c11\uff0c\u66f4\u5927\u7684\u64cd\u4f5c\u4e2d\u3002 \u8fd9\u4e9b\u6a21\u5f0f\u5728\u4e0d\u540c\u7684\u786c\u4ef6\u548c\u4e0d\u540c\u7684\u5e94\u7528\u4e2d\u5177\u6709\u4e0d\u540c\u7684\u6027\u80fd\u914d\u7f6e\u6587\u4ef6\u3002 return_sequences : \u5e03\u5c14\u503c\u3002\u662f\u8fd4\u56de\u8f93\u51fa\u5e8f\u5217\u4e2d\u7684\u6700\u540e\u4e00\u4e2a\u8f93\u51fa\uff0c\u8fd8\u662f\u5168\u90e8\u5e8f\u5217\u3002 return_state : \u5e03\u5c14\u503c\u3002\u9664\u4e86\u8f93\u51fa\u4e4b\u5916\u662f\u5426\u8fd4\u56de\u6700\u540e\u4e00\u4e2a\u72b6\u6001\u3002 go_backwards : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u5411\u540e\u5904\u7406\u8f93\u5165\u5e8f\u5217\u5e76\u8fd4\u56de\u76f8\u53cd\u7684\u5e8f\u5217\u3002 stateful : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u6279\u6b21\u4e2d\u7d22\u5f15 i \u5904\u7684\u6bcf\u4e2a\u6837\u54c1\u7684\u6700\u540e\u72b6\u6001 \u5c06\u7528\u4f5c\u4e0b\u4e00\u6279\u6b21\u4e2d\u7d22\u5f15 i \u6837\u54c1\u7684\u521d\u59cb\u72b6\u6001\u3002 unroll : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u7f51\u7edc\u5c06\u5c55\u5f00\uff0c\u5426\u5219\u5c06\u4f7f\u7528\u7b26\u53f7\u5faa\u73af\u3002 \u5c55\u5f00\u53ef\u4ee5\u52a0\u901f RNN\uff0c\u4f46\u5b83\u5f80\u5f80\u4f1a\u5360\u7528\u66f4\u591a\u7684\u5185\u5b58\u3002 \u5c55\u5f00\u53ea\u9002\u7528\u4e8e\u77ed\u5e8f\u5217\u3002 reset_after : GRU \u516c\u7ea6 (\u662f\u5426\u5728\u77e9\u9635\u4e58\u6cd5\u4e4b\u524d\u6216\u8005\u4e4b\u540e\u4f7f\u7528\u91cd\u7f6e\u95e8)\u3002 False =\u300c\u4e4b\u524d\u300d(\u9ed8\u8ba4)\uff0cTure =\u300c\u4e4b\u540e\u300d( CuDNN \u517c\u5bb9)\u3002 \u53c2\u8003\u6587\u732e Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation On the Properties of Neural Machine Translation: Encoder-Decoder Approaches Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling A Theoretically Grounded Application of Dropout in Recurrent Neural Networks [source]","title":"GRU"},{"location":"2-Layers/5.recurrent/#lstm","text":"keras.layers.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False) \u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\u5c42\uff08Long Short-Term Memory\uff09 - Hochreiter 1997. \u53c2\u6570 units : \u6b63\u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6\u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u5982\u679c\u4f20\u5165 None \uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373 \u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 recurrent_activation : \u7528\u4e8e\u5faa\u73af\u65f6\u95f4\u6b65\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u9ed8\u8ba4\uff1a\u5206\u6bb5\u7ebf\u6027\u8fd1\u4f3c sigmoid ( hard_sigmoid )\u3002 \u5982\u679c\u4f20\u5165 None \uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373 \u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668\uff0c \u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 recurrent_initializer : recurrent_kernel \u6743\u503c\u77e9\u9635 \u7684\u521d\u59cb\u5316\u5668\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 bias_initializer :\u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers ). unit_forget_bias : \u5e03\u5c14\u503c\u3002 \u5982\u679c\u4e3a True\uff0c\u521d\u59cb\u5316\u65f6\uff0c\u5c06\u5fd8\u8bb0\u95e8\u7684\u504f\u7f6e\u52a0 1\u3002 \u5c06\u5176\u8bbe\u7f6e\u4e3a True \u540c\u65f6\u8fd8\u4f1a\u5f3a\u5236 bias_initializer=\"zeros\" \u3002 \u8fd9\u4e2a\u5efa\u8bae\u6765\u81ea Jozefowicz et al. \u3002 kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 recurrent_regularizer : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u8f93\u51fa\uff08\u5b83\u7684\u6fc0\u6d3b\u503c\uff09\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 recurrent_constraint : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 dropout : \u5728 0 \u548c 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 \u5355\u5143\u7684\u4e22\u5f03\u6bd4\u4f8b\uff0c\u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362\u3002 recurrent_dropout : \u5728 0 \u548c 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 \u5355\u5143\u7684\u4e22\u5f03\u6bd4\u4f8b\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362\u3002 implementation : \u5b9e\u73b0\u6a21\u5f0f\uff0c1 \u6216 2\u3002 \u6a21\u5f0f 1 \u5c06\u628a\u5b83\u7684\u64cd\u4f5c\u7ed3\u6784\u5316\u4e3a\u66f4\u591a\u7684\u5c0f\u7684\u70b9\u79ef\u548c\u52a0\u6cd5\u64cd\u4f5c\uff0c \u800c\u6a21\u5f0f 2 \u5c06\u628a\u5b83\u4eec\u5206\u6279\u5230\u66f4\u5c11\uff0c\u66f4\u5927\u7684\u64cd\u4f5c\u4e2d\u3002 \u8fd9\u4e9b\u6a21\u5f0f\u5728\u4e0d\u540c\u7684\u786c\u4ef6\u548c\u4e0d\u540c\u7684\u5e94\u7528\u4e2d\u5177\u6709\u4e0d\u540c\u7684\u6027\u80fd\u914d\u7f6e\u6587\u4ef6\u3002 return_sequences : \u5e03\u5c14\u503c\u3002\u662f\u8fd4\u56de\u8f93\u51fa\u5e8f\u5217\u4e2d\u7684\u6700\u540e\u4e00\u4e2a\u8f93\u51fa\uff0c\u8fd8\u662f\u5168\u90e8\u5e8f\u5217\u3002 return_state : \u5e03\u5c14\u503c\u3002\u9664\u4e86\u8f93\u51fa\u4e4b\u5916\u662f\u5426\u8fd4\u56de\u6700\u540e\u4e00\u4e2a\u72b6\u6001\u3002 go_backwards : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u5411\u540e\u5904\u7406\u8f93\u5165\u5e8f\u5217\u5e76\u8fd4\u56de\u76f8\u53cd\u7684\u5e8f\u5217\u3002 stateful : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u6279\u6b21\u4e2d\u7d22\u5f15 i \u5904\u7684\u6bcf\u4e2a\u6837\u54c1\u7684\u6700\u540e\u72b6\u6001 \u5c06\u7528\u4f5c\u4e0b\u4e00\u6279\u6b21\u4e2d\u7d22\u5f15 i \u6837\u54c1\u7684\u521d\u59cb\u72b6\u6001\u3002 unroll : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u7f51\u7edc\u5c06\u5c55\u5f00\uff0c\u5426\u5219\u5c06\u4f7f\u7528\u7b26\u53f7\u5faa\u73af\u3002 \u5c55\u5f00\u53ef\u4ee5\u52a0\u901f RNN\uff0c\u4f46\u5b83\u5f80\u5f80\u4f1a\u5360\u7528\u66f4\u591a\u7684\u5185\u5b58\u3002 \u5c55\u5f00\u53ea\u9002\u7528\u4e8e\u77ed\u5e8f\u5217\u3002 \u53c2\u8003\u6587\u732e Long short-term memory (original 1997 paper) Learning to forget: Continual prediction with LSTM Supervised sequence labeling with recurrent neural networks A Theoretically Grounded Application of Dropout in Recurrent Neural Networks [source]","title":"LSTM"},{"location":"2-Layers/5.recurrent/#convlstm2d","text":"keras.layers.ConvLSTM2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, return_sequences=False, go_backwards=False, stateful=False, dropout=0.0, recurrent_dropout=0.0) \u5377\u79ef LSTM\u3002 \u5b83\u7c7b\u4f3c\u4e8e LSTM \u5c42\uff0c\u4f46\u8f93\u5165\u53d8\u6362\u548c\u5faa\u73af\u53d8\u6362\u90fd\u662f\u5377\u79ef\u7684\u3002 \u53c2\u6570 filters : \u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6 \uff08\u5373\u5377\u79ef\u4e2d\u6ee4\u6ce2\u5668\u7684\u8f93\u51fa\u6570\u91cf\uff09\u3002 kernel_size : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 n \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e\u5377\u79ef\u7a97\u53e3\u7684\u7ef4\u5ea6\u3002 strides : \u4e00\u4e2a\u6574\u6570\uff0c\u6216\u8005 n \u4e2a\u6574\u6570\u8868\u793a\u7684\u5143\u7ec4\u6216\u5217\u8868\uff0c \u6307\u660e\u5377\u79ef\u7684\u6b65\u957f\u3002 \u6307\u5b9a\u4efb\u4f55 stride \u503c != 1 \u4e0e\u6307\u5b9a dilation_rate \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 padding : \"valid\" \u6216 \"same\" \u4e4b\u4e00 (\u5927\u5c0f\u5199\u654f\u611f)\u3002 data_format : \u5b57\u7b26\u4e32\uff0c channels_last (\u9ed8\u8ba4) \u6216 channels_first \u4e4b\u4e00\u3002 \u8f93\u5165\u4e2d\u7ef4\u5ea6\u7684\u987a\u5e8f\u3002 channels_last \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, time, ..., channels) \uff0c channels_first \u5bf9\u5e94\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch, time, channels, ...) \u3002 \u5b83\u9ed8\u8ba4\u4e3a\u4ece Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d \u627e\u5230\u7684 image_data_format \u503c\u3002 \u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u5c06\u4f7f\u7528 \"channels_last\" \u3002 dilation_rate : \u4e00\u4e2a\u6574\u6570\uff0c\u6216 n \u4e2a\u6574\u6570\u7684\u5143\u7ec4/\u5217\u8868\uff0c\u6307\u5b9a\u7528\u4e8e\u81a8\u80c0\u5377\u79ef\u7684\u81a8\u80c0\u7387\u3002 \u76ee\u524d\uff0c\u6307\u5b9a\u4efb\u4f55 dilation_rate \u503c != 1 \u4e0e\u6307\u5b9a stride \u503c != 1 \u4e24\u8005\u4e0d\u517c\u5bb9\u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u5982\u679c\u4f20\u5165 None\uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373 \u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 recurrent_activation : \u7528\u4e8e\u5faa\u73af\u65f6\u95f4\u6b65\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668\uff0c \u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 recurrent_initializer : recurrent_kernel \u6743\u503c\u77e9\u9635 \u7684\u521d\u59cb\u5316\u5668\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 bias_initializer :\u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers ). unit_forget_bias : \u5e03\u5c14\u503c\u3002 \u5982\u679c\u4e3a True\uff0c\u521d\u59cb\u5316\u65f6\uff0c\u5c06\u5fd8\u8bb0\u95e8\u7684\u504f\u7f6e\u52a0 1\u3002 \u5c06\u5176\u8bbe\u7f6e\u4e3a True \u540c\u65f6\u8fd8\u4f1a\u5f3a\u5236 bias_initializer=\"zeros\" \u3002 \u8fd9\u4e2a\u5efa\u8bae\u6765\u81ea Jozefowicz et al. \u3002 kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 recurrent_regularizer : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u8f93\u51fa\uff08\u5b83\u7684\u6fc0\u6d3b\u503c\uff09\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 recurrent_constraint : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 return_sequences : \u5e03\u5c14\u503c\u3002\u662f\u8fd4\u56de\u8f93\u51fa\u5e8f\u5217\u4e2d\u7684\u6700\u540e\u4e00\u4e2a\u8f93\u51fa\uff0c\u8fd8\u662f\u5168\u90e8\u5e8f\u5217\u3002 go_backwards : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u5411\u540e\u5904\u7406\u8f93\u5165\u5e8f\u5217\u5e76\u8fd4\u56de\u76f8\u53cd\u7684\u5e8f\u5217\u3002 stateful : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u6279\u6b21\u4e2d\u7d22\u5f15 i \u5904\u7684\u6bcf\u4e2a\u6837\u54c1\u7684\u6700\u540e\u72b6\u6001 \u5c06\u7528\u4f5c\u4e0b\u4e00\u6279\u6b21\u4e2d\u7d22\u5f15 i \u6837\u54c1\u7684\u521d\u59cb\u72b6\u6001\u3002 dropout : \u5728 0 \u548c 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 \u5355\u5143\u7684\u4e22\u5f03\u6bd4\u4f8b\uff0c\u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362\u3002 recurrent_dropout : \u5728 0 \u548c 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 \u5355\u5143\u7684\u4e22\u5f03\u6bd4\u4f8b\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5982\u679c data_format='channels_first'\uff0c \u8f93\u5165 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (samples,time, channels, rows, cols) \u3002 \u5982\u679c data_format='channels_last'\uff0c \u8f93\u5165 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (samples,time, rows, cols, channels) \u3002 \u8f93\u51fa\u5c3a\u5bf8 \u5982\u679c return_sequences \uff0c \u5982\u679c data_format='channels_first'\uff0c\u8fd4\u56de 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (samples, time, filters, output_row, output_col) \u3002 \u5982\u679c data_format='channels_last'\uff0c\u8fd4\u56de 5D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (samples, time, output_row, output_col, filters) \u3002 \u5426\u5219\uff0c \u5982\u679c data_format ='channels_first'\uff0c\u8fd4\u56de 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (samples, filters, output_row, output_col) \u3002 \u5982\u679c data_format='channels_last'\uff0c\u8fd4\u56de 4D \u5f20\u91cf\uff0c\u5c3a\u5bf8\u4e3a\uff1a (samples, output_row, output_col, filters) \u3002 o_row \u548c o_col \u53d6\u51b3\u4e8e filter \u548c padding \u7684\u5c3a\u5bf8\u3002 \u5f02\u5e38 ValueError : \u65e0\u6548\u7684\u6784\u9020\u53c2\u6570\u3002 \u53c2\u8003\u6587\u732e Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting \u3002 \u5f53\u524d\u7684\u5b9e\u73b0\u4e0d\u5305\u62ec\u5355\u5143\u8f93\u51fa\u7684\u53cd\u9988\u56de\u8def\u3002 [source]","title":"ConvLSTM2D"},{"location":"2-Layers/5.recurrent/#simplernncell","text":"keras.layers.SimpleRNNCell(units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0) SimpleRNN \u7684\u5355\u5143\u7c7b\u3002 \u53c2\u6570 units : \u6b63\u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6\u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u9ed8\u8ba4\uff1a\u53cc\u66f2\u6b63\u5207 ( tanh )\u3002 \u5982\u679c\u4f20\u5165 None \uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373 \u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668\uff0c \u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 recurrent_initializer : recurrent_kernel \u6743\u503c\u77e9\u9635 \u7684\u521d\u59cb\u5316\u5668\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 bias_initializer :\u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers ). kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 recurrent_regularizer : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 recurrent_constraint : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 dropout : \u5728 0 \u548c 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 \u5355\u5143\u7684\u4e22\u5f03\u6bd4\u4f8b\uff0c\u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362\u3002 recurrent_dropout : \u5728 0 \u548c 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 \u5355\u5143\u7684\u4e22\u5f03\u6bd4\u4f8b\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362\u3002 [source]","title":"SimpleRNNCell"},{"location":"2-Layers/5.recurrent/#grucell","text":"keras.layers.GRUCell(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, reset_after=False) GRU \u5c42\u7684\u5355\u5143\u7c7b\u3002 \u53c2\u6570 units : \u6b63\u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6\u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u9ed8\u8ba4\uff1a\u53cc\u66f2\u6b63\u5207 ( tanh )\u3002 \u5982\u679c\u4f20\u5165 None \uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373 \u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 recurrent_activation : \u7528\u4e8e\u5faa\u73af\u65f6\u95f4\u6b65\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u9ed8\u8ba4\uff1a\u5206\u6bb5\u7ebf\u6027\u8fd1\u4f3c sigmoid ( hard_sigmoid )\u3002 \u5982\u679c\u4f20\u5165 None \uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373 \u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668\uff0c \u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 recurrent_initializer : recurrent_kernel \u6743\u503c\u77e9\u9635 \u7684\u521d\u59cb\u5316\u5668\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 bias_initializer :\u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers ). kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 recurrent_regularizer : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 recurrent_constraint : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 dropout : \u5728 0 \u548c 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 \u5355\u5143\u7684\u4e22\u5f03\u6bd4\u4f8b\uff0c\u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362\u3002 recurrent_dropout : \u5728 0 \u548c 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 \u5355\u5143\u7684\u4e22\u5f03\u6bd4\u4f8b\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362\u3002 implementation : \u5b9e\u73b0\u6a21\u5f0f\uff0c1 \u6216 2\u3002 \u6a21\u5f0f 1 \u5c06\u628a\u5b83\u7684\u64cd\u4f5c\u7ed3\u6784\u5316\u4e3a\u66f4\u591a\u7684\u5c0f\u7684\u70b9\u79ef\u548c\u52a0\u6cd5\u64cd\u4f5c\uff0c \u800c\u6a21\u5f0f 2 \u5c06\u628a\u5b83\u4eec\u5206\u6279\u5230\u66f4\u5c11\uff0c\u66f4\u5927\u7684\u64cd\u4f5c\u4e2d\u3002 \u8fd9\u4e9b\u6a21\u5f0f\u5728\u4e0d\u540c\u7684\u786c\u4ef6\u548c\u4e0d\u540c\u7684\u5e94\u7528\u4e2d\u5177\u6709\u4e0d\u540c\u7684\u6027\u80fd\u914d\u7f6e\u6587\u4ef6\u3002 reset_after : GRU \u516c\u7ea6 (\u662f\u5426\u5728\u77e9\u9635\u4e58\u6cd5\u4e4b\u524d\u6216\u8005\u4e4b\u540e\u4f7f\u7528\u91cd\u7f6e\u95e8)\u3002 False = \"before\" (\u9ed8\u8ba4)\uff0cTure = \"after\" ( CuDNN \u517c\u5bb9)\u3002 reset_after : GRU convention (whether to apply reset gate after or before matrix multiplication). False = \"before\" (default), True = \"after\" (CuDNN compatible). [source]","title":"GRUCell"},{"location":"2-Layers/5.recurrent/#lstmcell","text":"keras.layers.LSTMCell(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1) LSTM \u5c42\u7684\u5355\u5143\u7c7b\u3002 \u53c2\u6570 units : \u6b63\u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6\u3002 activation : \u8981\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u9ed8\u8ba4\uff1a\u53cc\u66f2\u6b63\u5207\uff08 tanh \uff09\u3002 \u5982\u679c\u4f20\u5165 None \uff0c\u5219\u4e0d\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570 (\u5373 \u7ebf\u6027\u6fc0\u6d3b\uff1a a(x) = x )\u3002 recurrent_activation : \u7528\u4e8e\u5faa\u73af\u65f6\u95f4\u6b65\u7684\u6fc0\u6d3b\u51fd\u6570 (\u8be6\u89c1 activations )\u3002 \u9ed8\u8ba4\uff1a\u5206\u6bb5\u7ebf\u6027\u8fd1\u4f3c sigmoid ( hard_sigmoid )\u3002 use_bias : \u5e03\u5c14\u503c\uff0c\u8be5\u5c42\u662f\u5426\u4f7f\u7528\u504f\u7f6e\u5411\u91cf\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668\uff0c \u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 recurrent_initializer : recurrent_kernel \u6743\u503c\u77e9\u9635 \u7684\u521d\u59cb\u5316\u5668\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 bias_initializer :\u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers ). unit_forget_bias : \u5e03\u5c14\u503c\u3002 \u5982\u679c\u4e3a True\uff0c\u521d\u59cb\u5316\u65f6\uff0c\u5c06\u5fd8\u8bb0\u95e8\u7684\u504f\u7f6e\u52a0 1\u3002 \u5c06\u5176\u8bbe\u7f6e\u4e3a True \u540c\u65f6\u8fd8\u4f1a\u5f3a\u5236 bias_initializer=\"zeros\" \u3002 \u8fd9\u4e2a\u5efa\u8bae\u6765\u81ea Jozefowicz et al. \u3002 kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 recurrent_regularizer : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 recurrent_constraint : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 dropout : \u5728 0 \u548c 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 \u5355\u5143\u7684\u4e22\u5f03\u6bd4\u4f8b\uff0c\u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362\u3002 recurrent_dropout : \u5728 0 \u548c 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 \u5355\u5143\u7684\u4e22\u5f03\u6bd4\u4f8b\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362\u3002 implementation : \u5b9e\u73b0\u6a21\u5f0f\uff0c1 \u6216 2\u3002 \u6a21\u5f0f 1 \u5c06\u628a\u5b83\u7684\u64cd\u4f5c\u7ed3\u6784\u5316\u4e3a\u66f4\u591a\u7684\u5c0f\u7684\u70b9\u79ef\u548c\u52a0\u6cd5\u64cd\u4f5c\uff0c \u800c\u6a21\u5f0f 2 \u5c06\u628a\u5b83\u4eec\u5206\u6279\u5230\u66f4\u5c11\uff0c\u66f4\u5927\u7684\u64cd\u4f5c\u4e2d\u3002 \u8fd9\u4e9b\u6a21\u5f0f\u5728\u4e0d\u540c\u7684\u786c\u4ef6\u548c\u4e0d\u540c\u7684\u5e94\u7528\u4e2d\u5177\u6709\u4e0d\u540c\u7684\u6027\u80fd\u914d\u7f6e\u6587\u4ef6\u3002 [source]","title":"LSTMCell"},{"location":"2-Layers/5.recurrent/#cudnngru","text":"keras.layers.CuDNNGRU(units, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, return_sequences=False, return_state=False, stateful=False) \u7531 CuDNN \u652f\u6301\u7684\u5feb\u901f GRU \u5b9e\u73b0\u3002 \u53ea\u80fd\u4ee5 TensorFlow \u540e\u7aef\u8fd0\u884c\u5728 GPU \u4e0a\u3002 \u53c2\u6570 units : \u6b63\u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668\uff0c \u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 recurrent_initializer : recurrent_kernel \u6743\u503c\u77e9\u9635 \u7684\u521d\u59cb\u5316\u5668\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 bias_initializer :\u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers ). kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 recurrent_regularizer : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 recurrent_constraint : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 return_sequences : \u5e03\u5c14\u503c\u3002\u662f\u8fd4\u56de\u8f93\u51fa\u5e8f\u5217\u4e2d\u7684\u6700\u540e\u4e00\u4e2a\u8f93\u51fa\uff0c\u8fd8\u662f\u5168\u90e8\u5e8f\u5217\u3002 return_state : \u5e03\u5c14\u503c\u3002\u9664\u4e86\u8f93\u51fa\u4e4b\u5916\u662f\u5426\u8fd4\u56de\u6700\u540e\u4e00\u4e2a\u72b6\u6001\u3002 stateful : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u6279\u6b21\u4e2d\u7d22\u5f15 i \u5904\u7684\u6bcf\u4e2a\u6837\u54c1\u7684\u6700\u540e\u72b6\u6001 \u5c06\u7528\u4f5c\u4e0b\u4e00\u6279\u6b21\u4e2d\u7d22\u5f15 i \u6837\u54c1\u7684\u521d\u59cb\u72b6\u6001\u3002 [source]","title":"CuDNNGRU"},{"location":"2-Layers/5.recurrent/#cudnnlstm","text":"keras.layers.CuDNNLSTM(units, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, return_sequences=False, return_state=False, stateful=False) \u7531 CuDNN \u652f\u6301\u7684\u5feb\u901f LSTM \u5b9e\u73b0\u3002 \u53ea\u80fd\u4ee5 TensorFlow \u540e\u7aef\u8fd0\u884c\u5728 GPU \u4e0a\u3002 \u53c2\u6570 units : \u6b63\u6574\u6570\uff0c\u8f93\u51fa\u7a7a\u95f4\u7684\u7ef4\u5ea6\u3002 kernel_initializer : kernel \u6743\u503c\u77e9\u9635\u7684\u521d\u59cb\u5316\u5668\uff0c\u7528\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u8f6c\u6362(\u8be6\u89c1 initializers )\u3002 unit_forget_bias : \u5e03\u5c14\u503c\u3002 \u5982\u679c\u4e3a True\uff0c\u521d\u59cb\u5316\u65f6\uff0c\u5c06\u5fd8\u8bb0\u95e8\u7684\u504f\u7f6e\u52a0 1\u3002 \u5c06\u5176\u8bbe\u7f6e\u4e3a True \u540c\u65f6\u8fd8\u4f1a\u5f3a\u5236 bias_initializer=\"zeros\" \u3002 \u8fd9\u4e2a\u5efa\u8bae\u6765\u81ea Jozefowicz et al. \u3002 recurrent_initializer : recurrent_kernel \u6743\u503c\u77e9\u9635 \u7684\u521d\u59cb\u5316\u5668\uff0c\u7528\u4e8e\u5faa\u73af\u5c42\u72b6\u6001\u7684\u7ebf\u6027\u8f6c\u6362 (\u8be6\u89c1 initializers )\u3002 bias_initializer :\u504f\u7f6e\u5411\u91cf\u7684\u521d\u59cb\u5316\u5668 (\u8be6\u89c1 initializers ). kernel_regularizer : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 recurrent_regularizer : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 bias_regularizer : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 activity_regularizer : \u8fd0\u7528\u5230\u5c42\u8f93\u51fa\uff08\u5b83\u7684\u6fc0\u6d3b\u503c\uff09\u7684\u6b63\u5219\u5316\u51fd\u6570 (\u8be6\u89c1 regularizer )\u3002 kernel_constraint : \u8fd0\u7528\u5230 kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 recurrent_constraint : \u8fd0\u7528\u5230 recurrent_kernel \u6743\u503c\u77e9\u9635\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 bias_constraint : \u8fd0\u7528\u5230\u504f\u7f6e\u5411\u91cf\u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 return_sequences : \u5e03\u5c14\u503c\u3002\u662f\u8fd4\u56de\u8f93\u51fa\u5e8f\u5217\u4e2d\u7684\u6700\u540e\u4e00\u4e2a\u8f93\u51fa\uff0c\u8fd8\u662f\u5168\u90e8\u5e8f\u5217\u3002 return_state : \u5e03\u5c14\u503c\u3002\u9664\u4e86\u8f93\u51fa\u4e4b\u5916\u662f\u5426\u8fd4\u56de\u6700\u540e\u4e00\u4e2a\u72b6\u6001\u3002 stateful : \u5e03\u5c14\u503c (\u9ed8\u8ba4 False)\u3002 \u5982\u679c\u4e3a True\uff0c\u5219\u6279\u6b21\u4e2d\u7d22\u5f15 i \u5904\u7684\u6bcf\u4e2a\u6837\u54c1\u7684\u6700\u540e\u72b6\u6001 \u5c06\u7528\u4f5c\u4e0b\u4e00\u6279\u6b21\u4e2d\u7d22\u5f15 i \u6837\u54c1\u7684\u521d\u59cb\u72b6\u6001\u3002","title":"CuDNNLSTM"},{"location":"2-Layers/6.embeddings/","text":"\u5d4c\u5165\u5c42 [source] Embedding keras.layers.Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None) \u5c06\u6b63\u6574\u6570\uff08\u7d22\u5f15\u503c\uff09\u8f6c\u6362\u4e3a\u56fa\u5b9a\u5c3a\u5bf8\u7684\u7a20\u5bc6\u5411\u91cf\u3002 \u4f8b\u5982\uff1a [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]] \u8be5\u5c42\u53ea\u80fd\u7528\u4f5c\u6a21\u578b\u4e2d\u7684\u7b2c\u4e00\u5c42\u3002 \u4f8b\u5b50 model = Sequential() model.add(Embedding(1000, 64, input_length=10)) # \u6a21\u578b\u5c06\u8f93\u5165\u4e00\u4e2a\u5927\u5c0f\u4e3a (batch, input_length) \u7684\u6574\u6570\u77e9\u9635\u3002 # \u8f93\u5165\u4e2d\u6700\u5927\u7684\u6574\u6570\uff08\u5373\u8bcd\u7d22\u5f15\uff09\u4e0d\u5e94\u8be5\u5927\u4e8e 999 \uff08\u8bcd\u6c47\u8868\u5927\u5c0f\uff09 # \u73b0\u5728 model.output_shape == (None, 10, 64)\uff0c\u5176\u4e2d None \u662f batch \u7684\u7ef4\u5ea6\u3002 input_array = np.random.randint(1000, size=(32, 10)) model.compile('rmsprop', 'mse') output_array = model.predict(input_array) assert output_array.shape == (32, 10, 64) \u53c2\u6570 input_dim : int > 0\u3002\u8bcd\u6c47\u8868\u5927\u5c0f\uff0c \u5373\uff0c\u6700\u5927\u6574\u6570 index + 1\u3002 output_dim : int >= 0\u3002\u8bcd\u5411\u91cf\u7684\u7ef4\u5ea6\u3002 embeddings_initializer : embeddings \u77e9\u9635\u7684\u521d\u59cb\u5316\u65b9\u6cd5 (\u8be6\u89c1 initializers )\u3002 embeddings_regularizer : embeddings matrix \u7684\u6b63\u5219\u5316\u65b9\u6cd5 (\u8be6\u89c1 regularizer )\u3002 embeddings_constraint : embeddings matrix \u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 mask_zero : \u662f\u5426\u628a 0 \u770b\u4f5c\u4e3a\u4e00\u4e2a\u5e94\u8be5\u88ab\u906e\u853d\u7684\u7279\u6b8a\u7684 \"padding\" \u503c\u3002 \u8fd9\u5bf9\u4e8e\u53ef\u53d8\u957f\u7684 \u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u5c42 \u5341\u5206\u6709\u7528\u3002 \u5982\u679c\u8bbe\u5b9a\u4e3a True \uff0c\u90a3\u4e48\u63a5\u4e0b\u6765\u7684\u6240\u6709\u5c42\u90fd\u5fc5\u987b\u652f\u6301 masking\uff0c\u5426\u5219\u5c31\u4f1a\u629b\u51fa\u5f02\u5e38\u3002 \u5982\u679c mask_zero \u4e3a True \uff0c\u4f5c\u4e3a\u7ed3\u679c\uff0c\u7d22\u5f15 0 \u5c31\u4e0d\u80fd\u88ab\u7528\u4e8e\u8bcd\u6c47\u8868\u4e2d \uff08input_dim \u5e94\u8be5\u4e0e vocabulary + 1 \u5927\u5c0f\u76f8\u540c\uff09\u3002 input_length : \u8f93\u5165\u5e8f\u5217\u7684\u957f\u5ea6\uff0c\u5f53\u5b83\u662f\u56fa\u5b9a\u7684\u65f6\u3002 \u5982\u679c\u4f60\u9700\u8981\u8fde\u63a5 Flatten \u548c Dense \u5c42\uff0c\u5219\u8fd9\u4e2a\u53c2\u6570\u662f\u5fc5\u987b\u7684 \uff08\u6ca1\u6709\u5b83\uff0cdense \u5c42\u7684\u8f93\u51fa\u5c3a\u5bf8\u5c31\u65e0\u6cd5\u8ba1\u7b97\uff09\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5c3a\u5bf8\u4e3a (batch_size, sequence_length) \u7684 2D \u5f20\u91cf\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u5c3a\u5bf8\u4e3a (batch_size, sequence_length, output_dim) \u7684 3D \u5f20\u91cf\u3002 \u53c2\u8003\u6587\u732e A Theoretically Grounded Application of Dropout in Recurrent Neural Networks","title":"\u5d4c\u5165\u5c42"},{"location":"2-Layers/6.embeddings/#_1","text":"[source]","title":"\u5d4c\u5165\u5c42"},{"location":"2-Layers/6.embeddings/#embedding","text":"keras.layers.Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None) \u5c06\u6b63\u6574\u6570\uff08\u7d22\u5f15\u503c\uff09\u8f6c\u6362\u4e3a\u56fa\u5b9a\u5c3a\u5bf8\u7684\u7a20\u5bc6\u5411\u91cf\u3002 \u4f8b\u5982\uff1a [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]] \u8be5\u5c42\u53ea\u80fd\u7528\u4f5c\u6a21\u578b\u4e2d\u7684\u7b2c\u4e00\u5c42\u3002 \u4f8b\u5b50 model = Sequential() model.add(Embedding(1000, 64, input_length=10)) # \u6a21\u578b\u5c06\u8f93\u5165\u4e00\u4e2a\u5927\u5c0f\u4e3a (batch, input_length) \u7684\u6574\u6570\u77e9\u9635\u3002 # \u8f93\u5165\u4e2d\u6700\u5927\u7684\u6574\u6570\uff08\u5373\u8bcd\u7d22\u5f15\uff09\u4e0d\u5e94\u8be5\u5927\u4e8e 999 \uff08\u8bcd\u6c47\u8868\u5927\u5c0f\uff09 # \u73b0\u5728 model.output_shape == (None, 10, 64)\uff0c\u5176\u4e2d None \u662f batch \u7684\u7ef4\u5ea6\u3002 input_array = np.random.randint(1000, size=(32, 10)) model.compile('rmsprop', 'mse') output_array = model.predict(input_array) assert output_array.shape == (32, 10, 64) \u53c2\u6570 input_dim : int > 0\u3002\u8bcd\u6c47\u8868\u5927\u5c0f\uff0c \u5373\uff0c\u6700\u5927\u6574\u6570 index + 1\u3002 output_dim : int >= 0\u3002\u8bcd\u5411\u91cf\u7684\u7ef4\u5ea6\u3002 embeddings_initializer : embeddings \u77e9\u9635\u7684\u521d\u59cb\u5316\u65b9\u6cd5 (\u8be6\u89c1 initializers )\u3002 embeddings_regularizer : embeddings matrix \u7684\u6b63\u5219\u5316\u65b9\u6cd5 (\u8be6\u89c1 regularizer )\u3002 embeddings_constraint : embeddings matrix \u7684\u7ea6\u675f\u51fd\u6570 (\u8be6\u89c1 constraints )\u3002 mask_zero : \u662f\u5426\u628a 0 \u770b\u4f5c\u4e3a\u4e00\u4e2a\u5e94\u8be5\u88ab\u906e\u853d\u7684\u7279\u6b8a\u7684 \"padding\" \u503c\u3002 \u8fd9\u5bf9\u4e8e\u53ef\u53d8\u957f\u7684 \u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u5c42 \u5341\u5206\u6709\u7528\u3002 \u5982\u679c\u8bbe\u5b9a\u4e3a True \uff0c\u90a3\u4e48\u63a5\u4e0b\u6765\u7684\u6240\u6709\u5c42\u90fd\u5fc5\u987b\u652f\u6301 masking\uff0c\u5426\u5219\u5c31\u4f1a\u629b\u51fa\u5f02\u5e38\u3002 \u5982\u679c mask_zero \u4e3a True \uff0c\u4f5c\u4e3a\u7ed3\u679c\uff0c\u7d22\u5f15 0 \u5c31\u4e0d\u80fd\u88ab\u7528\u4e8e\u8bcd\u6c47\u8868\u4e2d \uff08input_dim \u5e94\u8be5\u4e0e vocabulary + 1 \u5927\u5c0f\u76f8\u540c\uff09\u3002 input_length : \u8f93\u5165\u5e8f\u5217\u7684\u957f\u5ea6\uff0c\u5f53\u5b83\u662f\u56fa\u5b9a\u7684\u65f6\u3002 \u5982\u679c\u4f60\u9700\u8981\u8fde\u63a5 Flatten \u548c Dense \u5c42\uff0c\u5219\u8fd9\u4e2a\u53c2\u6570\u662f\u5fc5\u987b\u7684 \uff08\u6ca1\u6709\u5b83\uff0cdense \u5c42\u7684\u8f93\u51fa\u5c3a\u5bf8\u5c31\u65e0\u6cd5\u8ba1\u7b97\uff09\u3002 \u8f93\u5165\u5c3a\u5bf8 \u5c3a\u5bf8\u4e3a (batch_size, sequence_length) \u7684 2D \u5f20\u91cf\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u5c3a\u5bf8\u4e3a (batch_size, sequence_length, output_dim) \u7684 3D \u5f20\u91cf\u3002 \u53c2\u8003\u6587\u732e A Theoretically Grounded Application of Dropout in Recurrent Neural Networks","title":"Embedding"},{"location":"2-Layers/7.merge/","text":"\u878d\u5408\u5c42 [source] Add keras.layers.Add() \u8ba1\u7b97\u8f93\u5165\u5f20\u91cf\u5217\u8868\u7684\u548c\u3002 \u5b83\u63a5\u53d7\u4e00\u4e2a\u5f20\u91cf\u7684\u5217\u8868\uff0c \u6240\u6709\u7684\u5f20\u91cf\u5fc5\u987b\u6709\u76f8\u540c\u7684\u8f93\u5165\u5c3a\u5bf8\uff0c \u7136\u540e\u8fd4\u56de\u4e00\u4e2a\u5f20\u91cf\uff08\u548c\u8f93\u5165\u5f20\u91cf\u5c3a\u5bf8\u76f8\u540c\uff09\u3002 \u4f8b\u5b50 import keras input1 = keras.layers.Input(shape=(16,)) x1 = keras.layers.Dense(8, activation='relu')(input1) input2 = keras.layers.Input(shape=(32,)) x2 = keras.layers.Dense(8, activation='relu')(input2) # \u76f8\u5f53\u4e8e added = keras.layers.add([x1, x2]) added = keras.layers.Add()([x1, x2]) out = keras.layers.Dense(4)(added) model = keras.models.Model(inputs=[input1, input2], outputs=out) [source] Subtract keras.layers.Subtract() \u8ba1\u7b97\u4e24\u4e2a\u8f93\u5165\u5f20\u91cf\u7684\u5dee\u3002 \u5b83\u63a5\u53d7\u4e00\u4e2a\u957f\u5ea6\u4e3a 2 \u7684\u5f20\u91cf\u5217\u8868\uff0c \u4e24\u4e2a\u5f20\u91cf\u5fc5\u987b\u6709\u76f8\u540c\u7684\u5c3a\u5bf8\uff0c\u7136\u540e\u8fd4\u56de\u4e00\u4e2a\u503c\u4e3a (inputs[0] - inputs[1]) \u7684\u5f20\u91cf\uff0c \u8f93\u51fa\u5f20\u91cf\u548c\u8f93\u5165\u5f20\u91cf\u5c3a\u5bf8\u76f8\u540c\u3002 \u4f8b\u5b50 import keras input1 = keras.layers.Input(shape=(16,)) x1 = keras.layers.Dense(8, activation='relu')(input1) input2 = keras.layers.Input(shape=(32,)) x2 = keras.layers.Dense(8, activation='relu')(input2) # \u76f8\u5f53\u4e8e subtracted = keras.layers.subtract([x1, x2]) subtracted = keras.layers.Subtract()([x1, x2]) out = keras.layers.Dense(4)(subtracted) model = keras.models.Model(inputs=[input1, input2], outputs=out) [source] Multiply keras.layers.Multiply() \u8ba1\u7b97\u8f93\u5165\u5f20\u91cf\u5217\u8868\u7684\uff08\u9010\u5143\u7d20\u95f4\u7684\uff09\u4e58\u79ef\u3002 \u5b83\u63a5\u53d7\u4e00\u4e2a\u5f20\u91cf\u7684\u5217\u8868\uff0c \u6240\u6709\u7684\u5f20\u91cf\u5fc5\u987b\u6709\u76f8\u540c\u7684\u8f93\u5165\u5c3a\u5bf8\uff0c \u7136\u540e\u8fd4\u56de\u4e00\u4e2a\u5f20\u91cf\uff08\u548c\u8f93\u5165\u5f20\u91cf\u5c3a\u5bf8\u76f8\u540c\uff09\u3002 [[source]] [source] Average keras.layers.Average() \u8ba1\u7b97\u8f93\u5165\u5f20\u91cf\u5217\u8868\u7684\u5e73\u5747\u503c\u3002 \u5b83\u63a5\u53d7\u4e00\u4e2a\u5f20\u91cf\u7684\u5217\u8868\uff0c \u6240\u6709\u7684\u5f20\u91cf\u5fc5\u987b\u6709\u76f8\u540c\u7684\u8f93\u5165\u5c3a\u5bf8\uff0c \u7136\u540e\u8fd4\u56de\u4e00\u4e2a\u5f20\u91cf\uff08\u548c\u8f93\u5165\u5f20\u91cf\u5c3a\u5bf8\u76f8\u540c\uff09\u3002 [source] Maximum keras.layers.Maximum() \u8ba1\u7b97\u8f93\u5165\u5f20\u91cf\u5217\u8868\u7684\uff08\u9010\u5143\u7d20\u95f4\u7684\uff09\u6700\u5927\u503c\u3002 \u5b83\u63a5\u53d7\u4e00\u4e2a\u5f20\u91cf\u7684\u5217\u8868\uff0c \u6240\u6709\u7684\u5f20\u91cf\u5fc5\u987b\u6709\u76f8\u540c\u7684\u8f93\u5165\u5c3a\u5bf8\uff0c \u7136\u540e\u8fd4\u56de\u4e00\u4e2a\u5f20\u91cf\uff08\u548c\u8f93\u5165\u5f20\u91cf\u5c3a\u5bf8\u76f8\u540c\uff09\u3002 [source] Concatenate keras.layers.Concatenate(axis=-1) \u8fde\u63a5\u4e00\u4e2a\u8f93\u5165\u5f20\u91cf\u7684\u5217\u8868\u3002 \u5b83\u63a5\u53d7\u4e00\u4e2a\u5f20\u91cf\u7684\u5217\u8868\uff0c \u9664\u4e86\u8fde\u63a5\u8f74\u4e4b\u5916\uff0c\u5176\u4ed6\u7684\u5c3a\u5bf8\u90fd\u5fc5\u987b\u76f8\u540c\uff0c \u7136\u540e\u8fd4\u56de\u4e00\u4e2a\u7531\u6240\u6709\u8f93\u5165\u5f20\u91cf\u8fde\u63a5\u8d77\u6765\u7684\u8f93\u51fa\u5f20\u91cf\u3002 \u53c2\u6570 axis : \u8fde\u63a5\u7684\u8f74\u3002 **kwargs : \u5c42\u5173\u952e\u5b57\u53c2\u6570\u3002 [source] Dot keras.layers.Dot(axes, normalize=False) \u8ba1\u7b97\u4e24\u4e2a\u5f20\u91cf\u4e4b\u95f4\u6837\u672c\u7684\u70b9\u79ef\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u4f5c\u7528\u4e8e\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch_size, n) \u7684\u4e24\u4e2a\u5f20\u91cf a \u548c b \uff0c \u90a3\u4e48\u8f93\u51fa\u7ed3\u679c\u5c31\u4f1a\u662f\u5c3a\u5bf8\u4e3a (batch_size, 1) \u7684\u4e00\u4e2a\u5f20\u91cf\u3002 \u5728\u8fd9\u4e2a\u5f20\u91cf\u4e2d\uff0c\u6bcf\u4e00\u4e2a\u6761\u76ee i \u662f a[i] \u548c b[i] \u4e4b\u95f4\u7684\u70b9\u79ef\u3002 \u53c2\u6570 axes : \u6574\u6570\u6216\u8005\u6574\u6570\u5143\u7ec4\uff0c \u4e00\u4e2a\u6216\u8005\u51e0\u4e2a\u8fdb\u884c\u70b9\u79ef\u7684\u8f74\u3002 normalize : \u662f\u5426\u5728\u70b9\u79ef\u4e4b\u524d\u5bf9\u5373\u5c06\u8fdb\u884c\u70b9\u79ef\u7684\u8f74\u8fdb\u884c L2 \u6807\u51c6\u5316\u3002 \u5982\u679c\u8bbe\u7f6e\u6210 True \uff0c\u90a3\u4e48\u8f93\u51fa\u4e24\u4e2a\u6837\u672c\u4e4b\u95f4\u7684\u4f59\u5f26\u76f8\u4f3c\u503c\u3002 **kwargs : \u5c42\u5173\u952e\u5b57\u53c2\u6570\u3002 add keras.layers.add(inputs) Add \u5c42\u7684\u51fd\u6570\u5f0f\u63a5\u53e3\u3002 \u53c2\u6570 inputs : \u4e00\u4e2a\u8f93\u5165\u5f20\u91cf\u7684\u5217\u8868\uff08\u5217\u8868\u5927\u5c0f\u81f3\u5c11\u4e3a 2\uff09\u3002 **kwargs : \u5c42\u5173\u952e\u5b57\u53c2\u6570\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\uff0c\u6240\u6709\u8f93\u5165\u5f20\u91cf\u7684\u548c\u3002 \u4f8b\u5b50 import keras input1 = keras.layers.Input(shape=(16,)) x1 = keras.layers.Dense(8, activation='relu')(input1) input2 = keras.layers.Input(shape=(32,)) x2 = keras.layers.Dense(8, activation='relu')(input2) added = keras.layers.add([x1, x2]) out = keras.layers.Dense(4)(added) model = keras.models.Model(inputs=[input1, input2], outputs=out) subtract keras.layers.subtract(inputs) Subtract \u5c42\u7684\u51fd\u6570\u5f0f\u63a5\u53e3\u3002 \u53c2\u6570 inputs : \u4e00\u4e2a\u5217\u8868\u7684\u8f93\u5165\u5f20\u91cf\uff08\u5217\u8868\u5927\u5c0f\u51c6\u786e\u4e3a 2\uff09\u3002 **kwargs : \u5c42\u7684\u5173\u952e\u5b57\u53c2\u6570\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\uff0c\u4e24\u4e2a\u8f93\u5165\u5f20\u91cf\u7684\u5dee\u3002 \u4f8b\u5b50 import keras input1 = keras.layers.Input(shape=(16,)) x1 = keras.layers.Dense(8, activation='relu')(input1) input2 = keras.layers.Input(shape=(32,)) x2 = keras.layers.Dense(8, activation='relu')(input2) subtracted = keras.layers.subtract([x1, x2]) out = keras.layers.Dense(4)(subtracted) model = keras.models.Model(inputs=[input1, input2], outputs=out) multiply keras.layers.multiply(inputs) Multiply \u5c42\u7684\u51fd\u6570\u5f0f\u63a5\u53e3\u3002 \u53c2\u6570 inputs : \u4e00\u4e2a\u5217\u8868\u7684\u8f93\u5165\u5f20\u91cf\uff08\u5217\u8868\u5927\u5c0f\u81f3\u5c11\u4e3a 2\uff09\u3002 **kwargs : \u5c42\u7684\u5173\u952e\u5b57\u53c2\u6570\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\uff0c\u6240\u6709\u8f93\u5165\u5f20\u91cf\u7684\u9010\u5143\u7d20\u4e58\u79ef\u3002 average keras.layers.average(inputs) Average \u5c42\u7684\u51fd\u6570\u5f0f\u63a5\u53e3\u3002 \u53c2\u6570 inputs : \u4e00\u4e2a\u5217\u8868\u7684\u8f93\u5165\u5f20\u91cf\uff08\u5217\u8868\u5927\u5c0f\u81f3\u5c11\u4e3a 2\uff09\u3002 **kwargs : \u5c42\u7684\u5173\u952e\u5b57\u53c2\u6570\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\uff0c\u6240\u6709\u8f93\u5165\u5f20\u91cf\u7684\u5e73\u5747\u503c\u3002 maximum keras.layers.maximum(inputs) Maximum \u5c42\u7684\u51fd\u6570\u5f0f\u63a5\u53e3\u3002 \u53c2\u6570 inputs : \u4e00\u4e2a\u5217\u8868\u7684\u8f93\u5165\u5f20\u91cf\uff08\u5217\u8868\u5927\u5c0f\u81f3\u5c11\u4e3a 2\uff09\u3002 **kwargs : \u5c42\u7684\u5173\u952e\u5b57\u53c2\u6570\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\uff0c\u6240\u6709\u5f20\u91cf\u7684\u9010\u5143\u7d20\u7684\u6700\u5927\u503c\u3002 concatenate keras.layers.concatenate(inputs, axis=-1) Concatenate \u5c42\u7684\u51fd\u6570\u5f0f\u63a5\u53e3\u3002 \u53c2\u6570 inputs : \u4e00\u4e2a\u5217\u8868\u7684\u8f93\u5165\u5f20\u91cf\uff08\u5217\u8868\u5927\u5c0f\u81f3\u5c11\u4e3a 2\uff09\u3002 axis : \u4e32\u8054\u7684\u8f74\u3002 **kwargs : \u5c42\u7684\u5173\u952e\u5b57\u53c2\u6570\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\uff0c\u6240\u6709\u8f93\u5165\u5f20\u91cf\u901a\u8fc7 axis \u8f74\u4e32\u8054\u8d77\u6765\u7684\u8f93\u51fa\u5f20\u91cf\u3002 dot keras.layers.dot(inputs, axes, normalize=False) Dot \u5c42\u7684\u51fd\u6570\u5f0f\u63a5\u53e3\u3002 \u53c2\u6570 inputs : \u4e00\u4e2a\u5217\u8868\u7684\u8f93\u5165\u5f20\u91cf\uff08\u5217\u8868\u5927\u5c0f\u81f3\u5c11\u4e3a 2\uff09\u3002 axes : \u6574\u6570\u6216\u8005\u6574\u6570\u5143\u7ec4\uff0c \u4e00\u4e2a\u6216\u8005\u51e0\u4e2a\u8fdb\u884c\u70b9\u79ef\u7684\u8f74\u3002 normalize : \u662f\u5426\u5728\u70b9\u79ef\u4e4b\u524d\u5bf9\u5373\u5c06\u8fdb\u884c\u70b9\u79ef\u7684\u8f74\u8fdb\u884c L2 \u6807\u51c6\u5316\u3002 \u5982\u679c\u8bbe\u7f6e\u6210 True\uff0c\u90a3\u4e48\u8f93\u51fa\u4e24\u4e2a\u6837\u672c\u4e4b\u95f4\u7684\u4f59\u5f26\u76f8\u4f3c\u503c\u3002 **kwargs : \u5c42\u7684\u5173\u952e\u5b57\u53c2\u6570\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\uff0c\u6240\u6709\u8f93\u5165\u5f20\u91cf\u6837\u672c\u4e4b\u95f4\u7684\u70b9\u79ef\u3002","title":"\u878d\u5408\u5c42"},{"location":"2-Layers/7.merge/#_1","text":"[source]","title":"\u878d\u5408\u5c42"},{"location":"2-Layers/7.merge/#add","text":"keras.layers.Add() \u8ba1\u7b97\u8f93\u5165\u5f20\u91cf\u5217\u8868\u7684\u548c\u3002 \u5b83\u63a5\u53d7\u4e00\u4e2a\u5f20\u91cf\u7684\u5217\u8868\uff0c \u6240\u6709\u7684\u5f20\u91cf\u5fc5\u987b\u6709\u76f8\u540c\u7684\u8f93\u5165\u5c3a\u5bf8\uff0c \u7136\u540e\u8fd4\u56de\u4e00\u4e2a\u5f20\u91cf\uff08\u548c\u8f93\u5165\u5f20\u91cf\u5c3a\u5bf8\u76f8\u540c\uff09\u3002 \u4f8b\u5b50 import keras input1 = keras.layers.Input(shape=(16,)) x1 = keras.layers.Dense(8, activation='relu')(input1) input2 = keras.layers.Input(shape=(32,)) x2 = keras.layers.Dense(8, activation='relu')(input2) # \u76f8\u5f53\u4e8e added = keras.layers.add([x1, x2]) added = keras.layers.Add()([x1, x2]) out = keras.layers.Dense(4)(added) model = keras.models.Model(inputs=[input1, input2], outputs=out) [source]","title":"Add"},{"location":"2-Layers/7.merge/#subtract","text":"keras.layers.Subtract() \u8ba1\u7b97\u4e24\u4e2a\u8f93\u5165\u5f20\u91cf\u7684\u5dee\u3002 \u5b83\u63a5\u53d7\u4e00\u4e2a\u957f\u5ea6\u4e3a 2 \u7684\u5f20\u91cf\u5217\u8868\uff0c \u4e24\u4e2a\u5f20\u91cf\u5fc5\u987b\u6709\u76f8\u540c\u7684\u5c3a\u5bf8\uff0c\u7136\u540e\u8fd4\u56de\u4e00\u4e2a\u503c\u4e3a (inputs[0] - inputs[1]) \u7684\u5f20\u91cf\uff0c \u8f93\u51fa\u5f20\u91cf\u548c\u8f93\u5165\u5f20\u91cf\u5c3a\u5bf8\u76f8\u540c\u3002 \u4f8b\u5b50 import keras input1 = keras.layers.Input(shape=(16,)) x1 = keras.layers.Dense(8, activation='relu')(input1) input2 = keras.layers.Input(shape=(32,)) x2 = keras.layers.Dense(8, activation='relu')(input2) # \u76f8\u5f53\u4e8e subtracted = keras.layers.subtract([x1, x2]) subtracted = keras.layers.Subtract()([x1, x2]) out = keras.layers.Dense(4)(subtracted) model = keras.models.Model(inputs=[input1, input2], outputs=out) [source]","title":"Subtract"},{"location":"2-Layers/7.merge/#multiply","text":"keras.layers.Multiply() \u8ba1\u7b97\u8f93\u5165\u5f20\u91cf\u5217\u8868\u7684\uff08\u9010\u5143\u7d20\u95f4\u7684\uff09\u4e58\u79ef\u3002 \u5b83\u63a5\u53d7\u4e00\u4e2a\u5f20\u91cf\u7684\u5217\u8868\uff0c \u6240\u6709\u7684\u5f20\u91cf\u5fc5\u987b\u6709\u76f8\u540c\u7684\u8f93\u5165\u5c3a\u5bf8\uff0c \u7136\u540e\u8fd4\u56de\u4e00\u4e2a\u5f20\u91cf\uff08\u548c\u8f93\u5165\u5f20\u91cf\u5c3a\u5bf8\u76f8\u540c\uff09\u3002 [[source]] [source]","title":"Multiply"},{"location":"2-Layers/7.merge/#average","text":"keras.layers.Average() \u8ba1\u7b97\u8f93\u5165\u5f20\u91cf\u5217\u8868\u7684\u5e73\u5747\u503c\u3002 \u5b83\u63a5\u53d7\u4e00\u4e2a\u5f20\u91cf\u7684\u5217\u8868\uff0c \u6240\u6709\u7684\u5f20\u91cf\u5fc5\u987b\u6709\u76f8\u540c\u7684\u8f93\u5165\u5c3a\u5bf8\uff0c \u7136\u540e\u8fd4\u56de\u4e00\u4e2a\u5f20\u91cf\uff08\u548c\u8f93\u5165\u5f20\u91cf\u5c3a\u5bf8\u76f8\u540c\uff09\u3002 [source]","title":"Average"},{"location":"2-Layers/7.merge/#maximum","text":"keras.layers.Maximum() \u8ba1\u7b97\u8f93\u5165\u5f20\u91cf\u5217\u8868\u7684\uff08\u9010\u5143\u7d20\u95f4\u7684\uff09\u6700\u5927\u503c\u3002 \u5b83\u63a5\u53d7\u4e00\u4e2a\u5f20\u91cf\u7684\u5217\u8868\uff0c \u6240\u6709\u7684\u5f20\u91cf\u5fc5\u987b\u6709\u76f8\u540c\u7684\u8f93\u5165\u5c3a\u5bf8\uff0c \u7136\u540e\u8fd4\u56de\u4e00\u4e2a\u5f20\u91cf\uff08\u548c\u8f93\u5165\u5f20\u91cf\u5c3a\u5bf8\u76f8\u540c\uff09\u3002 [source]","title":"Maximum"},{"location":"2-Layers/7.merge/#concatenate","text":"keras.layers.Concatenate(axis=-1) \u8fde\u63a5\u4e00\u4e2a\u8f93\u5165\u5f20\u91cf\u7684\u5217\u8868\u3002 \u5b83\u63a5\u53d7\u4e00\u4e2a\u5f20\u91cf\u7684\u5217\u8868\uff0c \u9664\u4e86\u8fde\u63a5\u8f74\u4e4b\u5916\uff0c\u5176\u4ed6\u7684\u5c3a\u5bf8\u90fd\u5fc5\u987b\u76f8\u540c\uff0c \u7136\u540e\u8fd4\u56de\u4e00\u4e2a\u7531\u6240\u6709\u8f93\u5165\u5f20\u91cf\u8fde\u63a5\u8d77\u6765\u7684\u8f93\u51fa\u5f20\u91cf\u3002 \u53c2\u6570 axis : \u8fde\u63a5\u7684\u8f74\u3002 **kwargs : \u5c42\u5173\u952e\u5b57\u53c2\u6570\u3002 [source]","title":"Concatenate"},{"location":"2-Layers/7.merge/#dot","text":"keras.layers.Dot(axes, normalize=False) \u8ba1\u7b97\u4e24\u4e2a\u5f20\u91cf\u4e4b\u95f4\u6837\u672c\u7684\u70b9\u79ef\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u4f5c\u7528\u4e8e\u8f93\u5165\u5c3a\u5bf8\u4e3a (batch_size, n) \u7684\u4e24\u4e2a\u5f20\u91cf a \u548c b \uff0c \u90a3\u4e48\u8f93\u51fa\u7ed3\u679c\u5c31\u4f1a\u662f\u5c3a\u5bf8\u4e3a (batch_size, 1) \u7684\u4e00\u4e2a\u5f20\u91cf\u3002 \u5728\u8fd9\u4e2a\u5f20\u91cf\u4e2d\uff0c\u6bcf\u4e00\u4e2a\u6761\u76ee i \u662f a[i] \u548c b[i] \u4e4b\u95f4\u7684\u70b9\u79ef\u3002 \u53c2\u6570 axes : \u6574\u6570\u6216\u8005\u6574\u6570\u5143\u7ec4\uff0c \u4e00\u4e2a\u6216\u8005\u51e0\u4e2a\u8fdb\u884c\u70b9\u79ef\u7684\u8f74\u3002 normalize : \u662f\u5426\u5728\u70b9\u79ef\u4e4b\u524d\u5bf9\u5373\u5c06\u8fdb\u884c\u70b9\u79ef\u7684\u8f74\u8fdb\u884c L2 \u6807\u51c6\u5316\u3002 \u5982\u679c\u8bbe\u7f6e\u6210 True \uff0c\u90a3\u4e48\u8f93\u51fa\u4e24\u4e2a\u6837\u672c\u4e4b\u95f4\u7684\u4f59\u5f26\u76f8\u4f3c\u503c\u3002 **kwargs : \u5c42\u5173\u952e\u5b57\u53c2\u6570\u3002","title":"Dot"},{"location":"2-Layers/7.merge/#add_1","text":"keras.layers.add(inputs) Add \u5c42\u7684\u51fd\u6570\u5f0f\u63a5\u53e3\u3002 \u53c2\u6570 inputs : \u4e00\u4e2a\u8f93\u5165\u5f20\u91cf\u7684\u5217\u8868\uff08\u5217\u8868\u5927\u5c0f\u81f3\u5c11\u4e3a 2\uff09\u3002 **kwargs : \u5c42\u5173\u952e\u5b57\u53c2\u6570\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\uff0c\u6240\u6709\u8f93\u5165\u5f20\u91cf\u7684\u548c\u3002 \u4f8b\u5b50 import keras input1 = keras.layers.Input(shape=(16,)) x1 = keras.layers.Dense(8, activation='relu')(input1) input2 = keras.layers.Input(shape=(32,)) x2 = keras.layers.Dense(8, activation='relu')(input2) added = keras.layers.add([x1, x2]) out = keras.layers.Dense(4)(added) model = keras.models.Model(inputs=[input1, input2], outputs=out)","title":"add"},{"location":"2-Layers/7.merge/#subtract_1","text":"keras.layers.subtract(inputs) Subtract \u5c42\u7684\u51fd\u6570\u5f0f\u63a5\u53e3\u3002 \u53c2\u6570 inputs : \u4e00\u4e2a\u5217\u8868\u7684\u8f93\u5165\u5f20\u91cf\uff08\u5217\u8868\u5927\u5c0f\u51c6\u786e\u4e3a 2\uff09\u3002 **kwargs : \u5c42\u7684\u5173\u952e\u5b57\u53c2\u6570\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\uff0c\u4e24\u4e2a\u8f93\u5165\u5f20\u91cf\u7684\u5dee\u3002 \u4f8b\u5b50 import keras input1 = keras.layers.Input(shape=(16,)) x1 = keras.layers.Dense(8, activation='relu')(input1) input2 = keras.layers.Input(shape=(32,)) x2 = keras.layers.Dense(8, activation='relu')(input2) subtracted = keras.layers.subtract([x1, x2]) out = keras.layers.Dense(4)(subtracted) model = keras.models.Model(inputs=[input1, input2], outputs=out)","title":"subtract"},{"location":"2-Layers/7.merge/#multiply_1","text":"keras.layers.multiply(inputs) Multiply \u5c42\u7684\u51fd\u6570\u5f0f\u63a5\u53e3\u3002 \u53c2\u6570 inputs : \u4e00\u4e2a\u5217\u8868\u7684\u8f93\u5165\u5f20\u91cf\uff08\u5217\u8868\u5927\u5c0f\u81f3\u5c11\u4e3a 2\uff09\u3002 **kwargs : \u5c42\u7684\u5173\u952e\u5b57\u53c2\u6570\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\uff0c\u6240\u6709\u8f93\u5165\u5f20\u91cf\u7684\u9010\u5143\u7d20\u4e58\u79ef\u3002","title":"multiply"},{"location":"2-Layers/7.merge/#average_1","text":"keras.layers.average(inputs) Average \u5c42\u7684\u51fd\u6570\u5f0f\u63a5\u53e3\u3002 \u53c2\u6570 inputs : \u4e00\u4e2a\u5217\u8868\u7684\u8f93\u5165\u5f20\u91cf\uff08\u5217\u8868\u5927\u5c0f\u81f3\u5c11\u4e3a 2\uff09\u3002 **kwargs : \u5c42\u7684\u5173\u952e\u5b57\u53c2\u6570\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\uff0c\u6240\u6709\u8f93\u5165\u5f20\u91cf\u7684\u5e73\u5747\u503c\u3002","title":"average"},{"location":"2-Layers/7.merge/#maximum_1","text":"keras.layers.maximum(inputs) Maximum \u5c42\u7684\u51fd\u6570\u5f0f\u63a5\u53e3\u3002 \u53c2\u6570 inputs : \u4e00\u4e2a\u5217\u8868\u7684\u8f93\u5165\u5f20\u91cf\uff08\u5217\u8868\u5927\u5c0f\u81f3\u5c11\u4e3a 2\uff09\u3002 **kwargs : \u5c42\u7684\u5173\u952e\u5b57\u53c2\u6570\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\uff0c\u6240\u6709\u5f20\u91cf\u7684\u9010\u5143\u7d20\u7684\u6700\u5927\u503c\u3002","title":"maximum"},{"location":"2-Layers/7.merge/#concatenate_1","text":"keras.layers.concatenate(inputs, axis=-1) Concatenate \u5c42\u7684\u51fd\u6570\u5f0f\u63a5\u53e3\u3002 \u53c2\u6570 inputs : \u4e00\u4e2a\u5217\u8868\u7684\u8f93\u5165\u5f20\u91cf\uff08\u5217\u8868\u5927\u5c0f\u81f3\u5c11\u4e3a 2\uff09\u3002 axis : \u4e32\u8054\u7684\u8f74\u3002 **kwargs : \u5c42\u7684\u5173\u952e\u5b57\u53c2\u6570\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\uff0c\u6240\u6709\u8f93\u5165\u5f20\u91cf\u901a\u8fc7 axis \u8f74\u4e32\u8054\u8d77\u6765\u7684\u8f93\u51fa\u5f20\u91cf\u3002","title":"concatenate"},{"location":"2-Layers/7.merge/#dot_1","text":"keras.layers.dot(inputs, axes, normalize=False) Dot \u5c42\u7684\u51fd\u6570\u5f0f\u63a5\u53e3\u3002 \u53c2\u6570 inputs : \u4e00\u4e2a\u5217\u8868\u7684\u8f93\u5165\u5f20\u91cf\uff08\u5217\u8868\u5927\u5c0f\u81f3\u5c11\u4e3a 2\uff09\u3002 axes : \u6574\u6570\u6216\u8005\u6574\u6570\u5143\u7ec4\uff0c \u4e00\u4e2a\u6216\u8005\u51e0\u4e2a\u8fdb\u884c\u70b9\u79ef\u7684\u8f74\u3002 normalize : \u662f\u5426\u5728\u70b9\u79ef\u4e4b\u524d\u5bf9\u5373\u5c06\u8fdb\u884c\u70b9\u79ef\u7684\u8f74\u8fdb\u884c L2 \u6807\u51c6\u5316\u3002 \u5982\u679c\u8bbe\u7f6e\u6210 True\uff0c\u90a3\u4e48\u8f93\u51fa\u4e24\u4e2a\u6837\u672c\u4e4b\u95f4\u7684\u4f59\u5f26\u76f8\u4f3c\u503c\u3002 **kwargs : \u5c42\u7684\u5173\u952e\u5b57\u53c2\u6570\u3002 \u8fd4\u56de \u4e00\u4e2a\u5f20\u91cf\uff0c\u6240\u6709\u8f93\u5165\u5f20\u91cf\u6837\u672c\u4e4b\u95f4\u7684\u70b9\u79ef\u3002","title":"dot"},{"location":"2-Layers/8.advanced-activations/","text":"\u9ad8\u7ea7\u6fc0\u6d3b\u5c42 [source] LeakyReLU keras.layers.LeakyReLU(alpha=0.3) \u5e26\u6cc4\u6f0f\u7684 ReLU\u3002 \u5f53\u795e\u7ecf\u5143\u672a\u6fc0\u6d3b\u65f6\uff0c\u5b83\u4ecd\u5141\u8bb8\u8d4b\u4e88\u4e00\u4e2a\u5f88\u5c0f\u7684\u68af\u5ea6\uff1a f(x) = alpha * x for x < 0 , f(x) = x for x >= 0 . \u8f93\u5165\u5c3a\u5bf8 \u53ef\u4ee5\u662f\u4efb\u610f\u7684\u3002\u5982\u679c\u5c06\u8be5\u5c42\u4f5c\u4e3a\u6a21\u578b\u7684\u7b2c\u4e00\u5c42\uff0c \u5219\u9700\u8981\u6307\u5b9a input_shape \u53c2\u6570 \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u542b\u6837\u672c\u6570\u91cf\u7684\u7ef4\u5ea6\uff09\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u76f8\u540c\u3002 \u53c2\u6570 alpha : float >= 0\u3002\u8d1f\u659c\u7387\u7cfb\u6570\u3002 \u53c2\u8003\u6587\u732e Rectifier Nonlinearities Improve Neural Network Acoustic Models [source] PReLU keras.layers.PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None) \u53c2\u6570\u5316\u7684 ReLU\u3002 \u5f62\u5f0f\uff1a f(x) = alpha * x for x < 0 , f(x) = x for x >= 0 , \u5176\u4e2d alpha \u662f\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u6570\u7ec4\uff0c\u5c3a\u5bf8\u4e0e x \u76f8\u540c\u3002 \u8f93\u5165\u5c3a\u5bf8 \u53ef\u4ee5\u662f\u4efb\u610f\u7684\u3002\u5982\u679c\u5c06\u8fd9\u4e00\u5c42\u4f5c\u4e3a\u6a21\u578b\u7684\u7b2c\u4e00\u5c42\uff0c \u5219\u9700\u8981\u6307\u5b9a input_shape \u53c2\u6570 \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u542b\u6837\u672c\u6570\u91cf\u7684\u7ef4\u5ea6\uff09\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u76f8\u540c\u3002 \u53c2\u6570 alpha_initializer : \u6743\u91cd\u7684\u521d\u59cb\u5316\u51fd\u6570\u3002 alpha_regularizer : \u6743\u91cd\u7684\u6b63\u5219\u5316\u65b9\u6cd5\u3002 alpha_constraint : \u6743\u91cd\u7684\u7ea6\u675f\u3002 shared_axes : \u6fc0\u6d3b\u51fd\u6570\u5171\u4eab\u53ef\u5b66\u4e60\u53c2\u6570\u7684\u8f74\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u8f93\u5165\u7279\u5f81\u56fe\u6765\u81ea\u8f93\u51fa\u5f62\u72b6\u4e3a (batch, height, width, channels) \u7684 2D \u5377\u79ef\u5c42\uff0c\u800c\u4e14\u4f60\u5e0c\u671b\u8de8\u7a7a\u95f4\u5171\u4eab\u53c2\u6570\uff0c\u4ee5\u4fbf\u6bcf\u4e2a\u6ee4\u6ce2\u5668\u53ea\u6709\u4e00\u7ec4\u53c2\u6570\uff0c \u53ef\u8bbe\u7f6e shared_axes=[1, 2] \u3002 \u53c2\u8003\u6587\u732e Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification [source] ELU keras.layers.ELU(alpha=1.0) \u6307\u6570\u7ebf\u6027\u5355\u5143\u3002 \u5f62\u5f0f\uff1a f(x) = alpha * (exp(x) - 1.) for x < 0 , f(x) = x for x >= 0 . \u8f93\u5165\u5c3a\u5bf8 \u53ef\u4ee5\u662f\u4efb\u610f\u7684\u3002\u5982\u679c\u5c06\u8fd9\u4e00\u5c42\u4f5c\u4e3a\u6a21\u578b\u7684\u7b2c\u4e00\u5c42\uff0c \u5219\u9700\u8981\u6307\u5b9a input_shape \u53c2\u6570 \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u542b\u6837\u672c\u6570\u91cf\u7684\u7ef4\u5ea6\uff09\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u76f8\u540c\u3002 \u53c2\u6570 alpha : \u8d1f\u56e0\u5b50\u7684\u5c3a\u5ea6\u3002 \u53c2\u8003\u6587\u732e Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) [source] ThresholdedReLU keras.layers.ThresholdedReLU(theta=1.0) \u5e26\u9608\u503c\u7684\u4fee\u6b63\u7ebf\u6027\u5355\u5143\u3002 \u5f62\u5f0f\uff1a f(x) = x for x > theta , f(x) = 0 otherwise . \u8f93\u5165\u5c3a\u5bf8 \u53ef\u4ee5\u662f\u4efb\u610f\u7684\u3002\u5982\u679c\u5c06\u8fd9\u4e00\u5c42\u4f5c\u4e3a\u6a21\u578b\u7684\u7b2c\u4e00\u5c42\uff0c \u5219\u9700\u8981\u6307\u5b9a input_shape \u53c2\u6570 \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u542b\u6837\u672c\u6570\u91cf\u7684\u7ef4\u5ea6\uff09\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u76f8\u540c\u3002 \u53c2\u6570 theta : float >= 0\u3002\u6fc0\u6d3b\u7684\u9608\u503c\u4f4d\u3002 \u53c2\u8003\u6587\u732e Zero-Bias Autoencoders and the Benefits of Co-Adapting Features [source] Softmax keras.layers.Softmax(axis=-1) Softmax \u6fc0\u6d3b\u51fd\u6570\u3002 \u8f93\u5165\u5c3a\u5bf8 \u53ef\u4ee5\u662f\u4efb\u610f\u7684\u3002\u5982\u679c\u5c06\u8fd9\u4e00\u5c42\u4f5c\u4e3a\u6a21\u578b\u7684\u7b2c\u4e00\u5c42\uff0c \u5219\u9700\u8981\u6307\u5b9a input_shape \u53c2\u6570 \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u542b\u6837\u672c\u6570\u91cf\u7684\u7ef4\u5ea6\uff09\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u76f8\u540c\u3002 \u53c2\u6570 axis : \u6574\u6570\uff0c\u5e94\u7528 softmax \u6807\u51c6\u5316\u7684\u8f74\u3002 [source] ReLU keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0) ReLU \u6fc0\u6d3b\u51fd\u6570\u3002 \u4f7f\u7528\u9ed8\u8ba4\u503c\u65f6\uff0c\u5b83\u8fd4\u56de\u9010\u4e2a\u5143\u7d20\u7684 max(x\uff0c0) \u3002 \u5426\u5219\uff1a \u5982\u679c x >= max_value \uff0c\u8fd4\u56de f(x) = max_value \uff0c \u5982\u679c threshold <= x < max_value \uff0c\u8fd4\u56de f(x) = x , \u5426\u5219\uff0c\u8fd4\u56de f(x) = negative_slope * (x - threshold) \u3002 \u8f93\u5165\u5c3a\u5bf8 \u53ef\u4ee5\u662f\u4efb\u610f\u7684\u3002\u5982\u679c\u5c06\u8fd9\u4e00\u5c42\u4f5c\u4e3a\u6a21\u578b\u7684\u7b2c\u4e00\u5c42\uff0c \u5219\u9700\u8981\u6307\u5b9a input_shape \u53c2\u6570 \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u542b\u6837\u672c\u6570\u91cf\u7684\u7ef4\u5ea6\uff09\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u76f8\u540c\u3002 \u53c2\u6570 max_value : \u6d6e\u70b9\u6570\uff0c\u6700\u5927\u7684\u8f93\u51fa\u503c\u3002 negative_slope : float >= 0. \u8d1f\u659c\u7387\u7cfb\u6570\u3002 threshold : float\u3002\"thresholded activation\" \u7684\u9608\u503c\u3002","title":"\u9ad8\u7ea7\u6fc0\u6d3b\u5c42"},{"location":"2-Layers/8.advanced-activations/#_1","text":"[source]","title":"\u9ad8\u7ea7\u6fc0\u6d3b\u5c42"},{"location":"2-Layers/8.advanced-activations/#leakyrelu","text":"keras.layers.LeakyReLU(alpha=0.3) \u5e26\u6cc4\u6f0f\u7684 ReLU\u3002 \u5f53\u795e\u7ecf\u5143\u672a\u6fc0\u6d3b\u65f6\uff0c\u5b83\u4ecd\u5141\u8bb8\u8d4b\u4e88\u4e00\u4e2a\u5f88\u5c0f\u7684\u68af\u5ea6\uff1a f(x) = alpha * x for x < 0 , f(x) = x for x >= 0 . \u8f93\u5165\u5c3a\u5bf8 \u53ef\u4ee5\u662f\u4efb\u610f\u7684\u3002\u5982\u679c\u5c06\u8be5\u5c42\u4f5c\u4e3a\u6a21\u578b\u7684\u7b2c\u4e00\u5c42\uff0c \u5219\u9700\u8981\u6307\u5b9a input_shape \u53c2\u6570 \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u542b\u6837\u672c\u6570\u91cf\u7684\u7ef4\u5ea6\uff09\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u76f8\u540c\u3002 \u53c2\u6570 alpha : float >= 0\u3002\u8d1f\u659c\u7387\u7cfb\u6570\u3002 \u53c2\u8003\u6587\u732e Rectifier Nonlinearities Improve Neural Network Acoustic Models [source]","title":"LeakyReLU"},{"location":"2-Layers/8.advanced-activations/#prelu","text":"keras.layers.PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None) \u53c2\u6570\u5316\u7684 ReLU\u3002 \u5f62\u5f0f\uff1a f(x) = alpha * x for x < 0 , f(x) = x for x >= 0 , \u5176\u4e2d alpha \u662f\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u6570\u7ec4\uff0c\u5c3a\u5bf8\u4e0e x \u76f8\u540c\u3002 \u8f93\u5165\u5c3a\u5bf8 \u53ef\u4ee5\u662f\u4efb\u610f\u7684\u3002\u5982\u679c\u5c06\u8fd9\u4e00\u5c42\u4f5c\u4e3a\u6a21\u578b\u7684\u7b2c\u4e00\u5c42\uff0c \u5219\u9700\u8981\u6307\u5b9a input_shape \u53c2\u6570 \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u542b\u6837\u672c\u6570\u91cf\u7684\u7ef4\u5ea6\uff09\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u76f8\u540c\u3002 \u53c2\u6570 alpha_initializer : \u6743\u91cd\u7684\u521d\u59cb\u5316\u51fd\u6570\u3002 alpha_regularizer : \u6743\u91cd\u7684\u6b63\u5219\u5316\u65b9\u6cd5\u3002 alpha_constraint : \u6743\u91cd\u7684\u7ea6\u675f\u3002 shared_axes : \u6fc0\u6d3b\u51fd\u6570\u5171\u4eab\u53ef\u5b66\u4e60\u53c2\u6570\u7684\u8f74\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u8f93\u5165\u7279\u5f81\u56fe\u6765\u81ea\u8f93\u51fa\u5f62\u72b6\u4e3a (batch, height, width, channels) \u7684 2D \u5377\u79ef\u5c42\uff0c\u800c\u4e14\u4f60\u5e0c\u671b\u8de8\u7a7a\u95f4\u5171\u4eab\u53c2\u6570\uff0c\u4ee5\u4fbf\u6bcf\u4e2a\u6ee4\u6ce2\u5668\u53ea\u6709\u4e00\u7ec4\u53c2\u6570\uff0c \u53ef\u8bbe\u7f6e shared_axes=[1, 2] \u3002 \u53c2\u8003\u6587\u732e Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification [source]","title":"PReLU"},{"location":"2-Layers/8.advanced-activations/#elu","text":"keras.layers.ELU(alpha=1.0) \u6307\u6570\u7ebf\u6027\u5355\u5143\u3002 \u5f62\u5f0f\uff1a f(x) = alpha * (exp(x) - 1.) for x < 0 , f(x) = x for x >= 0 . \u8f93\u5165\u5c3a\u5bf8 \u53ef\u4ee5\u662f\u4efb\u610f\u7684\u3002\u5982\u679c\u5c06\u8fd9\u4e00\u5c42\u4f5c\u4e3a\u6a21\u578b\u7684\u7b2c\u4e00\u5c42\uff0c \u5219\u9700\u8981\u6307\u5b9a input_shape \u53c2\u6570 \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u542b\u6837\u672c\u6570\u91cf\u7684\u7ef4\u5ea6\uff09\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u76f8\u540c\u3002 \u53c2\u6570 alpha : \u8d1f\u56e0\u5b50\u7684\u5c3a\u5ea6\u3002 \u53c2\u8003\u6587\u732e Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) [source]","title":"ELU"},{"location":"2-Layers/8.advanced-activations/#thresholdedrelu","text":"keras.layers.ThresholdedReLU(theta=1.0) \u5e26\u9608\u503c\u7684\u4fee\u6b63\u7ebf\u6027\u5355\u5143\u3002 \u5f62\u5f0f\uff1a f(x) = x for x > theta , f(x) = 0 otherwise . \u8f93\u5165\u5c3a\u5bf8 \u53ef\u4ee5\u662f\u4efb\u610f\u7684\u3002\u5982\u679c\u5c06\u8fd9\u4e00\u5c42\u4f5c\u4e3a\u6a21\u578b\u7684\u7b2c\u4e00\u5c42\uff0c \u5219\u9700\u8981\u6307\u5b9a input_shape \u53c2\u6570 \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u542b\u6837\u672c\u6570\u91cf\u7684\u7ef4\u5ea6\uff09\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u76f8\u540c\u3002 \u53c2\u6570 theta : float >= 0\u3002\u6fc0\u6d3b\u7684\u9608\u503c\u4f4d\u3002 \u53c2\u8003\u6587\u732e Zero-Bias Autoencoders and the Benefits of Co-Adapting Features [source]","title":"ThresholdedReLU"},{"location":"2-Layers/8.advanced-activations/#softmax","text":"keras.layers.Softmax(axis=-1) Softmax \u6fc0\u6d3b\u51fd\u6570\u3002 \u8f93\u5165\u5c3a\u5bf8 \u53ef\u4ee5\u662f\u4efb\u610f\u7684\u3002\u5982\u679c\u5c06\u8fd9\u4e00\u5c42\u4f5c\u4e3a\u6a21\u578b\u7684\u7b2c\u4e00\u5c42\uff0c \u5219\u9700\u8981\u6307\u5b9a input_shape \u53c2\u6570 \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u542b\u6837\u672c\u6570\u91cf\u7684\u7ef4\u5ea6\uff09\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u76f8\u540c\u3002 \u53c2\u6570 axis : \u6574\u6570\uff0c\u5e94\u7528 softmax \u6807\u51c6\u5316\u7684\u8f74\u3002 [source]","title":"Softmax"},{"location":"2-Layers/8.advanced-activations/#relu","text":"keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0) ReLU \u6fc0\u6d3b\u51fd\u6570\u3002 \u4f7f\u7528\u9ed8\u8ba4\u503c\u65f6\uff0c\u5b83\u8fd4\u56de\u9010\u4e2a\u5143\u7d20\u7684 max(x\uff0c0) \u3002 \u5426\u5219\uff1a \u5982\u679c x >= max_value \uff0c\u8fd4\u56de f(x) = max_value \uff0c \u5982\u679c threshold <= x < max_value \uff0c\u8fd4\u56de f(x) = x , \u5426\u5219\uff0c\u8fd4\u56de f(x) = negative_slope * (x - threshold) \u3002 \u8f93\u5165\u5c3a\u5bf8 \u53ef\u4ee5\u662f\u4efb\u610f\u7684\u3002\u5982\u679c\u5c06\u8fd9\u4e00\u5c42\u4f5c\u4e3a\u6a21\u578b\u7684\u7b2c\u4e00\u5c42\uff0c \u5219\u9700\u8981\u6307\u5b9a input_shape \u53c2\u6570 \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u542b\u6837\u672c\u6570\u91cf\u7684\u7ef4\u5ea6\uff09\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u76f8\u540c\u3002 \u53c2\u6570 max_value : \u6d6e\u70b9\u6570\uff0c\u6700\u5927\u7684\u8f93\u51fa\u503c\u3002 negative_slope : float >= 0. \u8d1f\u659c\u7387\u7cfb\u6570\u3002 threshold : float\u3002\"thresholded activation\" \u7684\u9608\u503c\u3002","title":"ReLU"},{"location":"2-Layers/9.normalization/","text":"\u6807\u51c6\u5316\u5c42 [source] BatchNormalization keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None) \u6279\u91cf\u6807\u51c6\u5316\u5c42 (Ioffe and Szegedy, 2014)\u3002 \u5728\u6bcf\u4e00\u4e2a\u6279\u6b21\u7684\u6570\u636e\u4e2d\u6807\u51c6\u5316\u524d\u4e00\u5c42\u7684\u6fc0\u6d3b\u9879\uff0c \u5373\uff0c\u5e94\u7528\u4e00\u4e2a\u7ef4\u6301\u6fc0\u6d3b\u9879\u5e73\u5747\u503c\u63a5\u8fd1 0\uff0c\u6807\u51c6\u5dee\u63a5\u8fd1 1 \u7684\u8f6c\u6362\u3002 \u53c2\u6570 axis : \u6574\u6570\uff0c\u9700\u8981\u6807\u51c6\u5316\u7684\u8f74 \uff08\u901a\u5e38\u662f\u7279\u5f81\u8f74\uff09\u3002 \u4f8b\u5982\uff0c\u5728 data_format=\"channels_first\" \u7684 Conv2D \u5c42\u4e4b\u540e\uff0c \u5728 BatchNormalization \u4e2d\u8bbe\u7f6e axis=1 \u3002 momentum : \u79fb\u52a8\u5747\u503c\u548c\u79fb\u52a8\u65b9\u5dee\u7684\u52a8\u91cf\u3002 epsilon : \u589e\u52a0\u5230\u65b9\u5dee\u7684\u5c0f\u7684\u6d6e\u70b9\u6570\uff0c\u4ee5\u907f\u514d\u9664\u4ee5\u96f6\u3002 center : \u5982\u679c\u4e3a True\uff0c\u628a beta \u7684\u504f\u79fb\u91cf\u52a0\u5230\u6807\u51c6\u5316\u7684\u5f20\u91cf\u4e0a\u3002 \u5982\u679c\u4e3a False\uff0c beta \u88ab\u5ffd\u7565\u3002 scale : \u5982\u679c\u4e3a True\uff0c\u4e58\u4ee5 gamma \u3002 \u5982\u679c\u4e3a False\uff0c gamma \u4e0d\u4f7f\u7528\u3002 \u5f53\u4e0b\u4e00\u5c42\u4e3a\u7ebf\u6027\u5c42\uff08\u6216\u8005\u4f8b\u5982 nn.relu \uff09\uff0c \u8fd9\u53ef\u4ee5\u88ab\u7981\u7528\uff0c\u56e0\u4e3a\u7f29\u653e\u5c06\u7531\u4e0b\u4e00\u5c42\u5b8c\u6210\u3002 beta_initializer : beta \u6743\u91cd\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u3002 gamma_initializer : gamma \u6743\u91cd\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u3002 moving_mean_initializer : \u79fb\u52a8\u5747\u503c\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u3002 moving_variance_initializer : \u79fb\u52a8\u65b9\u5dee\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u3002 beta_regularizer : \u53ef\u9009\u7684 beta \u6743\u91cd\u7684\u6b63\u5219\u5316\u65b9\u6cd5\u3002 gamma_regularizer : \u53ef\u9009\u7684 gamma \u6743\u91cd\u7684\u6b63\u5219\u5316\u65b9\u6cd5\u3002 beta_constraint : \u53ef\u9009\u7684 beta \u6743\u91cd\u7684\u7ea6\u675f\u65b9\u6cd5\u3002 gamma_constraint : \u53ef\u9009\u7684 gamma \u6743\u91cd\u7684\u7ea6\u675f\u65b9\u6cd5\u3002 \u8f93\u5165\u5c3a\u5bf8 \u53ef\u4ee5\u662f\u4efb\u610f\u7684\u3002\u5982\u679c\u5c06\u8fd9\u4e00\u5c42\u4f5c\u4e3a\u6a21\u578b\u7684\u7b2c\u4e00\u5c42\uff0c \u5219\u9700\u8981\u6307\u5b9a input_shape \u53c2\u6570 \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u542b\u6837\u672c\u6570\u91cf\u7684\u7ef4\u5ea6\uff09\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u76f8\u540c\u3002 \u53c2\u8003\u6587\u732e Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift","title":"\u6807\u51c6\u5316\u5c42"},{"location":"2-Layers/9.normalization/#_1","text":"[source]","title":"\u6807\u51c6\u5316\u5c42"},{"location":"2-Layers/9.normalization/#batchnormalization","text":"keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None) \u6279\u91cf\u6807\u51c6\u5316\u5c42 (Ioffe and Szegedy, 2014)\u3002 \u5728\u6bcf\u4e00\u4e2a\u6279\u6b21\u7684\u6570\u636e\u4e2d\u6807\u51c6\u5316\u524d\u4e00\u5c42\u7684\u6fc0\u6d3b\u9879\uff0c \u5373\uff0c\u5e94\u7528\u4e00\u4e2a\u7ef4\u6301\u6fc0\u6d3b\u9879\u5e73\u5747\u503c\u63a5\u8fd1 0\uff0c\u6807\u51c6\u5dee\u63a5\u8fd1 1 \u7684\u8f6c\u6362\u3002 \u53c2\u6570 axis : \u6574\u6570\uff0c\u9700\u8981\u6807\u51c6\u5316\u7684\u8f74 \uff08\u901a\u5e38\u662f\u7279\u5f81\u8f74\uff09\u3002 \u4f8b\u5982\uff0c\u5728 data_format=\"channels_first\" \u7684 Conv2D \u5c42\u4e4b\u540e\uff0c \u5728 BatchNormalization \u4e2d\u8bbe\u7f6e axis=1 \u3002 momentum : \u79fb\u52a8\u5747\u503c\u548c\u79fb\u52a8\u65b9\u5dee\u7684\u52a8\u91cf\u3002 epsilon : \u589e\u52a0\u5230\u65b9\u5dee\u7684\u5c0f\u7684\u6d6e\u70b9\u6570\uff0c\u4ee5\u907f\u514d\u9664\u4ee5\u96f6\u3002 center : \u5982\u679c\u4e3a True\uff0c\u628a beta \u7684\u504f\u79fb\u91cf\u52a0\u5230\u6807\u51c6\u5316\u7684\u5f20\u91cf\u4e0a\u3002 \u5982\u679c\u4e3a False\uff0c beta \u88ab\u5ffd\u7565\u3002 scale : \u5982\u679c\u4e3a True\uff0c\u4e58\u4ee5 gamma \u3002 \u5982\u679c\u4e3a False\uff0c gamma \u4e0d\u4f7f\u7528\u3002 \u5f53\u4e0b\u4e00\u5c42\u4e3a\u7ebf\u6027\u5c42\uff08\u6216\u8005\u4f8b\u5982 nn.relu \uff09\uff0c \u8fd9\u53ef\u4ee5\u88ab\u7981\u7528\uff0c\u56e0\u4e3a\u7f29\u653e\u5c06\u7531\u4e0b\u4e00\u5c42\u5b8c\u6210\u3002 beta_initializer : beta \u6743\u91cd\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u3002 gamma_initializer : gamma \u6743\u91cd\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u3002 moving_mean_initializer : \u79fb\u52a8\u5747\u503c\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u3002 moving_variance_initializer : \u79fb\u52a8\u65b9\u5dee\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u3002 beta_regularizer : \u53ef\u9009\u7684 beta \u6743\u91cd\u7684\u6b63\u5219\u5316\u65b9\u6cd5\u3002 gamma_regularizer : \u53ef\u9009\u7684 gamma \u6743\u91cd\u7684\u6b63\u5219\u5316\u65b9\u6cd5\u3002 beta_constraint : \u53ef\u9009\u7684 beta \u6743\u91cd\u7684\u7ea6\u675f\u65b9\u6cd5\u3002 gamma_constraint : \u53ef\u9009\u7684 gamma \u6743\u91cd\u7684\u7ea6\u675f\u65b9\u6cd5\u3002 \u8f93\u5165\u5c3a\u5bf8 \u53ef\u4ee5\u662f\u4efb\u610f\u7684\u3002\u5982\u679c\u5c06\u8fd9\u4e00\u5c42\u4f5c\u4e3a\u6a21\u578b\u7684\u7b2c\u4e00\u5c42\uff0c \u5219\u9700\u8981\u6307\u5b9a input_shape \u53c2\u6570 \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u542b\u6837\u672c\u6570\u91cf\u7684\u7ef4\u5ea6\uff09\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u76f8\u540c\u3002 \u53c2\u8003\u6587\u732e Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift","title":"BatchNormalization"},{"location":"2-Layers/90.noise/","text":"\u566a\u58f0\u5c42 [source] GaussianNoise keras.layers.GaussianNoise(stddev) \u5e94\u7528\u4ee5 0 \u4e3a\u4e2d\u5fc3\u7684\u52a0\u6027\u9ad8\u65af\u566a\u58f0\u3002 \u8fd9\u5bf9\u7f13\u89e3\u8fc7\u62df\u5408\u5f88\u6709\u7528 \uff08\u4f60\u53ef\u4ee5\u5c06\u5176\u89c6\u4e3a\u968f\u673a\u6570\u636e\u589e\u5f3a\u7684\u4e00\u79cd\u5f62\u5f0f\uff09\u3002 \u9ad8\u65af\u566a\u58f0\uff08GS\uff09\u662f\u5bf9\u771f\u5b9e\u8f93\u5165\u7684\u8150\u8680\u8fc7\u7a0b\u7684\u81ea\u7136\u9009\u62e9\u3002 \u7531\u4e8e\u5b83\u662f\u4e00\u4e2a\u6b63\u5219\u5316\u5c42\uff0c\u56e0\u6b64\u5b83\u53ea\u5728\u8bad\u7ec3\u65f6\u624d\u88ab\u6fc0\u6d3b\u3002 \u53c2\u6570 stddev : float\uff0c\u566a\u58f0\u5206\u5e03\u7684\u6807\u51c6\u5dee\u3002 \u8f93\u5165\u5c3a\u5bf8 \u53ef\u4ee5\u662f\u4efb\u610f\u7684\u3002 \u5982\u679c\u5c06\u8be5\u5c42\u4f5c\u4e3a\u6a21\u578b\u7684\u7b2c\u4e00\u5c42\uff0c\u5219\u9700\u8981\u6307\u5b9a input_shape \u53c2\u6570 \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u542b\u6837\u672c\u6570\u91cf\u7684\u7ef4\u5ea6\uff09\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u76f8\u540c\u3002 [source] GaussianDropout keras.layers.GaussianDropout(rate) \u5e94\u7528\u4ee5 1 \u4e3a\u4e2d\u5fc3\u7684 \u4e58\u6027\u9ad8\u65af\u566a\u58f0\u3002 \u7531\u4e8e\u5b83\u662f\u4e00\u4e2a\u6b63\u5219\u5316\u5c42\uff0c\u56e0\u6b64\u5b83\u53ea\u5728\u8bad\u7ec3\u65f6\u624d\u88ab\u6fc0\u6d3b\u3002 \u53c2\u6570 rate : float\uff0c\u4e22\u5f03\u6982\u7387\uff08\u4e0e Dropout \u76f8\u540c\uff09\u3002 \u8fd9\u4e2a\u4e58\u6027\u566a\u58f0\u7684\u6807\u51c6\u5dee\u4e3a sqrt(rate / (1 - rate)) \u3002 \u8f93\u5165\u5c3a\u5bf8 \u53ef\u4ee5\u662f\u4efb\u610f\u7684\u3002 \u5982\u679c\u5c06\u8be5\u5c42\u4f5c\u4e3a\u6a21\u578b\u7684\u7b2c\u4e00\u5c42\uff0c\u5219\u9700\u8981\u6307\u5b9a input_shape \u53c2\u6570 \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u542b\u6837\u672c\u6570\u91cf\u7684\u7ef4\u5ea6\uff09\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u76f8\u540c\u3002 \u53c2\u8003\u6587\u732e Dropout: A Simple Way to Prevent Neural Networks from Overfitting Srivastava, Hinton, et al. 2014 [source] AlphaDropout keras.layers.AlphaDropout(rate, noise_shape=None, seed=None) \u5c06 Alpha Dropout \u5e94\u7528\u5230\u8f93\u5165\u3002 Alpha Dropout \u662f\u4e00\u79cd Dropout \uff0c \u5b83\u4fdd\u6301\u8f93\u5165\u7684\u5e73\u5747\u503c\u548c\u65b9\u5dee\u4e0e\u539f\u6765\u7684\u503c\u4e0d\u53d8\uff0c \u4ee5\u786e\u4fdd\u5373\u4f7f\u5728 dropout \u540e\u4e5f\u80fd\u5b9e\u73b0\u81ea\u6211\u5f52\u4e00\u5316\u3002 \u901a\u8fc7\u968f\u673a\u5c06\u6fc0\u6d3b\u8bbe\u7f6e\u4e3a\u8d1f\u9971\u548c\u503c\uff0c Alpha Dropout \u975e\u5e38\u9002\u5408\u6309\u6bd4\u4f8b\u7f29\u653e\u7684\u6307\u6570\u7ebf\u6027\u5355\u5143\uff08SELU\uff09\u3002 \u53c2\u6570 rate : float\uff0c\u4e22\u5f03\u6982\u7387\uff08\u4e0e Dropout \u76f8\u540c\uff09\u3002 \u8fd9\u4e2a\u4e58\u6027\u566a\u58f0\u7684\u6807\u51c6\u5dee\u4e3a sqrt(rate / (1 - rate)) \u3002 seed : \u7528\u4f5c\u968f\u673a\u79cd\u5b50\u7684 Python \u6574\u6570\u3002 \u8f93\u5165\u5c3a\u5bf8 \u53ef\u4ee5\u662f\u4efb\u610f\u7684\u3002 \u5982\u679c\u5c06\u8be5\u5c42\u4f5c\u4e3a\u6a21\u578b\u7684\u7b2c\u4e00\u5c42\uff0c\u5219\u9700\u8981\u6307\u5b9a input_shape \u53c2\u6570 \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u542b\u6837\u672c\u6570\u91cf\u7684\u7ef4\u5ea6\uff09\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u76f8\u540c\u3002 \u53c2\u8003\u6587\u732e Self-Normalizing Neural Networks","title":"\u566a\u58f0\u5c42"},{"location":"2-Layers/90.noise/#_1","text":"[source]","title":"\u566a\u58f0\u5c42"},{"location":"2-Layers/90.noise/#gaussiannoise","text":"keras.layers.GaussianNoise(stddev) \u5e94\u7528\u4ee5 0 \u4e3a\u4e2d\u5fc3\u7684\u52a0\u6027\u9ad8\u65af\u566a\u58f0\u3002 \u8fd9\u5bf9\u7f13\u89e3\u8fc7\u62df\u5408\u5f88\u6709\u7528 \uff08\u4f60\u53ef\u4ee5\u5c06\u5176\u89c6\u4e3a\u968f\u673a\u6570\u636e\u589e\u5f3a\u7684\u4e00\u79cd\u5f62\u5f0f\uff09\u3002 \u9ad8\u65af\u566a\u58f0\uff08GS\uff09\u662f\u5bf9\u771f\u5b9e\u8f93\u5165\u7684\u8150\u8680\u8fc7\u7a0b\u7684\u81ea\u7136\u9009\u62e9\u3002 \u7531\u4e8e\u5b83\u662f\u4e00\u4e2a\u6b63\u5219\u5316\u5c42\uff0c\u56e0\u6b64\u5b83\u53ea\u5728\u8bad\u7ec3\u65f6\u624d\u88ab\u6fc0\u6d3b\u3002 \u53c2\u6570 stddev : float\uff0c\u566a\u58f0\u5206\u5e03\u7684\u6807\u51c6\u5dee\u3002 \u8f93\u5165\u5c3a\u5bf8 \u53ef\u4ee5\u662f\u4efb\u610f\u7684\u3002 \u5982\u679c\u5c06\u8be5\u5c42\u4f5c\u4e3a\u6a21\u578b\u7684\u7b2c\u4e00\u5c42\uff0c\u5219\u9700\u8981\u6307\u5b9a input_shape \u53c2\u6570 \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u542b\u6837\u672c\u6570\u91cf\u7684\u7ef4\u5ea6\uff09\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u76f8\u540c\u3002 [source]","title":"GaussianNoise"},{"location":"2-Layers/90.noise/#gaussiandropout","text":"keras.layers.GaussianDropout(rate) \u5e94\u7528\u4ee5 1 \u4e3a\u4e2d\u5fc3\u7684 \u4e58\u6027\u9ad8\u65af\u566a\u58f0\u3002 \u7531\u4e8e\u5b83\u662f\u4e00\u4e2a\u6b63\u5219\u5316\u5c42\uff0c\u56e0\u6b64\u5b83\u53ea\u5728\u8bad\u7ec3\u65f6\u624d\u88ab\u6fc0\u6d3b\u3002 \u53c2\u6570 rate : float\uff0c\u4e22\u5f03\u6982\u7387\uff08\u4e0e Dropout \u76f8\u540c\uff09\u3002 \u8fd9\u4e2a\u4e58\u6027\u566a\u58f0\u7684\u6807\u51c6\u5dee\u4e3a sqrt(rate / (1 - rate)) \u3002 \u8f93\u5165\u5c3a\u5bf8 \u53ef\u4ee5\u662f\u4efb\u610f\u7684\u3002 \u5982\u679c\u5c06\u8be5\u5c42\u4f5c\u4e3a\u6a21\u578b\u7684\u7b2c\u4e00\u5c42\uff0c\u5219\u9700\u8981\u6307\u5b9a input_shape \u53c2\u6570 \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u542b\u6837\u672c\u6570\u91cf\u7684\u7ef4\u5ea6\uff09\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u76f8\u540c\u3002 \u53c2\u8003\u6587\u732e Dropout: A Simple Way to Prevent Neural Networks from Overfitting Srivastava, Hinton, et al. 2014 [source]","title":"GaussianDropout"},{"location":"2-Layers/90.noise/#alphadropout","text":"keras.layers.AlphaDropout(rate, noise_shape=None, seed=None) \u5c06 Alpha Dropout \u5e94\u7528\u5230\u8f93\u5165\u3002 Alpha Dropout \u662f\u4e00\u79cd Dropout \uff0c \u5b83\u4fdd\u6301\u8f93\u5165\u7684\u5e73\u5747\u503c\u548c\u65b9\u5dee\u4e0e\u539f\u6765\u7684\u503c\u4e0d\u53d8\uff0c \u4ee5\u786e\u4fdd\u5373\u4f7f\u5728 dropout \u540e\u4e5f\u80fd\u5b9e\u73b0\u81ea\u6211\u5f52\u4e00\u5316\u3002 \u901a\u8fc7\u968f\u673a\u5c06\u6fc0\u6d3b\u8bbe\u7f6e\u4e3a\u8d1f\u9971\u548c\u503c\uff0c Alpha Dropout \u975e\u5e38\u9002\u5408\u6309\u6bd4\u4f8b\u7f29\u653e\u7684\u6307\u6570\u7ebf\u6027\u5355\u5143\uff08SELU\uff09\u3002 \u53c2\u6570 rate : float\uff0c\u4e22\u5f03\u6982\u7387\uff08\u4e0e Dropout \u76f8\u540c\uff09\u3002 \u8fd9\u4e2a\u4e58\u6027\u566a\u58f0\u7684\u6807\u51c6\u5dee\u4e3a sqrt(rate / (1 - rate)) \u3002 seed : \u7528\u4f5c\u968f\u673a\u79cd\u5b50\u7684 Python \u6574\u6570\u3002 \u8f93\u5165\u5c3a\u5bf8 \u53ef\u4ee5\u662f\u4efb\u610f\u7684\u3002 \u5982\u679c\u5c06\u8be5\u5c42\u4f5c\u4e3a\u6a21\u578b\u7684\u7b2c\u4e00\u5c42\uff0c\u5219\u9700\u8981\u6307\u5b9a input_shape \u53c2\u6570 \uff08\u6574\u6570\u5143\u7ec4\uff0c\u4e0d\u5305\u542b\u6837\u672c\u6570\u91cf\u7684\u7ef4\u5ea6\uff09\u3002 \u8f93\u51fa\u5c3a\u5bf8 \u4e0e\u8f93\u5165\u76f8\u540c\u3002 \u53c2\u8003\u6587\u732e Self-Normalizing Neural Networks","title":"AlphaDropout"},{"location":"2-Layers/91.wrappers/","text":"\u5c42\u5c01\u88c5\u5668 [source] TimeDistributed keras.layers.TimeDistributed(layer) \u8fd9\u4e2a\u5c01\u88c5\u5668\u5c06\u4e00\u4e2a\u5c42\u5e94\u7528\u4e8e\u8f93\u5165\u7684\u6bcf\u4e2a\u65f6\u95f4\u7247\u3002 \u8f93\u5165\u81f3\u5c11\u4e3a 3D\uff0c\u4e14\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\u5e94\u8be5\u662f\u65f6\u95f4\u6240\u8868\u793a\u7684\u7ef4\u5ea6\u3002 \u8003\u8651 32 \u4e2a\u6837\u672c\u7684\u4e00\u4e2a batch\uff0c \u5176\u4e2d\u6bcf\u4e2a\u6837\u672c\u662f 10 \u4e2a 16 \u7ef4\u5411\u91cf\u7684\u5e8f\u5217\u3002 \u90a3\u4e48\u8fd9\u4e2a batch \u7684\u8f93\u5165\u5c3a\u5bf8\u4e3a (32, 10, 16) \uff0c \u800c input_shape \u4e0d\u5305\u542b\u6837\u672c\u6570\u91cf\u7684\u7ef4\u5ea6\uff0c\u4e3a (10, 16) \u3002 \u4f60\u53ef\u4ee5\u4f7f\u7528 TimeDistributed \u6765\u5c06 Dense \u5c42\u72ec\u7acb\u5730\u5e94\u7528\u5230 \u8fd9 10 \u4e2a\u65f6\u95f4\u6b65\u7684\u6bcf\u4e00\u4e2a\uff1a # \u4f5c\u4e3a\u6a21\u578b\u7b2c\u4e00\u5c42 model = Sequential() model.add(TimeDistributed(Dense(8), input_shape=(10, 16))) # \u73b0\u5728 model.output_shape == (None, 10, 8) \u8f93\u51fa\u7684\u5c3a\u5bf8\u4e3a (32, 10, 8) \u3002 \u5728\u540e\u7eed\u7684\u5c42\u4e2d\uff0c\u5c06\u4e0d\u518d\u9700\u8981 input_shape \uff1a model.add(TimeDistributed(Dense(32))) # \u73b0\u5728 model.output_shape == (None, 10, 32) \u8f93\u51fa\u7684\u5c3a\u5bf8\u4e3a (32, 10, 32) \u3002 TimeDistributed \u53ef\u4ee5\u5e94\u7528\u4e8e\u4efb\u610f\u5c42\uff0c\u4e0d\u4ec5\u4ec5\u662f Dense \uff0c \u4f8b\u5982\u8fd0\u7528\u4e8e Conv2D \u5c42\uff1a model = Sequential() model.add(TimeDistributed(Conv2D(64, (3, 3)), input_shape=(10, 299, 299, 3))) \u53c2\u6570 layer : \u4e00\u4e2a\u7f51\u7edc\u5c42\u5b9e\u4f8b\u3002 [source] Bidirectional keras.layers.Bidirectional(layer, merge_mode='concat', weights=None) RNN \u7684\u53cc\u5411\u5c01\u88c5\u5668\uff0c\u5bf9\u5e8f\u5217\u8fdb\u884c\u524d\u5411\u548c\u540e\u5411\u8ba1\u7b97\u3002 \u53c2\u6570 layer : Recurrent \u5b9e\u4f8b\u3002 merge_mode : \u524d\u5411\u548c\u540e\u5411 RNN \u7684\u8f93\u51fa\u7684\u7ed3\u5408\u6a21\u5f0f\u3002 \u4e3a {'sum', 'mul', 'concat', 'ave', None} \u5176\u4e2d\u4e4b\u4e00\u3002 \u5982\u679c\u662f None\uff0c\u8f93\u51fa\u4e0d\u4f1a\u88ab\u7ed3\u5408\uff0c\u800c\u662f\u4f5c\u4e3a\u4e00\u4e2a\u5217\u8868\u88ab\u8fd4\u56de\u3002 \u5f02\u5e38 ValueError : \u5982\u679c\u53c2\u6570 merge_mode \u975e\u6cd5\u3002 \u4f8b model = Sequential() model.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(5, 10))) model.add(Bidirectional(LSTM(10))) model.add(Dense(5)) model.add(Activation('softmax')) model.compile(loss='categorical_crossentropy', optimizer='rmsprop')","title":"\u5c42\u5c01\u88c5\u5668"},{"location":"2-Layers/91.wrappers/#_1","text":"[source]","title":"\u5c42\u5c01\u88c5\u5668"},{"location":"2-Layers/91.wrappers/#timedistributed","text":"keras.layers.TimeDistributed(layer) \u8fd9\u4e2a\u5c01\u88c5\u5668\u5c06\u4e00\u4e2a\u5c42\u5e94\u7528\u4e8e\u8f93\u5165\u7684\u6bcf\u4e2a\u65f6\u95f4\u7247\u3002 \u8f93\u5165\u81f3\u5c11\u4e3a 3D\uff0c\u4e14\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\u5e94\u8be5\u662f\u65f6\u95f4\u6240\u8868\u793a\u7684\u7ef4\u5ea6\u3002 \u8003\u8651 32 \u4e2a\u6837\u672c\u7684\u4e00\u4e2a batch\uff0c \u5176\u4e2d\u6bcf\u4e2a\u6837\u672c\u662f 10 \u4e2a 16 \u7ef4\u5411\u91cf\u7684\u5e8f\u5217\u3002 \u90a3\u4e48\u8fd9\u4e2a batch \u7684\u8f93\u5165\u5c3a\u5bf8\u4e3a (32, 10, 16) \uff0c \u800c input_shape \u4e0d\u5305\u542b\u6837\u672c\u6570\u91cf\u7684\u7ef4\u5ea6\uff0c\u4e3a (10, 16) \u3002 \u4f60\u53ef\u4ee5\u4f7f\u7528 TimeDistributed \u6765\u5c06 Dense \u5c42\u72ec\u7acb\u5730\u5e94\u7528\u5230 \u8fd9 10 \u4e2a\u65f6\u95f4\u6b65\u7684\u6bcf\u4e00\u4e2a\uff1a # \u4f5c\u4e3a\u6a21\u578b\u7b2c\u4e00\u5c42 model = Sequential() model.add(TimeDistributed(Dense(8), input_shape=(10, 16))) # \u73b0\u5728 model.output_shape == (None, 10, 8) \u8f93\u51fa\u7684\u5c3a\u5bf8\u4e3a (32, 10, 8) \u3002 \u5728\u540e\u7eed\u7684\u5c42\u4e2d\uff0c\u5c06\u4e0d\u518d\u9700\u8981 input_shape \uff1a model.add(TimeDistributed(Dense(32))) # \u73b0\u5728 model.output_shape == (None, 10, 32) \u8f93\u51fa\u7684\u5c3a\u5bf8\u4e3a (32, 10, 32) \u3002 TimeDistributed \u53ef\u4ee5\u5e94\u7528\u4e8e\u4efb\u610f\u5c42\uff0c\u4e0d\u4ec5\u4ec5\u662f Dense \uff0c \u4f8b\u5982\u8fd0\u7528\u4e8e Conv2D \u5c42\uff1a model = Sequential() model.add(TimeDistributed(Conv2D(64, (3, 3)), input_shape=(10, 299, 299, 3))) \u53c2\u6570 layer : \u4e00\u4e2a\u7f51\u7edc\u5c42\u5b9e\u4f8b\u3002 [source]","title":"TimeDistributed"},{"location":"2-Layers/91.wrappers/#bidirectional","text":"keras.layers.Bidirectional(layer, merge_mode='concat', weights=None) RNN \u7684\u53cc\u5411\u5c01\u88c5\u5668\uff0c\u5bf9\u5e8f\u5217\u8fdb\u884c\u524d\u5411\u548c\u540e\u5411\u8ba1\u7b97\u3002 \u53c2\u6570 layer : Recurrent \u5b9e\u4f8b\u3002 merge_mode : \u524d\u5411\u548c\u540e\u5411 RNN \u7684\u8f93\u51fa\u7684\u7ed3\u5408\u6a21\u5f0f\u3002 \u4e3a {'sum', 'mul', 'concat', 'ave', None} \u5176\u4e2d\u4e4b\u4e00\u3002 \u5982\u679c\u662f None\uff0c\u8f93\u51fa\u4e0d\u4f1a\u88ab\u7ed3\u5408\uff0c\u800c\u662f\u4f5c\u4e3a\u4e00\u4e2a\u5217\u8868\u88ab\u8fd4\u56de\u3002 \u5f02\u5e38 ValueError : \u5982\u679c\u53c2\u6570 merge_mode \u975e\u6cd5\u3002 \u4f8b model = Sequential() model.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(5, 10))) model.add(Bidirectional(LSTM(10))) model.add(Dense(5)) model.add(Activation('softmax')) model.compile(loss='categorical_crossentropy', optimizer='rmsprop')","title":"Bidirectional"},{"location":"2-Layers/92.writing-your-own-keras-layers/","text":"\u7f16\u5199\u4f60\u81ea\u5df1\u7684 Keras \u5c42 \u5bf9\u4e8e\u7b80\u5355\u3001\u65e0\u72b6\u6001\u7684\u81ea\u5b9a\u4e49\u64cd\u4f5c\uff0c\u4f60\u4e5f\u8bb8\u53ef\u4ee5\u901a\u8fc7 layers.core.Lambda \u5c42\u6765\u5b9e\u73b0\u3002\u4f46\u662f\u5bf9\u4e8e\u90a3\u4e9b\u5305\u542b\u4e86\u53ef\u8bad\u7ec3\u6743\u91cd\u7684\u81ea\u5b9a\u4e49\u5c42\uff0c\u4f60\u5e94\u8be5\u81ea\u5df1\u5b9e\u73b0\u8fd9\u79cd\u5c42\u3002 \u8fd9\u662f\u4e00\u4e2a Keras2.0 \u4e2d\uff0cKeras \u5c42\u7684\u9aa8\u67b6\uff08\u5982\u679c\u4f60\u7528\u7684\u662f\u65e7\u7684\u7248\u672c\uff0c\u8bf7\u66f4\u65b0\u5230\u65b0\u7248\uff09\u3002\u4f60\u53ea\u9700\u8981\u5b9e\u73b0\u4e09\u4e2a\u65b9\u6cd5\u5373\u53ef: build(input_shape) : \u8fd9\u662f\u4f60\u5b9a\u4e49\u6743\u91cd\u7684\u5730\u65b9\u3002\u8fd9\u4e2a\u65b9\u6cd5\u5fc5\u987b\u8bbe self.built = True \uff0c\u53ef\u4ee5\u901a\u8fc7\u8c03\u7528 super([Layer], self).build() \u5b8c\u6210\u3002 call(x) : \u8fd9\u91cc\u662f\u7f16\u5199\u5c42\u7684\u529f\u80fd\u903b\u8f91\u7684\u5730\u65b9\u3002\u4f60\u53ea\u9700\u8981\u5173\u6ce8\u4f20\u5165 call \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\uff1a\u8f93\u5165\u5f20\u91cf\uff0c\u9664\u975e\u4f60\u5e0c\u671b\u4f60\u7684\u5c42\u652f\u6301masking\u3002 compute_output_shape(input_shape) : \u5982\u679c\u4f60\u7684\u5c42\u66f4\u6539\u4e86\u8f93\u5165\u5f20\u91cf\u7684\u5f62\u72b6\uff0c\u4f60\u5e94\u8be5\u5728\u8fd9\u91cc\u5b9a\u4e49\u5f62\u72b6\u53d8\u5316\u7684\u903b\u8f91\uff0c\u8fd9\u8ba9Keras\u80fd\u591f\u81ea\u52a8\u63a8\u65ad\u5404\u5c42\u7684\u5f62\u72b6\u3002 from keras import backend as K from keras.engine.topology import Layer class MyLayer(Layer): def __init__(self, output_dim, **kwargs): self.output_dim = output_dim super(MyLayer, self).__init__(**kwargs) def build(self, input_shape): # \u4e3a\u8be5\u5c42\u521b\u5efa\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u6743\u91cd self.kernel = self.add_weight(name='kernel', shape=(input_shape[1], self.output_dim), initializer='uniform', trainable=True) super(MyLayer, self).build(input_shape) # \u4e00\u5b9a\u8981\u5728\u6700\u540e\u8c03\u7528\u5b83 def call(self, x): return K.dot(x, self.kernel) def compute_output_shape(self, input_shape): return (input_shape[0], self.output_dim) \u8fd8\u53ef\u4ee5\u5b9a\u4e49\u5177\u6709\u591a\u4e2a\u8f93\u5165\u5f20\u91cf\u548c\u591a\u4e2a\u8f93\u51fa\u5f20\u91cf\u7684 Keras \u5c42\u3002 \u4e3a\u6b64\uff0c\u4f60\u5e94\u8be5\u5047\u8bbe\u65b9\u6cd5 build(input_shape) \uff0c call(x) \u548c compute_output_shape(input_shape) \u7684\u8f93\u5165\u8f93\u51fa\u90fd\u662f\u5217\u8868\u3002 \u8fd9\u91cc\u662f\u4e00\u4e2a\u4f8b\u5b50\uff0c\u4e0e\u4e0a\u9762\u90a3\u4e2a\u76f8\u4f3c\uff1a from keras import backend as K from keras.engine.topology import Layer class MyLayer(Layer): def __init__(self, output_dim, **kwargs): self.output_dim = output_dim super(MyLayer, self).__init__(**kwargs) def build(self, input_shape): assert isinstance(input_shape, list) # \u4e3a\u8be5\u5c42\u521b\u5efa\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u6743\u91cd self.kernel = self.add_weight(name='kernel', shape=(input_shape[0][1], self.output_dim), initializer='uniform', trainable=True) super(MyLayer, self).build(input_shape) # \u4e00\u5b9a\u8981\u5728\u6700\u540e\u8c03\u7528\u5b83 def call(self, x): assert isinstance(x, list) a, b = x return [K.dot(a, self.kernel) + b, K.mean(b, axis=-1)] def compute_output_shape(self, input_shape): assert isinstance(input_shape, list) shape_a, shape_b = input_shape return [(shape_a[0], self.output_dim), shape_b[:-1]] \u5df2\u6709\u7684 Keras \u5c42\u5c31\u662f\u5b9e\u73b0\u4efb\u4f55\u5c42\u7684\u5f88\u597d\u4f8b\u5b50\u3002\u4e0d\u8981\u72b9\u8c6b\u9605\u8bfb\u6e90\u7801\uff01","title":"\u7f16\u5199\u4f60\u81ea\u5df1\u7684 Keras \u5c42"},{"location":"2-Layers/92.writing-your-own-keras-layers/#keras","text":"\u5bf9\u4e8e\u7b80\u5355\u3001\u65e0\u72b6\u6001\u7684\u81ea\u5b9a\u4e49\u64cd\u4f5c\uff0c\u4f60\u4e5f\u8bb8\u53ef\u4ee5\u901a\u8fc7 layers.core.Lambda \u5c42\u6765\u5b9e\u73b0\u3002\u4f46\u662f\u5bf9\u4e8e\u90a3\u4e9b\u5305\u542b\u4e86\u53ef\u8bad\u7ec3\u6743\u91cd\u7684\u81ea\u5b9a\u4e49\u5c42\uff0c\u4f60\u5e94\u8be5\u81ea\u5df1\u5b9e\u73b0\u8fd9\u79cd\u5c42\u3002 \u8fd9\u662f\u4e00\u4e2a Keras2.0 \u4e2d\uff0cKeras \u5c42\u7684\u9aa8\u67b6\uff08\u5982\u679c\u4f60\u7528\u7684\u662f\u65e7\u7684\u7248\u672c\uff0c\u8bf7\u66f4\u65b0\u5230\u65b0\u7248\uff09\u3002\u4f60\u53ea\u9700\u8981\u5b9e\u73b0\u4e09\u4e2a\u65b9\u6cd5\u5373\u53ef: build(input_shape) : \u8fd9\u662f\u4f60\u5b9a\u4e49\u6743\u91cd\u7684\u5730\u65b9\u3002\u8fd9\u4e2a\u65b9\u6cd5\u5fc5\u987b\u8bbe self.built = True \uff0c\u53ef\u4ee5\u901a\u8fc7\u8c03\u7528 super([Layer], self).build() \u5b8c\u6210\u3002 call(x) : \u8fd9\u91cc\u662f\u7f16\u5199\u5c42\u7684\u529f\u80fd\u903b\u8f91\u7684\u5730\u65b9\u3002\u4f60\u53ea\u9700\u8981\u5173\u6ce8\u4f20\u5165 call \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\uff1a\u8f93\u5165\u5f20\u91cf\uff0c\u9664\u975e\u4f60\u5e0c\u671b\u4f60\u7684\u5c42\u652f\u6301masking\u3002 compute_output_shape(input_shape) : \u5982\u679c\u4f60\u7684\u5c42\u66f4\u6539\u4e86\u8f93\u5165\u5f20\u91cf\u7684\u5f62\u72b6\uff0c\u4f60\u5e94\u8be5\u5728\u8fd9\u91cc\u5b9a\u4e49\u5f62\u72b6\u53d8\u5316\u7684\u903b\u8f91\uff0c\u8fd9\u8ba9Keras\u80fd\u591f\u81ea\u52a8\u63a8\u65ad\u5404\u5c42\u7684\u5f62\u72b6\u3002 from keras import backend as K from keras.engine.topology import Layer class MyLayer(Layer): def __init__(self, output_dim, **kwargs): self.output_dim = output_dim super(MyLayer, self).__init__(**kwargs) def build(self, input_shape): # \u4e3a\u8be5\u5c42\u521b\u5efa\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u6743\u91cd self.kernel = self.add_weight(name='kernel', shape=(input_shape[1], self.output_dim), initializer='uniform', trainable=True) super(MyLayer, self).build(input_shape) # \u4e00\u5b9a\u8981\u5728\u6700\u540e\u8c03\u7528\u5b83 def call(self, x): return K.dot(x, self.kernel) def compute_output_shape(self, input_shape): return (input_shape[0], self.output_dim) \u8fd8\u53ef\u4ee5\u5b9a\u4e49\u5177\u6709\u591a\u4e2a\u8f93\u5165\u5f20\u91cf\u548c\u591a\u4e2a\u8f93\u51fa\u5f20\u91cf\u7684 Keras \u5c42\u3002 \u4e3a\u6b64\uff0c\u4f60\u5e94\u8be5\u5047\u8bbe\u65b9\u6cd5 build(input_shape) \uff0c call(x) \u548c compute_output_shape(input_shape) \u7684\u8f93\u5165\u8f93\u51fa\u90fd\u662f\u5217\u8868\u3002 \u8fd9\u91cc\u662f\u4e00\u4e2a\u4f8b\u5b50\uff0c\u4e0e\u4e0a\u9762\u90a3\u4e2a\u76f8\u4f3c\uff1a from keras import backend as K from keras.engine.topology import Layer class MyLayer(Layer): def __init__(self, output_dim, **kwargs): self.output_dim = output_dim super(MyLayer, self).__init__(**kwargs) def build(self, input_shape): assert isinstance(input_shape, list) # \u4e3a\u8be5\u5c42\u521b\u5efa\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u6743\u91cd self.kernel = self.add_weight(name='kernel', shape=(input_shape[0][1], self.output_dim), initializer='uniform', trainable=True) super(MyLayer, self).build(input_shape) # \u4e00\u5b9a\u8981\u5728\u6700\u540e\u8c03\u7528\u5b83 def call(self, x): assert isinstance(x, list) a, b = x return [K.dot(a, self.kernel) + b, K.mean(b, axis=-1)] def compute_output_shape(self, input_shape): assert isinstance(input_shape, list) shape_a, shape_b = input_shape return [(shape_a[0], self.output_dim), shape_b[:-1]] \u5df2\u6709\u7684 Keras \u5c42\u5c31\u662f\u5b9e\u73b0\u4efb\u4f55\u5c42\u7684\u5f88\u597d\u4f8b\u5b50\u3002\u4e0d\u8981\u72b9\u8c6b\u9605\u8bfb\u6e90\u7801\uff01","title":"\u7f16\u5199\u4f60\u81ea\u5df1\u7684 Keras \u5c42"},{"location":"3-Preprocessing/0.sequence/","text":"\u5e8f\u5217\u9884\u5904\u7406 [source] TimeseriesGenerator keras.preprocessing.sequence.TimeseriesGenerator(data, targets, length, sampling_rate=1, stride=1, start_index=0, end_index=None, shuffle=False, reverse=False, batch_size=128) \u7528\u4e8e\u751f\u6210\u6279\u91cf\u65f6\u5e8f\u6570\u636e\u7684\u5b9e\u7528\u5de5\u5177\u7c7b\u3002 \u8fd9\u4e2a\u7c7b\u4ee5\u4e00\u7cfb\u5217\u7531\u76f8\u7b49\u95f4\u9694\u4ee5\u53ca\u4e00\u4e9b\u65f6\u95f4\u5e8f\u5217\u53c2\u6570\uff08\u4f8b\u5982\u6b65\u957f\u3001\u5386\u53f2\u957f\u5ea6\u7b49\uff09\u6c47\u96c6\u7684\u6570\u636e\u70b9\u4f5c\u4e3a\u8f93\u5165\uff0c\u4ee5\u751f\u6210\u7528\u4e8e\u8bad\u7ec3/\u9a8c\u8bc1\u7684\u6279\u6b21\u6570\u636e\u3002 \u53c2\u6570 data : \u53ef\u7d22\u5f15\u7684\u751f\u6210\u5668\uff08\u4f8b\u5982\u5217\u8868\u6216 Numpy \u6570\u7ec4\uff09\uff0c\u5305\u542b\u8fde\u7eed\u6570\u636e\u70b9\uff08\u65f6\u95f4\u6b65\uff09\u3002\u6570\u636e\u5e94\u8be5\u662f 2D \u7684\uff0c\u4e14\u7b2c 0 \u4e2a\u8f74\u4e3a\u65f6\u95f4\u7ef4\u5ea6\u3002 targets : \u5bf9\u5e94\u4e8e data \u7684\u65f6\u95f4\u6b65\u7684\u76ee\u6807\u503c\u3002\u5b83\u5e94\u8be5\u4e0e data \u7684\u957f\u5ea6\u76f8\u540c\u3002 length : \u8f93\u51fa\u5e8f\u5217\u7684\u957f\u5ea6\uff08\u4ee5\u65f6\u95f4\u6b65\u6570\u8868\u793a\uff09\u3002 sampling_rate : \u5e8f\u5217\u5185\u8fde\u7eed\u5404\u4e2a\u65f6\u95f4\u6b65\u4e4b\u95f4\u7684\u5468\u671f\u3002\u5bf9\u4e8e\u5468\u671f r , \u65f6\u95f4\u6b65 data[i] , data[i-r] , ... data[i - length] \u88ab\u7528\u4e8e\u751f\u6210\u6837\u672c\u5e8f\u5217\u3002 stride : \u8fde\u7eed\u8f93\u51fa\u5e8f\u5217\u4e4b\u95f4\u7684\u5468\u671f. \u5bf9\u4e8e\u5468\u671f s , \u8fde\u7eed\u8f93\u51fa\u6837\u672c\u5c06\u4e3a data[i] , data[i+s] , data[i+2*s] \u7b49\u3002 start_index : \u5728 start_index \u4e4b\u524d\u7684\u6570\u636e\u70b9\u5728\u8f93\u51fa\u5e8f\u5217\u4e2d\u5c06\u4e0d\u88ab\u4f7f\u7528\u3002\u8fd9\u5bf9\u4fdd\u7559\u90e8\u5206\u6570\u636e\u4ee5\u8fdb\u884c\u6d4b\u8bd5\u6216\u9a8c\u8bc1\u5f88\u6709\u7528\u3002 end_index : \u5728 end_index \u4e4b\u540e\u7684\u6570\u636e\u70b9\u5728\u8f93\u51fa\u5e8f\u5217\u4e2d\u5c06\u4e0d\u88ab\u4f7f\u7528\u3002\u8fd9\u5bf9\u4fdd\u7559\u90e8\u5206\u6570\u636e\u4ee5\u8fdb\u884c\u6d4b\u8bd5\u6216\u9a8c\u8bc1\u5f88\u6709\u7528\u3002 shuffle : \u662f\u5426\u6253\u4e71\u8f93\u51fa\u6837\u672c\uff0c\u8fd8\u662f\u6309\u7167\u65f6\u95f4\u987a\u5e8f\u7ed8\u5236\u5b83\u4eec\u3002 reverse : \u5e03\u5c14\u503c: \u5982\u679c true , \u6bcf\u4e2a\u8f93\u51fa\u6837\u672c\u4e2d\u7684\u65f6\u95f4\u6b65\u5c06\u6309\u7167\u65f6\u95f4\u5012\u5e8f\u6392\u5217\u3002 batch_size : \u6bcf\u4e2a\u6279\u6b21\u4e2d\u7684\u65f6\u95f4\u5e8f\u5217\u6837\u672c\u6570\uff08\u53ef\u80fd\u9664\u6700\u540e\u4e00\u4e2a\u5916\uff09\u3002 \u8fd4\u56de \u4e00\u4e2a Sequence \u5b9e\u4f8b\u3002 \u4f8b\u5b50 from keras.preprocessing.sequence import TimeseriesGenerator import numpy as np data = np.array([[i] for i in range(50)]) targets = np.array([[i] for i in range(50)]) data_gen = TimeseriesGenerator(data, targets, length=10, sampling_rate=2, batch_size=2) assert len(data_gen) == 20 batch_0 = data_gen[0] x, y = batch_0 assert np.array_equal(x, np.array([[[0], [2], [4], [6], [8]], [[1], [3], [5], [7], [9]]])) assert np.array_equal(y, np.array([[10], [11]])) pad_sequences keras.preprocessing.sequence.pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.0) \u5c06\u591a\u4e2a\u5e8f\u5217\u622a\u65ad\u6216\u8865\u9f50\u4e3a\u76f8\u540c\u957f\u5ea6\u3002 \u8be5\u51fd\u6570\u5c06\u4e00\u4e2a num_samples \u7684\u5e8f\u5217\uff08\u6574\u6570\u5217\u8868\uff09\u8f6c\u5316\u4e3a\u4e00\u4e2a 2D Numpy \u77e9\u9635\uff0c\u5176\u5c3a\u5bf8\u4e3a (num_samples, num_timesteps) \u3002 num_timesteps \u8981\u4e48\u662f\u7ed9\u5b9a\u7684 maxlen \u53c2\u6570\uff0c\u8981\u4e48\u662f\u6700\u957f\u5e8f\u5217\u7684\u957f\u5ea6\u3002 \u6bd4 num_timesteps \u77ed\u7684\u5e8f\u5217\u5c06\u5728\u672b\u7aef\u4ee5 value \u503c\u8865\u9f50\u3002 \u6bd4 num_timesteps \u957f\u7684\u5e8f\u5217\u5c06\u4f1a\u88ab\u622a\u65ad\u4ee5\u6ee1\u8db3\u6240\u9700\u8981\u7684\u957f\u5ea6\u3002\u8865\u9f50\u6216\u622a\u65ad\u53d1\u751f\u7684\u4f4d\u7f6e\u5206\u522b\u7531\u53c2\u6570 pading \u548c truncating \u51b3\u5b9a\u3002 \u5411\u524d\u8865\u9f50\u4e3a\u9ed8\u8ba4\u64cd\u4f5c\u3002 \u53c2\u6570 sequences : \u5217\u8868\u7684\u5217\u8868\uff0c\u6bcf\u4e00\u4e2a\u5143\u7d20\u662f\u4e00\u4e2a\u5e8f\u5217\u3002 maxlen : \u6574\u6570\uff0c\u6240\u6709\u5e8f\u5217\u7684\u6700\u5927\u957f\u5ea6\u3002 dtype : \u8f93\u51fa\u5e8f\u5217\u7684\u7c7b\u578b\u3002 \u8981\u4f7f\u7528\u53ef\u53d8\u957f\u5ea6\u5b57\u7b26\u4e32\u586b\u5145\u5e8f\u5217\uff0c\u53ef\u4ee5\u4f7f\u7528 object \u3002 padding : \u5b57\u7b26\u4e32\uff0c'pre' \u6216 'post' \uff0c\u5728\u5e8f\u5217\u7684\u524d\u7aef\u8865\u9f50\u8fd8\u662f\u5728\u540e\u7aef\u8865\u9f50\u3002 truncating : \u5b57\u7b26\u4e32\uff0c'pre' \u6216 'post' \uff0c\u79fb\u9664\u957f\u5ea6\u5927\u4e8e maxlen \u7684\u5e8f\u5217\u7684\u503c\uff0c\u8981\u4e48\u5728\u5e8f\u5217\u524d\u7aef\u622a\u65ad\uff0c\u8981\u4e48\u5728\u540e\u7aef\u3002 value : \u6d6e\u70b9\u6570\uff0c\u8868\u793a\u7528\u6765\u8865\u9f50\u7684\u503c\u3002 \u8fd4\u56de x : Numpy \u77e9\u9635\uff0c\u5c3a\u5bf8\u4e3a (len(sequences), maxlen) \u3002 \u5f02\u5e38 ValueError: \u5982\u679c\u622a\u65ad\u6216\u8865\u9f50\u7684\u503c\u65e0\u6548\uff0c\u6216\u8005\u5e8f\u5217\u6761\u76ee\u7684\u5f62\u72b6\u65e0\u6548\u3002 skipgrams keras.preprocessing.sequence.skipgrams(sequence, vocabulary_size, window_size=4, negative_samples=1.0, shuffle=True, categorical=False, sampling_table=None, seed=None) \u751f\u6210 skipgram \u8bcd\u5bf9\u3002 \u8be5\u51fd\u6570\u5c06\u4e00\u4e2a\u5355\u8bcd\u7d22\u5f15\u5e8f\u5217\uff08\u6574\u6570\u5217\u8868\uff09\u8f6c\u5316\u4e3a\u4ee5\u4e0b\u5f62\u5f0f\u7684\u5355\u8bcd\u5143\u7ec4\uff1a \uff08\u5355\u8bcd, \u540c\u7a97\u53e3\u7684\u5355\u8bcd\uff09\uff0c\u6807\u7b7e\u4e3a 1\uff08\u6b63\u6837\u672c\uff09\u3002 \uff08\u5355\u8bcd, \u6765\u81ea\u8bcd\u6c47\u8868\u7684\u968f\u673a\u5355\u8bcd\uff09\uff0c\u6807\u7b7e\u4e3a 0\uff08\u8d1f\u6837\u672c\uff09\u3002 \u82e5\u8981\u4e86\u89e3\u66f4\u591a\u548c Skipgram \u6709\u5173\u7684\u77e5\u8bc6\uff0c\u8bf7\u53c2\u9605\u8fd9\u4efd\u7531 Mikolov \u7b49\u4eba\u53d1\u8868\u7684\u7ecf\u5178\u8bba\u6587\uff1a Efficient Estimation of Word Representations in Vector Space \u53c2\u6570 sequence : \u4e00\u4e2a\u7f16\u7801\u4e3a\u5355\u8bcd\u7d22\u5f15\uff08\u6574\u6570\uff09\u5217\u8868\u7684\u8bcd\u5e8f\u5217\uff08\u53e5\u5b50\uff09\u3002\u5982\u679c\u4f7f\u7528\u4e00\u4e2a sampling_table \uff0c\u8bcd\u7d22\u5f15\u5e94\u8be5\u4ee5\u4e00\u4e2a\u76f8\u5173\u6570\u636e\u96c6\u7684\u8bcd\u7684\u6392\u540d\u5339\u914d\uff08\u4f8b\u5982\uff0c10 \u5c06\u4f1a\u7f16\u7801\u4e3a\u7b2c 10 \u4e2a\u6700\u957f\u51fa\u73b0\u7684\u8bcd\uff09\u3002\u6ce8\u610f\u8bcd\u6c47\u8868\u4e2d\u7684\u7d22\u5f15 0 \u662f\u975e\u5355\u8bcd\uff0c\u5c06\u88ab\u8df3\u8fc7\u3002 vocabulary_size : \u6574\u6570\uff0c\u6700\u5927\u53ef\u80fd\u8bcd\u7d22\u5f15 + 1 window_size : \u6574\u6570\uff0c\u91c7\u6837\u7a97\u53e3\u5927\u5c0f\uff08\u6280\u672f\u4e0a\u662f\u534a\u4e2a\u7a97\u53e3\uff09\u3002\u8bcd w_i \u7684\u7a97\u53e3\u662f [i - window_size, i + window_size+1] \u3002 negative_samples : \u5927\u4e8e\u7b49\u4e8e 0 \u7684\u6d6e\u70b9\u6570\u30020 \u8868\u793a\u975e\u8d1f\uff08\u5373\u968f\u673a\uff09\u91c7\u6837\u30021 \u8868\u793a\u4e0e\u6b63\u6837\u672c\u6570\u76f8\u540c\u3002 shuffle : \u662f\u5426\u5728\u8fd4\u56de\u4e4b\u524d\u5c06\u8fd9\u4e9b\u8bcd\u8bed\u6253\u4e71\u3002 categorical : \u5e03\u5c14\u503c\u3002\u5982\u679c False\uff0c\u6807\u7b7e\u5c06\u4e3a\u6574\u6570\uff08\u4f8b\u5982 [0, 1, 1 .. ] \uff09\uff0c\u5982\u679c True\uff0c\u6807\u7b7e\u5c06\u4e3a\u5206\u7c7b\uff0c\u4f8b\u5982 [[1,0],[0,1],[0,1] .. ] \u3002 sampling_table : \u5c3a\u5bf8\u4e3a vocabulary_size \u7684 1D \u6570\u7ec4\uff0c\u5176\u4e2d\u7b2c i \u9879\u7f16\u7801\u4e86\u6392\u540d\u4e3a i \u7684\u8bcd\u7684\u91c7\u6837\u6982\u7387\u3002 seed : \u968f\u673a\u79cd\u5b50\u3002 \u8fd4\u56de couples, labels: \u5176\u4e2d couples \u662f\u6574\u6570\u5bf9\uff0c labels \u662f 0 \u6216 1\u3002 \u6ce8\u610f \u6309\u7167\u60ef\u4f8b\uff0c\u8bcd\u6c47\u8868\u4e2d\u7684\u7d22\u5f15 0 \u662f\u975e\u5355\u8bcd\uff0c\u5c06\u88ab\u8df3\u8fc7\u3002 make_sampling_table keras.preprocessing.sequence.make_sampling_table(size, sampling_factor=1e-05) \u751f\u6210\u4e00\u4e2a\u57fa\u4e8e\u5355\u8bcd\u7684\u6982\u7387\u91c7\u6837\u8868\u3002 \u7528\u6765\u751f\u6210 skipgrams \u7684 sampling_table \u53c2\u6570\u3002 sampling_table[i] \u662f\u6570\u636e\u96c6\u4e2d\u7b2c i \u4e2a\u6700\u5e38\u89c1\u8bcd\u7684\u91c7\u6837\u6982\u7387\uff08\u51fa\u4e8e\u5e73\u8861\u8003\u8651\uff0c\u51fa\u73b0\u66f4\u9891\u7e41\u7684\u8bcd\u5e94\u8be5\u88ab\u66f4\u5c11\u5730\u91c7\u6837\uff09\u3002 \u91c7\u6837\u6982\u7387\u6839\u636e word2vec \u4e2d\u4f7f\u7528\u7684\u91c7\u6837\u5206\u5e03\u751f\u6210\uff1a p(word) = (min(1, sqrt(word_frequency / sampling_factor) / (word_frequency / sampling_factor))) \u6211\u4eec\u5047\u8bbe\u5355\u8bcd\u9891\u7387\u9075\u5faa Zipf \u5b9a\u5f8b\uff08s=1\uff09\uff0c\u6765\u5bfc\u51fa frequency(rank) \u7684\u6570\u503c\u8fd1\u4f3c\uff1a frequency(rank) ~ 1/(rank * (log(rank) + gamma) + 1/2 - 1/(12*rank)) \uff0c\u5176\u4e2d gamma \u4e3a Euler-Mascheroni \u5e38\u91cf\u3002 \u53c2\u6570 size : \u6574\u6570\uff0c\u53ef\u80fd\u91c7\u6837\u7684\u5355\u8bcd\u6570\u91cf\u3002 sampling_factor : word2vec \u516c\u5f0f\u4e2d\u7684\u91c7\u6837\u56e0\u5b50\u3002 \u8fd4\u56de \u4e00\u4e2a\u957f\u5ea6\u4e3a size \u5927\u5c0f\u7684 1D Numpy \u6570\u7ec4\uff0c\u5176\u4e2d\u7b2c i \u9879\u662f\u6392\u540d\u4e3a i \u7684\u5355\u8bcd\u7684\u91c7\u6837\u6982\u7387\u3002","title":"\u5e8f\u5217\u9884\u5904\u7406"},{"location":"3-Preprocessing/0.sequence/#_1","text":"[source]","title":"\u5e8f\u5217\u9884\u5904\u7406"},{"location":"3-Preprocessing/0.sequence/#timeseriesgenerator","text":"keras.preprocessing.sequence.TimeseriesGenerator(data, targets, length, sampling_rate=1, stride=1, start_index=0, end_index=None, shuffle=False, reverse=False, batch_size=128) \u7528\u4e8e\u751f\u6210\u6279\u91cf\u65f6\u5e8f\u6570\u636e\u7684\u5b9e\u7528\u5de5\u5177\u7c7b\u3002 \u8fd9\u4e2a\u7c7b\u4ee5\u4e00\u7cfb\u5217\u7531\u76f8\u7b49\u95f4\u9694\u4ee5\u53ca\u4e00\u4e9b\u65f6\u95f4\u5e8f\u5217\u53c2\u6570\uff08\u4f8b\u5982\u6b65\u957f\u3001\u5386\u53f2\u957f\u5ea6\u7b49\uff09\u6c47\u96c6\u7684\u6570\u636e\u70b9\u4f5c\u4e3a\u8f93\u5165\uff0c\u4ee5\u751f\u6210\u7528\u4e8e\u8bad\u7ec3/\u9a8c\u8bc1\u7684\u6279\u6b21\u6570\u636e\u3002 \u53c2\u6570 data : \u53ef\u7d22\u5f15\u7684\u751f\u6210\u5668\uff08\u4f8b\u5982\u5217\u8868\u6216 Numpy \u6570\u7ec4\uff09\uff0c\u5305\u542b\u8fde\u7eed\u6570\u636e\u70b9\uff08\u65f6\u95f4\u6b65\uff09\u3002\u6570\u636e\u5e94\u8be5\u662f 2D \u7684\uff0c\u4e14\u7b2c 0 \u4e2a\u8f74\u4e3a\u65f6\u95f4\u7ef4\u5ea6\u3002 targets : \u5bf9\u5e94\u4e8e data \u7684\u65f6\u95f4\u6b65\u7684\u76ee\u6807\u503c\u3002\u5b83\u5e94\u8be5\u4e0e data \u7684\u957f\u5ea6\u76f8\u540c\u3002 length : \u8f93\u51fa\u5e8f\u5217\u7684\u957f\u5ea6\uff08\u4ee5\u65f6\u95f4\u6b65\u6570\u8868\u793a\uff09\u3002 sampling_rate : \u5e8f\u5217\u5185\u8fde\u7eed\u5404\u4e2a\u65f6\u95f4\u6b65\u4e4b\u95f4\u7684\u5468\u671f\u3002\u5bf9\u4e8e\u5468\u671f r , \u65f6\u95f4\u6b65 data[i] , data[i-r] , ... data[i - length] \u88ab\u7528\u4e8e\u751f\u6210\u6837\u672c\u5e8f\u5217\u3002 stride : \u8fde\u7eed\u8f93\u51fa\u5e8f\u5217\u4e4b\u95f4\u7684\u5468\u671f. \u5bf9\u4e8e\u5468\u671f s , \u8fde\u7eed\u8f93\u51fa\u6837\u672c\u5c06\u4e3a data[i] , data[i+s] , data[i+2*s] \u7b49\u3002 start_index : \u5728 start_index \u4e4b\u524d\u7684\u6570\u636e\u70b9\u5728\u8f93\u51fa\u5e8f\u5217\u4e2d\u5c06\u4e0d\u88ab\u4f7f\u7528\u3002\u8fd9\u5bf9\u4fdd\u7559\u90e8\u5206\u6570\u636e\u4ee5\u8fdb\u884c\u6d4b\u8bd5\u6216\u9a8c\u8bc1\u5f88\u6709\u7528\u3002 end_index : \u5728 end_index \u4e4b\u540e\u7684\u6570\u636e\u70b9\u5728\u8f93\u51fa\u5e8f\u5217\u4e2d\u5c06\u4e0d\u88ab\u4f7f\u7528\u3002\u8fd9\u5bf9\u4fdd\u7559\u90e8\u5206\u6570\u636e\u4ee5\u8fdb\u884c\u6d4b\u8bd5\u6216\u9a8c\u8bc1\u5f88\u6709\u7528\u3002 shuffle : \u662f\u5426\u6253\u4e71\u8f93\u51fa\u6837\u672c\uff0c\u8fd8\u662f\u6309\u7167\u65f6\u95f4\u987a\u5e8f\u7ed8\u5236\u5b83\u4eec\u3002 reverse : \u5e03\u5c14\u503c: \u5982\u679c true , \u6bcf\u4e2a\u8f93\u51fa\u6837\u672c\u4e2d\u7684\u65f6\u95f4\u6b65\u5c06\u6309\u7167\u65f6\u95f4\u5012\u5e8f\u6392\u5217\u3002 batch_size : \u6bcf\u4e2a\u6279\u6b21\u4e2d\u7684\u65f6\u95f4\u5e8f\u5217\u6837\u672c\u6570\uff08\u53ef\u80fd\u9664\u6700\u540e\u4e00\u4e2a\u5916\uff09\u3002 \u8fd4\u56de \u4e00\u4e2a Sequence \u5b9e\u4f8b\u3002 \u4f8b\u5b50 from keras.preprocessing.sequence import TimeseriesGenerator import numpy as np data = np.array([[i] for i in range(50)]) targets = np.array([[i] for i in range(50)]) data_gen = TimeseriesGenerator(data, targets, length=10, sampling_rate=2, batch_size=2) assert len(data_gen) == 20 batch_0 = data_gen[0] x, y = batch_0 assert np.array_equal(x, np.array([[[0], [2], [4], [6], [8]], [[1], [3], [5], [7], [9]]])) assert np.array_equal(y, np.array([[10], [11]]))","title":"TimeseriesGenerator"},{"location":"3-Preprocessing/0.sequence/#pad_sequences","text":"keras.preprocessing.sequence.pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.0) \u5c06\u591a\u4e2a\u5e8f\u5217\u622a\u65ad\u6216\u8865\u9f50\u4e3a\u76f8\u540c\u957f\u5ea6\u3002 \u8be5\u51fd\u6570\u5c06\u4e00\u4e2a num_samples \u7684\u5e8f\u5217\uff08\u6574\u6570\u5217\u8868\uff09\u8f6c\u5316\u4e3a\u4e00\u4e2a 2D Numpy \u77e9\u9635\uff0c\u5176\u5c3a\u5bf8\u4e3a (num_samples, num_timesteps) \u3002 num_timesteps \u8981\u4e48\u662f\u7ed9\u5b9a\u7684 maxlen \u53c2\u6570\uff0c\u8981\u4e48\u662f\u6700\u957f\u5e8f\u5217\u7684\u957f\u5ea6\u3002 \u6bd4 num_timesteps \u77ed\u7684\u5e8f\u5217\u5c06\u5728\u672b\u7aef\u4ee5 value \u503c\u8865\u9f50\u3002 \u6bd4 num_timesteps \u957f\u7684\u5e8f\u5217\u5c06\u4f1a\u88ab\u622a\u65ad\u4ee5\u6ee1\u8db3\u6240\u9700\u8981\u7684\u957f\u5ea6\u3002\u8865\u9f50\u6216\u622a\u65ad\u53d1\u751f\u7684\u4f4d\u7f6e\u5206\u522b\u7531\u53c2\u6570 pading \u548c truncating \u51b3\u5b9a\u3002 \u5411\u524d\u8865\u9f50\u4e3a\u9ed8\u8ba4\u64cd\u4f5c\u3002 \u53c2\u6570 sequences : \u5217\u8868\u7684\u5217\u8868\uff0c\u6bcf\u4e00\u4e2a\u5143\u7d20\u662f\u4e00\u4e2a\u5e8f\u5217\u3002 maxlen : \u6574\u6570\uff0c\u6240\u6709\u5e8f\u5217\u7684\u6700\u5927\u957f\u5ea6\u3002 dtype : \u8f93\u51fa\u5e8f\u5217\u7684\u7c7b\u578b\u3002 \u8981\u4f7f\u7528\u53ef\u53d8\u957f\u5ea6\u5b57\u7b26\u4e32\u586b\u5145\u5e8f\u5217\uff0c\u53ef\u4ee5\u4f7f\u7528 object \u3002 padding : \u5b57\u7b26\u4e32\uff0c'pre' \u6216 'post' \uff0c\u5728\u5e8f\u5217\u7684\u524d\u7aef\u8865\u9f50\u8fd8\u662f\u5728\u540e\u7aef\u8865\u9f50\u3002 truncating : \u5b57\u7b26\u4e32\uff0c'pre' \u6216 'post' \uff0c\u79fb\u9664\u957f\u5ea6\u5927\u4e8e maxlen \u7684\u5e8f\u5217\u7684\u503c\uff0c\u8981\u4e48\u5728\u5e8f\u5217\u524d\u7aef\u622a\u65ad\uff0c\u8981\u4e48\u5728\u540e\u7aef\u3002 value : \u6d6e\u70b9\u6570\uff0c\u8868\u793a\u7528\u6765\u8865\u9f50\u7684\u503c\u3002 \u8fd4\u56de x : Numpy \u77e9\u9635\uff0c\u5c3a\u5bf8\u4e3a (len(sequences), maxlen) \u3002 \u5f02\u5e38 ValueError: \u5982\u679c\u622a\u65ad\u6216\u8865\u9f50\u7684\u503c\u65e0\u6548\uff0c\u6216\u8005\u5e8f\u5217\u6761\u76ee\u7684\u5f62\u72b6\u65e0\u6548\u3002","title":"pad_sequences"},{"location":"3-Preprocessing/0.sequence/#skipgrams","text":"keras.preprocessing.sequence.skipgrams(sequence, vocabulary_size, window_size=4, negative_samples=1.0, shuffle=True, categorical=False, sampling_table=None, seed=None) \u751f\u6210 skipgram \u8bcd\u5bf9\u3002 \u8be5\u51fd\u6570\u5c06\u4e00\u4e2a\u5355\u8bcd\u7d22\u5f15\u5e8f\u5217\uff08\u6574\u6570\u5217\u8868\uff09\u8f6c\u5316\u4e3a\u4ee5\u4e0b\u5f62\u5f0f\u7684\u5355\u8bcd\u5143\u7ec4\uff1a \uff08\u5355\u8bcd, \u540c\u7a97\u53e3\u7684\u5355\u8bcd\uff09\uff0c\u6807\u7b7e\u4e3a 1\uff08\u6b63\u6837\u672c\uff09\u3002 \uff08\u5355\u8bcd, \u6765\u81ea\u8bcd\u6c47\u8868\u7684\u968f\u673a\u5355\u8bcd\uff09\uff0c\u6807\u7b7e\u4e3a 0\uff08\u8d1f\u6837\u672c\uff09\u3002 \u82e5\u8981\u4e86\u89e3\u66f4\u591a\u548c Skipgram \u6709\u5173\u7684\u77e5\u8bc6\uff0c\u8bf7\u53c2\u9605\u8fd9\u4efd\u7531 Mikolov \u7b49\u4eba\u53d1\u8868\u7684\u7ecf\u5178\u8bba\u6587\uff1a Efficient Estimation of Word Representations in Vector Space \u53c2\u6570 sequence : \u4e00\u4e2a\u7f16\u7801\u4e3a\u5355\u8bcd\u7d22\u5f15\uff08\u6574\u6570\uff09\u5217\u8868\u7684\u8bcd\u5e8f\u5217\uff08\u53e5\u5b50\uff09\u3002\u5982\u679c\u4f7f\u7528\u4e00\u4e2a sampling_table \uff0c\u8bcd\u7d22\u5f15\u5e94\u8be5\u4ee5\u4e00\u4e2a\u76f8\u5173\u6570\u636e\u96c6\u7684\u8bcd\u7684\u6392\u540d\u5339\u914d\uff08\u4f8b\u5982\uff0c10 \u5c06\u4f1a\u7f16\u7801\u4e3a\u7b2c 10 \u4e2a\u6700\u957f\u51fa\u73b0\u7684\u8bcd\uff09\u3002\u6ce8\u610f\u8bcd\u6c47\u8868\u4e2d\u7684\u7d22\u5f15 0 \u662f\u975e\u5355\u8bcd\uff0c\u5c06\u88ab\u8df3\u8fc7\u3002 vocabulary_size : \u6574\u6570\uff0c\u6700\u5927\u53ef\u80fd\u8bcd\u7d22\u5f15 + 1 window_size : \u6574\u6570\uff0c\u91c7\u6837\u7a97\u53e3\u5927\u5c0f\uff08\u6280\u672f\u4e0a\u662f\u534a\u4e2a\u7a97\u53e3\uff09\u3002\u8bcd w_i \u7684\u7a97\u53e3\u662f [i - window_size, i + window_size+1] \u3002 negative_samples : \u5927\u4e8e\u7b49\u4e8e 0 \u7684\u6d6e\u70b9\u6570\u30020 \u8868\u793a\u975e\u8d1f\uff08\u5373\u968f\u673a\uff09\u91c7\u6837\u30021 \u8868\u793a\u4e0e\u6b63\u6837\u672c\u6570\u76f8\u540c\u3002 shuffle : \u662f\u5426\u5728\u8fd4\u56de\u4e4b\u524d\u5c06\u8fd9\u4e9b\u8bcd\u8bed\u6253\u4e71\u3002 categorical : \u5e03\u5c14\u503c\u3002\u5982\u679c False\uff0c\u6807\u7b7e\u5c06\u4e3a\u6574\u6570\uff08\u4f8b\u5982 [0, 1, 1 .. ] \uff09\uff0c\u5982\u679c True\uff0c\u6807\u7b7e\u5c06\u4e3a\u5206\u7c7b\uff0c\u4f8b\u5982 [[1,0],[0,1],[0,1] .. ] \u3002 sampling_table : \u5c3a\u5bf8\u4e3a vocabulary_size \u7684 1D \u6570\u7ec4\uff0c\u5176\u4e2d\u7b2c i \u9879\u7f16\u7801\u4e86\u6392\u540d\u4e3a i \u7684\u8bcd\u7684\u91c7\u6837\u6982\u7387\u3002 seed : \u968f\u673a\u79cd\u5b50\u3002 \u8fd4\u56de couples, labels: \u5176\u4e2d couples \u662f\u6574\u6570\u5bf9\uff0c labels \u662f 0 \u6216 1\u3002 \u6ce8\u610f \u6309\u7167\u60ef\u4f8b\uff0c\u8bcd\u6c47\u8868\u4e2d\u7684\u7d22\u5f15 0 \u662f\u975e\u5355\u8bcd\uff0c\u5c06\u88ab\u8df3\u8fc7\u3002","title":"skipgrams"},{"location":"3-Preprocessing/0.sequence/#make_sampling_table","text":"keras.preprocessing.sequence.make_sampling_table(size, sampling_factor=1e-05) \u751f\u6210\u4e00\u4e2a\u57fa\u4e8e\u5355\u8bcd\u7684\u6982\u7387\u91c7\u6837\u8868\u3002 \u7528\u6765\u751f\u6210 skipgrams \u7684 sampling_table \u53c2\u6570\u3002 sampling_table[i] \u662f\u6570\u636e\u96c6\u4e2d\u7b2c i \u4e2a\u6700\u5e38\u89c1\u8bcd\u7684\u91c7\u6837\u6982\u7387\uff08\u51fa\u4e8e\u5e73\u8861\u8003\u8651\uff0c\u51fa\u73b0\u66f4\u9891\u7e41\u7684\u8bcd\u5e94\u8be5\u88ab\u66f4\u5c11\u5730\u91c7\u6837\uff09\u3002 \u91c7\u6837\u6982\u7387\u6839\u636e word2vec \u4e2d\u4f7f\u7528\u7684\u91c7\u6837\u5206\u5e03\u751f\u6210\uff1a p(word) = (min(1, sqrt(word_frequency / sampling_factor) / (word_frequency / sampling_factor))) \u6211\u4eec\u5047\u8bbe\u5355\u8bcd\u9891\u7387\u9075\u5faa Zipf \u5b9a\u5f8b\uff08s=1\uff09\uff0c\u6765\u5bfc\u51fa frequency(rank) \u7684\u6570\u503c\u8fd1\u4f3c\uff1a frequency(rank) ~ 1/(rank * (log(rank) + gamma) + 1/2 - 1/(12*rank)) \uff0c\u5176\u4e2d gamma \u4e3a Euler-Mascheroni \u5e38\u91cf\u3002 \u53c2\u6570 size : \u6574\u6570\uff0c\u53ef\u80fd\u91c7\u6837\u7684\u5355\u8bcd\u6570\u91cf\u3002 sampling_factor : word2vec \u516c\u5f0f\u4e2d\u7684\u91c7\u6837\u56e0\u5b50\u3002 \u8fd4\u56de \u4e00\u4e2a\u957f\u5ea6\u4e3a size \u5927\u5c0f\u7684 1D Numpy \u6570\u7ec4\uff0c\u5176\u4e2d\u7b2c i \u9879\u662f\u6392\u540d\u4e3a i \u7684\u5355\u8bcd\u7684\u91c7\u6837\u6982\u7387\u3002","title":"make_sampling_table"},{"location":"3-Preprocessing/1.text/","text":"\u6587\u672c\u9884\u5904\u7406 Text Preprocessing [source] Tokenizer keras.preprocessing.text.Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ ', lower=True, split=' ', char_level=False, oov_token=None, document_count=0) \u6587\u672c\u6807\u8bb0\u5b9e\u7528\u7c7b\u3002 \u8be5\u7c7b\u5141\u8bb8\u4f7f\u7528\u4e24\u79cd\u65b9\u6cd5\u5411\u91cf\u5316\u4e00\u4e2a\u6587\u672c\u8bed\u6599\u5e93\uff1a \u5c06\u6bcf\u4e2a\u6587\u672c\u8f6c\u5316\u4e3a\u4e00\u4e2a\u6574\u6570\u5e8f\u5217\uff08\u6bcf\u4e2a\u6574\u6570\u90fd\u662f\u8bcd\u5178\u4e2d\u6807\u8bb0\u7684\u7d22\u5f15\uff09\uff1b \u6216\u8005\u5c06\u5176\u8f6c\u5316\u4e3a\u4e00\u4e2a\u5411\u91cf\uff0c\u5176\u4e2d\u6bcf\u4e2a\u6807\u8bb0\u7684\u7cfb\u6570\u53ef\u4ee5\u662f\u4e8c\u8fdb\u5236\u503c\u3001\u8bcd\u9891\u3001TF-IDF \u6743\u91cd\u7b49\u3002 \u53c2\u6570 num_words : \u9700\u8981\u4fdd\u7559\u7684\u6700\u5927\u8bcd\u6570\uff0c\u57fa\u4e8e\u8bcd\u9891\u3002\u53ea\u6709\u6700\u5e38\u51fa\u73b0\u7684 num_words \u8bcd\u4f1a\u88ab\u4fdd\u7559\u3002 filters : \u4e00\u4e2a\u5b57\u7b26\u4e32\uff0c\u5176\u4e2d\u6bcf\u4e2a\u5143\u7d20\u662f\u4e00\u4e2a\u5c06\u4ece\u6587\u672c\u4e2d\u8fc7\u6ee4\u6389\u7684\u5b57\u7b26\u3002\u9ed8\u8ba4\u503c\u662f\u6240\u6709\u6807\u70b9\u7b26\u53f7\uff0c\u52a0\u4e0a\u5236\u8868\u7b26\u548c\u6362\u884c\u7b26\uff0c\u51cf\u53bb ' \u5b57\u7b26\u3002 lower : \u5e03\u5c14\u503c\u3002\u662f\u5426\u5c06\u6587\u672c\u8f6c\u6362\u4e3a\u5c0f\u5199\u3002 split : \u5b57\u7b26\u4e32\u3002\u6309\u8be5\u5b57\u7b26\u4e32\u5207\u5272\u6587\u672c\u3002 char_level : \u5982\u679c\u4e3a True\uff0c\u5219\u6bcf\u4e2a\u5b57\u7b26\u90fd\u5c06\u88ab\u89c6\u4e3a\u6807\u8bb0\u3002 oov_token : \u5982\u679c\u7ed9\u51fa\uff0c\u5b83\u5c06\u88ab\u6dfb\u52a0\u5230 word_index \u4e2d\uff0c\u5e76\u7528\u4e8e\u5728 text_to_sequence \u8c03\u7528\u671f\u95f4\u66ff\u6362\u8bcd\u6c47\u8868\u5916\u7684\u5355\u8bcd\u3002 \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u5220\u9664\u6240\u6709\u6807\u70b9\u7b26\u53f7\uff0c\u5c06\u6587\u672c\u8f6c\u6362\u4e3a\u7a7a\u683c\u5206\u9694\u7684\u5355\u8bcd\u5e8f\u5217\uff08\u5355\u8bcd\u53ef\u80fd\u5305\u542b ' \u5b57\u7b26\uff09\u3002 \u8fd9\u4e9b\u5e8f\u5217\u7136\u540e\u88ab\u5206\u5272\u6210\u6807\u8bb0\u5217\u8868\u3002\u7136\u540e\u5b83\u4eec\u5c06\u88ab\u7d22\u5f15\u6216\u5411\u91cf\u5316\u3002 0 \u662f\u4e0d\u4f1a\u88ab\u5206\u914d\u7ed9\u4efb\u4f55\u5355\u8bcd\u7684\u4fdd\u7559\u7d22\u5f15\u3002 hashing_trick keras.preprocessing.text.hashing_trick(text, n, hash_function=None, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ ', lower=True, split=' ') \u5c06\u6587\u672c\u8f6c\u6362\u4e3a\u56fa\u5b9a\u5927\u5c0f\u6563\u5217\u7a7a\u95f4\u4e2d\u7684\u7d22\u5f15\u5e8f\u5217\u3002 \u53c2\u6570 text : \u8f93\u5165\u6587\u672c\uff08\u5b57\u7b26\u4e32\uff09\u3002 n : \u6563\u5217\u7a7a\u95f4\u7ef4\u5ea6\u3002 hash_function : \u9ed8\u8ba4\u4e3a python \u6563\u5217\u51fd\u6570\uff0c\u53ef\u4ee5\u662f 'md5' \u6216\u4efb\u610f\u63a5\u53d7\u8f93\u5165\u5b57\u7b26\u4e32\u5e76\u8fd4\u56de\u6574\u6570\u7684\u51fd\u6570\u3002\u6ce8\u610f 'hash' \u4e0d\u662f\u7a33\u5b9a\u7684\u6563\u5217\u51fd\u6570\uff0c\u6240\u4ee5\u5b83\u5728\u4e0d\u540c\u7684\u8fd0\u884c\u4e2d\u4e0d\u4e00\u81f4\uff0c\u800c 'md5' \u662f\u4e00\u4e2a\u7a33\u5b9a\u7684\u6563\u5217\u51fd\u6570\u3002 filters : \u8981\u8fc7\u6ee4\u7684\u5b57\u7b26\u5217\u8868\uff08\u6216\u8fde\u63a5\uff09\uff0c\u5982\u6807\u70b9\u7b26\u53f7\u3002\u9ed8\u8ba4\uff1a !\"#$%&()*+,-./:;<=>?@[\\]^_{|}~ \uff0c\u5305\u542b\u57fa\u672c\u6807\u70b9\u7b26\u53f7\uff0c\u5236\u8868\u7b26\u548c\u6362\u884c\u7b26\u3002 lower : \u5e03\u5c14\u503c\u3002\u662f\u5426\u5c06\u6587\u672c\u8f6c\u6362\u4e3a\u5c0f\u5199\u3002 split : \u5b57\u7b26\u4e32\u3002\u6309\u8be5\u5b57\u7b26\u4e32\u5207\u5272\u6587\u672c\u3002 \u8fd4\u56de \u6574\u6570\u8bcd\u7d22\u5f15\u5217\u8868\uff08\u552f\u4e00\u6027\u65e0\u6cd5\u4fdd\u8bc1\uff09\u3002 0 \u662f\u4e0d\u4f1a\u88ab\u5206\u914d\u7ed9\u4efb\u4f55\u5355\u8bcd\u7684\u4fdd\u7559\u7d22\u5f15\u3002 \u7531\u4e8e\u54c8\u5e0c\u51fd\u6570\u53ef\u80fd\u53d1\u751f\u51b2\u7a81\uff0c\u53ef\u80fd\u4f1a\u5c06\u4e24\u4e2a\u6216\u66f4\u591a\u5b57\u5206\u914d\u7ed9\u540c\u4e00\u7d22\u5f15\u3002 \u78b0\u649e\u7684 \u6982\u7387 \u4e0e\u6563\u5217\u7a7a\u95f4\u7684\u7ef4\u5ea6\u548c\u4e0d\u540c\u5bf9\u8c61\u7684\u6570\u91cf\u6709\u5173\u3002 one_hot keras.preprocessing.text.one_hot(text, n, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True, split=' ') One-hot \u5c06\u6587\u672c\u7f16\u7801\u4e3a\u5927\u5c0f\u4e3a n \u7684\u5355\u8bcd\u7d22\u5f15\u5217\u8868\u3002 \u8fd9\u662f hashing_trick \u51fd\u6570\u7684\u4e00\u4e2a\u5c01\u88c5\uff0c \u4f7f\u7528 hash \u4f5c\u4e3a\u6563\u5217\u51fd\u6570\uff1b\u5355\u8bcd\u7d22\u5f15\u6620\u5c04\u65e0\u4fdd\u8bc1\u552f\u4e00\u6027\u3002 \u53c2\u6570 text : \u8f93\u5165\u6587\u672c\uff08\u5b57\u7b26\u4e32\uff09\u3002 n : \u6574\u6570\u3002\u8bcd\u6c47\u8868\u5c3a\u5bf8\u3002 filters : \u8981\u8fc7\u6ee4\u7684\u5b57\u7b26\u5217\u8868\uff08\u6216\u8fde\u63a5\uff09\uff0c\u5982\u6807\u70b9\u7b26\u53f7\u3002\u9ed8\u8ba4\uff1a !\"#$%&()*+,-./:;<=>?@[\\]^_{|}~ \uff0c\u5305\u542b\u57fa\u672c\u6807\u70b9\u7b26\u53f7\uff0c\u5236\u8868\u7b26\u548c\u6362\u884c\u7b26\u3002 lower : \u5e03\u5c14\u503c\u3002\u662f\u5426\u5c06\u6587\u672c\u8f6c\u6362\u4e3a\u5c0f\u5199\u3002 split : \u5b57\u7b26\u4e32\u3002\u6309\u8be5\u5b57\u7b26\u4e32\u5207\u5272\u6587\u672c\u3002 \u8fd4\u56de [1, n] \u4e4b\u95f4\u7684\u6574\u6570\u5217\u8868\u3002\u6bcf\u4e2a\u6574\u6570\u7f16\u7801\u4e00\u4e2a\u8bcd\uff08\u552f\u4e00\u6027\u65e0\u6cd5\u4fdd\u8bc1\uff09\u3002 text_to_word_sequence keras.preprocessing.text.text_to_word_sequence(text, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ ', lower=True, split=' ') \u5c06\u6587\u672c\u8f6c\u6362\u4e3a\u5355\u8bcd\uff08\u6216\u6807\u8bb0\uff09\u7684\u5e8f\u5217\u3002 \u53c2\u6570 text : \u8f93\u5165\u6587\u672c\uff08\u5b57\u7b26\u4e32\uff09\u3002 filters : \u8981\u8fc7\u6ee4\u7684\u5b57\u7b26\u5217\u8868\uff08\u6216\u8fde\u63a5\uff09\uff0c\u5982\u6807\u70b9\u7b26\u53f7\u3002\u9ed8\u8ba4\uff1a !\"#$%&()*+,-./:;<=>?@[\\]^_{|}~ \uff0c\u5305\u542b\u57fa\u672c\u6807\u70b9\u7b26\u53f7\uff0c\u5236\u8868\u7b26\u548c\u6362\u884c\u7b26\u3002 lower : \u5e03\u5c14\u503c\u3002\u662f\u5426\u5c06\u6587\u672c\u8f6c\u6362\u4e3a\u5c0f\u5199\u3002 split : \u5b57\u7b26\u4e32\u3002\u6309\u8be5\u5b57\u7b26\u4e32\u5207\u5272\u6587\u672c\u3002 \u8fd4\u56de \u8bcd\u6216\u6807\u8bb0\u7684\u5217\u8868\u3002","title":"\u6587\u672c\u9884\u5904\u7406"},{"location":"3-Preprocessing/1.text/#_1","text":"","title":"\u6587\u672c\u9884\u5904\u7406"},{"location":"3-Preprocessing/1.text/#text-preprocessing","text":"[source]","title":"Text Preprocessing"},{"location":"3-Preprocessing/1.text/#tokenizer","text":"keras.preprocessing.text.Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ ', lower=True, split=' ', char_level=False, oov_token=None, document_count=0) \u6587\u672c\u6807\u8bb0\u5b9e\u7528\u7c7b\u3002 \u8be5\u7c7b\u5141\u8bb8\u4f7f\u7528\u4e24\u79cd\u65b9\u6cd5\u5411\u91cf\u5316\u4e00\u4e2a\u6587\u672c\u8bed\u6599\u5e93\uff1a \u5c06\u6bcf\u4e2a\u6587\u672c\u8f6c\u5316\u4e3a\u4e00\u4e2a\u6574\u6570\u5e8f\u5217\uff08\u6bcf\u4e2a\u6574\u6570\u90fd\u662f\u8bcd\u5178\u4e2d\u6807\u8bb0\u7684\u7d22\u5f15\uff09\uff1b \u6216\u8005\u5c06\u5176\u8f6c\u5316\u4e3a\u4e00\u4e2a\u5411\u91cf\uff0c\u5176\u4e2d\u6bcf\u4e2a\u6807\u8bb0\u7684\u7cfb\u6570\u53ef\u4ee5\u662f\u4e8c\u8fdb\u5236\u503c\u3001\u8bcd\u9891\u3001TF-IDF \u6743\u91cd\u7b49\u3002 \u53c2\u6570 num_words : \u9700\u8981\u4fdd\u7559\u7684\u6700\u5927\u8bcd\u6570\uff0c\u57fa\u4e8e\u8bcd\u9891\u3002\u53ea\u6709\u6700\u5e38\u51fa\u73b0\u7684 num_words \u8bcd\u4f1a\u88ab\u4fdd\u7559\u3002 filters : \u4e00\u4e2a\u5b57\u7b26\u4e32\uff0c\u5176\u4e2d\u6bcf\u4e2a\u5143\u7d20\u662f\u4e00\u4e2a\u5c06\u4ece\u6587\u672c\u4e2d\u8fc7\u6ee4\u6389\u7684\u5b57\u7b26\u3002\u9ed8\u8ba4\u503c\u662f\u6240\u6709\u6807\u70b9\u7b26\u53f7\uff0c\u52a0\u4e0a\u5236\u8868\u7b26\u548c\u6362\u884c\u7b26\uff0c\u51cf\u53bb ' \u5b57\u7b26\u3002 lower : \u5e03\u5c14\u503c\u3002\u662f\u5426\u5c06\u6587\u672c\u8f6c\u6362\u4e3a\u5c0f\u5199\u3002 split : \u5b57\u7b26\u4e32\u3002\u6309\u8be5\u5b57\u7b26\u4e32\u5207\u5272\u6587\u672c\u3002 char_level : \u5982\u679c\u4e3a True\uff0c\u5219\u6bcf\u4e2a\u5b57\u7b26\u90fd\u5c06\u88ab\u89c6\u4e3a\u6807\u8bb0\u3002 oov_token : \u5982\u679c\u7ed9\u51fa\uff0c\u5b83\u5c06\u88ab\u6dfb\u52a0\u5230 word_index \u4e2d\uff0c\u5e76\u7528\u4e8e\u5728 text_to_sequence \u8c03\u7528\u671f\u95f4\u66ff\u6362\u8bcd\u6c47\u8868\u5916\u7684\u5355\u8bcd\u3002 \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u5220\u9664\u6240\u6709\u6807\u70b9\u7b26\u53f7\uff0c\u5c06\u6587\u672c\u8f6c\u6362\u4e3a\u7a7a\u683c\u5206\u9694\u7684\u5355\u8bcd\u5e8f\u5217\uff08\u5355\u8bcd\u53ef\u80fd\u5305\u542b ' \u5b57\u7b26\uff09\u3002 \u8fd9\u4e9b\u5e8f\u5217\u7136\u540e\u88ab\u5206\u5272\u6210\u6807\u8bb0\u5217\u8868\u3002\u7136\u540e\u5b83\u4eec\u5c06\u88ab\u7d22\u5f15\u6216\u5411\u91cf\u5316\u3002 0 \u662f\u4e0d\u4f1a\u88ab\u5206\u914d\u7ed9\u4efb\u4f55\u5355\u8bcd\u7684\u4fdd\u7559\u7d22\u5f15\u3002","title":"Tokenizer"},{"location":"3-Preprocessing/1.text/#hashing_trick","text":"keras.preprocessing.text.hashing_trick(text, n, hash_function=None, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ ', lower=True, split=' ') \u5c06\u6587\u672c\u8f6c\u6362\u4e3a\u56fa\u5b9a\u5927\u5c0f\u6563\u5217\u7a7a\u95f4\u4e2d\u7684\u7d22\u5f15\u5e8f\u5217\u3002 \u53c2\u6570 text : \u8f93\u5165\u6587\u672c\uff08\u5b57\u7b26\u4e32\uff09\u3002 n : \u6563\u5217\u7a7a\u95f4\u7ef4\u5ea6\u3002 hash_function : \u9ed8\u8ba4\u4e3a python \u6563\u5217\u51fd\u6570\uff0c\u53ef\u4ee5\u662f 'md5' \u6216\u4efb\u610f\u63a5\u53d7\u8f93\u5165\u5b57\u7b26\u4e32\u5e76\u8fd4\u56de\u6574\u6570\u7684\u51fd\u6570\u3002\u6ce8\u610f 'hash' \u4e0d\u662f\u7a33\u5b9a\u7684\u6563\u5217\u51fd\u6570\uff0c\u6240\u4ee5\u5b83\u5728\u4e0d\u540c\u7684\u8fd0\u884c\u4e2d\u4e0d\u4e00\u81f4\uff0c\u800c 'md5' \u662f\u4e00\u4e2a\u7a33\u5b9a\u7684\u6563\u5217\u51fd\u6570\u3002 filters : \u8981\u8fc7\u6ee4\u7684\u5b57\u7b26\u5217\u8868\uff08\u6216\u8fde\u63a5\uff09\uff0c\u5982\u6807\u70b9\u7b26\u53f7\u3002\u9ed8\u8ba4\uff1a !\"#$%&()*+,-./:;<=>?@[\\]^_{|}~ \uff0c\u5305\u542b\u57fa\u672c\u6807\u70b9\u7b26\u53f7\uff0c\u5236\u8868\u7b26\u548c\u6362\u884c\u7b26\u3002 lower : \u5e03\u5c14\u503c\u3002\u662f\u5426\u5c06\u6587\u672c\u8f6c\u6362\u4e3a\u5c0f\u5199\u3002 split : \u5b57\u7b26\u4e32\u3002\u6309\u8be5\u5b57\u7b26\u4e32\u5207\u5272\u6587\u672c\u3002 \u8fd4\u56de \u6574\u6570\u8bcd\u7d22\u5f15\u5217\u8868\uff08\u552f\u4e00\u6027\u65e0\u6cd5\u4fdd\u8bc1\uff09\u3002 0 \u662f\u4e0d\u4f1a\u88ab\u5206\u914d\u7ed9\u4efb\u4f55\u5355\u8bcd\u7684\u4fdd\u7559\u7d22\u5f15\u3002 \u7531\u4e8e\u54c8\u5e0c\u51fd\u6570\u53ef\u80fd\u53d1\u751f\u51b2\u7a81\uff0c\u53ef\u80fd\u4f1a\u5c06\u4e24\u4e2a\u6216\u66f4\u591a\u5b57\u5206\u914d\u7ed9\u540c\u4e00\u7d22\u5f15\u3002 \u78b0\u649e\u7684 \u6982\u7387 \u4e0e\u6563\u5217\u7a7a\u95f4\u7684\u7ef4\u5ea6\u548c\u4e0d\u540c\u5bf9\u8c61\u7684\u6570\u91cf\u6709\u5173\u3002","title":"hashing_trick"},{"location":"3-Preprocessing/1.text/#one_hot","text":"keras.preprocessing.text.one_hot(text, n, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True, split=' ') One-hot \u5c06\u6587\u672c\u7f16\u7801\u4e3a\u5927\u5c0f\u4e3a n \u7684\u5355\u8bcd\u7d22\u5f15\u5217\u8868\u3002 \u8fd9\u662f hashing_trick \u51fd\u6570\u7684\u4e00\u4e2a\u5c01\u88c5\uff0c \u4f7f\u7528 hash \u4f5c\u4e3a\u6563\u5217\u51fd\u6570\uff1b\u5355\u8bcd\u7d22\u5f15\u6620\u5c04\u65e0\u4fdd\u8bc1\u552f\u4e00\u6027\u3002 \u53c2\u6570 text : \u8f93\u5165\u6587\u672c\uff08\u5b57\u7b26\u4e32\uff09\u3002 n : \u6574\u6570\u3002\u8bcd\u6c47\u8868\u5c3a\u5bf8\u3002 filters : \u8981\u8fc7\u6ee4\u7684\u5b57\u7b26\u5217\u8868\uff08\u6216\u8fde\u63a5\uff09\uff0c\u5982\u6807\u70b9\u7b26\u53f7\u3002\u9ed8\u8ba4\uff1a !\"#$%&()*+,-./:;<=>?@[\\]^_{|}~ \uff0c\u5305\u542b\u57fa\u672c\u6807\u70b9\u7b26\u53f7\uff0c\u5236\u8868\u7b26\u548c\u6362\u884c\u7b26\u3002 lower : \u5e03\u5c14\u503c\u3002\u662f\u5426\u5c06\u6587\u672c\u8f6c\u6362\u4e3a\u5c0f\u5199\u3002 split : \u5b57\u7b26\u4e32\u3002\u6309\u8be5\u5b57\u7b26\u4e32\u5207\u5272\u6587\u672c\u3002 \u8fd4\u56de [1, n] \u4e4b\u95f4\u7684\u6574\u6570\u5217\u8868\u3002\u6bcf\u4e2a\u6574\u6570\u7f16\u7801\u4e00\u4e2a\u8bcd\uff08\u552f\u4e00\u6027\u65e0\u6cd5\u4fdd\u8bc1\uff09\u3002","title":"one_hot"},{"location":"3-Preprocessing/1.text/#text_to_word_sequence","text":"keras.preprocessing.text.text_to_word_sequence(text, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ ', lower=True, split=' ') \u5c06\u6587\u672c\u8f6c\u6362\u4e3a\u5355\u8bcd\uff08\u6216\u6807\u8bb0\uff09\u7684\u5e8f\u5217\u3002 \u53c2\u6570 text : \u8f93\u5165\u6587\u672c\uff08\u5b57\u7b26\u4e32\uff09\u3002 filters : \u8981\u8fc7\u6ee4\u7684\u5b57\u7b26\u5217\u8868\uff08\u6216\u8fde\u63a5\uff09\uff0c\u5982\u6807\u70b9\u7b26\u53f7\u3002\u9ed8\u8ba4\uff1a !\"#$%&()*+,-./:;<=>?@[\\]^_{|}~ \uff0c\u5305\u542b\u57fa\u672c\u6807\u70b9\u7b26\u53f7\uff0c\u5236\u8868\u7b26\u548c\u6362\u884c\u7b26\u3002 lower : \u5e03\u5c14\u503c\u3002\u662f\u5426\u5c06\u6587\u672c\u8f6c\u6362\u4e3a\u5c0f\u5199\u3002 split : \u5b57\u7b26\u4e32\u3002\u6309\u8be5\u5b57\u7b26\u4e32\u5207\u5272\u6587\u672c\u3002 \u8fd4\u56de \u8bcd\u6216\u6807\u8bb0\u7684\u5217\u8868\u3002","title":"text_to_word_sequence"},{"location":"3-Preprocessing/2.image/","text":"\u56fe\u50cf\u9884\u5904\u7406 [source] ImageDataGenerator \u7c7b keras.preprocessing.image.ImageDataGenerator(featurewise_center=False, samplewise_center=False, featurewise_std_normalization=False, samplewise_std_normalization=False, zca_whitening=False, zca_epsilon=1e-06, rotation_range=0, width_shift_range=0.0, height_shift_range=0.0, brightness_range=None, shear_range=0.0, zoom_range=0.0, channel_shift_range=0.0, fill_mode='nearest', cval=0.0, horizontal_flip=False, vertical_flip=False, rescale=None, preprocessing_function=None, data_format=None, validation_split=0.0, dtype=None) \u901a\u8fc7\u5b9e\u65f6\u6570\u636e\u589e\u5f3a\u751f\u6210\u5f20\u91cf\u56fe\u50cf\u6570\u636e\u6279\u6b21\u3002\u6570\u636e\u5c06\u4e0d\u65ad\u5faa\u73af\uff08\u6309\u6279\u6b21\uff09\u3002 \u53c2\u6570 featurewise_center : \u5e03\u5c14\u503c\u3002\u5c06\u8f93\u5165\u6570\u636e\u7684\u5747\u503c\u8bbe\u7f6e\u4e3a 0\uff0c\u9010\u7279\u5f81\u8fdb\u884c\u3002 samplewise_center : \u5e03\u5c14\u503c\u3002\u5c06\u6bcf\u4e2a\u6837\u672c\u7684\u5747\u503c\u8bbe\u7f6e\u4e3a 0\u3002 featurewise_std_normalization : Boolean. \u5e03\u5c14\u503c\u3002\u5c06\u8f93\u5165\u9664\u4ee5\u6570\u636e\u6807\u51c6\u5dee\uff0c\u9010\u7279\u5f81\u8fdb\u884c\u3002 samplewise_std_normalization : \u5e03\u5c14\u503c\u3002\u5c06\u6bcf\u4e2a\u8f93\u5165\u9664\u4ee5\u5176\u6807\u51c6\u5dee\u3002 zca_epsilon : ZCA \u767d\u5316\u7684 epsilon \u503c\uff0c\u9ed8\u8ba4\u4e3a 1e-6\u3002 zca_whitening : \u5e03\u5c14\u503c\u3002\u662f\u5426\u5e94\u7528 ZCA \u767d\u5316\u3002 rotation_range : \u6574\u6570\u3002\u968f\u673a\u65cb\u8f6c\u7684\u5ea6\u6570\u8303\u56f4\u3002 width_shift_range : \u6d6e\u70b9\u6570\u3001\u4e00\u7ef4\u6570\u7ec4\u6216\u6574\u6570 float: \u5982\u679c <1\uff0c\u5219\u662f\u9664\u4ee5\u603b\u5bbd\u5ea6\u7684\u503c\uff0c\u6216\u8005\u5982\u679c >=1\uff0c\u5219\u4e3a\u50cf\u7d20\u503c\u3002 1-D \u6570\u7ec4: \u6570\u7ec4\u4e2d\u7684\u968f\u673a\u5143\u7d20\u3002 int: \u6765\u81ea\u95f4\u9694 (-width_shift_range, +width_shift_range) \u4e4b\u95f4\u7684\u6574\u6570\u4e2a\u50cf\u7d20\u3002 width_shift_range=2 \u65f6\uff0c\u53ef\u80fd\u503c\u662f\u6574\u6570 [-1, 0, +1] \uff0c\u4e0e width_shift_range=[-1, 0, +1] \u76f8\u540c\uff1b\u800c width_shift_range=1.0 \u65f6\uff0c\u53ef\u80fd\u503c\u662f [-1.0, +1.0) \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 height_shift_range : \u6d6e\u70b9\u6570\u3001\u4e00\u7ef4\u6570\u7ec4\u6216\u6574\u6570 float: \u5982\u679c <1\uff0c\u5219\u662f\u9664\u4ee5\u603b\u5bbd\u5ea6\u7684\u503c\uff0c\u6216\u8005\u5982\u679c >=1\uff0c\u5219\u4e3a\u50cf\u7d20\u503c\u3002 1-D array-like: \u6570\u7ec4\u4e2d\u7684\u968f\u673a\u5143\u7d20\u3002 int: \u6765\u81ea\u95f4\u9694 (-height_shift_range, +height_shift_range) \u4e4b\u95f4\u7684\u6574\u6570\u4e2a\u50cf\u7d20\u3002 height_shift_range=2 \u65f6\uff0c\u53ef\u80fd\u503c\u662f\u6574\u6570 [-1, 0, +1] \uff0c\u4e0e height_shift_range=[-1, 0, +1] \u76f8\u540c\uff1b\u800c height_shift_range=1.0 \u65f6\uff0c\u53ef\u80fd\u503c\u662f [-1.0, +1.0) \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 shear_range : \u6d6e\u70b9\u6570\u3002\u526a\u5207\u5f3a\u5ea6\uff08\u4ee5\u5f27\u5ea6\u9006\u65f6\u9488\u65b9\u5411\u526a\u5207\u89d2\u5ea6\uff09\u3002 zoom_range : \u6d6e\u70b9\u6570 \u6216 [lower, upper] \u3002\u968f\u673a\u7f29\u653e\u8303\u56f4\u3002\u5982\u679c\u662f\u6d6e\u70b9\u6570\uff0c [lower, upper] = [1-zoom_range, 1+zoom_range] \u3002 channel_shift_range : \u6d6e\u70b9\u6570\u3002\u968f\u673a\u901a\u9053\u8f6c\u6362\u7684\u8303\u56f4\u3002 fill_mode : {\"constant\", \"nearest\", \"reflect\" or \"wrap\"} \u4e4b\u4e00\u3002\u9ed8\u8ba4\u4e3a 'nearest'\u3002\u8f93\u5165\u8fb9\u754c\u4ee5\u5916\u7684\u70b9\u6839\u636e\u7ed9\u5b9a\u7684\u6a21\u5f0f\u586b\u5145\uff1a 'constant': kkkkkkkk|abcd|kkkkkkkk (cval=k) 'nearest': aaaaaaaa|abcd|dddddddd 'reflect': abcddcba|abcd|dcbaabcd 'wrap': abcdabcd|abcd|abcdabcd cval : \u6d6e\u70b9\u6570\u6216\u6574\u6570\u3002\u7528\u4e8e\u8fb9\u754c\u4e4b\u5916\u7684\u70b9\u7684\u503c\uff0c\u5f53 fill_mode = \"constant\" \u65f6\u3002 horizontal_flip : \u5e03\u5c14\u503c\u3002\u968f\u673a\u6c34\u5e73\u7ffb\u8f6c\u3002 vertical_flip : \u5e03\u5c14\u503c\u3002\u968f\u673a\u5782\u76f4\u7ffb\u8f6c\u3002 rescale : \u91cd\u7f29\u653e\u56e0\u5b50\u3002\u9ed8\u8ba4\u4e3a None\u3002\u5982\u679c\u662f None \u6216 0\uff0c\u4e0d\u8fdb\u884c\u7f29\u653e\uff0c\u5426\u5219\u5c06\u6570\u636e\u4e58\u4ee5\u6240\u63d0\u4f9b\u7684\u503c\uff08\u5728\u5e94\u7528\u4efb\u4f55\u5176\u4ed6\u8f6c\u6362\u4e4b\u524d\uff09\u3002 preprocessing_function : \u5e94\u7528\u4e8e\u6bcf\u4e2a\u8f93\u5165\u7684\u51fd\u6570\u3002\u8fd9\u4e2a\u51fd\u6570\u4f1a\u5728\u4efb\u4f55\u5176\u4ed6\u6539\u53d8\u4e4b\u524d\u8fd0\u884c\u3002\u8fd9\u4e2a\u51fd\u6570\u9700\u8981\u4e00\u4e2a\u53c2\u6570\uff1a\u4e00\u5f20\u56fe\u50cf\uff08\u79e9\u4e3a 3 \u7684 Numpy \u5f20\u91cf\uff09\uff0c\u5e76\u4e14\u5e94\u8be5\u8f93\u51fa\u4e00\u4e2a\u540c\u5c3a\u5bf8\u7684 Numpy \u5f20\u91cf\u3002 data_format : \u56fe\u50cf\u6570\u636e\u683c\u5f0f\uff0c{\"channels_first\", \"channels_last\"} \u4e4b\u4e00\u3002\"channels_last\" \u6a21\u5f0f\u8868\u793a\u56fe\u50cf\u8f93\u5165\u5c3a\u5bf8\u5e94\u8be5\u4e3a (samples, height, width, channels) \uff0c\"channels_first\" \u6a21\u5f0f\u8868\u793a\u8f93\u5165\u5c3a\u5bf8\u5e94\u8be5\u4e3a (samples, channels, height, width) \u3002\u9ed8\u8ba4\u4e3a \u5728 Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d\u7684 image_data_format \u503c\u3002\u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u90a3\u5b83\u5c31\u662f \"channels_last\"\u3002 validation_split : \u6d6e\u70b9\u6570\u3002Float. \u4fdd\u7559\u7528\u4e8e\u9a8c\u8bc1\u7684\u56fe\u50cf\u7684\u6bd4\u4f8b\uff08\u4e25\u683c\u57280\u548c1\u4e4b\u95f4\uff09\u3002 dtype : \u751f\u6210\u6570\u7ec4\u4f7f\u7528\u7684\u6570\u636e\u7c7b\u578b\u3002 \u4f8b\u5b50 \u4f7f\u7528 .flow(x, y) \u7684\u4f8b\u5b50\uff1a (x_train, y_train), (x_test, y_test) = cifar10.load_data() y_train = np_utils.to_categorical(y_train, num_classes) y_test = np_utils.to_categorical(y_test, num_classes) datagen = ImageDataGenerator( featurewise_center=True, featurewise_std_normalization=True, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, horizontal_flip=True) # \u8ba1\u7b97\u7279\u5f81\u5f52\u4e00\u5316\u6240\u9700\u7684\u6570\u91cf # \uff08\u5982\u679c\u5e94\u7528 ZCA \u767d\u5316\uff0c\u5c06\u8ba1\u7b97\u6807\u51c6\u5dee\uff0c\u5747\u503c\uff0c\u4e3b\u6210\u5206\uff09 datagen.fit(x_train) # \u4f7f\u7528\u5b9e\u65f6\u6570\u636e\u589e\u76ca\u7684\u6279\u6570\u636e\u5bf9\u6a21\u578b\u8fdb\u884c\u62df\u5408\uff1a model.fit_generator(datagen.flow(x_train, y_train, batch_size=32), steps_per_epoch=len(x_train) / 32, epochs=epochs) # \u8fd9\u91cc\u6709\u4e00\u4e2a\u66f4 \u300c\u624b\u52a8\u300d\u7684\u4f8b\u5b50 for e in range(epochs): print('Epoch', e) batches = 0 for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=32): model.fit(x_batch, y_batch) batches += 1 if batches >= len(x_train) / 32: # \u6211\u4eec\u9700\u8981\u624b\u52a8\u6253\u7834\u5faa\u73af\uff0c # \u56e0\u4e3a\u751f\u6210\u5668\u4f1a\u65e0\u9650\u5faa\u73af break \u4f7f\u7528 .flow_from_directory(directory) \u7684\u4f8b\u5b50\uff1a train_datagen = ImageDataGenerator( rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True) test_datagen = ImageDataGenerator(rescale=1./255) train_generator = train_datagen.flow_from_directory( 'data/train', target_size=(150, 150), batch_size=32, class_mode='binary') validation_generator = test_datagen.flow_from_directory( 'data/validation', target_size=(150, 150), batch_size=32, class_mode='binary') model.fit_generator( train_generator, steps_per_epoch=2000, epochs=50, validation_data=validation_generator, validation_steps=800) \u540c\u65f6\u8f6c\u6362\u56fe\u50cf\u548c\u8499\u7248 (mask) \u7684\u4f8b\u5b50\u3002 # \u521b\u5efa\u4e24\u4e2a\u76f8\u540c\u53c2\u6570\u7684\u5b9e\u4f8b data_gen_args = dict(featurewise_center=True, featurewise_std_normalization=True, rotation_range=90, width_shift_range=0.1, height_shift_range=0.1, zoom_range=0.2) image_datagen = ImageDataGenerator(**data_gen_args) mask_datagen = ImageDataGenerator(**data_gen_args) # \u4e3a fit \u548c flow \u51fd\u6570\u63d0\u4f9b\u76f8\u540c\u7684\u79cd\u5b50\u548c\u5173\u952e\u5b57\u53c2\u6570 seed = 1 image_datagen.fit(images, augment=True, seed=seed) mask_datagen.fit(masks, augment=True, seed=seed) image_generator = image_datagen.flow_from_directory( 'data/images', class_mode=None, seed=seed) mask_generator = mask_datagen.flow_from_directory( 'data/masks', class_mode=None, seed=seed) # \u5c06\u751f\u6210\u5668\u7ec4\u5408\u6210\u4e00\u4e2a\u4ea7\u751f\u56fe\u50cf\u548c\u8499\u7248\uff08mask\uff09\u7684\u751f\u6210\u5668 train_generator = zip(image_generator, mask_generator) model.fit_generator( train_generator, steps_per_epoch=2000, epochs=50) ImageDataGenerator \u7c7b\u65b9\u6cd5 apply_transform apply_transform(x, transform_parameters) \u6839\u636e\u7ed9\u5b9a\u7684\u53c2\u6570\u5c06\u53d8\u6362\u5e94\u7528\u4e8e\u56fe\u50cf\u3002 \u53c2\u6570 x : 3D \u5f20\u91cf\uff0c\u5355\u5f20\u56fe\u50cf\u3002 transform_parameters : \u5b57\u7b26\u4e32 - \u53c2\u6570 \u5bf9\u8868\u793a\u7684\u5b57\u5178\uff0c\u7528\u4e8e\u63cf\u8ff0\u8f6c\u6362\u3002\u76ee\u524d\uff0c\u4f7f\u7528\u5b57\u5178\u4e2d\u7684\u4ee5\u4e0b\u53c2\u6570\uff1a 'theta': \u6d6e\u70b9\u6570\u3002\u65cb\u8f6c\u89d2\u5ea6\uff08\u5ea6\uff09\u3002 'tx': \u6d6e\u70b9\u6570\u3002\u5728 x \u65b9\u5411\u4e0a\u79fb\u52a8\u3002 'ty': \u6d6e\u70b9\u6570\u3002\u5728 y \u65b9\u5411\u4e0a\u79fb\u52a8\u3002 shear': \u6d6e\u70b9\u6570\u3002\u526a\u5207\u89d2\u5ea6\uff08\u5ea6\uff09\u3002 'zx': \u6d6e\u70b9\u6570\u3002\u653e\u5927 x \u65b9\u5411\u3002 'zy': \u6d6e\u70b9\u6570\u3002\u653e\u5927 y \u65b9\u5411\u3002 'flip_horizontal': \u5e03\u5c14 \u503c\u3002\u6c34\u5e73\u7ffb\u8f6c\u3002 'flip_vertical': \u5e03\u5c14\u503c\u3002\u5782\u76f4\u7ffb\u8f6c\u3002 'channel_shift_intencity': \u6d6e\u70b9\u6570\u3002\u9891\u9053\u8f6c\u6362\u5f3a\u5ea6\u3002 'brightness': \u6d6e\u70b9\u6570\u3002\u4eae\u5ea6\u8f6c\u6362\u5f3a\u5ea6\u3002 \u8fd4\u56de \u8f93\u5165\u7684\u8f6c\u6362\u540e\u7248\u672c\uff08\u76f8\u540c\u5c3a\u5bf8\uff09\u3002 fit fit(x, augment=False, rounds=1, seed=None) \u5c06\u6570\u636e\u751f\u6210\u5668\u7528\u4e8e\u67d0\u4e9b\u793a\u4f8b\u6570\u636e\u3002 \u5b83\u57fa\u4e8e\u4e00\u7ec4\u6837\u672c\u6570\u636e\uff0c\u8ba1\u7b97\u4e0e\u6570\u636e\u8f6c\u6362\u76f8\u5173\u7684\u5185\u90e8\u6570\u636e\u7edf\u8ba1\u3002 \u5f53\u4e14\u4ec5\u5f53 featurewise_center \u6216 featurewise_std_normalization \u6216 zca_whitening \u8bbe\u7f6e\u4e3a True \u65f6\u624d\u9700\u8981\u3002 \u53c2\u6570 x : \u6837\u672c\u6570\u636e\u3002\u79e9\u5e94\u8be5\u4e3a 4\u3002\u5bf9\u4e8e\u7070\u5ea6\u6570\u636e\uff0c\u901a\u9053\u8f74\u7684\u503c\u5e94\u8be5\u4e3a 1\uff1b\u5bf9\u4e8e RGB \u6570\u636e\uff0c\u503c\u5e94\u8be5\u4e3a 3\u3002 augment : \u5e03\u5c14\u503c\uff08\u9ed8\u8ba4\u4e3a False\uff09\u3002\u662f\u5426\u4f7f\u7528\u968f\u673a\u6837\u672c\u6269\u5f20\u3002 rounds : \u6574\u6570\uff08\u9ed8\u8ba4\u4e3a 1\uff09\u3002\u5982\u679c\u6570\u636e\u6570\u636e\u589e\u5f3a\uff08augment=True\uff09\uff0c\u8868\u660e\u5728\u6570\u636e\u4e0a\u8fdb\u884c\u591a\u5c11\u6b21\u589e\u5f3a\u3002 seed : \u6574\u6570\uff08\u9ed8\u8ba4 None\uff09\u3002\u968f\u673a\u79cd\u5b50\u3002 flow flow(x, y=None, batch_size=32, shuffle=True, sample_weight=None, seed=None, save_to_dir=None, save_prefix='', save_format='png', subset=None) \u91c7\u96c6\u6570\u636e\u548c\u6807\u7b7e\u6570\u7ec4\uff0c\u751f\u6210\u6279\u91cf\u589e\u5f3a\u6570\u636e\u3002 \u53c2\u6570 x : \u8f93\u5165\u6570\u636e\u3002\u79e9\u4e3a 4 \u7684 Numpy \u77e9\u9635\u6216\u5143\u7ec4\u3002\u5982\u679c\u662f\u5143\u7ec4\uff0c\u7b2c\u4e00\u4e2a\u5143\u7d20\u5e94\u8be5\u5305\u542b\u56fe\u50cf\uff0c\u7b2c\u4e8c\u4e2a\u5143\u7d20\u662f\u53e6\u4e00\u4e2a Numpy \u6570\u7ec4\u6216\u4e00\u5217 Numpy \u6570\u7ec4\uff0c\u5b83\u4eec\u4e0d\u7ecf\u8fc7\u4efb\u4f55\u4fee\u6539\u5c31\u4f20\u9012\u7ed9\u8f93\u51fa\u3002\u53ef\u7528\u4e8e\u5c06\u6a21\u578b\u6742\u9879\u6570\u636e\u4e0e\u56fe\u50cf\u4e00\u8d77\u8f93\u5165\u3002\u5bf9\u4e8e\u7070\u5ea6\u6570\u636e\uff0c\u56fe\u50cf\u6570\u7ec4\u7684\u901a\u9053\u8f74\u7684\u503c\u5e94\u8be5\u4e3a 1\uff0c\u800c\u5bf9\u4e8e RGB \u6570\u636e\uff0c\u5176\u503c\u5e94\u8be5\u4e3a 3\u3002 y : \u6807\u7b7e\u3002 batch_size : \u6574\u6570 (\u9ed8\u8ba4\u4e3a 32)\u3002 shuffle : \u5e03\u5c14\u503c (\u9ed8\u8ba4\u4e3a True)\u3002 sample_weight : \u6837\u672c\u6743\u91cd\u3002 seed : \u6574\u6570\uff08\u9ed8\u8ba4\u4e3a None\uff09\u3002 save_to_dir : None \u6216 \u5b57\u7b26\u4e32\uff08\u9ed8\u8ba4\u4e3a None\uff09\u3002\u8fd9\u4f7f\u60a8\u53ef\u4ee5\u9009\u62e9\u6307\u5b9a\u8981\u4fdd\u5b58\u7684\u6b63\u5728\u751f\u6210\u7684\u589e\u5f3a\u56fe\u7247\u7684\u76ee\u5f55\uff08\u7528\u4e8e\u53ef\u89c6\u5316\u60a8\u6b63\u5728\u6267\u884c\u7684\u64cd\u4f5c\uff09\u3002 save_prefix : \u5b57\u7b26\u4e32\uff08\u9ed8\u8ba4 '' \uff09\u3002\u4fdd\u5b58\u56fe\u7247\u7684\u6587\u4ef6\u540d\u524d\u7f00\uff08\u4ec5\u5f53 save_to_dir \u8bbe\u7f6e\u65f6\u53ef\u7528\uff09\u3002 save_format : \"png\", \"jpeg\" \u4e4b\u4e00\uff08\u4ec5\u5f53 save_to_dir \u8bbe\u7f6e\u65f6\u53ef\u7528\uff09\u3002\u9ed8\u8ba4\uff1a\"png\"\u3002 subset : \u6570\u636e\u5b50\u96c6 (\"training\" \u6216 \"validation\")\uff0c\u5982\u679c \u5728 ImageDataGenerator \u4e2d\u8bbe\u7f6e\u4e86 validation_split \u3002 \u8fd4\u56de \u4e00\u4e2a\u751f\u6210\u5143\u7ec4 (x, y) \u7684 Iterator \uff0c\u5176\u4e2d x \u662f\u56fe\u50cf\u6570\u636e\u7684 Numpy \u6570\u7ec4\uff08\u5728\u5355\u5f20\u56fe\u50cf\u8f93\u5165\u65f6\uff09\uff0c\u6216 Numpy \u6570\u7ec4\u5217\u8868\uff08\u5728\u989d\u5916\u591a\u4e2a\u8f93\u5165\u65f6\uff09\uff0c y \u662f\u5bf9\u5e94\u7684\u6807\u7b7e\u7684 Numpy \u6570\u7ec4\u3002\u5982\u679c 'sample_weight' \u4e0d\u662f None\uff0c\u751f\u6210\u7684\u5143\u7ec4\u5f62\u5f0f\u4e3a (x, y, sample_weight) \u3002\u5982\u679c y \u662f None, \u53ea\u6709 Numpy \u6570\u7ec4 x \u88ab\u8fd4\u56de\u3002 flow_from_dataframe flow_from_dataframe(dataframe, directory, x_col='filename', y_col='class', has_ext=True, target_size=(256, 256), color_mode='rgb', classes=None, class_mode='categorical', batch_size=32, shuffle=True, seed=None, save_to_dir=None, save_prefix='', save_format='png', subset=None, interpolation='nearest') \u8f93\u5165 dataframe \u548c\u76ee\u5f55\u7684\u8def\u5f84\uff0c\u5e76\u751f\u6210\u6279\u91cf\u7684\u589e\u5f3a/\u6807\u51c6\u5316\u7684\u6570\u636e\u3002 \u8fd9\u91cc\u6709\u4e00\u4e2a\u7b80\u5355\u7684\u6559\u7a0b\uff1a http://bit.ly/keras_flow_from_dataframe \u53c2\u6570 dataframe : Pandas dataframe\uff0c\u4e00\u5217\u4e3a\u56fe\u50cf\u7684\u6587\u4ef6\u540d\uff0c\u53e6\u4e00\u5217\u4e3a\u56fe\u50cf\u7684\u7c7b\u522b\uff0c \u6216\u8005\u662f\u53ef\u4ee5\u4f5c\u4e3a\u539f\u59cb\u76ee\u6807\u6570\u636e\u591a\u4e2a\u5217\u3002 directory : \u5b57\u7b26\u4e32\uff0c\u76ee\u6807\u76ee\u5f55\u7684\u8def\u5f84\uff0c\u5176\u4e2d\u5305\u542b\u5728 dataframe \u4e2d\u6620\u5c04\u7684\u6240\u6709\u56fe\u50cf\u3002 x_col : \u5b57\u7b26\u4e32\uff0cdataframe \u4e2d\u5305\u542b\u76ee\u6807\u56fe\u50cf\u6587\u4ef6\u5939\u7684\u76ee\u5f55\u7684\u5217\u3002 y_col : \u5b57\u7b26\u4e32\u6216\u5b57\u7b26\u4e32\u5217\u8868\uff0cdataframe \u4e2d\u5c06\u4f5c\u4e3a\u76ee\u6807\u6570\u636e\u7684\u5217\u3002 has_ext : \u5e03\u5c14\u503c\uff0c\u5982\u679c dataframe[x_col] \u4e2d\u7684\u6587\u4ef6\u540d\u5177\u6709\u6269\u5c55\u540d\u5219\u4e3a True\uff0c\u5426\u5219\u4e3a False\u3002 target_size : \u6574\u6570\u5143\u7ec4 (height, width) \uff0c\u9ed8\u8ba4\u4e3a (256, 256) \u3002 \u6240\u6709\u627e\u5230\u7684\u56fe\u90fd\u4f1a\u8c03\u6574\u5230\u8fd9\u4e2a\u7ef4\u5ea6\u3002 color_mode : \"grayscale\", \"rbg\" \u4e4b\u4e00\u3002\u9ed8\u8ba4\uff1a\"rgb\"\u3002 \u56fe\u50cf\u662f\u5426\u8f6c\u6362\u4e3a 1 \u4e2a\u6216 3 \u4e2a\u989c\u8272\u901a\u9053\u3002 classes : \u53ef\u9009\u7684\u7c7b\u522b\u5217\u8868 (\u4f8b\u5982\uff0c ['dogs', 'cats'] )\u3002\u9ed8\u8ba4\uff1aNone\u3002 \u5982\u672a\u63d0\u4f9b\uff0c\u7c7b\u6bd4\u5217\u8868\u5c06\u81ea\u52a8\u4ece y_col \u4e2d\u63a8\u7406\u51fa\u6765\uff0cy_col \u5c06\u4f1a\u88ab\u6620\u5c04\u4e3a\u7c7b\u522b\u7d22\u5f15\uff09\u3002 \u5305\u542b\u4ece\u7c7b\u540d\u5230\u7c7b\u7d22\u5f15\u7684\u6620\u5c04\u7684\u5b57\u5178\u53ef\u4ee5\u901a\u8fc7\u5c5e\u6027 class_indices \u83b7\u5f97\u3002 class_mode : \"categorical\", \"binary\", \"sparse\", \"input\", \"other\" or None \u4e4b\u4e00\u3002 \u9ed8\u8ba4\uff1a\"categorical\"\u3002\u51b3\u5b9a\u8fd4\u56de\u6807\u7b7e\u6570\u7ec4\u7684\u7c7b\u578b\uff1a \"categorical\" \u5c06\u662f 2D one-hot \u7f16\u7801\u6807\u7b7e\uff0c \"binary\" \u5c06\u662f 1D \u4e8c\u8fdb\u5236\u6807\u7b7e\uff0c \"sparse\" \u5c06\u662f 1D \u6574\u6570\u6807\u7b7e\uff0c \"input\" \u5c06\u662f\u4e0e\u8f93\u5165\u56fe\u50cf\u76f8\u540c\u7684\u56fe\u50cf\uff08\u4e3b\u8981\u7528\u4e8e\u4e0e\u81ea\u52a8\u7f16\u7801\u5668\u4e00\u8d77\u4f7f\u7528\uff09\uff0c \"other\" \u5c06\u662f y_col \u6570\u636e\u7684 numpy \u6570\u7ec4\uff0c None, \u4e0d\u8fd4\u56de\u4efb\u4f55\u6807\u7b7e\uff08\u751f\u6210\u5668\u53ea\u4f1a\u4ea7\u751f\u6279\u91cf\u7684\u56fe\u50cf\u6570\u636e\uff0c\u8fd9\u5bf9\u4f7f\u7528 model.predict_generator() , model.evaluate_generator() \u7b49\u5f88\u6709\u7528\uff09\u3002 batch_size : \u6279\u91cf\u6570\u636e\u7684\u5c3a\u5bf8\uff08\u9ed8\u8ba4\uff1a32\uff09\u3002 shuffle : \u662f\u5426\u6df7\u6d17\u6570\u636e\uff08\u9ed8\u8ba4\uff1aTrue\uff09 seed : \u53ef\u9009\u7684\u6df7\u6d17\u548c\u8f6c\u6362\u7684\u968f\u5373\u79cd\u5b50\u3002 save_to_dir : None \u6216 str (\u9ed8\u8ba4: None). \u8fd9\u5141\u8bb8\u4f60\u53ef\u9009\u5730\u6307\u5b9a\u8981\u4fdd\u5b58\u6b63\u5728\u751f\u6210\u7684\u589e\u5f3a\u56fe\u7247\u7684\u76ee\u5f55\uff08\u7528\u4e8e\u53ef\u89c6\u5316\u60a8\u6b63\u5728\u6267\u884c\u7684\u64cd\u4f5c\uff09\u3002 save_prefix : \u5b57\u7b26\u4e32\u3002\u4fdd\u5b58\u56fe\u7247\u7684\u6587\u4ef6\u540d\u524d\u7f00\uff08\u4ec5\u5f53 save_to_dir \u8bbe\u7f6e\u65f6\u53ef\u7528\uff09\u3002 save_format : \"png\", \"jpeg\" \u4e4b\u4e00\uff08\u4ec5\u5f53 save_to_dir \u8bbe\u7f6e\u65f6\u53ef\u7528\uff09\u3002\u9ed8\u8ba4\uff1a\"png\"\u3002 follow_links : \u662f\u5426\u8ddf\u968f\u7c7b\u5b50\u76ee\u5f55\u4e2d\u7684\u7b26\u53f7\u94fe\u63a5\uff08\u9ed8\u8ba4\uff1aFalse\uff09\u3002 subset : \u6570\u636e\u5b50\u96c6 ( \"training\" \u6216 \"validation\" )\uff0c\u5982\u679c\u5728 ImageDataGenerator \u4e2d\u8bbe\u7f6e\u4e86 validation_split \u3002 interpolation : \u5728\u76ee\u6807\u5927\u5c0f\u4e0e\u52a0\u8f7d\u56fe\u50cf\u7684\u5927\u5c0f\u4e0d\u540c\u65f6\uff0c\u7528\u4e8e\u91cd\u65b0\u91c7\u6837\u56fe\u50cf\u7684\u63d2\u503c\u65b9\u6cd5\u3002 \u652f\u6301\u7684\u65b9\u6cd5\u6709 \"nearest\" , \"bilinear\" , and \"bicubic\" \u3002 \u5982\u679c\u5b89\u88c5\u4e86 1.1.3 \u4ee5\u4e0a\u7248\u672c\u7684 PIL \u7684\u8bdd\uff0c\u540c\u6837\u652f\u6301 \"lanczos\" \u3002 \u5982\u679c\u5b89\u88c5\u4e86 3.4.0 \u4ee5\u4e0a\u7248\u672c\u7684 PIL \u7684\u8bdd\uff0c\u540c\u6837\u652f\u6301 \"box\" \u548c \"hamming\" \u3002 \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528 \"nearest\" \u3002 Returns \u4e00\u4e2a\u751f\u6210 (x, y) \u5143\u7ec4\u7684 DataFrameIterator\uff0c \u5176\u4e2d x \u662f\u4e00\u4e2a\u5305\u542b\u4e00\u6279\u5c3a\u5bf8\u4e3a (batch_size, *target_size, channels) \u7684\u56fe\u50cf\u6837\u672c\u7684 numpy \u6570\u7ec4\uff0c y \u662f\u5bf9\u5e94\u7684\u6807\u7b7e\u7684 numpy \u6570\u7ec4\u3002 flow_from_directory flow_from_directory(directory, target_size=(256, 256), color_mode='rgb', classes=None, class_mode='categorical', batch_size=32, shuffle=True, seed=None, save_to_dir=None, save_prefix='', save_format='png', follow_links=False, subset=None, interpolation='nearest') \u53c2\u6570 directory : \u76ee\u6807\u76ee\u5f55\u7684\u8def\u5f84\u3002\u6bcf\u4e2a\u7c7b\u5e94\u8be5\u5305\u542b\u4e00\u4e2a\u5b50\u76ee\u5f55\u3002\u4efb\u4f55\u5728\u5b50\u76ee\u5f55\u6811\u4e0b\u7684 PNG, JPG, BMP, PPM \u6216 TIF \u56fe\u50cf\uff0c\u90fd\u5c06\u88ab\u5305\u542b\u5728\u751f\u6210\u5668\u4e2d\u3002\u66f4\u591a\u7ec6\u8282\uff0c\u8be6\u89c1 \u6b64\u811a\u672c \u3002 target_size : \u6574\u6570\u5143\u7ec4 (height, width) \uff0c\u9ed8\u8ba4\uff1a (256, 256) \u3002\u6240\u6709\u7684\u56fe\u50cf\u5c06\u88ab\u8c03\u6574\u5230\u7684\u5c3a\u5bf8\u3002 color_mode : \"grayscale\", \"rbg\" \u4e4b\u4e00\u3002\u9ed8\u8ba4\uff1a\"rgb\"\u3002\u56fe\u50cf\u662f\u5426\u88ab\u8f6c\u6362\u6210 1 \u6216 3 \u4e2a\u989c\u8272\u901a\u9053\u3002 classes : \u53ef\u9009\u7684\u7c7b\u7684\u5b50\u76ee\u5f55\u5217\u8868\uff08\u4f8b\u5982 ['dogs', 'cats'] \uff09\u3002\u9ed8\u8ba4\uff1aNone\u3002\u5982\u679c\u672a\u63d0\u4f9b\uff0c\u7c7b\u7684\u5217\u8868\u5c06\u81ea\u52a8\u4ece directory \u4e0b\u7684 \u5b50\u76ee\u5f55\u540d\u79f0/\u7ed3\u6784 \u4e2d\u63a8\u65ad\u51fa\u6765\uff0c\u5176\u4e2d\u6bcf\u4e2a\u5b50\u76ee\u5f55\u90fd\u5c06\u88ab\u4f5c\u4e3a\u4e0d\u540c\u7684\u7c7b\uff08\u7c7b\u540d\u5c06\u6309\u5b57\u5178\u5e8f\u6620\u5c04\u5230\u6807\u7b7e\u7684\u7d22\u5f15\uff09\u3002\u5305\u542b\u4ece\u7c7b\u540d\u5230\u7c7b\u7d22\u5f15\u7684\u6620\u5c04\u7684\u5b57\u5178\u53ef\u4ee5\u901a\u8fc7 class_indices \u5c5e\u6027\u83b7\u5f97\u3002 class_mode : \"categorical\", \"binary\", \"sparse\", \"input\" \u6216 None \u4e4b\u4e00\u3002\u9ed8\u8ba4\uff1a\"categorical\"\u3002\u51b3\u5b9a\u8fd4\u56de\u7684\u6807\u7b7e\u6570\u7ec4\u7684\u7c7b\u578b\uff1a \"categorical\" \u5c06\u662f 2D one-hot \u7f16\u7801\u6807\u7b7e\uff0c \"binary\" \u5c06\u662f 1D \u4e8c\u8fdb\u5236\u6807\u7b7e\uff0c\"sparse\" \u5c06\u662f 1D \u6574\u6570\u6807\u7b7e\uff0c \"input\" \u5c06\u662f\u4e0e\u8f93\u5165\u56fe\u50cf\u76f8\u540c\u7684\u56fe\u50cf\uff08\u4e3b\u8981\u7528\u4e8e\u81ea\u52a8\u7f16\u7801\u5668\uff09\u3002 \u5982\u679c\u4e3a None\uff0c\u4e0d\u8fd4\u56de\u6807\u7b7e\uff08\u751f\u6210\u5668\u5c06\u53ea\u4ea7\u751f\u6279\u91cf\u7684\u56fe\u50cf\u6570\u636e\uff0c\u5bf9\u4e8e model.predict_generator() , model.evaluate_generator() \u7b49\u5f88\u6709\u7528\uff09\u3002\u8bf7\u6ce8\u610f\uff0c\u5982\u679c class_mode \u4e3a None\uff0c\u90a3\u4e48\u6570\u636e\u4ecd\u7136\u9700\u8981\u9a7b\u7559\u5728 directory \u7684\u5b50\u76ee\u5f55\u4e2d\u624d\u80fd\u6b63\u5e38\u5de5\u4f5c\u3002 batch_size : \u4e00\u6279\u6570\u636e\u7684\u5927\u5c0f\uff08\u9ed8\u8ba4 32\uff09\u3002 shuffle : \u662f\u5426\u6df7\u6d17\u6570\u636e\uff08\u9ed8\u8ba4 True\uff09\u3002 seed : \u53ef\u9009\u968f\u673a\u79cd\u5b50\uff0c\u7528\u4e8e\u6df7\u6d17\u548c\u8f6c\u6362\u3002 save_to_dir : None \u6216 \u5b57\u7b26\u4e32\uff08\u9ed8\u8ba4 None\uff09\u3002\u8fd9\u4f7f\u4f60\u53ef\u4ee5\u6700\u4f73\u5730\u6307\u5b9a\u6b63\u5728\u751f\u6210\u7684\u589e\u5f3a\u56fe\u7247\u8981\u4fdd\u5b58\u7684\u76ee\u5f55\uff08\u7528\u4e8e\u53ef\u89c6\u5316\u4f60\u5728\u505a\u4ec0\u4e48\uff09\u3002 save_prefix : \u5b57\u7b26\u4e32\u3002 \u4fdd\u5b58\u56fe\u7247\u7684\u6587\u4ef6\u540d\u524d\u7f00\uff08\u4ec5\u5f53 save_to_dir \u8bbe\u7f6e\u65f6\u53ef\u7528\uff09\u3002 save_format : \"png\", \"jpeg\" \u4e4b\u4e00\uff08\u4ec5\u5f53 save_to_dir \u8bbe\u7f6e\u65f6\u53ef\u7528\uff09\u3002\u9ed8\u8ba4\uff1a\"png\"\u3002 follow_links : \u662f\u5426\u8ddf\u8e2a\u7c7b\u5b50\u76ee\u5f55\u4e2d\u7684\u7b26\u53f7\u94fe\u63a5\uff08\u9ed8\u8ba4\u4e3a False\uff09\u3002 subset : \u6570\u636e\u5b50\u96c6 (\"training\" \u6216 \"validation\")\uff0c\u5982\u679c \u5728 ImageDataGenerator \u4e2d\u8bbe\u7f6e\u4e86 validation_split \u3002 interpolation : \u5728\u76ee\u6807\u5927\u5c0f\u4e0e\u52a0\u8f7d\u56fe\u50cf\u7684\u5927\u5c0f\u4e0d\u540c\u65f6\uff0c\u7528\u4e8e\u91cd\u65b0\u91c7\u6837\u56fe\u50cf\u7684\u63d2\u503c\u65b9\u6cd5\u3002 \u652f\u6301\u7684\u65b9\u6cd5\u6709 \"nearest\" , \"bilinear\" , and \"bicubic\" \u3002 \u5982\u679c\u5b89\u88c5\u4e86 1.1.3 \u4ee5\u4e0a\u7248\u672c\u7684 PIL \u7684\u8bdd\uff0c\u540c\u6837\u652f\u6301 \"lanczos\" \u3002 \u5982\u679c\u5b89\u88c5\u4e86 3.4.0 \u4ee5\u4e0a\u7248\u672c\u7684 PIL \u7684\u8bdd\uff0c\u540c\u6837\u652f\u6301 \"box\" \u548c \"hamming\" \u3002 \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528 \"nearest\" \u3002 \u8fd4\u56de \u4e00\u4e2a\u751f\u6210 (x, y) \u5143\u7ec4\u7684 DirectoryIterator \uff0c\u5176\u4e2d x \u662f\u4e00\u4e2a\u5305\u542b\u4e00\u6279\u5c3a\u5bf8\u4e3a (batch_size, *target_size, channels) \u7684\u56fe\u50cf\u7684 Numpy \u6570\u7ec4\uff0c y \u662f\u5bf9\u5e94\u6807\u7b7e\u7684 Numpy \u6570\u7ec4\u3002 get_random_transform get_random_transform(img_shape, seed=None) \u4e3a\u8f6c\u6362\u751f\u6210\u968f\u673a\u53c2\u6570\u3002 \u53c2\u6570 seed : \u968f\u673a\u79cd\u5b50 img_shape : \u6574\u6570\u5143\u7ec4\u3002\u88ab\u8f6c\u6362\u7684\u56fe\u50cf\u7684\u5c3a\u5bf8\u3002 \u8fd4\u56de \u5305\u542b\u968f\u673a\u9009\u62e9\u7684\u63cf\u8ff0\u53d8\u6362\u7684\u53c2\u6570\u7684\u5b57\u5178\u3002 random_transform random_transform(x, seed=None) \u5c06\u968f\u673a\u53d8\u6362\u5e94\u7528\u4e8e\u56fe\u50cf\u3002 \u53c2\u6570 x : 3D \u5f20\u91cf\uff0c\u5355\u5f20\u56fe\u50cf\u3002 seed : \u968f\u673a\u79cd\u5b50\u3002 \u8fd4\u56de \u8f93\u5165\u7684\u968f\u673a\u8f6c\u6362\u7248\u672c\uff08\u76f8\u540c\u5f62\u72b6\uff09\u3002 standardize standardize(x) \u5c06\u6807\u51c6\u5316\u914d\u7f6e\u5e94\u7528\u4e8e\u4e00\u6279\u8f93\u5165\u3002 \u53c2\u6570 x : \u9700\u8981\u6807\u51c6\u5316\u7684\u4e00\u6279\u8f93\u5165\u3002 \u8fd4\u56de \u6807\u51c6\u5316\u540e\u7684\u8f93\u5165\u3002","title":"\u56fe\u50cf\u9884\u5904\u7406"},{"location":"3-Preprocessing/2.image/#_1","text":"[source]","title":"\u56fe\u50cf\u9884\u5904\u7406"},{"location":"3-Preprocessing/2.image/#imagedatagenerator","text":"keras.preprocessing.image.ImageDataGenerator(featurewise_center=False, samplewise_center=False, featurewise_std_normalization=False, samplewise_std_normalization=False, zca_whitening=False, zca_epsilon=1e-06, rotation_range=0, width_shift_range=0.0, height_shift_range=0.0, brightness_range=None, shear_range=0.0, zoom_range=0.0, channel_shift_range=0.0, fill_mode='nearest', cval=0.0, horizontal_flip=False, vertical_flip=False, rescale=None, preprocessing_function=None, data_format=None, validation_split=0.0, dtype=None) \u901a\u8fc7\u5b9e\u65f6\u6570\u636e\u589e\u5f3a\u751f\u6210\u5f20\u91cf\u56fe\u50cf\u6570\u636e\u6279\u6b21\u3002\u6570\u636e\u5c06\u4e0d\u65ad\u5faa\u73af\uff08\u6309\u6279\u6b21\uff09\u3002 \u53c2\u6570 featurewise_center : \u5e03\u5c14\u503c\u3002\u5c06\u8f93\u5165\u6570\u636e\u7684\u5747\u503c\u8bbe\u7f6e\u4e3a 0\uff0c\u9010\u7279\u5f81\u8fdb\u884c\u3002 samplewise_center : \u5e03\u5c14\u503c\u3002\u5c06\u6bcf\u4e2a\u6837\u672c\u7684\u5747\u503c\u8bbe\u7f6e\u4e3a 0\u3002 featurewise_std_normalization : Boolean. \u5e03\u5c14\u503c\u3002\u5c06\u8f93\u5165\u9664\u4ee5\u6570\u636e\u6807\u51c6\u5dee\uff0c\u9010\u7279\u5f81\u8fdb\u884c\u3002 samplewise_std_normalization : \u5e03\u5c14\u503c\u3002\u5c06\u6bcf\u4e2a\u8f93\u5165\u9664\u4ee5\u5176\u6807\u51c6\u5dee\u3002 zca_epsilon : ZCA \u767d\u5316\u7684 epsilon \u503c\uff0c\u9ed8\u8ba4\u4e3a 1e-6\u3002 zca_whitening : \u5e03\u5c14\u503c\u3002\u662f\u5426\u5e94\u7528 ZCA \u767d\u5316\u3002 rotation_range : \u6574\u6570\u3002\u968f\u673a\u65cb\u8f6c\u7684\u5ea6\u6570\u8303\u56f4\u3002 width_shift_range : \u6d6e\u70b9\u6570\u3001\u4e00\u7ef4\u6570\u7ec4\u6216\u6574\u6570 float: \u5982\u679c <1\uff0c\u5219\u662f\u9664\u4ee5\u603b\u5bbd\u5ea6\u7684\u503c\uff0c\u6216\u8005\u5982\u679c >=1\uff0c\u5219\u4e3a\u50cf\u7d20\u503c\u3002 1-D \u6570\u7ec4: \u6570\u7ec4\u4e2d\u7684\u968f\u673a\u5143\u7d20\u3002 int: \u6765\u81ea\u95f4\u9694 (-width_shift_range, +width_shift_range) \u4e4b\u95f4\u7684\u6574\u6570\u4e2a\u50cf\u7d20\u3002 width_shift_range=2 \u65f6\uff0c\u53ef\u80fd\u503c\u662f\u6574\u6570 [-1, 0, +1] \uff0c\u4e0e width_shift_range=[-1, 0, +1] \u76f8\u540c\uff1b\u800c width_shift_range=1.0 \u65f6\uff0c\u53ef\u80fd\u503c\u662f [-1.0, +1.0) \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 height_shift_range : \u6d6e\u70b9\u6570\u3001\u4e00\u7ef4\u6570\u7ec4\u6216\u6574\u6570 float: \u5982\u679c <1\uff0c\u5219\u662f\u9664\u4ee5\u603b\u5bbd\u5ea6\u7684\u503c\uff0c\u6216\u8005\u5982\u679c >=1\uff0c\u5219\u4e3a\u50cf\u7d20\u503c\u3002 1-D array-like: \u6570\u7ec4\u4e2d\u7684\u968f\u673a\u5143\u7d20\u3002 int: \u6765\u81ea\u95f4\u9694 (-height_shift_range, +height_shift_range) \u4e4b\u95f4\u7684\u6574\u6570\u4e2a\u50cf\u7d20\u3002 height_shift_range=2 \u65f6\uff0c\u53ef\u80fd\u503c\u662f\u6574\u6570 [-1, 0, +1] \uff0c\u4e0e height_shift_range=[-1, 0, +1] \u76f8\u540c\uff1b\u800c height_shift_range=1.0 \u65f6\uff0c\u53ef\u80fd\u503c\u662f [-1.0, +1.0) \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\u3002 shear_range : \u6d6e\u70b9\u6570\u3002\u526a\u5207\u5f3a\u5ea6\uff08\u4ee5\u5f27\u5ea6\u9006\u65f6\u9488\u65b9\u5411\u526a\u5207\u89d2\u5ea6\uff09\u3002 zoom_range : \u6d6e\u70b9\u6570 \u6216 [lower, upper] \u3002\u968f\u673a\u7f29\u653e\u8303\u56f4\u3002\u5982\u679c\u662f\u6d6e\u70b9\u6570\uff0c [lower, upper] = [1-zoom_range, 1+zoom_range] \u3002 channel_shift_range : \u6d6e\u70b9\u6570\u3002\u968f\u673a\u901a\u9053\u8f6c\u6362\u7684\u8303\u56f4\u3002 fill_mode : {\"constant\", \"nearest\", \"reflect\" or \"wrap\"} \u4e4b\u4e00\u3002\u9ed8\u8ba4\u4e3a 'nearest'\u3002\u8f93\u5165\u8fb9\u754c\u4ee5\u5916\u7684\u70b9\u6839\u636e\u7ed9\u5b9a\u7684\u6a21\u5f0f\u586b\u5145\uff1a 'constant': kkkkkkkk|abcd|kkkkkkkk (cval=k) 'nearest': aaaaaaaa|abcd|dddddddd 'reflect': abcddcba|abcd|dcbaabcd 'wrap': abcdabcd|abcd|abcdabcd cval : \u6d6e\u70b9\u6570\u6216\u6574\u6570\u3002\u7528\u4e8e\u8fb9\u754c\u4e4b\u5916\u7684\u70b9\u7684\u503c\uff0c\u5f53 fill_mode = \"constant\" \u65f6\u3002 horizontal_flip : \u5e03\u5c14\u503c\u3002\u968f\u673a\u6c34\u5e73\u7ffb\u8f6c\u3002 vertical_flip : \u5e03\u5c14\u503c\u3002\u968f\u673a\u5782\u76f4\u7ffb\u8f6c\u3002 rescale : \u91cd\u7f29\u653e\u56e0\u5b50\u3002\u9ed8\u8ba4\u4e3a None\u3002\u5982\u679c\u662f None \u6216 0\uff0c\u4e0d\u8fdb\u884c\u7f29\u653e\uff0c\u5426\u5219\u5c06\u6570\u636e\u4e58\u4ee5\u6240\u63d0\u4f9b\u7684\u503c\uff08\u5728\u5e94\u7528\u4efb\u4f55\u5176\u4ed6\u8f6c\u6362\u4e4b\u524d\uff09\u3002 preprocessing_function : \u5e94\u7528\u4e8e\u6bcf\u4e2a\u8f93\u5165\u7684\u51fd\u6570\u3002\u8fd9\u4e2a\u51fd\u6570\u4f1a\u5728\u4efb\u4f55\u5176\u4ed6\u6539\u53d8\u4e4b\u524d\u8fd0\u884c\u3002\u8fd9\u4e2a\u51fd\u6570\u9700\u8981\u4e00\u4e2a\u53c2\u6570\uff1a\u4e00\u5f20\u56fe\u50cf\uff08\u79e9\u4e3a 3 \u7684 Numpy \u5f20\u91cf\uff09\uff0c\u5e76\u4e14\u5e94\u8be5\u8f93\u51fa\u4e00\u4e2a\u540c\u5c3a\u5bf8\u7684 Numpy \u5f20\u91cf\u3002 data_format : \u56fe\u50cf\u6570\u636e\u683c\u5f0f\uff0c{\"channels_first\", \"channels_last\"} \u4e4b\u4e00\u3002\"channels_last\" \u6a21\u5f0f\u8868\u793a\u56fe\u50cf\u8f93\u5165\u5c3a\u5bf8\u5e94\u8be5\u4e3a (samples, height, width, channels) \uff0c\"channels_first\" \u6a21\u5f0f\u8868\u793a\u8f93\u5165\u5c3a\u5bf8\u5e94\u8be5\u4e3a (samples, channels, height, width) \u3002\u9ed8\u8ba4\u4e3a \u5728 Keras \u914d\u7f6e\u6587\u4ef6 ~/.keras/keras.json \u4e2d\u7684 image_data_format \u503c\u3002\u5982\u679c\u4f60\u4ece\u672a\u8bbe\u7f6e\u5b83\uff0c\u90a3\u5b83\u5c31\u662f \"channels_last\"\u3002 validation_split : \u6d6e\u70b9\u6570\u3002Float. \u4fdd\u7559\u7528\u4e8e\u9a8c\u8bc1\u7684\u56fe\u50cf\u7684\u6bd4\u4f8b\uff08\u4e25\u683c\u57280\u548c1\u4e4b\u95f4\uff09\u3002 dtype : \u751f\u6210\u6570\u7ec4\u4f7f\u7528\u7684\u6570\u636e\u7c7b\u578b\u3002 \u4f8b\u5b50 \u4f7f\u7528 .flow(x, y) \u7684\u4f8b\u5b50\uff1a (x_train, y_train), (x_test, y_test) = cifar10.load_data() y_train = np_utils.to_categorical(y_train, num_classes) y_test = np_utils.to_categorical(y_test, num_classes) datagen = ImageDataGenerator( featurewise_center=True, featurewise_std_normalization=True, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, horizontal_flip=True) # \u8ba1\u7b97\u7279\u5f81\u5f52\u4e00\u5316\u6240\u9700\u7684\u6570\u91cf # \uff08\u5982\u679c\u5e94\u7528 ZCA \u767d\u5316\uff0c\u5c06\u8ba1\u7b97\u6807\u51c6\u5dee\uff0c\u5747\u503c\uff0c\u4e3b\u6210\u5206\uff09 datagen.fit(x_train) # \u4f7f\u7528\u5b9e\u65f6\u6570\u636e\u589e\u76ca\u7684\u6279\u6570\u636e\u5bf9\u6a21\u578b\u8fdb\u884c\u62df\u5408\uff1a model.fit_generator(datagen.flow(x_train, y_train, batch_size=32), steps_per_epoch=len(x_train) / 32, epochs=epochs) # \u8fd9\u91cc\u6709\u4e00\u4e2a\u66f4 \u300c\u624b\u52a8\u300d\u7684\u4f8b\u5b50 for e in range(epochs): print('Epoch', e) batches = 0 for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=32): model.fit(x_batch, y_batch) batches += 1 if batches >= len(x_train) / 32: # \u6211\u4eec\u9700\u8981\u624b\u52a8\u6253\u7834\u5faa\u73af\uff0c # \u56e0\u4e3a\u751f\u6210\u5668\u4f1a\u65e0\u9650\u5faa\u73af break \u4f7f\u7528 .flow_from_directory(directory) \u7684\u4f8b\u5b50\uff1a train_datagen = ImageDataGenerator( rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True) test_datagen = ImageDataGenerator(rescale=1./255) train_generator = train_datagen.flow_from_directory( 'data/train', target_size=(150, 150), batch_size=32, class_mode='binary') validation_generator = test_datagen.flow_from_directory( 'data/validation', target_size=(150, 150), batch_size=32, class_mode='binary') model.fit_generator( train_generator, steps_per_epoch=2000, epochs=50, validation_data=validation_generator, validation_steps=800) \u540c\u65f6\u8f6c\u6362\u56fe\u50cf\u548c\u8499\u7248 (mask) \u7684\u4f8b\u5b50\u3002 # \u521b\u5efa\u4e24\u4e2a\u76f8\u540c\u53c2\u6570\u7684\u5b9e\u4f8b data_gen_args = dict(featurewise_center=True, featurewise_std_normalization=True, rotation_range=90, width_shift_range=0.1, height_shift_range=0.1, zoom_range=0.2) image_datagen = ImageDataGenerator(**data_gen_args) mask_datagen = ImageDataGenerator(**data_gen_args) # \u4e3a fit \u548c flow \u51fd\u6570\u63d0\u4f9b\u76f8\u540c\u7684\u79cd\u5b50\u548c\u5173\u952e\u5b57\u53c2\u6570 seed = 1 image_datagen.fit(images, augment=True, seed=seed) mask_datagen.fit(masks, augment=True, seed=seed) image_generator = image_datagen.flow_from_directory( 'data/images', class_mode=None, seed=seed) mask_generator = mask_datagen.flow_from_directory( 'data/masks', class_mode=None, seed=seed) # \u5c06\u751f\u6210\u5668\u7ec4\u5408\u6210\u4e00\u4e2a\u4ea7\u751f\u56fe\u50cf\u548c\u8499\u7248\uff08mask\uff09\u7684\u751f\u6210\u5668 train_generator = zip(image_generator, mask_generator) model.fit_generator( train_generator, steps_per_epoch=2000, epochs=50)","title":"ImageDataGenerator \u7c7b"},{"location":"3-Preprocessing/2.image/#imagedatagenerator_1","text":"","title":"ImageDataGenerator \u7c7b\u65b9\u6cd5"},{"location":"3-Preprocessing/2.image/#apply_transform","text":"apply_transform(x, transform_parameters) \u6839\u636e\u7ed9\u5b9a\u7684\u53c2\u6570\u5c06\u53d8\u6362\u5e94\u7528\u4e8e\u56fe\u50cf\u3002 \u53c2\u6570 x : 3D \u5f20\u91cf\uff0c\u5355\u5f20\u56fe\u50cf\u3002 transform_parameters : \u5b57\u7b26\u4e32 - \u53c2\u6570 \u5bf9\u8868\u793a\u7684\u5b57\u5178\uff0c\u7528\u4e8e\u63cf\u8ff0\u8f6c\u6362\u3002\u76ee\u524d\uff0c\u4f7f\u7528\u5b57\u5178\u4e2d\u7684\u4ee5\u4e0b\u53c2\u6570\uff1a 'theta': \u6d6e\u70b9\u6570\u3002\u65cb\u8f6c\u89d2\u5ea6\uff08\u5ea6\uff09\u3002 'tx': \u6d6e\u70b9\u6570\u3002\u5728 x \u65b9\u5411\u4e0a\u79fb\u52a8\u3002 'ty': \u6d6e\u70b9\u6570\u3002\u5728 y \u65b9\u5411\u4e0a\u79fb\u52a8\u3002 shear': \u6d6e\u70b9\u6570\u3002\u526a\u5207\u89d2\u5ea6\uff08\u5ea6\uff09\u3002 'zx': \u6d6e\u70b9\u6570\u3002\u653e\u5927 x \u65b9\u5411\u3002 'zy': \u6d6e\u70b9\u6570\u3002\u653e\u5927 y \u65b9\u5411\u3002 'flip_horizontal': \u5e03\u5c14 \u503c\u3002\u6c34\u5e73\u7ffb\u8f6c\u3002 'flip_vertical': \u5e03\u5c14\u503c\u3002\u5782\u76f4\u7ffb\u8f6c\u3002 'channel_shift_intencity': \u6d6e\u70b9\u6570\u3002\u9891\u9053\u8f6c\u6362\u5f3a\u5ea6\u3002 'brightness': \u6d6e\u70b9\u6570\u3002\u4eae\u5ea6\u8f6c\u6362\u5f3a\u5ea6\u3002 \u8fd4\u56de \u8f93\u5165\u7684\u8f6c\u6362\u540e\u7248\u672c\uff08\u76f8\u540c\u5c3a\u5bf8\uff09\u3002","title":"apply_transform"},{"location":"3-Preprocessing/2.image/#fit","text":"fit(x, augment=False, rounds=1, seed=None) \u5c06\u6570\u636e\u751f\u6210\u5668\u7528\u4e8e\u67d0\u4e9b\u793a\u4f8b\u6570\u636e\u3002 \u5b83\u57fa\u4e8e\u4e00\u7ec4\u6837\u672c\u6570\u636e\uff0c\u8ba1\u7b97\u4e0e\u6570\u636e\u8f6c\u6362\u76f8\u5173\u7684\u5185\u90e8\u6570\u636e\u7edf\u8ba1\u3002 \u5f53\u4e14\u4ec5\u5f53 featurewise_center \u6216 featurewise_std_normalization \u6216 zca_whitening \u8bbe\u7f6e\u4e3a True \u65f6\u624d\u9700\u8981\u3002 \u53c2\u6570 x : \u6837\u672c\u6570\u636e\u3002\u79e9\u5e94\u8be5\u4e3a 4\u3002\u5bf9\u4e8e\u7070\u5ea6\u6570\u636e\uff0c\u901a\u9053\u8f74\u7684\u503c\u5e94\u8be5\u4e3a 1\uff1b\u5bf9\u4e8e RGB \u6570\u636e\uff0c\u503c\u5e94\u8be5\u4e3a 3\u3002 augment : \u5e03\u5c14\u503c\uff08\u9ed8\u8ba4\u4e3a False\uff09\u3002\u662f\u5426\u4f7f\u7528\u968f\u673a\u6837\u672c\u6269\u5f20\u3002 rounds : \u6574\u6570\uff08\u9ed8\u8ba4\u4e3a 1\uff09\u3002\u5982\u679c\u6570\u636e\u6570\u636e\u589e\u5f3a\uff08augment=True\uff09\uff0c\u8868\u660e\u5728\u6570\u636e\u4e0a\u8fdb\u884c\u591a\u5c11\u6b21\u589e\u5f3a\u3002 seed : \u6574\u6570\uff08\u9ed8\u8ba4 None\uff09\u3002\u968f\u673a\u79cd\u5b50\u3002","title":"fit"},{"location":"3-Preprocessing/2.image/#flow","text":"flow(x, y=None, batch_size=32, shuffle=True, sample_weight=None, seed=None, save_to_dir=None, save_prefix='', save_format='png', subset=None) \u91c7\u96c6\u6570\u636e\u548c\u6807\u7b7e\u6570\u7ec4\uff0c\u751f\u6210\u6279\u91cf\u589e\u5f3a\u6570\u636e\u3002 \u53c2\u6570 x : \u8f93\u5165\u6570\u636e\u3002\u79e9\u4e3a 4 \u7684 Numpy \u77e9\u9635\u6216\u5143\u7ec4\u3002\u5982\u679c\u662f\u5143\u7ec4\uff0c\u7b2c\u4e00\u4e2a\u5143\u7d20\u5e94\u8be5\u5305\u542b\u56fe\u50cf\uff0c\u7b2c\u4e8c\u4e2a\u5143\u7d20\u662f\u53e6\u4e00\u4e2a Numpy \u6570\u7ec4\u6216\u4e00\u5217 Numpy \u6570\u7ec4\uff0c\u5b83\u4eec\u4e0d\u7ecf\u8fc7\u4efb\u4f55\u4fee\u6539\u5c31\u4f20\u9012\u7ed9\u8f93\u51fa\u3002\u53ef\u7528\u4e8e\u5c06\u6a21\u578b\u6742\u9879\u6570\u636e\u4e0e\u56fe\u50cf\u4e00\u8d77\u8f93\u5165\u3002\u5bf9\u4e8e\u7070\u5ea6\u6570\u636e\uff0c\u56fe\u50cf\u6570\u7ec4\u7684\u901a\u9053\u8f74\u7684\u503c\u5e94\u8be5\u4e3a 1\uff0c\u800c\u5bf9\u4e8e RGB \u6570\u636e\uff0c\u5176\u503c\u5e94\u8be5\u4e3a 3\u3002 y : \u6807\u7b7e\u3002 batch_size : \u6574\u6570 (\u9ed8\u8ba4\u4e3a 32)\u3002 shuffle : \u5e03\u5c14\u503c (\u9ed8\u8ba4\u4e3a True)\u3002 sample_weight : \u6837\u672c\u6743\u91cd\u3002 seed : \u6574\u6570\uff08\u9ed8\u8ba4\u4e3a None\uff09\u3002 save_to_dir : None \u6216 \u5b57\u7b26\u4e32\uff08\u9ed8\u8ba4\u4e3a None\uff09\u3002\u8fd9\u4f7f\u60a8\u53ef\u4ee5\u9009\u62e9\u6307\u5b9a\u8981\u4fdd\u5b58\u7684\u6b63\u5728\u751f\u6210\u7684\u589e\u5f3a\u56fe\u7247\u7684\u76ee\u5f55\uff08\u7528\u4e8e\u53ef\u89c6\u5316\u60a8\u6b63\u5728\u6267\u884c\u7684\u64cd\u4f5c\uff09\u3002 save_prefix : \u5b57\u7b26\u4e32\uff08\u9ed8\u8ba4 '' \uff09\u3002\u4fdd\u5b58\u56fe\u7247\u7684\u6587\u4ef6\u540d\u524d\u7f00\uff08\u4ec5\u5f53 save_to_dir \u8bbe\u7f6e\u65f6\u53ef\u7528\uff09\u3002 save_format : \"png\", \"jpeg\" \u4e4b\u4e00\uff08\u4ec5\u5f53 save_to_dir \u8bbe\u7f6e\u65f6\u53ef\u7528\uff09\u3002\u9ed8\u8ba4\uff1a\"png\"\u3002 subset : \u6570\u636e\u5b50\u96c6 (\"training\" \u6216 \"validation\")\uff0c\u5982\u679c \u5728 ImageDataGenerator \u4e2d\u8bbe\u7f6e\u4e86 validation_split \u3002 \u8fd4\u56de \u4e00\u4e2a\u751f\u6210\u5143\u7ec4 (x, y) \u7684 Iterator \uff0c\u5176\u4e2d x \u662f\u56fe\u50cf\u6570\u636e\u7684 Numpy \u6570\u7ec4\uff08\u5728\u5355\u5f20\u56fe\u50cf\u8f93\u5165\u65f6\uff09\uff0c\u6216 Numpy \u6570\u7ec4\u5217\u8868\uff08\u5728\u989d\u5916\u591a\u4e2a\u8f93\u5165\u65f6\uff09\uff0c y \u662f\u5bf9\u5e94\u7684\u6807\u7b7e\u7684 Numpy \u6570\u7ec4\u3002\u5982\u679c 'sample_weight' \u4e0d\u662f None\uff0c\u751f\u6210\u7684\u5143\u7ec4\u5f62\u5f0f\u4e3a (x, y, sample_weight) \u3002\u5982\u679c y \u662f None, \u53ea\u6709 Numpy \u6570\u7ec4 x \u88ab\u8fd4\u56de\u3002","title":"flow"},{"location":"3-Preprocessing/2.image/#flow_from_dataframe","text":"flow_from_dataframe(dataframe, directory, x_col='filename', y_col='class', has_ext=True, target_size=(256, 256), color_mode='rgb', classes=None, class_mode='categorical', batch_size=32, shuffle=True, seed=None, save_to_dir=None, save_prefix='', save_format='png', subset=None, interpolation='nearest') \u8f93\u5165 dataframe \u548c\u76ee\u5f55\u7684\u8def\u5f84\uff0c\u5e76\u751f\u6210\u6279\u91cf\u7684\u589e\u5f3a/\u6807\u51c6\u5316\u7684\u6570\u636e\u3002 \u8fd9\u91cc\u6709\u4e00\u4e2a\u7b80\u5355\u7684\u6559\u7a0b\uff1a http://bit.ly/keras_flow_from_dataframe \u53c2\u6570 dataframe : Pandas dataframe\uff0c\u4e00\u5217\u4e3a\u56fe\u50cf\u7684\u6587\u4ef6\u540d\uff0c\u53e6\u4e00\u5217\u4e3a\u56fe\u50cf\u7684\u7c7b\u522b\uff0c \u6216\u8005\u662f\u53ef\u4ee5\u4f5c\u4e3a\u539f\u59cb\u76ee\u6807\u6570\u636e\u591a\u4e2a\u5217\u3002 directory : \u5b57\u7b26\u4e32\uff0c\u76ee\u6807\u76ee\u5f55\u7684\u8def\u5f84\uff0c\u5176\u4e2d\u5305\u542b\u5728 dataframe \u4e2d\u6620\u5c04\u7684\u6240\u6709\u56fe\u50cf\u3002 x_col : \u5b57\u7b26\u4e32\uff0cdataframe \u4e2d\u5305\u542b\u76ee\u6807\u56fe\u50cf\u6587\u4ef6\u5939\u7684\u76ee\u5f55\u7684\u5217\u3002 y_col : \u5b57\u7b26\u4e32\u6216\u5b57\u7b26\u4e32\u5217\u8868\uff0cdataframe \u4e2d\u5c06\u4f5c\u4e3a\u76ee\u6807\u6570\u636e\u7684\u5217\u3002 has_ext : \u5e03\u5c14\u503c\uff0c\u5982\u679c dataframe[x_col] \u4e2d\u7684\u6587\u4ef6\u540d\u5177\u6709\u6269\u5c55\u540d\u5219\u4e3a True\uff0c\u5426\u5219\u4e3a False\u3002 target_size : \u6574\u6570\u5143\u7ec4 (height, width) \uff0c\u9ed8\u8ba4\u4e3a (256, 256) \u3002 \u6240\u6709\u627e\u5230\u7684\u56fe\u90fd\u4f1a\u8c03\u6574\u5230\u8fd9\u4e2a\u7ef4\u5ea6\u3002 color_mode : \"grayscale\", \"rbg\" \u4e4b\u4e00\u3002\u9ed8\u8ba4\uff1a\"rgb\"\u3002 \u56fe\u50cf\u662f\u5426\u8f6c\u6362\u4e3a 1 \u4e2a\u6216 3 \u4e2a\u989c\u8272\u901a\u9053\u3002 classes : \u53ef\u9009\u7684\u7c7b\u522b\u5217\u8868 (\u4f8b\u5982\uff0c ['dogs', 'cats'] )\u3002\u9ed8\u8ba4\uff1aNone\u3002 \u5982\u672a\u63d0\u4f9b\uff0c\u7c7b\u6bd4\u5217\u8868\u5c06\u81ea\u52a8\u4ece y_col \u4e2d\u63a8\u7406\u51fa\u6765\uff0cy_col \u5c06\u4f1a\u88ab\u6620\u5c04\u4e3a\u7c7b\u522b\u7d22\u5f15\uff09\u3002 \u5305\u542b\u4ece\u7c7b\u540d\u5230\u7c7b\u7d22\u5f15\u7684\u6620\u5c04\u7684\u5b57\u5178\u53ef\u4ee5\u901a\u8fc7\u5c5e\u6027 class_indices \u83b7\u5f97\u3002 class_mode : \"categorical\", \"binary\", \"sparse\", \"input\", \"other\" or None \u4e4b\u4e00\u3002 \u9ed8\u8ba4\uff1a\"categorical\"\u3002\u51b3\u5b9a\u8fd4\u56de\u6807\u7b7e\u6570\u7ec4\u7684\u7c7b\u578b\uff1a \"categorical\" \u5c06\u662f 2D one-hot \u7f16\u7801\u6807\u7b7e\uff0c \"binary\" \u5c06\u662f 1D \u4e8c\u8fdb\u5236\u6807\u7b7e\uff0c \"sparse\" \u5c06\u662f 1D \u6574\u6570\u6807\u7b7e\uff0c \"input\" \u5c06\u662f\u4e0e\u8f93\u5165\u56fe\u50cf\u76f8\u540c\u7684\u56fe\u50cf\uff08\u4e3b\u8981\u7528\u4e8e\u4e0e\u81ea\u52a8\u7f16\u7801\u5668\u4e00\u8d77\u4f7f\u7528\uff09\uff0c \"other\" \u5c06\u662f y_col \u6570\u636e\u7684 numpy \u6570\u7ec4\uff0c None, \u4e0d\u8fd4\u56de\u4efb\u4f55\u6807\u7b7e\uff08\u751f\u6210\u5668\u53ea\u4f1a\u4ea7\u751f\u6279\u91cf\u7684\u56fe\u50cf\u6570\u636e\uff0c\u8fd9\u5bf9\u4f7f\u7528 model.predict_generator() , model.evaluate_generator() \u7b49\u5f88\u6709\u7528\uff09\u3002 batch_size : \u6279\u91cf\u6570\u636e\u7684\u5c3a\u5bf8\uff08\u9ed8\u8ba4\uff1a32\uff09\u3002 shuffle : \u662f\u5426\u6df7\u6d17\u6570\u636e\uff08\u9ed8\u8ba4\uff1aTrue\uff09 seed : \u53ef\u9009\u7684\u6df7\u6d17\u548c\u8f6c\u6362\u7684\u968f\u5373\u79cd\u5b50\u3002 save_to_dir : None \u6216 str (\u9ed8\u8ba4: None). \u8fd9\u5141\u8bb8\u4f60\u53ef\u9009\u5730\u6307\u5b9a\u8981\u4fdd\u5b58\u6b63\u5728\u751f\u6210\u7684\u589e\u5f3a\u56fe\u7247\u7684\u76ee\u5f55\uff08\u7528\u4e8e\u53ef\u89c6\u5316\u60a8\u6b63\u5728\u6267\u884c\u7684\u64cd\u4f5c\uff09\u3002 save_prefix : \u5b57\u7b26\u4e32\u3002\u4fdd\u5b58\u56fe\u7247\u7684\u6587\u4ef6\u540d\u524d\u7f00\uff08\u4ec5\u5f53 save_to_dir \u8bbe\u7f6e\u65f6\u53ef\u7528\uff09\u3002 save_format : \"png\", \"jpeg\" \u4e4b\u4e00\uff08\u4ec5\u5f53 save_to_dir \u8bbe\u7f6e\u65f6\u53ef\u7528\uff09\u3002\u9ed8\u8ba4\uff1a\"png\"\u3002 follow_links : \u662f\u5426\u8ddf\u968f\u7c7b\u5b50\u76ee\u5f55\u4e2d\u7684\u7b26\u53f7\u94fe\u63a5\uff08\u9ed8\u8ba4\uff1aFalse\uff09\u3002 subset : \u6570\u636e\u5b50\u96c6 ( \"training\" \u6216 \"validation\" )\uff0c\u5982\u679c\u5728 ImageDataGenerator \u4e2d\u8bbe\u7f6e\u4e86 validation_split \u3002 interpolation : \u5728\u76ee\u6807\u5927\u5c0f\u4e0e\u52a0\u8f7d\u56fe\u50cf\u7684\u5927\u5c0f\u4e0d\u540c\u65f6\uff0c\u7528\u4e8e\u91cd\u65b0\u91c7\u6837\u56fe\u50cf\u7684\u63d2\u503c\u65b9\u6cd5\u3002 \u652f\u6301\u7684\u65b9\u6cd5\u6709 \"nearest\" , \"bilinear\" , and \"bicubic\" \u3002 \u5982\u679c\u5b89\u88c5\u4e86 1.1.3 \u4ee5\u4e0a\u7248\u672c\u7684 PIL \u7684\u8bdd\uff0c\u540c\u6837\u652f\u6301 \"lanczos\" \u3002 \u5982\u679c\u5b89\u88c5\u4e86 3.4.0 \u4ee5\u4e0a\u7248\u672c\u7684 PIL \u7684\u8bdd\uff0c\u540c\u6837\u652f\u6301 \"box\" \u548c \"hamming\" \u3002 \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528 \"nearest\" \u3002 Returns \u4e00\u4e2a\u751f\u6210 (x, y) \u5143\u7ec4\u7684 DataFrameIterator\uff0c \u5176\u4e2d x \u662f\u4e00\u4e2a\u5305\u542b\u4e00\u6279\u5c3a\u5bf8\u4e3a (batch_size, *target_size, channels) \u7684\u56fe\u50cf\u6837\u672c\u7684 numpy \u6570\u7ec4\uff0c y \u662f\u5bf9\u5e94\u7684\u6807\u7b7e\u7684 numpy \u6570\u7ec4\u3002","title":"flow_from_dataframe"},{"location":"3-Preprocessing/2.image/#flow_from_directory","text":"flow_from_directory(directory, target_size=(256, 256), color_mode='rgb', classes=None, class_mode='categorical', batch_size=32, shuffle=True, seed=None, save_to_dir=None, save_prefix='', save_format='png', follow_links=False, subset=None, interpolation='nearest') \u53c2\u6570 directory : \u76ee\u6807\u76ee\u5f55\u7684\u8def\u5f84\u3002\u6bcf\u4e2a\u7c7b\u5e94\u8be5\u5305\u542b\u4e00\u4e2a\u5b50\u76ee\u5f55\u3002\u4efb\u4f55\u5728\u5b50\u76ee\u5f55\u6811\u4e0b\u7684 PNG, JPG, BMP, PPM \u6216 TIF \u56fe\u50cf\uff0c\u90fd\u5c06\u88ab\u5305\u542b\u5728\u751f\u6210\u5668\u4e2d\u3002\u66f4\u591a\u7ec6\u8282\uff0c\u8be6\u89c1 \u6b64\u811a\u672c \u3002 target_size : \u6574\u6570\u5143\u7ec4 (height, width) \uff0c\u9ed8\u8ba4\uff1a (256, 256) \u3002\u6240\u6709\u7684\u56fe\u50cf\u5c06\u88ab\u8c03\u6574\u5230\u7684\u5c3a\u5bf8\u3002 color_mode : \"grayscale\", \"rbg\" \u4e4b\u4e00\u3002\u9ed8\u8ba4\uff1a\"rgb\"\u3002\u56fe\u50cf\u662f\u5426\u88ab\u8f6c\u6362\u6210 1 \u6216 3 \u4e2a\u989c\u8272\u901a\u9053\u3002 classes : \u53ef\u9009\u7684\u7c7b\u7684\u5b50\u76ee\u5f55\u5217\u8868\uff08\u4f8b\u5982 ['dogs', 'cats'] \uff09\u3002\u9ed8\u8ba4\uff1aNone\u3002\u5982\u679c\u672a\u63d0\u4f9b\uff0c\u7c7b\u7684\u5217\u8868\u5c06\u81ea\u52a8\u4ece directory \u4e0b\u7684 \u5b50\u76ee\u5f55\u540d\u79f0/\u7ed3\u6784 \u4e2d\u63a8\u65ad\u51fa\u6765\uff0c\u5176\u4e2d\u6bcf\u4e2a\u5b50\u76ee\u5f55\u90fd\u5c06\u88ab\u4f5c\u4e3a\u4e0d\u540c\u7684\u7c7b\uff08\u7c7b\u540d\u5c06\u6309\u5b57\u5178\u5e8f\u6620\u5c04\u5230\u6807\u7b7e\u7684\u7d22\u5f15\uff09\u3002\u5305\u542b\u4ece\u7c7b\u540d\u5230\u7c7b\u7d22\u5f15\u7684\u6620\u5c04\u7684\u5b57\u5178\u53ef\u4ee5\u901a\u8fc7 class_indices \u5c5e\u6027\u83b7\u5f97\u3002 class_mode : \"categorical\", \"binary\", \"sparse\", \"input\" \u6216 None \u4e4b\u4e00\u3002\u9ed8\u8ba4\uff1a\"categorical\"\u3002\u51b3\u5b9a\u8fd4\u56de\u7684\u6807\u7b7e\u6570\u7ec4\u7684\u7c7b\u578b\uff1a \"categorical\" \u5c06\u662f 2D one-hot \u7f16\u7801\u6807\u7b7e\uff0c \"binary\" \u5c06\u662f 1D \u4e8c\u8fdb\u5236\u6807\u7b7e\uff0c\"sparse\" \u5c06\u662f 1D \u6574\u6570\u6807\u7b7e\uff0c \"input\" \u5c06\u662f\u4e0e\u8f93\u5165\u56fe\u50cf\u76f8\u540c\u7684\u56fe\u50cf\uff08\u4e3b\u8981\u7528\u4e8e\u81ea\u52a8\u7f16\u7801\u5668\uff09\u3002 \u5982\u679c\u4e3a None\uff0c\u4e0d\u8fd4\u56de\u6807\u7b7e\uff08\u751f\u6210\u5668\u5c06\u53ea\u4ea7\u751f\u6279\u91cf\u7684\u56fe\u50cf\u6570\u636e\uff0c\u5bf9\u4e8e model.predict_generator() , model.evaluate_generator() \u7b49\u5f88\u6709\u7528\uff09\u3002\u8bf7\u6ce8\u610f\uff0c\u5982\u679c class_mode \u4e3a None\uff0c\u90a3\u4e48\u6570\u636e\u4ecd\u7136\u9700\u8981\u9a7b\u7559\u5728 directory \u7684\u5b50\u76ee\u5f55\u4e2d\u624d\u80fd\u6b63\u5e38\u5de5\u4f5c\u3002 batch_size : \u4e00\u6279\u6570\u636e\u7684\u5927\u5c0f\uff08\u9ed8\u8ba4 32\uff09\u3002 shuffle : \u662f\u5426\u6df7\u6d17\u6570\u636e\uff08\u9ed8\u8ba4 True\uff09\u3002 seed : \u53ef\u9009\u968f\u673a\u79cd\u5b50\uff0c\u7528\u4e8e\u6df7\u6d17\u548c\u8f6c\u6362\u3002 save_to_dir : None \u6216 \u5b57\u7b26\u4e32\uff08\u9ed8\u8ba4 None\uff09\u3002\u8fd9\u4f7f\u4f60\u53ef\u4ee5\u6700\u4f73\u5730\u6307\u5b9a\u6b63\u5728\u751f\u6210\u7684\u589e\u5f3a\u56fe\u7247\u8981\u4fdd\u5b58\u7684\u76ee\u5f55\uff08\u7528\u4e8e\u53ef\u89c6\u5316\u4f60\u5728\u505a\u4ec0\u4e48\uff09\u3002 save_prefix : \u5b57\u7b26\u4e32\u3002 \u4fdd\u5b58\u56fe\u7247\u7684\u6587\u4ef6\u540d\u524d\u7f00\uff08\u4ec5\u5f53 save_to_dir \u8bbe\u7f6e\u65f6\u53ef\u7528\uff09\u3002 save_format : \"png\", \"jpeg\" \u4e4b\u4e00\uff08\u4ec5\u5f53 save_to_dir \u8bbe\u7f6e\u65f6\u53ef\u7528\uff09\u3002\u9ed8\u8ba4\uff1a\"png\"\u3002 follow_links : \u662f\u5426\u8ddf\u8e2a\u7c7b\u5b50\u76ee\u5f55\u4e2d\u7684\u7b26\u53f7\u94fe\u63a5\uff08\u9ed8\u8ba4\u4e3a False\uff09\u3002 subset : \u6570\u636e\u5b50\u96c6 (\"training\" \u6216 \"validation\")\uff0c\u5982\u679c \u5728 ImageDataGenerator \u4e2d\u8bbe\u7f6e\u4e86 validation_split \u3002 interpolation : \u5728\u76ee\u6807\u5927\u5c0f\u4e0e\u52a0\u8f7d\u56fe\u50cf\u7684\u5927\u5c0f\u4e0d\u540c\u65f6\uff0c\u7528\u4e8e\u91cd\u65b0\u91c7\u6837\u56fe\u50cf\u7684\u63d2\u503c\u65b9\u6cd5\u3002 \u652f\u6301\u7684\u65b9\u6cd5\u6709 \"nearest\" , \"bilinear\" , and \"bicubic\" \u3002 \u5982\u679c\u5b89\u88c5\u4e86 1.1.3 \u4ee5\u4e0a\u7248\u672c\u7684 PIL \u7684\u8bdd\uff0c\u540c\u6837\u652f\u6301 \"lanczos\" \u3002 \u5982\u679c\u5b89\u88c5\u4e86 3.4.0 \u4ee5\u4e0a\u7248\u672c\u7684 PIL \u7684\u8bdd\uff0c\u540c\u6837\u652f\u6301 \"box\" \u548c \"hamming\" \u3002 \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528 \"nearest\" \u3002 \u8fd4\u56de \u4e00\u4e2a\u751f\u6210 (x, y) \u5143\u7ec4\u7684 DirectoryIterator \uff0c\u5176\u4e2d x \u662f\u4e00\u4e2a\u5305\u542b\u4e00\u6279\u5c3a\u5bf8\u4e3a (batch_size, *target_size, channels) \u7684\u56fe\u50cf\u7684 Numpy \u6570\u7ec4\uff0c y \u662f\u5bf9\u5e94\u6807\u7b7e\u7684 Numpy \u6570\u7ec4\u3002","title":"flow_from_directory"},{"location":"3-Preprocessing/2.image/#get_random_transform","text":"get_random_transform(img_shape, seed=None) \u4e3a\u8f6c\u6362\u751f\u6210\u968f\u673a\u53c2\u6570\u3002 \u53c2\u6570 seed : \u968f\u673a\u79cd\u5b50 img_shape : \u6574\u6570\u5143\u7ec4\u3002\u88ab\u8f6c\u6362\u7684\u56fe\u50cf\u7684\u5c3a\u5bf8\u3002 \u8fd4\u56de \u5305\u542b\u968f\u673a\u9009\u62e9\u7684\u63cf\u8ff0\u53d8\u6362\u7684\u53c2\u6570\u7684\u5b57\u5178\u3002","title":"get_random_transform"},{"location":"3-Preprocessing/2.image/#random_transform","text":"random_transform(x, seed=None) \u5c06\u968f\u673a\u53d8\u6362\u5e94\u7528\u4e8e\u56fe\u50cf\u3002 \u53c2\u6570 x : 3D \u5f20\u91cf\uff0c\u5355\u5f20\u56fe\u50cf\u3002 seed : \u968f\u673a\u79cd\u5b50\u3002 \u8fd4\u56de \u8f93\u5165\u7684\u968f\u673a\u8f6c\u6362\u7248\u672c\uff08\u76f8\u540c\u5f62\u72b6\uff09\u3002","title":"random_transform"},{"location":"3-Preprocessing/2.image/#standardize","text":"standardize(x) \u5c06\u6807\u51c6\u5316\u914d\u7f6e\u5e94\u7528\u4e8e\u4e00\u6279\u8f93\u5165\u3002 \u53c2\u6570 x : \u9700\u8981\u6807\u51c6\u5316\u7684\u4e00\u6279\u8f93\u5165\u3002 \u8fd4\u56de \u6807\u51c6\u5316\u540e\u7684\u8f93\u5165\u3002","title":"standardize"},{"location":"4-Examples/","text":"\u7ecf\u5178\u6837\u4f8b Addition RNN: examples/addition_rnn.md Baby RNN: examples/babi_rnn.md Baby MemNN: examples/babi_memnn.md CIFAR-10 CNN: examples/cifar10_cnn.md CIFAR-10 CNN-Capsule: examples/cifar10_cnn_capsule.md CIFAR-10 CNN with augmentation (TF): examples/cifar10_cnn_tfaugment2d.md CIFAR-10 ResNet: examples/cifar10_resnet.md Convolution filter visualization: examples/conv_filter_visualization.md Image OCR: examples/image_ocr.md Bidirectional LSTM: examples/imdb_bidirectional_lstm.md","title":"\u7ecf\u5178\u6837\u4f8b"},{"location":"4-Examples/#_1","text":"Addition RNN: examples/addition_rnn.md Baby RNN: examples/babi_rnn.md Baby MemNN: examples/babi_memnn.md CIFAR-10 CNN: examples/cifar10_cnn.md CIFAR-10 CNN-Capsule: examples/cifar10_cnn_capsule.md CIFAR-10 CNN with augmentation (TF): examples/cifar10_cnn_tfaugment2d.md CIFAR-10 ResNet: examples/cifar10_resnet.md Convolution filter visualization: examples/conv_filter_visualization.md Image OCR: examples/image_ocr.md Bidirectional LSTM: examples/imdb_bidirectional_lstm.md","title":"\u7ecf\u5178\u6837\u4f8b"},{"location":"4-Examples/0.pretrained_word_embeddings/","text":"\u9884\u8bad\u7ec3\u7684\u5355\u8bcd\u5d4c\u5165 \u6b64\u811a\u672c\u5c06\u9884\u5148\u8bad\u7ec3\u7684\u5355\u8bcd\u5d4c\u5165\uff08GloVe \u5d4c\u5165\uff09\u52a0\u8f7d\u5230\u51bb\u7ed3\u7684 Keras \u5d4c\u5165\u5c42\u4e2d\uff0c\u5e76\u4f7f\u7528\u5b83\u5728 20 \u4e2a\u65b0\u95fb\u7ec4\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6587\u672c\u5206\u7c7b\u6a21\u578b\uff08\u5c06\u65b0\u95fb\u7ec4\u6d88\u606f\u5206\u7c7b\u4e3a 20 \u4e2a\u4e0d\u540c\u7684\u7c7b\u522b\uff09\u3002 \u53ef\u4ee5\u5728\u4ee5\u4e0b\u4f4d\u7f6e\u627e\u5230 GloVe \u5d4c\u5165\u6570\u636e: http://nlp.stanford.edu/data/glove.6B.zip(\u6e90\u9875\u9762: http://nlp.stanford.edu/projects/glove/) 20 \u4e2a\u65b0\u95fb\u7ec4\u6570\u636e\u53ef\u4ee5\u5728: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html from __future__ import print_function import os import sys import numpy as np from keras.preprocessing.text import Tokenizer from keras.preprocessing.sequence import pad_sequences from keras.utils import to_categorical from keras.layers import Dense, Input, GlobalMaxPooling1D from keras.layers import Conv1D, MaxPooling1D, Embedding from keras.models import Model from keras.initializers import Constant BASE_DIR = '' GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B') TEXT_DATA_DIR = os.path.join(BASE_DIR, '20_newsgroup') MAX_SEQUENCE_LENGTH = 1000 MAX_NUM_WORDS = 20000 EMBEDDING_DIM = 100 VALIDATION_SPLIT = 0.2 # first, build index mapping words in the embeddings set # to their embedding vector print('Indexing word vectors.') embeddings_index = {} with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f: for line in f: values = line.split() word = values[0] coefs = np.asarray(values[1:], dtype='float32') embeddings_index[word] = coefs print('Found %s word vectors.' % len(embeddings_index)) # second, prepare text samples and their labels print('Processing text dataset') texts = [] # list of text samples labels_index = {} # dictionary mapping label name to numeric id labels = [] # list of label ids for name in sorted(os.listdir(TEXT_DATA_DIR)): path = os.path.join(TEXT_DATA_DIR, name) if os.path.isdir(path): label_id = len(labels_index) labels_index[name] = label_id for fname in sorted(os.listdir(path)): if fname.isdigit(): fpath = os.path.join(path, fname) args = {} if sys.version_info < (3,) else {'encoding': 'latin-1'} with open(fpath, **args) as f: t = f.read() i = t.find('\\n\\n') # skip header if 0 < i: t = t[i:] texts.append(t) labels.append(label_id) print('Found %s texts.' % len(texts)) # finally, vectorize the text samples into a 2D integer tensor tokenizer = Tokenizer(num_words=MAX_NUM_WORDS) tokenizer.fit_on_texts(texts) sequences = tokenizer.texts_to_sequences(texts) word_index = tokenizer.word_index print('Found %s unique tokens.' % len(word_index)) data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH) labels = to_categorical(np.asarray(labels)) print('Shape of data tensor:', data.shape) print('Shape of label tensor:', labels.shape) # split the data into a training set and a validation set indices = np.arange(data.shape[0]) np.random.shuffle(indices) data = data[indices] labels = labels[indices] num_validation_samples = int(VALIDATION_SPLIT * data.shape[0]) x_train = data[:-num_validation_samples] y_train = labels[:-num_validation_samples] x_val = data[-num_validation_samples:] y_val = labels[-num_validation_samples:] print('Preparing embedding matrix.') # prepare embedding matrix num_words = min(MAX_NUM_WORDS, len(word_index)) + 1 embedding_matrix = np.zeros((num_words, EMBEDDING_DIM)) for word, i in word_index.items(): if i > MAX_NUM_WORDS: continue embedding_vector = embeddings_index.get(word) if embedding_vector is not None: # words not found in embedding index will be all-zeros. embedding_matrix[i] = embedding_vector # load pre-trained word embeddings into an Embedding layer # note that we set trainable = False so as to keep the embeddings fixed embedding_layer = Embedding(num_words, EMBEDDING_DIM, embeddings_initializer=Constant(embedding_matrix), input_length=MAX_SEQUENCE_LENGTH, trainable=False) print('Training model.') # train a 1D convnet with global maxpooling sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32') embedded_sequences = embedding_layer(sequence_input) x = Conv1D(128, 5, activation='relu')(embedded_sequences) x = MaxPooling1D(5)(x) x = Conv1D(128, 5, activation='relu')(x) x = MaxPooling1D(5)(x) x = Conv1D(128, 5, activation='relu')(x) x = GlobalMaxPooling1D()(x) x = Dense(128, activation='relu')(x) preds = Dense(len(labels_index), activation='softmax')(x) model = Model(sequence_input, preds) model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc']) model.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_val, y_val))","title":"\u9884\u8bad\u7ec3\u7684\u5355\u8bcd\u5d4c\u5165"},{"location":"4-Examples/0.pretrained_word_embeddings/#_1","text":"\u6b64\u811a\u672c\u5c06\u9884\u5148\u8bad\u7ec3\u7684\u5355\u8bcd\u5d4c\u5165\uff08GloVe \u5d4c\u5165\uff09\u52a0\u8f7d\u5230\u51bb\u7ed3\u7684 Keras \u5d4c\u5165\u5c42\u4e2d\uff0c\u5e76\u4f7f\u7528\u5b83\u5728 20 \u4e2a\u65b0\u95fb\u7ec4\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6587\u672c\u5206\u7c7b\u6a21\u578b\uff08\u5c06\u65b0\u95fb\u7ec4\u6d88\u606f\u5206\u7c7b\u4e3a 20 \u4e2a\u4e0d\u540c\u7684\u7c7b\u522b\uff09\u3002 \u53ef\u4ee5\u5728\u4ee5\u4e0b\u4f4d\u7f6e\u627e\u5230 GloVe \u5d4c\u5165\u6570\u636e: http://nlp.stanford.edu/data/glove.6B.zip(\u6e90\u9875\u9762: http://nlp.stanford.edu/projects/glove/) 20 \u4e2a\u65b0\u95fb\u7ec4\u6570\u636e\u53ef\u4ee5\u5728: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html from __future__ import print_function import os import sys import numpy as np from keras.preprocessing.text import Tokenizer from keras.preprocessing.sequence import pad_sequences from keras.utils import to_categorical from keras.layers import Dense, Input, GlobalMaxPooling1D from keras.layers import Conv1D, MaxPooling1D, Embedding from keras.models import Model from keras.initializers import Constant BASE_DIR = '' GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B') TEXT_DATA_DIR = os.path.join(BASE_DIR, '20_newsgroup') MAX_SEQUENCE_LENGTH = 1000 MAX_NUM_WORDS = 20000 EMBEDDING_DIM = 100 VALIDATION_SPLIT = 0.2 # first, build index mapping words in the embeddings set # to their embedding vector print('Indexing word vectors.') embeddings_index = {} with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f: for line in f: values = line.split() word = values[0] coefs = np.asarray(values[1:], dtype='float32') embeddings_index[word] = coefs print('Found %s word vectors.' % len(embeddings_index)) # second, prepare text samples and their labels print('Processing text dataset') texts = [] # list of text samples labels_index = {} # dictionary mapping label name to numeric id labels = [] # list of label ids for name in sorted(os.listdir(TEXT_DATA_DIR)): path = os.path.join(TEXT_DATA_DIR, name) if os.path.isdir(path): label_id = len(labels_index) labels_index[name] = label_id for fname in sorted(os.listdir(path)): if fname.isdigit(): fpath = os.path.join(path, fname) args = {} if sys.version_info < (3,) else {'encoding': 'latin-1'} with open(fpath, **args) as f: t = f.read() i = t.find('\\n\\n') # skip header if 0 < i: t = t[i:] texts.append(t) labels.append(label_id) print('Found %s texts.' % len(texts)) # finally, vectorize the text samples into a 2D integer tensor tokenizer = Tokenizer(num_words=MAX_NUM_WORDS) tokenizer.fit_on_texts(texts) sequences = tokenizer.texts_to_sequences(texts) word_index = tokenizer.word_index print('Found %s unique tokens.' % len(word_index)) data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH) labels = to_categorical(np.asarray(labels)) print('Shape of data tensor:', data.shape) print('Shape of label tensor:', labels.shape) # split the data into a training set and a validation set indices = np.arange(data.shape[0]) np.random.shuffle(indices) data = data[indices] labels = labels[indices] num_validation_samples = int(VALIDATION_SPLIT * data.shape[0]) x_train = data[:-num_validation_samples] y_train = labels[:-num_validation_samples] x_val = data[-num_validation_samples:] y_val = labels[-num_validation_samples:] print('Preparing embedding matrix.') # prepare embedding matrix num_words = min(MAX_NUM_WORDS, len(word_index)) + 1 embedding_matrix = np.zeros((num_words, EMBEDDING_DIM)) for word, i in word_index.items(): if i > MAX_NUM_WORDS: continue embedding_vector = embeddings_index.get(word) if embedding_vector is not None: # words not found in embedding index will be all-zeros. embedding_matrix[i] = embedding_vector # load pre-trained word embeddings into an Embedding layer # note that we set trainable = False so as to keep the embeddings fixed embedding_layer = Embedding(num_words, EMBEDDING_DIM, embeddings_initializer=Constant(embedding_matrix), input_length=MAX_SEQUENCE_LENGTH, trainable=False) print('Training model.') # train a 1D convnet with global maxpooling sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32') embedded_sequences = embedding_layer(sequence_input) x = Conv1D(128, 5, activation='relu')(embedded_sequences) x = MaxPooling1D(5)(x) x = Conv1D(128, 5, activation='relu')(x) x = MaxPooling1D(5)(x) x = Conv1D(128, 5, activation='relu')(x) x = GlobalMaxPooling1D()(x) x = Dense(128, activation='relu')(x) preds = Dense(len(labels_index), activation='softmax')(x) model = Model(sequence_input, preds) model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc']) model.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_val, y_val))","title":"\u9884\u8bad\u7ec3\u7684\u5355\u8bcd\u5d4c\u5165"},{"location":"4-Examples/addition_rnn/","text":"\u5b9e\u73b0\u4e00\u4e2a\u7528\u6765\u6267\u884c\u52a0\u6cd5\u7684\u5e8f\u5217\u5230\u5e8f\u5217\u5b66\u4e60\u6a21\u578b \u8f93\u5165: \"535+61\" \u8f93\u51fa: \"596\" \u4f7f\u7528\u91cd\u590d\u7684\u6807\u8bb0\u5b57\u7b26\uff08\u7a7a\u683c\uff09\u5904\u7406\u586b\u5145\u3002 \u8f93\u5165\u53ef\u4ee5\u9009\u62e9\u6027\u5730\u53cd\u8f6c\uff0c\u5b83\u88ab\u8ba4\u4e3a\u53ef\u4ee5\u63d0\u9ad8\u8bb8\u591a\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4f8b\u5982\uff1a Learning to Execute \u4ee5\u53ca Sequence to Sequence Learning with Neural Networks \u3002 \u4ece\u7406\u8bba\u4e0a\u8bb2\uff0c\u5b83\u5f15\u5165\u4e86\u6e90\u548c\u76ee\u6807\u4e4b\u95f4\u7684\u77ed\u671f\u4f9d\u8d56\u5173\u7cfb\u3002 \u4e24\u4e2a\u53cd\u8f6c\u7684\u6570\u5b57 + \u4e00\u4e2a LSTM \u5c42\uff08128\u4e2a\u9690\u85cf\u5355\u5143\uff09\uff0c\u5728 55 \u4e2a epochs \u540e\uff0c5k \u7684\u8bad\u7ec3\u6837\u672c\u53d6\u5f97\u4e86 99% \u7684\u8bad\u7ec3/\u6d4b\u8bd5\u51c6\u786e\u7387\u3002 \u4e09\u4e2a\u53cd\u8f6c\u7684\u6570\u5b57 + \u4e00\u4e2a LSTM \u5c42\uff08128\u4e2a\u9690\u85cf\u5355\u5143\uff09\uff0c\u5728 100 \u4e2a epochs \u540e\uff0c50k \u7684\u8bad\u7ec3\u6837\u672c\u53d6\u5f97\u4e86 99% \u7684\u8bad\u7ec3/\u6d4b\u8bd5\u51c6\u786e\u7387\u3002 \u56db\u4e2a\u53cd\u8f6c\u7684\u6570\u5b57 + \u4e00\u4e2a LSTM \u5c42\uff08128\u4e2a\u9690\u85cf\u5355\u5143\uff09\uff0c\u5728 20 \u4e2a epochs \u540e\uff0c400k \u7684\u8bad\u7ec3\u6837\u672c\u53d6\u5f97\u4e86 99% \u7684\u8bad\u7ec3/\u6d4b\u8bd5\u51c6\u786e\u7387\u3002 \u4e94\u4e2a\u53cd\u8f6c\u7684\u6570\u5b57 + \u4e00\u4e2a LSTM \u5c42\uff08128\u4e2a\u9690\u85cf\u5355\u5143\uff09\uff0c\u5728 30 \u4e2a epochs \u540e\uff0c550k \u7684\u8bad\u7ec3\u6837\u672c\u53d6\u5f97\u4e86 99% \u7684\u8bad\u7ec3/\u6d4b\u8bd5\u51c6\u786e\u7387\u3002 from __future__ import print_function from keras.models import Sequential from keras import layers import numpy as np from six.moves import range class CharacterTable(object): \"\"\"\u7ed9\u5b9a\u4e00\u7ec4\u5b57\u7b26\uff1a + \u5c06\u5b83\u4eec\u7f16\u7801\u4e3a one-hot \u6574\u6570\u8868\u793a + \u5c06 one-hot \u6216\u6574\u6570\u8868\u793a\u89e3\u7801\u4e3a\u5b57\u7b26\u8f93\u51fa + \u5c06\u4e00\u4e2a\u6982\u7387\u5411\u91cf\u89e3\u7801\u4e3a\u5b57\u7b26\u8f93\u51fa \"\"\" def __init__(self, chars): \"\"\"\u521d\u59cb\u5316\u5b57\u7b26\u8868\u3002 # \u53c2\u6570\uff1a chars: \u53ef\u4ee5\u51fa\u73b0\u5728\u8f93\u5165\u4e2d\u7684\u5b57\u7b26\u3002 \"\"\" self.chars = sorted(set(chars)) self.char_indices = dict((c, i) for i, c in enumerate(self.chars)) self.indices_char = dict((i, c) for i, c in enumerate(self.chars)) def encode(self, C, num_rows): \"\"\"\u7ed9\u5b9a\u5b57\u7b26\u4e32 C \u7684 one-hot \u7f16\u7801\u3002 # \u53c2\u6570 C: \u9700\u8981\u7f16\u7801\u7684\u5b57\u7b26\u4e32\u3002 num_rows: \u8fd4\u56de\u7684 one-hot \u7f16\u7801\u7684\u884c\u6570\u3002 \u8fd9\u7528\u6765\u4fdd\u8bc1\u6bcf\u4e2a\u6570\u636e\u7684\u884c\u6570\u76f8\u540c\u3002 \"\"\" x = np.zeros((num_rows, len(self.chars))) for i, c in enumerate(C): x[i, self.char_indices[c]] = 1 return x def decode(self, x, calc_argmax=True): \"\"\"\u5c06\u7ed9\u5b9a\u7684\u5411\u91cf\u6216 2D array \u89e3\u7801\u4e3a\u5b83\u4eec\u7684\u5b57\u7b26\u8f93\u51fa\u3002 # \u53c2\u6570 x: \u4e00\u4e2a\u5411\u91cf\u6216 2D \u6982\u7387\u6570\u7ec4\u6216 one-hot \u8868\u793a\uff0c \u6216 \u5b57\u7b26\u7d22\u5f15\u7684\u5411\u91cf\uff08\u5982\u679c `calc_argmax=False`\uff09\u3002 calc_argmax: \u662f\u5426\u6839\u636e\u6700\u5927\u6982\u7387\u6765\u627e\u5230\u5b57\u7b26\uff0c\u9ed8\u8ba4\u4e3a `True`\u3002 \"\"\" if calc_argmax: x = x.argmax(axis=-1) return ''.join(self.indices_char[x] for x in x) class colors: ok = '\\033[92m' fail = '\\033[91m' close = '\\033[0m' # \u6a21\u578b\u548c\u6570\u636e\u7684\u53c2\u6570 TRAINING_SIZE = 50000 DIGITS = 3 REVERSE = True # \u8f93\u5165\u7684\u6700\u5927\u957f\u5ea6\u662f 'int+int' (\u4f8b\u5982, '345+678'). int \u7684\u6700\u5927\u957f\u5ea6\u4e3a DIGITS\u3002 MAXLEN = DIGITS + 1 + DIGITS # \u6240\u6709\u7684\u6570\u5b57\uff0c\u52a0\u4e0a\u7b26\u53f7\uff0c\u4ee5\u53ca\u7528\u4e8e\u586b\u5145\u7684\u7a7a\u683c\u3002 chars = '0123456789+ ' ctable = CharacterTable(chars) questions = [] expected = [] seen = set() print('Generating data...') while len(questions) < TRAINING_SIZE: f = lambda: int(''.join(np.random.choice(list('0123456789')) for i in range(np.random.randint(1, DIGITS + 1)))) a, b = f(), f() # \u8df3\u8fc7\u4efb\u4f55\u5df2\u6709\u7684\u52a0\u6cd5\u95ee\u9898 # \u540c\u4e8b\u8df3\u8fc7\u4efb\u4f55 x+Y == Y+x \u7684\u60c5\u51b5(\u5373\u6392\u5e8f)\u3002 key = tuple(sorted((a, b))) if key in seen: continue seen.add(key) # \u5229\u7528\u7a7a\u683c\u586b\u5145\uff0c\u662f\u7684\u957f\u5ea6\u59cb\u7ec8\u4e3a MAXLEN\u3002 q = '{}+{}'.format(a, b) query = q + ' ' * (MAXLEN - len(q)) ans = str(a + b) # \u7b54\u6848\u53ef\u80fd\u7684\u6700\u957f\u957f\u5ea6\u4e3a DIGITS + 1\u3002 ans += ' ' * (DIGITS + 1 - len(ans)) if REVERSE: # \u53cd\u8f6c\u67e5\u8be2\uff0c\u4f8b\u5982\uff0c'12+345 ' \u53d8\u6210 ' 543+21'. # (\u6ce8\u610f\u7528\u4e8e\u586b\u5145\u7684\u7a7a\u683c) query = query[::-1] questions.append(query) expected.append(ans) print('Total addition questions:', len(questions)) print('Vectorization...') x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool) y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool) for i, sentence in enumerate(questions): x[i] = ctable.encode(sentence, MAXLEN) for i, sentence in enumerate(expected): y[i] = ctable.encode(sentence, DIGITS + 1) # \u6df7\u6d17 (x, y)\uff0c\u56e0\u4e3a x \u7684\u540e\u534a\u6bb5\u51e0\u4e4e\u90fd\u662f\u6bd4\u8f83\u5927\u7684\u6570\u5b57\u3002 indices = np.arange(len(y)) np.random.shuffle(indices) x = x[indices] y = y[indices] # \u663e\u5f0f\u5730\u5206\u79bb\u51fa 10% \u7684\u8bad\u7ec3\u6570\u636e\u4f5c\u4e3a\u9a8c\u8bc1\u96c6\u3002 split_at = len(x) - len(x) // 10 (x_train, x_val) = x[:split_at], x[split_at:] (y_train, y_val) = y[:split_at], y[split_at:] print('Training Data:') print(x_train.shape) print(y_train.shape) print('Validation Data:') print(x_val.shape) print(y_val.shape) # \u53ef\u4ee5\u5c1d\u8bd5\u66f4\u6539\u4e3a GRU, \u6216 SimpleRNN\u3002 RNN = layers.LSTM HIDDEN_SIZE = 128 BATCH_SIZE = 128 LAYERS = 1 print('Build model...') model = Sequential() # \u5229\u7528 RNN \u5c06\u8f93\u5165\u5e8f\u5217\u300c\u7f16\u7801\u300d\u4e3a\u4e00\u4e2a HIDDEN_SIZE \u957f\u5ea6\u7684\u8f93\u51fa\u5411\u91cf\u3002 # \u6ce8\u610f\uff1a\u5728\u8f93\u5165\u5e8f\u5217\u5177\u6709\u53ef\u53d8\u957f\u5ea6\u7684\u60c5\u51b5\u4e0b, # \u4f7f\u7528 input_shape=(None, num_feature). model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars)))) # \u4f5c\u4e3a\u89e3\u7801\u5668 RNN \u7684\u8f93\u5165\uff0c\u4e3a\u6bcf\u4e2a\u65f6\u95f4\u6b65\u91cd\u590d\u5730\u63d0\u4f9b RNN \u7684\u6700\u540e\u8f93\u51fa\u3002 # \u91cd\u590d 'DIGITS + 1' \u6b21\uff0c\u56e0\u4e3a\u5b83\u662f\u6700\u5927\u8f93\u51fa\u957f\u5ea6\u3002 # \u4f8b\u5982\uff0c\u5f53 DIGITS=3, \u6700\u5927\u8f93\u51fa\u4e3a 999+999=1998\u3002 model.add(layers.RepeatVector(DIGITS + 1)) # \u89e3\u7801\u5668 RNN \u53ef\u4ee5\u662f\u591a\u4e2a\u5806\u53e0\u7684\u5c42\uff0c\u6216\u4e00\u4e2a\u5355\u72ec\u7684\u5c42\u3002 for _ in range(LAYERS): # \u901a\u8fc7\u8bbe\u7f6e return_sequences \u4e3a True, \u5c06\u4e0d\u4ec5\u8fd4\u56de\u6700\u540e\u4e00\u4e2a\u8f93\u51fa\uff0c\u800c\u662f\u8fd4\u56de\u76ee\u524d\u7684\u6240\u6709\u8f93\u51fa\uff0c\u5f62\u5f0f\u4e3a(num_samples, timesteps, output_dim)\u3002 # \u8fd9\u662f\u5fc5\u987b\u7684\uff0c\u56e0\u4e3a\u540e\u9762\u7684 TimeDistributed \u9700\u8981\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\u662f\u65f6\u95f4\u6b65\u3002 model.add(RNN(HIDDEN_SIZE, return_sequences=True)) # \u5c06\u5168\u8fde\u63a5\u5c42\u5e94\u7528\u4e8e\u8f93\u5165\u7684\u6bcf\u4e2a\u65f6\u95f4\u7247\u3002 # \u5bf9\u4e8e\u8f93\u51fa\u5e8f\u5217\u7684\u6bcf\u4e00\u6b65\uff0c\u51b3\u5b9a\u5e94\u9009\u54ea\u4e2a\u5b57\u7b26\u3002 model.add(layers.TimeDistributed(layers.Dense(len(chars), activation='softmax'))) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) model.summary() # \u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u5728\u6bcf\u4e00\u4ee3\u663e\u793a\u9a8c\u8bc1\u6570\u636e\u7684\u9884\u6d4b\u3002 for iteration in range(1, 200): print() print('-' * 50) print('Iteration', iteration) model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=1, validation_data=(x_val, y_val)) # \u4ece\u968f\u673a\u9a8c\u8bc1\u96c6\u4e2d\u9009\u62e9 10 \u4e2a\u6837\u672c\uff0c\u4ee5\u4fbf\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u9519\u8bef\u3002 for i in range(10): ind = np.random.randint(0, len(x_val)) rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])] preds = model.predict_classes(rowx, verbose=0) q = ctable.decode(rowx[0]) correct = ctable.decode(rowy[0]) guess = ctable.decode(preds[0], calc_argmax=False) print('Q', q[::-1] if REVERSE else q, end=' ') print('T', correct, end=' ') if correct == guess: print(colors.ok + '\u2611' + colors.close, end=' ') else: print(colors.fail + '\u2612' + colors.close, end=' ') print(guess)","title":"\u5b9e\u73b0\u4e00\u4e2a\u7528\u6765\u6267\u884c\u52a0\u6cd5\u7684\u5e8f\u5217\u5230\u5e8f\u5217\u5b66\u4e60\u6a21\u578b"},{"location":"4-Examples/addition_rnn/#_1","text":"\u8f93\u5165: \"535+61\" \u8f93\u51fa: \"596\" \u4f7f\u7528\u91cd\u590d\u7684\u6807\u8bb0\u5b57\u7b26\uff08\u7a7a\u683c\uff09\u5904\u7406\u586b\u5145\u3002 \u8f93\u5165\u53ef\u4ee5\u9009\u62e9\u6027\u5730\u53cd\u8f6c\uff0c\u5b83\u88ab\u8ba4\u4e3a\u53ef\u4ee5\u63d0\u9ad8\u8bb8\u591a\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4f8b\u5982\uff1a Learning to Execute \u4ee5\u53ca Sequence to Sequence Learning with Neural Networks \u3002 \u4ece\u7406\u8bba\u4e0a\u8bb2\uff0c\u5b83\u5f15\u5165\u4e86\u6e90\u548c\u76ee\u6807\u4e4b\u95f4\u7684\u77ed\u671f\u4f9d\u8d56\u5173\u7cfb\u3002 \u4e24\u4e2a\u53cd\u8f6c\u7684\u6570\u5b57 + \u4e00\u4e2a LSTM \u5c42\uff08128\u4e2a\u9690\u85cf\u5355\u5143\uff09\uff0c\u5728 55 \u4e2a epochs \u540e\uff0c5k \u7684\u8bad\u7ec3\u6837\u672c\u53d6\u5f97\u4e86 99% \u7684\u8bad\u7ec3/\u6d4b\u8bd5\u51c6\u786e\u7387\u3002 \u4e09\u4e2a\u53cd\u8f6c\u7684\u6570\u5b57 + \u4e00\u4e2a LSTM \u5c42\uff08128\u4e2a\u9690\u85cf\u5355\u5143\uff09\uff0c\u5728 100 \u4e2a epochs \u540e\uff0c50k \u7684\u8bad\u7ec3\u6837\u672c\u53d6\u5f97\u4e86 99% \u7684\u8bad\u7ec3/\u6d4b\u8bd5\u51c6\u786e\u7387\u3002 \u56db\u4e2a\u53cd\u8f6c\u7684\u6570\u5b57 + \u4e00\u4e2a LSTM \u5c42\uff08128\u4e2a\u9690\u85cf\u5355\u5143\uff09\uff0c\u5728 20 \u4e2a epochs \u540e\uff0c400k \u7684\u8bad\u7ec3\u6837\u672c\u53d6\u5f97\u4e86 99% \u7684\u8bad\u7ec3/\u6d4b\u8bd5\u51c6\u786e\u7387\u3002 \u4e94\u4e2a\u53cd\u8f6c\u7684\u6570\u5b57 + \u4e00\u4e2a LSTM \u5c42\uff08128\u4e2a\u9690\u85cf\u5355\u5143\uff09\uff0c\u5728 30 \u4e2a epochs \u540e\uff0c550k \u7684\u8bad\u7ec3\u6837\u672c\u53d6\u5f97\u4e86 99% \u7684\u8bad\u7ec3/\u6d4b\u8bd5\u51c6\u786e\u7387\u3002 from __future__ import print_function from keras.models import Sequential from keras import layers import numpy as np from six.moves import range class CharacterTable(object): \"\"\"\u7ed9\u5b9a\u4e00\u7ec4\u5b57\u7b26\uff1a + \u5c06\u5b83\u4eec\u7f16\u7801\u4e3a one-hot \u6574\u6570\u8868\u793a + \u5c06 one-hot \u6216\u6574\u6570\u8868\u793a\u89e3\u7801\u4e3a\u5b57\u7b26\u8f93\u51fa + \u5c06\u4e00\u4e2a\u6982\u7387\u5411\u91cf\u89e3\u7801\u4e3a\u5b57\u7b26\u8f93\u51fa \"\"\" def __init__(self, chars): \"\"\"\u521d\u59cb\u5316\u5b57\u7b26\u8868\u3002 # \u53c2\u6570\uff1a chars: \u53ef\u4ee5\u51fa\u73b0\u5728\u8f93\u5165\u4e2d\u7684\u5b57\u7b26\u3002 \"\"\" self.chars = sorted(set(chars)) self.char_indices = dict((c, i) for i, c in enumerate(self.chars)) self.indices_char = dict((i, c) for i, c in enumerate(self.chars)) def encode(self, C, num_rows): \"\"\"\u7ed9\u5b9a\u5b57\u7b26\u4e32 C \u7684 one-hot \u7f16\u7801\u3002 # \u53c2\u6570 C: \u9700\u8981\u7f16\u7801\u7684\u5b57\u7b26\u4e32\u3002 num_rows: \u8fd4\u56de\u7684 one-hot \u7f16\u7801\u7684\u884c\u6570\u3002 \u8fd9\u7528\u6765\u4fdd\u8bc1\u6bcf\u4e2a\u6570\u636e\u7684\u884c\u6570\u76f8\u540c\u3002 \"\"\" x = np.zeros((num_rows, len(self.chars))) for i, c in enumerate(C): x[i, self.char_indices[c]] = 1 return x def decode(self, x, calc_argmax=True): \"\"\"\u5c06\u7ed9\u5b9a\u7684\u5411\u91cf\u6216 2D array \u89e3\u7801\u4e3a\u5b83\u4eec\u7684\u5b57\u7b26\u8f93\u51fa\u3002 # \u53c2\u6570 x: \u4e00\u4e2a\u5411\u91cf\u6216 2D \u6982\u7387\u6570\u7ec4\u6216 one-hot \u8868\u793a\uff0c \u6216 \u5b57\u7b26\u7d22\u5f15\u7684\u5411\u91cf\uff08\u5982\u679c `calc_argmax=False`\uff09\u3002 calc_argmax: \u662f\u5426\u6839\u636e\u6700\u5927\u6982\u7387\u6765\u627e\u5230\u5b57\u7b26\uff0c\u9ed8\u8ba4\u4e3a `True`\u3002 \"\"\" if calc_argmax: x = x.argmax(axis=-1) return ''.join(self.indices_char[x] for x in x) class colors: ok = '\\033[92m' fail = '\\033[91m' close = '\\033[0m' # \u6a21\u578b\u548c\u6570\u636e\u7684\u53c2\u6570 TRAINING_SIZE = 50000 DIGITS = 3 REVERSE = True # \u8f93\u5165\u7684\u6700\u5927\u957f\u5ea6\u662f 'int+int' (\u4f8b\u5982, '345+678'). int \u7684\u6700\u5927\u957f\u5ea6\u4e3a DIGITS\u3002 MAXLEN = DIGITS + 1 + DIGITS # \u6240\u6709\u7684\u6570\u5b57\uff0c\u52a0\u4e0a\u7b26\u53f7\uff0c\u4ee5\u53ca\u7528\u4e8e\u586b\u5145\u7684\u7a7a\u683c\u3002 chars = '0123456789+ ' ctable = CharacterTable(chars) questions = [] expected = [] seen = set() print('Generating data...') while len(questions) < TRAINING_SIZE: f = lambda: int(''.join(np.random.choice(list('0123456789')) for i in range(np.random.randint(1, DIGITS + 1)))) a, b = f(), f() # \u8df3\u8fc7\u4efb\u4f55\u5df2\u6709\u7684\u52a0\u6cd5\u95ee\u9898 # \u540c\u4e8b\u8df3\u8fc7\u4efb\u4f55 x+Y == Y+x \u7684\u60c5\u51b5(\u5373\u6392\u5e8f)\u3002 key = tuple(sorted((a, b))) if key in seen: continue seen.add(key) # \u5229\u7528\u7a7a\u683c\u586b\u5145\uff0c\u662f\u7684\u957f\u5ea6\u59cb\u7ec8\u4e3a MAXLEN\u3002 q = '{}+{}'.format(a, b) query = q + ' ' * (MAXLEN - len(q)) ans = str(a + b) # \u7b54\u6848\u53ef\u80fd\u7684\u6700\u957f\u957f\u5ea6\u4e3a DIGITS + 1\u3002 ans += ' ' * (DIGITS + 1 - len(ans)) if REVERSE: # \u53cd\u8f6c\u67e5\u8be2\uff0c\u4f8b\u5982\uff0c'12+345 ' \u53d8\u6210 ' 543+21'. # (\u6ce8\u610f\u7528\u4e8e\u586b\u5145\u7684\u7a7a\u683c) query = query[::-1] questions.append(query) expected.append(ans) print('Total addition questions:', len(questions)) print('Vectorization...') x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool) y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool) for i, sentence in enumerate(questions): x[i] = ctable.encode(sentence, MAXLEN) for i, sentence in enumerate(expected): y[i] = ctable.encode(sentence, DIGITS + 1) # \u6df7\u6d17 (x, y)\uff0c\u56e0\u4e3a x \u7684\u540e\u534a\u6bb5\u51e0\u4e4e\u90fd\u662f\u6bd4\u8f83\u5927\u7684\u6570\u5b57\u3002 indices = np.arange(len(y)) np.random.shuffle(indices) x = x[indices] y = y[indices] # \u663e\u5f0f\u5730\u5206\u79bb\u51fa 10% \u7684\u8bad\u7ec3\u6570\u636e\u4f5c\u4e3a\u9a8c\u8bc1\u96c6\u3002 split_at = len(x) - len(x) // 10 (x_train, x_val) = x[:split_at], x[split_at:] (y_train, y_val) = y[:split_at], y[split_at:] print('Training Data:') print(x_train.shape) print(y_train.shape) print('Validation Data:') print(x_val.shape) print(y_val.shape) # \u53ef\u4ee5\u5c1d\u8bd5\u66f4\u6539\u4e3a GRU, \u6216 SimpleRNN\u3002 RNN = layers.LSTM HIDDEN_SIZE = 128 BATCH_SIZE = 128 LAYERS = 1 print('Build model...') model = Sequential() # \u5229\u7528 RNN \u5c06\u8f93\u5165\u5e8f\u5217\u300c\u7f16\u7801\u300d\u4e3a\u4e00\u4e2a HIDDEN_SIZE \u957f\u5ea6\u7684\u8f93\u51fa\u5411\u91cf\u3002 # \u6ce8\u610f\uff1a\u5728\u8f93\u5165\u5e8f\u5217\u5177\u6709\u53ef\u53d8\u957f\u5ea6\u7684\u60c5\u51b5\u4e0b, # \u4f7f\u7528 input_shape=(None, num_feature). model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars)))) # \u4f5c\u4e3a\u89e3\u7801\u5668 RNN \u7684\u8f93\u5165\uff0c\u4e3a\u6bcf\u4e2a\u65f6\u95f4\u6b65\u91cd\u590d\u5730\u63d0\u4f9b RNN \u7684\u6700\u540e\u8f93\u51fa\u3002 # \u91cd\u590d 'DIGITS + 1' \u6b21\uff0c\u56e0\u4e3a\u5b83\u662f\u6700\u5927\u8f93\u51fa\u957f\u5ea6\u3002 # \u4f8b\u5982\uff0c\u5f53 DIGITS=3, \u6700\u5927\u8f93\u51fa\u4e3a 999+999=1998\u3002 model.add(layers.RepeatVector(DIGITS + 1)) # \u89e3\u7801\u5668 RNN \u53ef\u4ee5\u662f\u591a\u4e2a\u5806\u53e0\u7684\u5c42\uff0c\u6216\u4e00\u4e2a\u5355\u72ec\u7684\u5c42\u3002 for _ in range(LAYERS): # \u901a\u8fc7\u8bbe\u7f6e return_sequences \u4e3a True, \u5c06\u4e0d\u4ec5\u8fd4\u56de\u6700\u540e\u4e00\u4e2a\u8f93\u51fa\uff0c\u800c\u662f\u8fd4\u56de\u76ee\u524d\u7684\u6240\u6709\u8f93\u51fa\uff0c\u5f62\u5f0f\u4e3a(num_samples, timesteps, output_dim)\u3002 # \u8fd9\u662f\u5fc5\u987b\u7684\uff0c\u56e0\u4e3a\u540e\u9762\u7684 TimeDistributed \u9700\u8981\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\u662f\u65f6\u95f4\u6b65\u3002 model.add(RNN(HIDDEN_SIZE, return_sequences=True)) # \u5c06\u5168\u8fde\u63a5\u5c42\u5e94\u7528\u4e8e\u8f93\u5165\u7684\u6bcf\u4e2a\u65f6\u95f4\u7247\u3002 # \u5bf9\u4e8e\u8f93\u51fa\u5e8f\u5217\u7684\u6bcf\u4e00\u6b65\uff0c\u51b3\u5b9a\u5e94\u9009\u54ea\u4e2a\u5b57\u7b26\u3002 model.add(layers.TimeDistributed(layers.Dense(len(chars), activation='softmax'))) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) model.summary() # \u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u5728\u6bcf\u4e00\u4ee3\u663e\u793a\u9a8c\u8bc1\u6570\u636e\u7684\u9884\u6d4b\u3002 for iteration in range(1, 200): print() print('-' * 50) print('Iteration', iteration) model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=1, validation_data=(x_val, y_val)) # \u4ece\u968f\u673a\u9a8c\u8bc1\u96c6\u4e2d\u9009\u62e9 10 \u4e2a\u6837\u672c\uff0c\u4ee5\u4fbf\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u9519\u8bef\u3002 for i in range(10): ind = np.random.randint(0, len(x_val)) rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])] preds = model.predict_classes(rowx, verbose=0) q = ctable.decode(rowx[0]) correct = ctable.decode(rowy[0]) guess = ctable.decode(preds[0], calc_argmax=False) print('Q', q[::-1] if REVERSE else q, end=' ') print('T', correct, end=' ') if correct == guess: print(colors.ok + '\u2611' + colors.close, end=' ') else: print(colors.fail + '\u2612' + colors.close, end=' ') print(guess)","title":"\u5b9e\u73b0\u4e00\u4e2a\u7528\u6765\u6267\u884c\u52a0\u6cd5\u7684\u5e8f\u5217\u5230\u5e8f\u5217\u5b66\u4e60\u6a21\u578b"},{"location":"4-Examples/antirectifier/","text":"\u8be5\u793a\u4f8b\u6f14\u793a\u4e86\u5982\u4f55\u4e3a Keras \u7f16\u5199\u81ea\u5b9a\u4e49\u56fe\u5c42\u3002 We build a custom activation layer called 'Antirectifier', which modifies the shape of the tensor that passes through it. We need to specify two methods: compute_output_shape and call . Note that the same result can also be achieved via a Lambda layer. Because our custom layer is written with primitives from the Keras backend ( K ), our code can run both on TensorFlow and Theano. from __future__ import print_function import keras from keras.models import Sequential from keras import layers from keras.datasets import mnist from keras import backend as K class Antirectifier(layers.Layer): '''This is the combination of a sample-wise L2 normalization with the concatenation of the positive part of the input with the negative part of the input. The result is a tensor of samples that are twice as large as the input samples. It can be used in place of a ReLU. # Input shape 2D tensor of shape (samples, n) # Output shape 2D tensor of shape (samples, 2*n) # Theoretical justification When applying ReLU, assuming that the distribution of the previous output is approximately centered around 0., you are discarding half of your input. This is inefficient. Antirectifier allows to return all-positive outputs like ReLU, without discarding any data. Tests on MNIST show that Antirectifier allows to train networks with twice less parameters yet with comparable classification accuracy as an equivalent ReLU-based network. ''' def compute_output_shape(self, input_shape): shape = list(input_shape) assert len(shape) == 2 # only valid for 2D tensors shape[-1] *= 2 return tuple(shape) def call(self, inputs): inputs -= K.mean(inputs, axis=1, keepdims=True) inputs = K.l2_normalize(inputs, axis=1) pos = K.relu(inputs) neg = K.relu(-inputs) return K.concatenate([pos, neg], axis=1) # global parameters batch_size = 128 num_classes = 10 epochs = 40 # the data, split between train and test sets (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.reshape(60000, 784) x_test = x_test.reshape(10000, 784) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') # convert class vectors to binary class matrices y_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes) # build the model model = Sequential() model.add(layers.Dense(256, input_shape=(784,))) model.add(Antirectifier()) model.add(layers.Dropout(0.1)) model.add(layers.Dense(256)) model.add(Antirectifier()) model.add(layers.Dropout(0.1)) model.add(layers.Dense(num_classes)) model.add(layers.Activation('softmax')) # compile the model model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) # train the model model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test)) # next, compare with an equivalent network # with2x bigger Dense layers and ReLU","title":"Antirectifier"},{"location":"4-Examples/babi_memnn/","text":"\u5728 bAbI \u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e00\u4e2a\u8bb0\u5fc6\u7f51\u7edc\u3002 \u53c2\u8003\u6587\u732e\uff1a Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush, \"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\" Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus, \"End-To-End Memory Networks\" 120 \u8f6e\u8fed\u4ee3\u540e\uff0c\u5728 'single_supporting_fact_10k' \u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86 98.6% \u7684\u51c6\u786e\u7387\u3002 \u6bcf\u8f6e\u8fed\u4ee3\u65f6\u95f4: 3s on CPU (core i7). from __future__ import print_function from keras.models import Sequential, Model from keras.layers.embeddings import Embedding from keras.layers import Input, Activation, Dense, Permute, Dropout from keras.layers import add, dot, concatenate from keras.layers import LSTM from keras.utils.data_utils import get_file from keras.preprocessing.sequence import pad_sequences from functools import reduce import tarfile import numpy as np import re def tokenize(sent): '''\u8fd4\u56de\u5305\u542b\u6807\u70b9\u7b26\u53f7\u7684\u53e5\u5b50\u7684\u6807\u8bb0\u3002 >>> tokenize('Bob dropped the apple. Where is the apple?') ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?'] ''' return [x.strip() for x in re.split(r'(\\W+)?', sent) if x.strip()] def parse_stories(lines, only_supporting=False): '''\u89e3\u6790 bAbi \u4efb\u52a1\u683c\u5f0f\u4e2d\u63d0\u4f9b\u7684\u6545\u4e8b \u5982\u679c only_supporting \u4e3a true\uff0c \u5219\u53ea\u4fdd\u7559\u652f\u6301\u7b54\u6848\u7684\u53e5\u5b50\u3002 ''' data = [] story = [] for line in lines: line = line.decode('utf-8').strip() nid, line = line.split(' ', 1) nid = int(nid) if nid == 1: story = [] if '\\t' in line: q, a, supporting = line.split('\\t') q = tokenize(q) if only_supporting: # \u53ea\u9009\u62e9\u76f8\u5173\u7684\u5b50\u6545\u4e8b supporting = map(int, supporting.split()) substory = [story[i - 1] for i in supporting] else: # \u63d0\u4f9b\u6240\u6709\u5b50\u6545\u4e8b substory = [x for x in story if x] data.append((substory, q, a)) story.append('') else: sent = tokenize(line) story.append(sent) return data def get_stories(f, only_supporting=False, max_length=None): '''\u7ed9\u5b9a\u6587\u4ef6\u540d\uff0c\u8bfb\u53d6\u6587\u4ef6\uff0c\u68c0\u7d22\u6545\u4e8b\uff0c \u7136\u540e\u5c06\u53e5\u5b50\u8f6c\u6362\u4e3a\u4e00\u4e2a\u72ec\u7acb\u6545\u4e8b\u3002 \u5982\u679c\u63d0\u4f9b\u4e86 max_length, \u4efb\u4f55\u957f\u4e8e max_length \u7684\u6545\u4e8b\u90fd\u5c06\u88ab\u4e22\u5f03\u3002 ''' data = parse_stories(f.readlines(), only_supporting=only_supporting) flatten = lambda data: reduce(lambda x, y: x + y, data) data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length] return data def vectorize_stories(data): inputs, queries, answers = [], [], [] for story, query, answer in data: inputs.append([word_idx[w] for w in story]) queries.append([word_idx[w] for w in query]) answers.append(word_idx[answer]) return (pad_sequences(inputs, maxlen=story_maxlen), pad_sequences(queries, maxlen=query_maxlen), np.array(answers)) try: path = get_file('babi-tasks-v1-2.tar.gz', origin='https://s3.amazonaws.com/text-datasets/' 'babi_tasks_1-20_v1-2.tar.gz') except: print('Error downloading dataset, please download it manually:\\n' '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2' '.tar.gz\\n' '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz') raise challenges = { # QA1 \u4efb\u52a1\uff0c10,000 \u6837\u672c 'single_supporting_fact_10k': 'tasks_1-20_v1-2/en-10k/qa1_' 'single-supporting-fact_{}.txt', # QA2 \u4efb\u52a1\uff0c1000 \u6837\u672c 'two_supporting_facts_10k': 'tasks_1-20_v1-2/en-10k/qa2_' 'two-supporting-facts_{}.txt', } challenge_type = 'single_supporting_fact_10k' challenge = challenges[challenge_type] print('Extracting stories for the challenge:', challenge_type) with tarfile.open(path) as tar: train_stories = get_stories(tar.extractfile(challenge.format('train'))) test_stories = get_stories(tar.extractfile(challenge.format('test'))) vocab = set() for story, q, answer in train_stories + test_stories: vocab |= set(story + q + [answer]) vocab = sorted(vocab) # \u4fdd\u7559 0 \u4ee5\u7559\u4f5c pad_sequences \u8fdb\u884c masking vocab_size = len(vocab) + 1 story_maxlen = max(map(len, (x for x, _, _ in train_stories + test_stories))) query_maxlen = max(map(len, (x for _, x, _ in train_stories + test_stories))) print('-') print('Vocab size:', vocab_size, 'unique words') print('Story max length:', story_maxlen, 'words') print('Query max length:', query_maxlen, 'words') print('Number of training stories:', len(train_stories)) print('Number of test stories:', len(test_stories)) print('-') print('Here\\'s what a \"story\" tuple looks like (input, query, answer):') print(train_stories[0]) print('-') print('Vectorizing the word sequences...') word_idx = dict((c, i + 1) for i, c in enumerate(vocab)) inputs_train, queries_train, answers_train = vectorize_stories(train_stories) inputs_test, queries_test, answers_test = vectorize_stories(test_stories) print('-') print('inputs: integer tensor of shape (samples, max_length)') print('inputs_train shape:', inputs_train.shape) print('inputs_test shape:', inputs_test.shape) print('-') print('queries: integer tensor of shape (samples, max_length)') print('queries_train shape:', queries_train.shape) print('queries_test shape:', queries_test.shape) print('-') print('answers: binary (1 or 0) tensor of shape (samples, vocab_size)') print('answers_train shape:', answers_train.shape) print('answers_test shape:', answers_test.shape) print('-') print('Compiling...') # \u5360\u4f4d\u7b26 input_sequence = Input((story_maxlen,)) question = Input((query_maxlen,)) # \u7f16\u7801\u5668 # \u5c06\u8f93\u5165\u5e8f\u5217\u7f16\u7801\u4e3a\u5411\u91cf\u7684\u5e8f\u5217 input_encoder_m = Sequential() input_encoder_m.add(Embedding(input_dim=vocab_size, output_dim=64)) input_encoder_m.add(Dropout(0.3)) # \u8f93\u51fa: (samples, story_maxlen, embedding_dim) # \u5c06\u8f93\u5165\u7f16\u7801\u4e3a\u7684\u5411\u91cf\u7684\u5e8f\u5217\uff08\u5411\u91cf\u5c3a\u5bf8\u4e3a query_maxlen\uff09 input_encoder_c = Sequential() input_encoder_c.add(Embedding(input_dim=vocab_size, output_dim=query_maxlen)) input_encoder_c.add(Dropout(0.3)) # \u8f93\u51fa: (samples, story_maxlen, query_maxlen) # \u5c06\u95ee\u9898\u7f16\u7801\u4e3a\u5411\u91cf\u7684\u5e8f\u5217 question_encoder = Sequential() question_encoder.add(Embedding(input_dim=vocab_size, output_dim=64, input_length=query_maxlen)) question_encoder.add(Dropout(0.3)) # \u8f93\u51fa: (samples, query_maxlen, embedding_dim) # \u7f16\u7801\u8f93\u5165\u5e8f\u5217\u548c\u95ee\u9898\uff08\u5747\u5df2\u7d22\u5f15\u5316\uff09\u4e3a\u5bc6\u96c6\u5411\u91cf\u7684\u5e8f\u5217 input_encoded_m = input_encoder_m(input_sequence) input_encoded_c = input_encoder_c(input_sequence) question_encoded = question_encoder(question) # \u8ba1\u7b97\u7b2c\u4e00\u4e2a\u8f93\u5165\u5411\u91cf\u548c\u95ee\u9898\u5411\u91cf\u5e8f\u5217\u7684\u300e\u5339\u914d\u300f\uff08'match'\uff09 # \u5c3a\u5bf8: `(samples, story_maxlen, query_maxlen)` match = dot([input_encoded_m, question_encoded], axes=(2, 2)) match = Activation('softmax')(match) # \u5c06\u5339\u914d\u77e9\u9635\u4e0e\u7b2c\u4e8c\u4e2a\u8f93\u5165\u5411\u91cf\u5e8f\u5217\u76f8\u52a0 response = add([match, input_encoded_c]) # (samples, story_maxlen, query_maxlen) response = Permute((2, 1))(response) # (samples, query_maxlen, story_maxlen) # \u62fc\u63a5\u5339\u914d\u77e9\u9635\u548c\u95ee\u9898\u5411\u91cf\u5e8f\u5217 answer = concatenate([response, question_encoded]) # \u539f\u59cb\u8bba\u6587\u4f7f\u7528\u4e00\u4e2a\u77e9\u9635\u4e58\u6cd5\u6765\u8fdb\u884c\u5f52\u7ea6\u64cd\u4f5c\u3002 # \u6211\u4eec\u5728\u6b64\u9009\u62e9\u4f7f\u7528 RNN\u3002 answer = LSTM(32)(answer) # (samples, 32) # \u4e00\u4e2a\u6b63\u5219\u5316\u5c42 - \u53ef\u80fd\u8fd8\u9700\u8981\u66f4\u591a\u5c42 answer = Dropout(0.3)(answer) answer = Dense(vocab_size)(answer) # (samples, vocab_size) # \u8f93\u51fa\u8bcd\u6c47\u8868\u7684\u4e00\u4e2a\u6982\u7387\u5206\u5e03 answer = Activation('softmax')(answer) # \u6784\u5efa\u6700\u7ec8\u6a21\u578b model = Model([input_sequence, question], answer) model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy']) # \u8bad\u7ec3 model.fit([inputs_train, queries_train], answers_train, batch_size=32, epochs=120, validation_data=([inputs_test, queries_test], answers_test))","title":"\u5728 bAbI \u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e00\u4e2a\u8bb0\u5fc6\u7f51\u7edc\u3002"},{"location":"4-Examples/babi_memnn/#babi","text":"\u53c2\u8003\u6587\u732e\uff1a Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush, \"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\" Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus, \"End-To-End Memory Networks\" 120 \u8f6e\u8fed\u4ee3\u540e\uff0c\u5728 'single_supporting_fact_10k' \u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86 98.6% \u7684\u51c6\u786e\u7387\u3002 \u6bcf\u8f6e\u8fed\u4ee3\u65f6\u95f4: 3s on CPU (core i7). from __future__ import print_function from keras.models import Sequential, Model from keras.layers.embeddings import Embedding from keras.layers import Input, Activation, Dense, Permute, Dropout from keras.layers import add, dot, concatenate from keras.layers import LSTM from keras.utils.data_utils import get_file from keras.preprocessing.sequence import pad_sequences from functools import reduce import tarfile import numpy as np import re def tokenize(sent): '''\u8fd4\u56de\u5305\u542b\u6807\u70b9\u7b26\u53f7\u7684\u53e5\u5b50\u7684\u6807\u8bb0\u3002 >>> tokenize('Bob dropped the apple. Where is the apple?') ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?'] ''' return [x.strip() for x in re.split(r'(\\W+)?', sent) if x.strip()] def parse_stories(lines, only_supporting=False): '''\u89e3\u6790 bAbi \u4efb\u52a1\u683c\u5f0f\u4e2d\u63d0\u4f9b\u7684\u6545\u4e8b \u5982\u679c only_supporting \u4e3a true\uff0c \u5219\u53ea\u4fdd\u7559\u652f\u6301\u7b54\u6848\u7684\u53e5\u5b50\u3002 ''' data = [] story = [] for line in lines: line = line.decode('utf-8').strip() nid, line = line.split(' ', 1) nid = int(nid) if nid == 1: story = [] if '\\t' in line: q, a, supporting = line.split('\\t') q = tokenize(q) if only_supporting: # \u53ea\u9009\u62e9\u76f8\u5173\u7684\u5b50\u6545\u4e8b supporting = map(int, supporting.split()) substory = [story[i - 1] for i in supporting] else: # \u63d0\u4f9b\u6240\u6709\u5b50\u6545\u4e8b substory = [x for x in story if x] data.append((substory, q, a)) story.append('') else: sent = tokenize(line) story.append(sent) return data def get_stories(f, only_supporting=False, max_length=None): '''\u7ed9\u5b9a\u6587\u4ef6\u540d\uff0c\u8bfb\u53d6\u6587\u4ef6\uff0c\u68c0\u7d22\u6545\u4e8b\uff0c \u7136\u540e\u5c06\u53e5\u5b50\u8f6c\u6362\u4e3a\u4e00\u4e2a\u72ec\u7acb\u6545\u4e8b\u3002 \u5982\u679c\u63d0\u4f9b\u4e86 max_length, \u4efb\u4f55\u957f\u4e8e max_length \u7684\u6545\u4e8b\u90fd\u5c06\u88ab\u4e22\u5f03\u3002 ''' data = parse_stories(f.readlines(), only_supporting=only_supporting) flatten = lambda data: reduce(lambda x, y: x + y, data) data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length] return data def vectorize_stories(data): inputs, queries, answers = [], [], [] for story, query, answer in data: inputs.append([word_idx[w] for w in story]) queries.append([word_idx[w] for w in query]) answers.append(word_idx[answer]) return (pad_sequences(inputs, maxlen=story_maxlen), pad_sequences(queries, maxlen=query_maxlen), np.array(answers)) try: path = get_file('babi-tasks-v1-2.tar.gz', origin='https://s3.amazonaws.com/text-datasets/' 'babi_tasks_1-20_v1-2.tar.gz') except: print('Error downloading dataset, please download it manually:\\n' '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2' '.tar.gz\\n' '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz') raise challenges = { # QA1 \u4efb\u52a1\uff0c10,000 \u6837\u672c 'single_supporting_fact_10k': 'tasks_1-20_v1-2/en-10k/qa1_' 'single-supporting-fact_{}.txt', # QA2 \u4efb\u52a1\uff0c1000 \u6837\u672c 'two_supporting_facts_10k': 'tasks_1-20_v1-2/en-10k/qa2_' 'two-supporting-facts_{}.txt', } challenge_type = 'single_supporting_fact_10k' challenge = challenges[challenge_type] print('Extracting stories for the challenge:', challenge_type) with tarfile.open(path) as tar: train_stories = get_stories(tar.extractfile(challenge.format('train'))) test_stories = get_stories(tar.extractfile(challenge.format('test'))) vocab = set() for story, q, answer in train_stories + test_stories: vocab |= set(story + q + [answer]) vocab = sorted(vocab) # \u4fdd\u7559 0 \u4ee5\u7559\u4f5c pad_sequences \u8fdb\u884c masking vocab_size = len(vocab) + 1 story_maxlen = max(map(len, (x for x, _, _ in train_stories + test_stories))) query_maxlen = max(map(len, (x for _, x, _ in train_stories + test_stories))) print('-') print('Vocab size:', vocab_size, 'unique words') print('Story max length:', story_maxlen, 'words') print('Query max length:', query_maxlen, 'words') print('Number of training stories:', len(train_stories)) print('Number of test stories:', len(test_stories)) print('-') print('Here\\'s what a \"story\" tuple looks like (input, query, answer):') print(train_stories[0]) print('-') print('Vectorizing the word sequences...') word_idx = dict((c, i + 1) for i, c in enumerate(vocab)) inputs_train, queries_train, answers_train = vectorize_stories(train_stories) inputs_test, queries_test, answers_test = vectorize_stories(test_stories) print('-') print('inputs: integer tensor of shape (samples, max_length)') print('inputs_train shape:', inputs_train.shape) print('inputs_test shape:', inputs_test.shape) print('-') print('queries: integer tensor of shape (samples, max_length)') print('queries_train shape:', queries_train.shape) print('queries_test shape:', queries_test.shape) print('-') print('answers: binary (1 or 0) tensor of shape (samples, vocab_size)') print('answers_train shape:', answers_train.shape) print('answers_test shape:', answers_test.shape) print('-') print('Compiling...') # \u5360\u4f4d\u7b26 input_sequence = Input((story_maxlen,)) question = Input((query_maxlen,)) # \u7f16\u7801\u5668 # \u5c06\u8f93\u5165\u5e8f\u5217\u7f16\u7801\u4e3a\u5411\u91cf\u7684\u5e8f\u5217 input_encoder_m = Sequential() input_encoder_m.add(Embedding(input_dim=vocab_size, output_dim=64)) input_encoder_m.add(Dropout(0.3)) # \u8f93\u51fa: (samples, story_maxlen, embedding_dim) # \u5c06\u8f93\u5165\u7f16\u7801\u4e3a\u7684\u5411\u91cf\u7684\u5e8f\u5217\uff08\u5411\u91cf\u5c3a\u5bf8\u4e3a query_maxlen\uff09 input_encoder_c = Sequential() input_encoder_c.add(Embedding(input_dim=vocab_size, output_dim=query_maxlen)) input_encoder_c.add(Dropout(0.3)) # \u8f93\u51fa: (samples, story_maxlen, query_maxlen) # \u5c06\u95ee\u9898\u7f16\u7801\u4e3a\u5411\u91cf\u7684\u5e8f\u5217 question_encoder = Sequential() question_encoder.add(Embedding(input_dim=vocab_size, output_dim=64, input_length=query_maxlen)) question_encoder.add(Dropout(0.3)) # \u8f93\u51fa: (samples, query_maxlen, embedding_dim) # \u7f16\u7801\u8f93\u5165\u5e8f\u5217\u548c\u95ee\u9898\uff08\u5747\u5df2\u7d22\u5f15\u5316\uff09\u4e3a\u5bc6\u96c6\u5411\u91cf\u7684\u5e8f\u5217 input_encoded_m = input_encoder_m(input_sequence) input_encoded_c = input_encoder_c(input_sequence) question_encoded = question_encoder(question) # \u8ba1\u7b97\u7b2c\u4e00\u4e2a\u8f93\u5165\u5411\u91cf\u548c\u95ee\u9898\u5411\u91cf\u5e8f\u5217\u7684\u300e\u5339\u914d\u300f\uff08'match'\uff09 # \u5c3a\u5bf8: `(samples, story_maxlen, query_maxlen)` match = dot([input_encoded_m, question_encoded], axes=(2, 2)) match = Activation('softmax')(match) # \u5c06\u5339\u914d\u77e9\u9635\u4e0e\u7b2c\u4e8c\u4e2a\u8f93\u5165\u5411\u91cf\u5e8f\u5217\u76f8\u52a0 response = add([match, input_encoded_c]) # (samples, story_maxlen, query_maxlen) response = Permute((2, 1))(response) # (samples, query_maxlen, story_maxlen) # \u62fc\u63a5\u5339\u914d\u77e9\u9635\u548c\u95ee\u9898\u5411\u91cf\u5e8f\u5217 answer = concatenate([response, question_encoded]) # \u539f\u59cb\u8bba\u6587\u4f7f\u7528\u4e00\u4e2a\u77e9\u9635\u4e58\u6cd5\u6765\u8fdb\u884c\u5f52\u7ea6\u64cd\u4f5c\u3002 # \u6211\u4eec\u5728\u6b64\u9009\u62e9\u4f7f\u7528 RNN\u3002 answer = LSTM(32)(answer) # (samples, 32) # \u4e00\u4e2a\u6b63\u5219\u5316\u5c42 - \u53ef\u80fd\u8fd8\u9700\u8981\u66f4\u591a\u5c42 answer = Dropout(0.3)(answer) answer = Dense(vocab_size)(answer) # (samples, vocab_size) # \u8f93\u51fa\u8bcd\u6c47\u8868\u7684\u4e00\u4e2a\u6982\u7387\u5206\u5e03 answer = Activation('softmax')(answer) # \u6784\u5efa\u6700\u7ec8\u6a21\u578b model = Model([input_sequence, question], answer) model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy']) # \u8bad\u7ec3 model.fit([inputs_train, queries_train], answers_train, batch_size=32, epochs=120, validation_data=([inputs_test, queries_test], answers_test))","title":"\u5728 bAbI \u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e00\u4e2a\u8bb0\u5fc6\u7f51\u7edc\u3002"},{"location":"4-Examples/babi_rnn/","text":"\u57fa\u4e8e\u6545\u4e8b\u548c\u95ee\u9898\u8bad\u7ec3\u4e24\u4e2a\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u3002 \u4e24\u8005\u7684\u5408\u5e76\u5411\u91cf\u5c06\u7528\u4e8e\u56de\u7b54\u4e00\u7cfb\u5217 bAbI \u4efb\u52a1\u3002 \u8fd9\u4e9b\u7ed3\u679c\u4e0e Weston \u7b49\u4eba\u63d0\u4f9b\u7684 LSTM \u6a21\u578b\u7684\u7ed3\u679c\u76f8\u5f53\uff1a Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks \u3002 Task Number FB LSTM Baseline Keras QA QA1 - Single Supporting Fact 50 52.1 QA2 - Two Supporting Facts 20 37.0 QA3 - Three Supporting Facts 20 20.5 QA4 - Two Arg. Relations 61 62.9 QA5 - Three Arg. Relations 70 61.9 QA6 - yes/No Questions 48 50.7 QA7 - Counting 49 78.9 QA8 - Lists/Sets 45 77.2 QA9 - Simple Negation 64 64.0 QA10 - Indefinite Knowledge 44 47.7 QA11 - Basic Coreference 72 74.9 QA12 - Conjunction 74 76.4 QA13 - Compound Coreference 94 94.4 QA14 - Time Reasoning 27 34.8 QA15 - Basic Deduction 21 32.4 QA16 - Basic Induction 23 50.6 QA17 - Positional Reasoning 51 49.1 QA18 - Size Reasoning 52 90.8 QA19 - Path Finding 8 9.0 QA20 - Agent's Motivations 91 90.7 \u6709\u5173 bAbI \u9879\u76ee\u7684\u76f8\u5173\u8d44\u6e90\uff0c\u8bf7\u53c2\u8003: https://research.facebook.com/researchers/1543934539189348 \u6ce8\u610f \u4f7f\u7528\u9ed8\u8ba4\u7684\u5355\u8bcd\u3001\u53e5\u5b50\u548c\u67e5\u8be2\u5411\u91cf\u5c3a\u5bf8\uff0cGRU \u6a21\u578b\u5f97\u5230\u4e86\u4ee5\u4e0b\u6548\u679c\uff1a 20 \u8f6e\u8fed\u4ee3\u540e\uff0c\u5728 QA1 \u4e0a\u8fbe\u5230\u4e86 52.1% \u7684\u6d4b\u8bd5\u51c6\u786e\u7387\uff08\u5728 CPU \u4e0a\u6bcf\u8f6e\u8fed\u4ee3 2 \u79d2\uff09\uff1b 20 \u8f6e\u8fed\u4ee3\u540e\uff0c\u5728 QA2 \u4e0a\u8fbe\u5230\u4e86 37.0% \u7684\u6d4b\u8bd5\u51c6\u786e\u7387\uff08\u5728 CPU \u4e0a\u6bcf\u8f6e\u8fed\u4ee3 16 \u79d2\uff09\u3002 \u76f8\u6bd4\u4e4b\u4e0b\uff0cFacebook\u7684\u8bba\u6587\u4e2d LSTM baseline \u7684\u51c6\u786e\u7387\u5206\u522b\u662f 50% \u548c 20%\u3002 \u8fd9\u4e2a\u4efb\u52a1\u5e76\u4e0d\u662f\u7b3c\u7edf\u5730\u5355\u72ec\u53bb\u89e3\u6790\u95ee\u9898\u3002\u8fd9\u5e94\u8be5\u53ef\u4ee5\u63d0\u9ad8\u51c6\u786e\u7387\uff0c\u4e14\u662f\u5408\u5e76\u4e24\u4e2a RNN \u7684\u4e00\u6b21\u8f83\u597d\u5b9e\u8df5\u3002 \u6545\u4e8b\u548c\u95ee\u9898\u7684 RNN \u4e4b\u95f4\u4e0d\u5171\u4eab\u8bcd\u5411\u91cf\uff08\u8bcd\u5d4c\u5165\uff09\u3002 \u6ce8\u610f\u89c2\u5bdf 1000 \u4e2a\u8bad\u7ec3\u6837\u672c\uff08en-10k\uff09\u5230 10,000 \u4e2a\u7684\u51c6\u786e\u5ea6\u5982\u4f55\u53d8\u5316\u3002\u4f7f\u7528 1000 \u662f\u4e3a\u4e86\u4e0e\u539f\u59cb\u8bba\u6587\u8fdb\u884c\u5bf9\u6bd4\u3002 \u5c1d\u8bd5\u4f7f\u7528 GRU, LSTM \u548c JZS1-3\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f1a\u4ea7\u751f\u5fae\u5999\u7684\u4e0d\u540c\u7ed3\u679c\u3002 \u957f\u5ea6\u548c\u566a\u58f0\uff08\u5373\u300c\u65e0\u7528\u300d\u7684\u6545\u4e8b\u5185\u5bb9\uff09\u4f1a\u5f71\u54cd LSTM/GRU \u63d0\u4f9b\u6b63\u786e\u7b54\u6848\u7684\u80fd\u529b\u3002\u5728\u53ea\u63d0\u4f9b\u4e8b\u5b9e\u7684\u60c5\u51b5\u4e0b\uff0c\u8fd9\u4e9b RNN\u53ef\u4ee5\u5728\u8bb8\u591a\u4efb\u52a1\u4e0a\u8fbe\u5230 100% \u7684\u51c6\u786e\u6027\u3002 \u4f7f\u7528\u6ce8\u610f\u529b\u8fc7\u7a0b\u7684\u8bb0\u5fc6\u7f51\u7edc\u548c\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u6709\u6548\u5730\u641c\u7d22\u8fd9\u4e9b\u566a\u58f0\u4ee5\u627e\u5230\u76f8\u5173\u7684\u8bed\u53e5\uff0c\u4ece\u800c\u5927\u5927\u63d0\u9ad8\u6027\u80fd\u3002\u8fd9\u5728 QA2 \u548c QA3 \u4e0a\u53d8\u5f97\u5c24\u4e3a\u660e\u663e\uff0c\u4e24\u8005\u90fd\u8fdc\u8fdc\u663e\u8457\u4e8e QA1\u3002 from __future__ import print_function from functools import reduce import re import tarfile import numpy as np from keras.utils.data_utils import get_file from keras.layers.embeddings import Embedding from keras import layers from keras.layers import recurrent from keras.models import Model from keras.preprocessing.sequence import pad_sequences def tokenize(sent): '''\u8fd4\u56de\u5305\u542b\u6807\u70b9\u7b26\u53f7\u7684\u53e5\u5b50\u7684\u6807\u8bb0\u3002 >>> tokenize('Bob dropped the apple. Where is the apple?') ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?'] ''' return [x.strip() for x in re.split(r'(\\W+)?', sent) if x.strip()] def parse_stories(lines, only_supporting=False): '''\u89e3\u6790 bAbi \u4efb\u52a1\u683c\u5f0f\u4e2d\u63d0\u4f9b\u7684\u6545\u4e8b \u5982\u679c only_supporting \u4e3a true\uff0c \u5219\u53ea\u4fdd\u7559\u652f\u6301\u7b54\u6848\u7684\u53e5\u5b50\u3002 ''' data = [] story = [] for line in lines: line = line.decode('utf-8').strip() nid, line = line.split(' ', 1) nid = int(nid) if nid == 1: story = [] if '\\t' in line: q, a, supporting = line.split('\\t') q = tokenize(q) if only_supporting: # \u53ea\u9009\u62e9\u76f8\u5173\u7684\u5b50\u6545\u4e8b supporting = map(int, supporting.split()) substory = [story[i - 1] for i in supporting] else: # \u63d0\u4f9b\u6240\u6709\u5b50\u6545\u4e8b substory = [x for x in story if x] data.append((substory, q, a)) story.append('') else: sent = tokenize(line) story.append(sent) return data def get_stories(f, only_supporting=False, max_length=None): '''\u7ed9\u5b9a\u6587\u4ef6\u540d\uff0c\u8bfb\u53d6\u6587\u4ef6\uff0c\u68c0\u7d22\u6545\u4e8b\uff0c \u7136\u540e\u5c06\u53e5\u5b50\u8f6c\u6362\u4e3a\u4e00\u4e2a\u72ec\u7acb\u6545\u4e8b\u3002 \u5982\u679c\u63d0\u4f9b\u4e86 max_length, \u4efb\u4f55\u957f\u4e8e max_length \u7684\u6545\u4e8b\u90fd\u5c06\u88ab\u4e22\u5f03\u3002 ''' data = parse_stories(f.readlines(), only_supporting=only_supporting) flatten = lambda data: reduce(lambda x, y: x + y, data) data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length] return data def vectorize_stories(data, word_idx, story_maxlen, query_maxlen): xs = [] xqs = [] ys = [] for story, query, answer in data: x = [word_idx[w] for w in story] xq = [word_idx[w] for w in query] # let's not forget that index 0 is reserved y = np.zeros(len(word_idx) + 1) y[word_idx[answer]] = 1 xs.append(x) xqs.append(xq) ys.append(y) return (pad_sequences(xs, maxlen=story_maxlen), pad_sequences(xqs, maxlen=query_maxlen), np.array(ys)) RNN = recurrent.LSTM EMBED_HIDDEN_SIZE = 50 SENT_HIDDEN_SIZE = 100 QUERY_HIDDEN_SIZE = 100 BATCH_SIZE = 32 EPOCHS = 20 print('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN, EMBED_HIDDEN_SIZE, SENT_HIDDEN_SIZE, QUERY_HIDDEN_SIZE)) try: path = get_file('babi-tasks-v1-2.tar.gz', origin='https://s3.amazonaws.com/text-datasets/' 'babi_tasks_1-20_v1-2.tar.gz') except: print('Error downloading dataset, please download it manually:\\n' '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2' '.tar.gz\\n' '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz') raise # \u9ed8\u8ba4 QA1 \u4efb\u52a1\uff0c1000 \u6837\u672c # challenge = 'tasks_1-20_v1-2/en/qa1_single-supporting-fact_{}.txt' # QA1 \u4efb\u52a1\uff0c10,000 \u6837\u672c # challenge = 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt' # QA2 \u4efb\u52a1\uff0c1000 \u6837\u672c challenge = 'tasks_1-20_v1-2/en/qa2_two-supporting-facts_{}.txt' # QA2 \u4efb\u52a1\uff0c10,000 \u6837\u672c # challenge = 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt' with tarfile.open(path) as tar: train = get_stories(tar.extractfile(challenge.format('train'))) test = get_stories(tar.extractfile(challenge.format('test'))) vocab = set() for story, q, answer in train + test: vocab |= set(story + q + [answer]) vocab = sorted(vocab) # \u4fdd\u7559 0 \u4ee5\u7559\u4f5c pad_sequences \u8fdb\u884c masking vocab_size = len(vocab) + 1 word_idx = dict((c, i + 1) for i, c in enumerate(vocab)) story_maxlen = max(map(len, (x for x, _, _ in train + test))) query_maxlen = max(map(len, (x for _, x, _ in train + test))) x, xq, y = vectorize_stories(train, word_idx, story_maxlen, query_maxlen) tx, txq, ty = vectorize_stories(test, word_idx, story_maxlen, query_maxlen) print('vocab = {}'.format(vocab)) print('x.shape = {}'.format(x.shape)) print('xq.shape = {}'.format(xq.shape)) print('y.shape = {}'.format(y.shape)) print('story_maxlen, query_maxlen = {}, {}'.format(story_maxlen, query_maxlen)) print('Build model...') sentence = layers.Input(shape=(story_maxlen,), dtype='int32') encoded_sentence = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(sentence) encoded_sentence = RNN(SENT_HIDDEN_SIZE)(encoded_sentence) question = layers.Input(shape=(query_maxlen,), dtype='int32') encoded_question = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(question) encoded_question = RNN(QUERY_HIDDEN_SIZE)(encoded_question) merged = layers.concatenate([encoded_sentence, encoded_question]) preds = layers.Dense(vocab_size, activation='softmax')(merged) model = Model([sentence, question], preds) model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) print('Training') model.fit([x, xq], y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.05) print('Evaluation') loss, acc = model.evaluate([tx, txq], ty, batch_size=BATCH_SIZE) print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))","title":"\u57fa\u4e8e\u6545\u4e8b\u548c\u95ee\u9898\u8bad\u7ec3\u4e24\u4e2a\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u3002"},{"location":"4-Examples/babi_rnn/#_1","text":"\u4e24\u8005\u7684\u5408\u5e76\u5411\u91cf\u5c06\u7528\u4e8e\u56de\u7b54\u4e00\u7cfb\u5217 bAbI \u4efb\u52a1\u3002 \u8fd9\u4e9b\u7ed3\u679c\u4e0e Weston \u7b49\u4eba\u63d0\u4f9b\u7684 LSTM \u6a21\u578b\u7684\u7ed3\u679c\u76f8\u5f53\uff1a Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks \u3002 Task Number FB LSTM Baseline Keras QA QA1 - Single Supporting Fact 50 52.1 QA2 - Two Supporting Facts 20 37.0 QA3 - Three Supporting Facts 20 20.5 QA4 - Two Arg. Relations 61 62.9 QA5 - Three Arg. Relations 70 61.9 QA6 - yes/No Questions 48 50.7 QA7 - Counting 49 78.9 QA8 - Lists/Sets 45 77.2 QA9 - Simple Negation 64 64.0 QA10 - Indefinite Knowledge 44 47.7 QA11 - Basic Coreference 72 74.9 QA12 - Conjunction 74 76.4 QA13 - Compound Coreference 94 94.4 QA14 - Time Reasoning 27 34.8 QA15 - Basic Deduction 21 32.4 QA16 - Basic Induction 23 50.6 QA17 - Positional Reasoning 51 49.1 QA18 - Size Reasoning 52 90.8 QA19 - Path Finding 8 9.0 QA20 - Agent's Motivations 91 90.7 \u6709\u5173 bAbI \u9879\u76ee\u7684\u76f8\u5173\u8d44\u6e90\uff0c\u8bf7\u53c2\u8003: https://research.facebook.com/researchers/1543934539189348","title":"\u57fa\u4e8e\u6545\u4e8b\u548c\u95ee\u9898\u8bad\u7ec3\u4e24\u4e2a\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u3002"},{"location":"4-Examples/babi_rnn/#_2","text":"\u4f7f\u7528\u9ed8\u8ba4\u7684\u5355\u8bcd\u3001\u53e5\u5b50\u548c\u67e5\u8be2\u5411\u91cf\u5c3a\u5bf8\uff0cGRU \u6a21\u578b\u5f97\u5230\u4e86\u4ee5\u4e0b\u6548\u679c\uff1a 20 \u8f6e\u8fed\u4ee3\u540e\uff0c\u5728 QA1 \u4e0a\u8fbe\u5230\u4e86 52.1% \u7684\u6d4b\u8bd5\u51c6\u786e\u7387\uff08\u5728 CPU \u4e0a\u6bcf\u8f6e\u8fed\u4ee3 2 \u79d2\uff09\uff1b 20 \u8f6e\u8fed\u4ee3\u540e\uff0c\u5728 QA2 \u4e0a\u8fbe\u5230\u4e86 37.0% \u7684\u6d4b\u8bd5\u51c6\u786e\u7387\uff08\u5728 CPU \u4e0a\u6bcf\u8f6e\u8fed\u4ee3 16 \u79d2\uff09\u3002 \u76f8\u6bd4\u4e4b\u4e0b\uff0cFacebook\u7684\u8bba\u6587\u4e2d LSTM baseline \u7684\u51c6\u786e\u7387\u5206\u522b\u662f 50% \u548c 20%\u3002 \u8fd9\u4e2a\u4efb\u52a1\u5e76\u4e0d\u662f\u7b3c\u7edf\u5730\u5355\u72ec\u53bb\u89e3\u6790\u95ee\u9898\u3002\u8fd9\u5e94\u8be5\u53ef\u4ee5\u63d0\u9ad8\u51c6\u786e\u7387\uff0c\u4e14\u662f\u5408\u5e76\u4e24\u4e2a RNN \u7684\u4e00\u6b21\u8f83\u597d\u5b9e\u8df5\u3002 \u6545\u4e8b\u548c\u95ee\u9898\u7684 RNN \u4e4b\u95f4\u4e0d\u5171\u4eab\u8bcd\u5411\u91cf\uff08\u8bcd\u5d4c\u5165\uff09\u3002 \u6ce8\u610f\u89c2\u5bdf 1000 \u4e2a\u8bad\u7ec3\u6837\u672c\uff08en-10k\uff09\u5230 10,000 \u4e2a\u7684\u51c6\u786e\u5ea6\u5982\u4f55\u53d8\u5316\u3002\u4f7f\u7528 1000 \u662f\u4e3a\u4e86\u4e0e\u539f\u59cb\u8bba\u6587\u8fdb\u884c\u5bf9\u6bd4\u3002 \u5c1d\u8bd5\u4f7f\u7528 GRU, LSTM \u548c JZS1-3\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f1a\u4ea7\u751f\u5fae\u5999\u7684\u4e0d\u540c\u7ed3\u679c\u3002 \u957f\u5ea6\u548c\u566a\u58f0\uff08\u5373\u300c\u65e0\u7528\u300d\u7684\u6545\u4e8b\u5185\u5bb9\uff09\u4f1a\u5f71\u54cd LSTM/GRU \u63d0\u4f9b\u6b63\u786e\u7b54\u6848\u7684\u80fd\u529b\u3002\u5728\u53ea\u63d0\u4f9b\u4e8b\u5b9e\u7684\u60c5\u51b5\u4e0b\uff0c\u8fd9\u4e9b RNN\u53ef\u4ee5\u5728\u8bb8\u591a\u4efb\u52a1\u4e0a\u8fbe\u5230 100% \u7684\u51c6\u786e\u6027\u3002 \u4f7f\u7528\u6ce8\u610f\u529b\u8fc7\u7a0b\u7684\u8bb0\u5fc6\u7f51\u7edc\u548c\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u6709\u6548\u5730\u641c\u7d22\u8fd9\u4e9b\u566a\u58f0\u4ee5\u627e\u5230\u76f8\u5173\u7684\u8bed\u53e5\uff0c\u4ece\u800c\u5927\u5927\u63d0\u9ad8\u6027\u80fd\u3002\u8fd9\u5728 QA2 \u548c QA3 \u4e0a\u53d8\u5f97\u5c24\u4e3a\u660e\u663e\uff0c\u4e24\u8005\u90fd\u8fdc\u8fdc\u663e\u8457\u4e8e QA1\u3002 from __future__ import print_function from functools import reduce import re import tarfile import numpy as np from keras.utils.data_utils import get_file from keras.layers.embeddings import Embedding from keras import layers from keras.layers import recurrent from keras.models import Model from keras.preprocessing.sequence import pad_sequences def tokenize(sent): '''\u8fd4\u56de\u5305\u542b\u6807\u70b9\u7b26\u53f7\u7684\u53e5\u5b50\u7684\u6807\u8bb0\u3002 >>> tokenize('Bob dropped the apple. Where is the apple?') ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?'] ''' return [x.strip() for x in re.split(r'(\\W+)?', sent) if x.strip()] def parse_stories(lines, only_supporting=False): '''\u89e3\u6790 bAbi \u4efb\u52a1\u683c\u5f0f\u4e2d\u63d0\u4f9b\u7684\u6545\u4e8b \u5982\u679c only_supporting \u4e3a true\uff0c \u5219\u53ea\u4fdd\u7559\u652f\u6301\u7b54\u6848\u7684\u53e5\u5b50\u3002 ''' data = [] story = [] for line in lines: line = line.decode('utf-8').strip() nid, line = line.split(' ', 1) nid = int(nid) if nid == 1: story = [] if '\\t' in line: q, a, supporting = line.split('\\t') q = tokenize(q) if only_supporting: # \u53ea\u9009\u62e9\u76f8\u5173\u7684\u5b50\u6545\u4e8b supporting = map(int, supporting.split()) substory = [story[i - 1] for i in supporting] else: # \u63d0\u4f9b\u6240\u6709\u5b50\u6545\u4e8b substory = [x for x in story if x] data.append((substory, q, a)) story.append('') else: sent = tokenize(line) story.append(sent) return data def get_stories(f, only_supporting=False, max_length=None): '''\u7ed9\u5b9a\u6587\u4ef6\u540d\uff0c\u8bfb\u53d6\u6587\u4ef6\uff0c\u68c0\u7d22\u6545\u4e8b\uff0c \u7136\u540e\u5c06\u53e5\u5b50\u8f6c\u6362\u4e3a\u4e00\u4e2a\u72ec\u7acb\u6545\u4e8b\u3002 \u5982\u679c\u63d0\u4f9b\u4e86 max_length, \u4efb\u4f55\u957f\u4e8e max_length \u7684\u6545\u4e8b\u90fd\u5c06\u88ab\u4e22\u5f03\u3002 ''' data = parse_stories(f.readlines(), only_supporting=only_supporting) flatten = lambda data: reduce(lambda x, y: x + y, data) data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length] return data def vectorize_stories(data, word_idx, story_maxlen, query_maxlen): xs = [] xqs = [] ys = [] for story, query, answer in data: x = [word_idx[w] for w in story] xq = [word_idx[w] for w in query] # let's not forget that index 0 is reserved y = np.zeros(len(word_idx) + 1) y[word_idx[answer]] = 1 xs.append(x) xqs.append(xq) ys.append(y) return (pad_sequences(xs, maxlen=story_maxlen), pad_sequences(xqs, maxlen=query_maxlen), np.array(ys)) RNN = recurrent.LSTM EMBED_HIDDEN_SIZE = 50 SENT_HIDDEN_SIZE = 100 QUERY_HIDDEN_SIZE = 100 BATCH_SIZE = 32 EPOCHS = 20 print('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN, EMBED_HIDDEN_SIZE, SENT_HIDDEN_SIZE, QUERY_HIDDEN_SIZE)) try: path = get_file('babi-tasks-v1-2.tar.gz', origin='https://s3.amazonaws.com/text-datasets/' 'babi_tasks_1-20_v1-2.tar.gz') except: print('Error downloading dataset, please download it manually:\\n' '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2' '.tar.gz\\n' '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz') raise # \u9ed8\u8ba4 QA1 \u4efb\u52a1\uff0c1000 \u6837\u672c # challenge = 'tasks_1-20_v1-2/en/qa1_single-supporting-fact_{}.txt' # QA1 \u4efb\u52a1\uff0c10,000 \u6837\u672c # challenge = 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt' # QA2 \u4efb\u52a1\uff0c1000 \u6837\u672c challenge = 'tasks_1-20_v1-2/en/qa2_two-supporting-facts_{}.txt' # QA2 \u4efb\u52a1\uff0c10,000 \u6837\u672c # challenge = 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt' with tarfile.open(path) as tar: train = get_stories(tar.extractfile(challenge.format('train'))) test = get_stories(tar.extractfile(challenge.format('test'))) vocab = set() for story, q, answer in train + test: vocab |= set(story + q + [answer]) vocab = sorted(vocab) # \u4fdd\u7559 0 \u4ee5\u7559\u4f5c pad_sequences \u8fdb\u884c masking vocab_size = len(vocab) + 1 word_idx = dict((c, i + 1) for i, c in enumerate(vocab)) story_maxlen = max(map(len, (x for x, _, _ in train + test))) query_maxlen = max(map(len, (x for _, x, _ in train + test))) x, xq, y = vectorize_stories(train, word_idx, story_maxlen, query_maxlen) tx, txq, ty = vectorize_stories(test, word_idx, story_maxlen, query_maxlen) print('vocab = {}'.format(vocab)) print('x.shape = {}'.format(x.shape)) print('xq.shape = {}'.format(xq.shape)) print('y.shape = {}'.format(y.shape)) print('story_maxlen, query_maxlen = {}, {}'.format(story_maxlen, query_maxlen)) print('Build model...') sentence = layers.Input(shape=(story_maxlen,), dtype='int32') encoded_sentence = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(sentence) encoded_sentence = RNN(SENT_HIDDEN_SIZE)(encoded_sentence) question = layers.Input(shape=(query_maxlen,), dtype='int32') encoded_question = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(question) encoded_question = RNN(QUERY_HIDDEN_SIZE)(encoded_question) merged = layers.concatenate([encoded_sentence, encoded_question]) preds = layers.Dense(vocab_size, activation='softmax')(merged) model = Model([sentence, question], preds) model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) print('Training') model.fit([x, xq], y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.05) print('Evaluation') loss, acc = model.evaluate([tx, txq], ty, batch_size=BATCH_SIZE) print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))","title":"\u6ce8\u610f"},{"location":"4-Examples/cifar10_cnn/","text":"\u5728 CIFAR10 \u5c0f\u578b\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e00\u4e2a\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3002 \u5728 25 \u8f6e\u8fed\u4ee3\u540e \u9a8c\u8bc1\u96c6\u51c6\u786e\u7387\u8fbe\u5230 75%\uff0c\u5728 50 \u8f6e\u540e\u8fbe\u5230 79%\u3002 (\u5c3d\u7ba1\u76ee\u524d\u4ecd\u7136\u6b20\u62df\u5408)\u3002 from __future__ import print_function import keras from keras.datasets import cifar10 from keras.preprocessing.image import ImageDataGenerator from keras.models import Sequential from keras.layers import Dense, Dropout, Activation, Flatten from keras.layers import Conv2D, MaxPooling2D import os batch_size = 32 num_classes = 10 epochs = 100 data_augmentation = True num_predictions = 20 save_dir = os.path.join(os.getcwd(), 'saved_models') model_name = 'keras_cifar10_trained_model.h5' # \u6570\u636e\uff0c\u5207\u5206\u4e3a\u8bad\u7ec3\u548c\u6d4b\u8bd5\u96c6\u3002 (x_train, y_train), (x_test, y_test) = cifar10.load_data() print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') # \u5c06\u7c7b\u5411\u91cf\u8f6c\u6362\u4e3a\u4e8c\u8fdb\u5236\u7c7b\u77e9\u9635\u3002 y_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes) model = Sequential() model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:])) model.add(Activation('relu')) model.add(Conv2D(32, (3, 3))) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Conv2D(64, (3, 3), padding='same')) model.add(Activation('relu')) model.add(Conv2D(64, (3, 3))) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Flatten()) model.add(Dense(512)) model.add(Activation('relu')) model.add(Dropout(0.5)) model.add(Dense(num_classes)) model.add(Activation('softmax')) # \u521d\u59cb\u5316 RMSprop \u4f18\u5316\u5668\u3002 opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6) # \u5229\u7528 RMSprop \u6765\u8bad\u7ec3\u6a21\u578b\u3002 model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 if not data_augmentation: print('Not using data augmentation.') model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), shuffle=True) else: print('Using real-time data augmentation.') # \u8fd9\u4e00\u6b65\u5c06\u8fdb\u884c\u6570\u636e\u5904\u7406\u548c\u5b9e\u65f6\u6570\u636e\u589e\u76ca\u3002data augmentation: datagen = ImageDataGenerator( featurewise_center=False, # \u5c06\u6574\u4e2a\u6570\u636e\u96c6\u7684\u5747\u503c\u8bbe\u4e3a0 samplewise_center=False, # \u5c06\u6bcf\u4e2a\u6837\u672c\u7684\u5747\u503c\u8bbe\u4e3a0 featurewise_std_normalization=False, # \u5c06\u8f93\u5165\u9664\u4ee5\u6574\u4e2a\u6570\u636e\u96c6\u7684\u6807\u51c6\u5dee samplewise_std_normalization=False, # \u5c06\u8f93\u5165\u9664\u4ee5\u5176\u6807\u51c6\u5dee zca_whitening=False, # \u8fd0\u7528 ZCA \u767d\u5316 zca_epsilon=1e-06, # ZCA \u767d\u5316\u7684 epsilon\u503c rotation_range=0, # \u968f\u673a\u65cb\u8f6c\u56fe\u50cf\u8303\u56f4 (\u89d2\u5ea6, 0 to 180) # \u968f\u673a\u6c34\u5e73\u79fb\u52a8\u56fe\u50cf (\u603b\u5bbd\u5ea6\u7684\u767e\u5206\u6bd4) width_shift_range=0.1, # \u968f\u673a\u5782\u76f4\u79fb\u52a8\u56fe\u50cf (\u603b\u9ad8\u5ea6\u7684\u767e\u5206\u6bd4) height_shift_range=0.1, shear_range=0., # \u8bbe\u7f6e\u968f\u673a\u88c1\u526a\u8303\u56f4 zoom_range=0., # \u8bbe\u7f6e\u968f\u673a\u653e\u5927\u8303\u56f4 channel_shift_range=0., # \u8bbe\u7f6e\u968f\u673a\u901a\u9053\u5207\u6362\u7684\u8303\u56f4 # \u8bbe\u7f6e\u586b\u5145\u8f93\u5165\u8fb9\u754c\u4e4b\u5916\u7684\u70b9\u7684\u6a21\u5f0f fill_mode='nearest', cval=0., # \u5728 fill_mode = \"constant\" \u65f6\u4f7f\u7528\u7684\u503c horizontal_flip=True, # \u968f\u673a\u6c34\u5e73\u7ffb\u8f6c\u56fe\u50cf vertical_flip=False, # \u968f\u673a\u5782\u76f4\u7ffb\u8f6c\u56fe\u50cf # \u8bbe\u7f6e\u7f29\u653e\u56e0\u5b50 (\u5728\u5176\u4ed6\u8f6c\u6362\u4e4b\u524d\u4f7f\u7528) rescale=None, # \u8bbe\u7f6e\u5c06\u5e94\u7528\u4e8e\u6bcf\u4e00\u4e2a\u8f93\u5165\u7684\u51fd\u6570 preprocessing_function=None, # \u56fe\u50cf\u6570\u636e\u683c\u5f0f\uff0c\"channels_first\" \u6216 \"channels_last\" \u4e4b\u4e00 data_format=None, # \u4fdd\u7559\u7528\u4e8e\u9a8c\u8bc1\u7684\u56fe\u50cf\u6bd4\u4f8b\uff08\u4e25\u683c\u57280\u548c1\u4e4b\u95f4\uff09 validation_split=0.0) # \u8ba1\u7b97\u7279\u5f81\u6807\u51c6\u5316\u6240\u9700\u7684\u8ba1\u7b97\u91cf # (\u5982\u679c\u5e94\u7528 ZCA \u767d\u5316\uff0c\u5219\u4e3a std\uff0cmean\u548c\u4e3b\u6210\u5206). datagen.fit(x_train) # \u5229\u7528\u7531 datagen.flow() \u751f\u6210\u7684\u6279\u6765\u8bad\u7ec3\u6a21\u578b model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size), epochs=epochs, validation_data=(x_test, y_test), workers=4) # \u4fdd\u5b58\u6a21\u578b\u548c\u6743\u91cd if not os.path.isdir(save_dir): os.makedirs(save_dir) model_path = os.path.join(save_dir, model_name) model.save(model_path) print('Saved trained model at %s ' % model_path) # \u8bc4\u4f30\u8bad\u7ec3\u6a21\u578b scores = model.evaluate(x_test, y_test, verbose=1) print('Test loss:', scores[0]) print('Test accuracy:', scores[1])","title":"\u5728 CIFAR10 \u5c0f\u578b\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e00\u4e2a\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3002"},{"location":"4-Examples/cifar10_cnn/#cifar10","text":"\u5728 25 \u8f6e\u8fed\u4ee3\u540e \u9a8c\u8bc1\u96c6\u51c6\u786e\u7387\u8fbe\u5230 75%\uff0c\u5728 50 \u8f6e\u540e\u8fbe\u5230 79%\u3002 (\u5c3d\u7ba1\u76ee\u524d\u4ecd\u7136\u6b20\u62df\u5408)\u3002 from __future__ import print_function import keras from keras.datasets import cifar10 from keras.preprocessing.image import ImageDataGenerator from keras.models import Sequential from keras.layers import Dense, Dropout, Activation, Flatten from keras.layers import Conv2D, MaxPooling2D import os batch_size = 32 num_classes = 10 epochs = 100 data_augmentation = True num_predictions = 20 save_dir = os.path.join(os.getcwd(), 'saved_models') model_name = 'keras_cifar10_trained_model.h5' # \u6570\u636e\uff0c\u5207\u5206\u4e3a\u8bad\u7ec3\u548c\u6d4b\u8bd5\u96c6\u3002 (x_train, y_train), (x_test, y_test) = cifar10.load_data() print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') # \u5c06\u7c7b\u5411\u91cf\u8f6c\u6362\u4e3a\u4e8c\u8fdb\u5236\u7c7b\u77e9\u9635\u3002 y_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes) model = Sequential() model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:])) model.add(Activation('relu')) model.add(Conv2D(32, (3, 3))) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Conv2D(64, (3, 3), padding='same')) model.add(Activation('relu')) model.add(Conv2D(64, (3, 3))) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Flatten()) model.add(Dense(512)) model.add(Activation('relu')) model.add(Dropout(0.5)) model.add(Dense(num_classes)) model.add(Activation('softmax')) # \u521d\u59cb\u5316 RMSprop \u4f18\u5316\u5668\u3002 opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6) # \u5229\u7528 RMSprop \u6765\u8bad\u7ec3\u6a21\u578b\u3002 model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 if not data_augmentation: print('Not using data augmentation.') model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), shuffle=True) else: print('Using real-time data augmentation.') # \u8fd9\u4e00\u6b65\u5c06\u8fdb\u884c\u6570\u636e\u5904\u7406\u548c\u5b9e\u65f6\u6570\u636e\u589e\u76ca\u3002data augmentation: datagen = ImageDataGenerator( featurewise_center=False, # \u5c06\u6574\u4e2a\u6570\u636e\u96c6\u7684\u5747\u503c\u8bbe\u4e3a0 samplewise_center=False, # \u5c06\u6bcf\u4e2a\u6837\u672c\u7684\u5747\u503c\u8bbe\u4e3a0 featurewise_std_normalization=False, # \u5c06\u8f93\u5165\u9664\u4ee5\u6574\u4e2a\u6570\u636e\u96c6\u7684\u6807\u51c6\u5dee samplewise_std_normalization=False, # \u5c06\u8f93\u5165\u9664\u4ee5\u5176\u6807\u51c6\u5dee zca_whitening=False, # \u8fd0\u7528 ZCA \u767d\u5316 zca_epsilon=1e-06, # ZCA \u767d\u5316\u7684 epsilon\u503c rotation_range=0, # \u968f\u673a\u65cb\u8f6c\u56fe\u50cf\u8303\u56f4 (\u89d2\u5ea6, 0 to 180) # \u968f\u673a\u6c34\u5e73\u79fb\u52a8\u56fe\u50cf (\u603b\u5bbd\u5ea6\u7684\u767e\u5206\u6bd4) width_shift_range=0.1, # \u968f\u673a\u5782\u76f4\u79fb\u52a8\u56fe\u50cf (\u603b\u9ad8\u5ea6\u7684\u767e\u5206\u6bd4) height_shift_range=0.1, shear_range=0., # \u8bbe\u7f6e\u968f\u673a\u88c1\u526a\u8303\u56f4 zoom_range=0., # \u8bbe\u7f6e\u968f\u673a\u653e\u5927\u8303\u56f4 channel_shift_range=0., # \u8bbe\u7f6e\u968f\u673a\u901a\u9053\u5207\u6362\u7684\u8303\u56f4 # \u8bbe\u7f6e\u586b\u5145\u8f93\u5165\u8fb9\u754c\u4e4b\u5916\u7684\u70b9\u7684\u6a21\u5f0f fill_mode='nearest', cval=0., # \u5728 fill_mode = \"constant\" \u65f6\u4f7f\u7528\u7684\u503c horizontal_flip=True, # \u968f\u673a\u6c34\u5e73\u7ffb\u8f6c\u56fe\u50cf vertical_flip=False, # \u968f\u673a\u5782\u76f4\u7ffb\u8f6c\u56fe\u50cf # \u8bbe\u7f6e\u7f29\u653e\u56e0\u5b50 (\u5728\u5176\u4ed6\u8f6c\u6362\u4e4b\u524d\u4f7f\u7528) rescale=None, # \u8bbe\u7f6e\u5c06\u5e94\u7528\u4e8e\u6bcf\u4e00\u4e2a\u8f93\u5165\u7684\u51fd\u6570 preprocessing_function=None, # \u56fe\u50cf\u6570\u636e\u683c\u5f0f\uff0c\"channels_first\" \u6216 \"channels_last\" \u4e4b\u4e00 data_format=None, # \u4fdd\u7559\u7528\u4e8e\u9a8c\u8bc1\u7684\u56fe\u50cf\u6bd4\u4f8b\uff08\u4e25\u683c\u57280\u548c1\u4e4b\u95f4\uff09 validation_split=0.0) # \u8ba1\u7b97\u7279\u5f81\u6807\u51c6\u5316\u6240\u9700\u7684\u8ba1\u7b97\u91cf # (\u5982\u679c\u5e94\u7528 ZCA \u767d\u5316\uff0c\u5219\u4e3a std\uff0cmean\u548c\u4e3b\u6210\u5206). datagen.fit(x_train) # \u5229\u7528\u7531 datagen.flow() \u751f\u6210\u7684\u6279\u6765\u8bad\u7ec3\u6a21\u578b model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size), epochs=epochs, validation_data=(x_test, y_test), workers=4) # \u4fdd\u5b58\u6a21\u578b\u548c\u6743\u91cd if not os.path.isdir(save_dir): os.makedirs(save_dir) model_path = os.path.join(save_dir, model_name) model.save(model_path) print('Saved trained model at %s ' % model_path) # \u8bc4\u4f30\u8bad\u7ec3\u6a21\u578b scores = model.evaluate(x_test, y_test, verbose=1) print('Test loss:', scores[0]) print('Test accuracy:', scores[1])","title":"\u5728 CIFAR10 \u5c0f\u578b\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e00\u4e2a\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3002"},{"location":"4-Examples/cifar10_cnn_capsule/","text":"\u5728 CIFAR10 \u5c0f\u578b\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e00\u4e2a\u7b80\u5355\u7684 CNN-Capsule Network\u3002 \u65e0\u6570\u636e\u589e\u76ca\u7684\u60c5\u51b5\u4e0b\uff1a \u5728 10 \u8f6e\u8fed\u4ee3\u540e\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387\u8fbe\u5230 75%\uff0c\u5728 15 \u8f6e\u540e\u8fbe\u5230 79%\uff0c\u5728 20 \u8f6e\u540e\u8fc7\u62df\u5408\u3002 \u6709\u6570\u636e\u589e\u76ca\u60c5\u51b5\u4e0b: \u5728 10 \u8f6e\u8fed\u4ee3\u540e\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387\u8fbe\u5230 75%\uff0c\u5728 15 \u8f6e\u540e\u8fbe\u5230 79%\uff0c\u5728 30 \u8f6e\u540e\u8fbe\u5230 83%\u3002 \u5728\u6211\u7684\u6d4b\u8bd5\u4e2d\uff0c50 \u8f6e\u540e\u6700\u9ad8\u7684\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387\u4e3a 83.79%\u3002 \u8fd9\u662f\u4e00\u4e2a\u5feb\u901f\u7248\u7684\u5b9e\u73b0\uff0c\u5728 GTX 1070 GPU \u4e0a\u8fed\u4ee3\u53ea\u9700 20s/epoch\u3002 from __future__ import print_function from keras import backend as K from keras.layers import Layer from keras import activations from keras import utils from keras.datasets import cifar10 from keras.models import Model from keras.layers import * from keras.preprocessing.image import ImageDataGenerator # \u6324\u538b\u51fd\u6570 # \u6211\u4eec\u5728\u6b64\u4f7f\u7528 0.5\uff0c\u800c\u4e0d\u662f Hinton \u8bba\u6587\u4e2d\u7ed9\u51fa\u7684 1 # \u5982\u679c\u4e3a 1\uff0c\u5219\u5411\u91cf\u7684\u8303\u6570\u5c06\u88ab\u7f29\u5c0f\u3002 # \u5982\u679c\u4e3a 0.5\uff0c\u5219\u5f53\u539f\u59cb\u8303\u6570\u5c0f\u4e8e 0.5 \u65f6\uff0c\u8303\u6570\u5c06\u88ab\u653e\u5927\uff0c # \u5f53\u539f\u59cb\u8303\u6570\u5927\u4e8e 0.5 \u65f6\uff0c\u8303\u6570\u5c06\u88ab\u7f29\u5c0f\u3002 def squash(x, axis=-1): s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon() scale = K.sqrt(s_squared_norm) / (0.5 + s_squared_norm) return scale * x # \u4f7f\u7528\u81ea\u5b9a\u4e49\u7684 softmax \u51fd\u6570\uff0c\u800c\u975e K.softmax\uff0c # \u56e0\u4e3a K.softmax \u4e0d\u80fd\u6307\u5b9a\u8f74\u3002 def softmax(x, axis=-1): ex = K.exp(x - K.max(x, axis=axis, keepdims=True)) return ex / K.sum(ex, axis=axis, keepdims=True) # \u5b9a\u4e49 margin loss\uff0c\u7c7b\u4f3c\u4e8e hinge loss def margin_loss(y_true, y_pred): lamb, margin = 0.5, 0.1 return K.sum(y_true * K.square(K.relu(1 - margin - y_pred)) + lamb * ( 1 - y_true) * K.square(K.relu(y_pred - margin)), axis=-1) class Capsule(Layer): \"\"\"\u4e00\u4e2a\u7531\u7eaf Keras \u5b9e\u73b0\u7684 Capsule \u7f51\u7edc\u3002 \u603b\u5171\u6709\u4e24\u4e2a\u7248\u672c\u7684 Capsule\u3002 \u4e00\u4e2a\u7c7b\u4f3c\u4e8e\u5168\u8fde\u63a5\u5c42 (\u7528\u4e8e\u56fa\u5b9a\u5c3a\u5bf8\u7684\u8f93\u5165)\uff0c \u53e6\u4e00\u4e2a\u7c7b\u4f3c\u4e8e\u65f6\u5e8f\u5206\u5e03\u5168\u8fde\u63a5\u5c42 (\u7528\u4e8e\u53d8\u6210\u8f93\u5165)\u3002 Capsure \u7684\u8f93\u5165\u5c3a\u5bf8\u5fc5\u987b\u4e3a (batch_size, input_num_capsule, input_dim_capsule ) \u4ee5\u53ca\u8f93\u51fa\u5c3a\u5bf8\u5fc5\u987b\u4e3a (batch_size, num_capsule, dim_capsule ) Capsule \u5b9e\u73b0\u6765\u81ea\u4e8e https://github.com/bojone/Capsule/ Capsule \u8bba\u6587: https://arxiv.org/abs/1710.09829 \"\"\" def __init__(self, num_capsule, dim_capsule, routings=3, share_weights=True, activation='squash', **kwargs): super(Capsule, self).__init__(**kwargs) self.num_capsule = num_capsule self.dim_capsule = dim_capsule self.routings = routings self.share_weights = share_weights if activation == 'squash': self.activation = squash else: self.activation = activations.get(activation) def build(self, input_shape): input_dim_capsule = input_shape[-1] if self.share_weights: self.kernel = self.add_weight( name='capsule_kernel', shape=(1, input_dim_capsule, self.num_capsule * self.dim_capsule), initializer='glorot_uniform', trainable=True) else: input_num_capsule = input_shape[-2] self.kernel = self.add_weight( name='capsule_kernel', shape=(input_num_capsule, input_dim_capsule, self.num_capsule * self.dim_capsule), initializer='glorot_uniform', trainable=True) def call(self, inputs): \"\"\"\u9075\u5faa Hinton \u8bba\u6587\u4e2d\u7684\u8def\u7531\u7b97\u6cd5\uff0c \u4f46\u662f\u5c06 b = b + <u,v> \u66ff\u6362\u4e3a b = <u,v>\u3002 \u8fd9\u4e00\u6539\u53d8\u5c06\u63d0\u5347 Capsule \u7684\u7279\u5f81\u8868\u793a\u80fd\u529b\u3002 \u7136\u800c\uff0c\u4f60\u4ecd\u53ef\u4ee5\u5c06 b = K.batch_dot(outputs, hat_inputs, [2, 3]) \u66ff\u6362\u4e3a b += K.batch_dot(outputs, hat_inputs, [2, 3]) \u6765\u5b9e\u73b0\u4e00\u4e2a\u6807\u51c6\u7684\u8def\u7531\u3002 \"\"\" if self.share_weights: hat_inputs = K.conv1d(inputs, self.kernel) else: hat_inputs = K.local_conv1d(inputs, self.kernel, [1], [1]) batch_size = K.shape(inputs)[0] input_num_capsule = K.shape(inputs)[1] hat_inputs = K.reshape(hat_inputs, (batch_size, input_num_capsule, self.num_capsule, self.dim_capsule)) hat_inputs = K.permute_dimensions(hat_inputs, (0, 2, 1, 3)) b = K.zeros_like(hat_inputs[:, :, :, 0]) for i in range(self.routings): c = softmax(b, 1) o = self.activation(K.batch_dot(c, hat_inputs, [2, 2])) if i < self.routings - 1: b = K.batch_dot(o, hat_inputs, [2, 3]) if K.backend() == 'theano': o = K.sum(o, axis=1) return o def compute_output_shape(self, input_shape): return (None, self.num_capsule, self.dim_capsule) batch_size = 128 num_classes = 10 epochs = 100 (x_train, y_train), (x_test, y_test) = cifar10.load_data() x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 y_train = utils.to_categorical(y_train, num_classes) y_test = utils.to_categorical(y_test, num_classes) # \u4e00\u4e2a\u5e38\u89c4\u7684 Conv2D \u6a21\u578b input_image = Input(shape=(None, None, 3)) x = Conv2D(64, (3, 3), activation='relu')(input_image) x = Conv2D(64, (3, 3), activation='relu')(x) x = AveragePooling2D((2, 2))(x) x = Conv2D(128, (3, 3), activation='relu')(x) x = Conv2D(128, (3, 3), activation='relu')(x) \"\"\"\u73b0\u5728\u6211\u4eec\u5c06\u5176\u5c3a\u5bf8\u91cd\u65b0\u8c03\u6574\u4e3a (batch_size, input_num_capsule, input_dim_capsule)\uff0c\u518d\u8fde\u63a5\u4e00\u4e2a Capsule \u7f51\u7edc\u3002 \u6700\u7ec8\u6a21\u578b\u7684\u8f93\u51fa\u4e3a\u957f\u5ea6\u4e3a 10 \u7684 Capsure\uff0c\u5176 dim=16\u3002 Capsule \u7684\u957f\u5ea6\u8868\u793a\u4e3a proba\uff0c \u56e0\u6b64\u95ee\u9898\u53d8\u6210\u4e86\u4e00\u4e2a\u300e10\u4e2a\u4e8c\u5206\u7c7b\u300f\u7684\u95ee\u9898\u3002 \"\"\" x = Reshape((-1, 128))(x) capsule = Capsule(10, 16, 3, True)(x) output = Lambda(lambda z: K.sqrt(K.sum(K.square(z), 2)))(capsule) model = Model(inputs=input_image, outputs=output) # \u4f7f\u7528 margin loss model.compile(loss=margin_loss, optimizer='adam', metrics=['accuracy']) model.summary() # \u53ef\u4ee5\u6bd4\u8f83\u6709\u65e0\u6570\u636e\u589e\u76ca\u5bf9\u5e94\u7684\u6027\u80fd data_augmentation = True if not data_augmentation: print('Not using data augmentation.') model.fit( x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), shuffle=True) else: print('Using real-time data augmentation.') # \u8fd9\u4e00\u6b65\u5c06\u8fdb\u884c\u6570\u636e\u5904\u7406\u548c\u5b9e\u65f6\u6570\u636e\u589e\u76ca: datagen = ImageDataGenerator( featurewise_center=False, # \u5c06\u6574\u4e2a\u6570\u636e\u96c6\u7684\u5747\u503c\u8bbe\u4e3a 0 samplewise_center=False, # \u5c06\u6bcf\u4e2a\u6837\u672c\u7684\u5747\u503c\u8bbe\u4e3a 0 featurewise_std_normalization=False, # \u5c06\u8f93\u5165\u9664\u4ee5\u6574\u4e2a\u6570\u636e\u96c6\u7684\u6807\u51c6\u5dee samplewise_std_normalization=False, # \u5c06\u8f93\u5165\u9664\u4ee5\u5176\u6807\u51c6\u5dee zca_whitening=False, # \u8fd0\u7528 ZCA \u767d\u5316 zca_epsilon=1e-06, # ZCA \u767d\u5316\u7684 epsilon\u503c rotation_range=0, # \u968f\u673a\u65cb\u8f6c\u56fe\u50cf\u8303\u56f4 (\u89d2\u5ea6, 0 to 180) width_shift_range=0.1, # \u968f\u673a\u6c34\u5e73\u79fb\u52a8\u56fe\u50cf (\u603b\u5bbd\u5ea6\u7684\u767e\u5206\u6bd4) height_shift_range=0.1, # \u968f\u673a\u5782\u76f4\u79fb\u52a8\u56fe\u50cf (\u603b\u9ad8\u5ea6\u7684\u767e\u5206\u6bd4) shear_range=0., # \u8bbe\u7f6e\u968f\u673a\u88c1\u526a\u8303\u56f4 zoom_range=0., # \u8bbe\u7f6e\u968f\u673a\u653e\u5927\u8303\u56f4 channel_shift_range=0., # \u8bbe\u7f6e\u968f\u673a\u901a\u9053\u5207\u6362\u7684\u8303\u56f4 # \u8bbe\u7f6e\u586b\u5145\u8f93\u5165\u8fb9\u754c\u4e4b\u5916\u7684\u70b9\u7684\u6a21\u5f0f fill_mode='nearest', cval=0., # \u5728 fill_mode = \"constant\" \u65f6\u4f7f\u7528\u7684\u503c horizontal_flip=True, # \u968f\u673a\u6c34\u5e73\u7ffb\u8f6c\u56fe\u50cf vertical_flip=False, # \u968f\u673a\u5782\u76f4\u7ffb\u8f6c\u56fe\u50cf # \u8bbe\u7f6e\u7f29\u653e\u56e0\u5b50 (\u5728\u5176\u4ed6\u8f6c\u6362\u4e4b\u524d\u4f7f\u7528) rescale=None, # \u8bbe\u7f6e\u5c06\u5e94\u7528\u4e8e\u6bcf\u4e00\u4e2a\u8f93\u5165\u7684\u51fd\u6570 preprocessing_function=None, # \u56fe\u50cf\u6570\u636e\u683c\u5f0f\uff0c\"channels_first\" \u6216 \"channels_last\" \u4e4b\u4e00 data_format=None, # \u4fdd\u7559\u7528\u4e8e\u9a8c\u8bc1\u7684\u56fe\u50cf\u6bd4\u4f8b\uff08\u4e25\u683c\u57280\u548c1\u4e4b\u95f4\uff09 validation_split=0.0) # \u8ba1\u7b97\u7279\u5f81\u6807\u51c6\u5316\u6240\u9700\u7684\u8ba1\u7b97\u91cf # (\u5982\u679c\u5e94\u7528 ZCA \u767d\u5316\uff0c\u5219\u4e3a std\uff0cmean\u548c\u4e3b\u6210\u5206)\u3002 datagen.fit(x_train) # \u5229\u7528\u7531 datagen.flow() \u751f\u6210\u7684\u6279\u6765\u8bad\u7ec3\u6a21\u578b\u3002 model.fit_generator( datagen.flow(x_train, y_train, batch_size=batch_size), epochs=epochs, validation_data=(x_test, y_test), workers=4)","title":"\u5728 CIFAR10 \u5c0f\u578b\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e00\u4e2a\u7b80\u5355\u7684 CNN-Capsule Network\u3002"},{"location":"4-Examples/cifar10_cnn_capsule/#cifar10-cnn-capsule-network","text":"\u65e0\u6570\u636e\u589e\u76ca\u7684\u60c5\u51b5\u4e0b\uff1a \u5728 10 \u8f6e\u8fed\u4ee3\u540e\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387\u8fbe\u5230 75%\uff0c\u5728 15 \u8f6e\u540e\u8fbe\u5230 79%\uff0c\u5728 20 \u8f6e\u540e\u8fc7\u62df\u5408\u3002 \u6709\u6570\u636e\u589e\u76ca\u60c5\u51b5\u4e0b: \u5728 10 \u8f6e\u8fed\u4ee3\u540e\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387\u8fbe\u5230 75%\uff0c\u5728 15 \u8f6e\u540e\u8fbe\u5230 79%\uff0c\u5728 30 \u8f6e\u540e\u8fbe\u5230 83%\u3002 \u5728\u6211\u7684\u6d4b\u8bd5\u4e2d\uff0c50 \u8f6e\u540e\u6700\u9ad8\u7684\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387\u4e3a 83.79%\u3002 \u8fd9\u662f\u4e00\u4e2a\u5feb\u901f\u7248\u7684\u5b9e\u73b0\uff0c\u5728 GTX 1070 GPU \u4e0a\u8fed\u4ee3\u53ea\u9700 20s/epoch\u3002 from __future__ import print_function from keras import backend as K from keras.layers import Layer from keras import activations from keras import utils from keras.datasets import cifar10 from keras.models import Model from keras.layers import * from keras.preprocessing.image import ImageDataGenerator # \u6324\u538b\u51fd\u6570 # \u6211\u4eec\u5728\u6b64\u4f7f\u7528 0.5\uff0c\u800c\u4e0d\u662f Hinton \u8bba\u6587\u4e2d\u7ed9\u51fa\u7684 1 # \u5982\u679c\u4e3a 1\uff0c\u5219\u5411\u91cf\u7684\u8303\u6570\u5c06\u88ab\u7f29\u5c0f\u3002 # \u5982\u679c\u4e3a 0.5\uff0c\u5219\u5f53\u539f\u59cb\u8303\u6570\u5c0f\u4e8e 0.5 \u65f6\uff0c\u8303\u6570\u5c06\u88ab\u653e\u5927\uff0c # \u5f53\u539f\u59cb\u8303\u6570\u5927\u4e8e 0.5 \u65f6\uff0c\u8303\u6570\u5c06\u88ab\u7f29\u5c0f\u3002 def squash(x, axis=-1): s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon() scale = K.sqrt(s_squared_norm) / (0.5 + s_squared_norm) return scale * x # \u4f7f\u7528\u81ea\u5b9a\u4e49\u7684 softmax \u51fd\u6570\uff0c\u800c\u975e K.softmax\uff0c # \u56e0\u4e3a K.softmax \u4e0d\u80fd\u6307\u5b9a\u8f74\u3002 def softmax(x, axis=-1): ex = K.exp(x - K.max(x, axis=axis, keepdims=True)) return ex / K.sum(ex, axis=axis, keepdims=True) # \u5b9a\u4e49 margin loss\uff0c\u7c7b\u4f3c\u4e8e hinge loss def margin_loss(y_true, y_pred): lamb, margin = 0.5, 0.1 return K.sum(y_true * K.square(K.relu(1 - margin - y_pred)) + lamb * ( 1 - y_true) * K.square(K.relu(y_pred - margin)), axis=-1) class Capsule(Layer): \"\"\"\u4e00\u4e2a\u7531\u7eaf Keras \u5b9e\u73b0\u7684 Capsule \u7f51\u7edc\u3002 \u603b\u5171\u6709\u4e24\u4e2a\u7248\u672c\u7684 Capsule\u3002 \u4e00\u4e2a\u7c7b\u4f3c\u4e8e\u5168\u8fde\u63a5\u5c42 (\u7528\u4e8e\u56fa\u5b9a\u5c3a\u5bf8\u7684\u8f93\u5165)\uff0c \u53e6\u4e00\u4e2a\u7c7b\u4f3c\u4e8e\u65f6\u5e8f\u5206\u5e03\u5168\u8fde\u63a5\u5c42 (\u7528\u4e8e\u53d8\u6210\u8f93\u5165)\u3002 Capsure \u7684\u8f93\u5165\u5c3a\u5bf8\u5fc5\u987b\u4e3a (batch_size, input_num_capsule, input_dim_capsule ) \u4ee5\u53ca\u8f93\u51fa\u5c3a\u5bf8\u5fc5\u987b\u4e3a (batch_size, num_capsule, dim_capsule ) Capsule \u5b9e\u73b0\u6765\u81ea\u4e8e https://github.com/bojone/Capsule/ Capsule \u8bba\u6587: https://arxiv.org/abs/1710.09829 \"\"\" def __init__(self, num_capsule, dim_capsule, routings=3, share_weights=True, activation='squash', **kwargs): super(Capsule, self).__init__(**kwargs) self.num_capsule = num_capsule self.dim_capsule = dim_capsule self.routings = routings self.share_weights = share_weights if activation == 'squash': self.activation = squash else: self.activation = activations.get(activation) def build(self, input_shape): input_dim_capsule = input_shape[-1] if self.share_weights: self.kernel = self.add_weight( name='capsule_kernel', shape=(1, input_dim_capsule, self.num_capsule * self.dim_capsule), initializer='glorot_uniform', trainable=True) else: input_num_capsule = input_shape[-2] self.kernel = self.add_weight( name='capsule_kernel', shape=(input_num_capsule, input_dim_capsule, self.num_capsule * self.dim_capsule), initializer='glorot_uniform', trainable=True) def call(self, inputs): \"\"\"\u9075\u5faa Hinton \u8bba\u6587\u4e2d\u7684\u8def\u7531\u7b97\u6cd5\uff0c \u4f46\u662f\u5c06 b = b + <u,v> \u66ff\u6362\u4e3a b = <u,v>\u3002 \u8fd9\u4e00\u6539\u53d8\u5c06\u63d0\u5347 Capsule \u7684\u7279\u5f81\u8868\u793a\u80fd\u529b\u3002 \u7136\u800c\uff0c\u4f60\u4ecd\u53ef\u4ee5\u5c06 b = K.batch_dot(outputs, hat_inputs, [2, 3]) \u66ff\u6362\u4e3a b += K.batch_dot(outputs, hat_inputs, [2, 3]) \u6765\u5b9e\u73b0\u4e00\u4e2a\u6807\u51c6\u7684\u8def\u7531\u3002 \"\"\" if self.share_weights: hat_inputs = K.conv1d(inputs, self.kernel) else: hat_inputs = K.local_conv1d(inputs, self.kernel, [1], [1]) batch_size = K.shape(inputs)[0] input_num_capsule = K.shape(inputs)[1] hat_inputs = K.reshape(hat_inputs, (batch_size, input_num_capsule, self.num_capsule, self.dim_capsule)) hat_inputs = K.permute_dimensions(hat_inputs, (0, 2, 1, 3)) b = K.zeros_like(hat_inputs[:, :, :, 0]) for i in range(self.routings): c = softmax(b, 1) o = self.activation(K.batch_dot(c, hat_inputs, [2, 2])) if i < self.routings - 1: b = K.batch_dot(o, hat_inputs, [2, 3]) if K.backend() == 'theano': o = K.sum(o, axis=1) return o def compute_output_shape(self, input_shape): return (None, self.num_capsule, self.dim_capsule) batch_size = 128 num_classes = 10 epochs = 100 (x_train, y_train), (x_test, y_test) = cifar10.load_data() x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 y_train = utils.to_categorical(y_train, num_classes) y_test = utils.to_categorical(y_test, num_classes) # \u4e00\u4e2a\u5e38\u89c4\u7684 Conv2D \u6a21\u578b input_image = Input(shape=(None, None, 3)) x = Conv2D(64, (3, 3), activation='relu')(input_image) x = Conv2D(64, (3, 3), activation='relu')(x) x = AveragePooling2D((2, 2))(x) x = Conv2D(128, (3, 3), activation='relu')(x) x = Conv2D(128, (3, 3), activation='relu')(x) \"\"\"\u73b0\u5728\u6211\u4eec\u5c06\u5176\u5c3a\u5bf8\u91cd\u65b0\u8c03\u6574\u4e3a (batch_size, input_num_capsule, input_dim_capsule)\uff0c\u518d\u8fde\u63a5\u4e00\u4e2a Capsule \u7f51\u7edc\u3002 \u6700\u7ec8\u6a21\u578b\u7684\u8f93\u51fa\u4e3a\u957f\u5ea6\u4e3a 10 \u7684 Capsure\uff0c\u5176 dim=16\u3002 Capsule \u7684\u957f\u5ea6\u8868\u793a\u4e3a proba\uff0c \u56e0\u6b64\u95ee\u9898\u53d8\u6210\u4e86\u4e00\u4e2a\u300e10\u4e2a\u4e8c\u5206\u7c7b\u300f\u7684\u95ee\u9898\u3002 \"\"\" x = Reshape((-1, 128))(x) capsule = Capsule(10, 16, 3, True)(x) output = Lambda(lambda z: K.sqrt(K.sum(K.square(z), 2)))(capsule) model = Model(inputs=input_image, outputs=output) # \u4f7f\u7528 margin loss model.compile(loss=margin_loss, optimizer='adam', metrics=['accuracy']) model.summary() # \u53ef\u4ee5\u6bd4\u8f83\u6709\u65e0\u6570\u636e\u589e\u76ca\u5bf9\u5e94\u7684\u6027\u80fd data_augmentation = True if not data_augmentation: print('Not using data augmentation.') model.fit( x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), shuffle=True) else: print('Using real-time data augmentation.') # \u8fd9\u4e00\u6b65\u5c06\u8fdb\u884c\u6570\u636e\u5904\u7406\u548c\u5b9e\u65f6\u6570\u636e\u589e\u76ca: datagen = ImageDataGenerator( featurewise_center=False, # \u5c06\u6574\u4e2a\u6570\u636e\u96c6\u7684\u5747\u503c\u8bbe\u4e3a 0 samplewise_center=False, # \u5c06\u6bcf\u4e2a\u6837\u672c\u7684\u5747\u503c\u8bbe\u4e3a 0 featurewise_std_normalization=False, # \u5c06\u8f93\u5165\u9664\u4ee5\u6574\u4e2a\u6570\u636e\u96c6\u7684\u6807\u51c6\u5dee samplewise_std_normalization=False, # \u5c06\u8f93\u5165\u9664\u4ee5\u5176\u6807\u51c6\u5dee zca_whitening=False, # \u8fd0\u7528 ZCA \u767d\u5316 zca_epsilon=1e-06, # ZCA \u767d\u5316\u7684 epsilon\u503c rotation_range=0, # \u968f\u673a\u65cb\u8f6c\u56fe\u50cf\u8303\u56f4 (\u89d2\u5ea6, 0 to 180) width_shift_range=0.1, # \u968f\u673a\u6c34\u5e73\u79fb\u52a8\u56fe\u50cf (\u603b\u5bbd\u5ea6\u7684\u767e\u5206\u6bd4) height_shift_range=0.1, # \u968f\u673a\u5782\u76f4\u79fb\u52a8\u56fe\u50cf (\u603b\u9ad8\u5ea6\u7684\u767e\u5206\u6bd4) shear_range=0., # \u8bbe\u7f6e\u968f\u673a\u88c1\u526a\u8303\u56f4 zoom_range=0., # \u8bbe\u7f6e\u968f\u673a\u653e\u5927\u8303\u56f4 channel_shift_range=0., # \u8bbe\u7f6e\u968f\u673a\u901a\u9053\u5207\u6362\u7684\u8303\u56f4 # \u8bbe\u7f6e\u586b\u5145\u8f93\u5165\u8fb9\u754c\u4e4b\u5916\u7684\u70b9\u7684\u6a21\u5f0f fill_mode='nearest', cval=0., # \u5728 fill_mode = \"constant\" \u65f6\u4f7f\u7528\u7684\u503c horizontal_flip=True, # \u968f\u673a\u6c34\u5e73\u7ffb\u8f6c\u56fe\u50cf vertical_flip=False, # \u968f\u673a\u5782\u76f4\u7ffb\u8f6c\u56fe\u50cf # \u8bbe\u7f6e\u7f29\u653e\u56e0\u5b50 (\u5728\u5176\u4ed6\u8f6c\u6362\u4e4b\u524d\u4f7f\u7528) rescale=None, # \u8bbe\u7f6e\u5c06\u5e94\u7528\u4e8e\u6bcf\u4e00\u4e2a\u8f93\u5165\u7684\u51fd\u6570 preprocessing_function=None, # \u56fe\u50cf\u6570\u636e\u683c\u5f0f\uff0c\"channels_first\" \u6216 \"channels_last\" \u4e4b\u4e00 data_format=None, # \u4fdd\u7559\u7528\u4e8e\u9a8c\u8bc1\u7684\u56fe\u50cf\u6bd4\u4f8b\uff08\u4e25\u683c\u57280\u548c1\u4e4b\u95f4\uff09 validation_split=0.0) # \u8ba1\u7b97\u7279\u5f81\u6807\u51c6\u5316\u6240\u9700\u7684\u8ba1\u7b97\u91cf # (\u5982\u679c\u5e94\u7528 ZCA \u767d\u5316\uff0c\u5219\u4e3a std\uff0cmean\u548c\u4e3b\u6210\u5206)\u3002 datagen.fit(x_train) # \u5229\u7528\u7531 datagen.flow() \u751f\u6210\u7684\u6279\u6765\u8bad\u7ec3\u6a21\u578b\u3002 model.fit_generator( datagen.flow(x_train, y_train, batch_size=batch_size), epochs=epochs, validation_data=(x_test, y_test), workers=4)","title":"\u5728 CIFAR10 \u5c0f\u578b\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e00\u4e2a\u7b80\u5355\u7684 CNN-Capsule Network\u3002"},{"location":"4-Examples/cifar10_cnn_tfaugment2d/","text":"Train a simple deep CNN on the CIFAR10 small images dataset using augmentation. Using TensorFlow internal augmentation APIs by replacing ImageGenerator with an embedded AugmentLayer using LambdaLayer, which is faster on GPU. Benchmark of ImageGenerator (IG) vs AugmentLayer (AL) both using augmentation 2D: (backend = Tensorflow-GPU, Nvidia Tesla P100-SXM2) Epoch no. IG %Accuracy IG Performance AL %Accuracy AL Performance 1 44.84 15 ms/step 45.54 358 us/step 2 52.34 8 ms/step 50.55 285 us/step 8 65.45 8 ms/step 65.59 281 us/step 25 76.74 8 ms/step 76.17 280 us/step 100 78.81 8 ms/step 78.70 285 us/step Settings: horizontal_flip = True Epoch no. IG %Accuracy IG Performance AL %Accuracy AL Performance 1 43.46 15 ms/step 42.21 334 us/step 2 48.95 11 ms/step 48.06 282 us/step 8 63.59 11 ms/step 61.35 290 us/step 25 72.25 12 ms/step 71.08 287 us/step 100 76.35 11 ms/step 74.62 286 us/step Settings: rotation = 30.0 (Corner process and rotation precision by ImageGenerator and AugmentLayer are slightly different.) from __future__ import print_function import keras from keras.datasets import cifar10 from keras.models import Sequential from keras.layers import Dense, Dropout, Activation, Flatten from keras.layers import Conv2D, Lambda, MaxPooling2D from keras import backend as K import os if K.backend() != 'tensorflow': raise RuntimeError('This example can only run with the ' 'TensorFlow backend, ' 'because it requires TF-native augmentation APIs') import tensorflow as tf def augment_2d(inputs, rotation=0, horizontal_flip=False, vertical_flip=False): \"\"\"Apply additive augmentation on 2D data. # Arguments rotation: A float, the degree range for rotation (0 <= rotation < 180), e.g. 3 for random image rotation between (-3.0, 3.0). horizontal_flip: A boolean, whether to allow random horizontal flip, e.g. true for 50% possibility to flip image horizontally. vertical_flip: A boolean, whether to allow random vertical flip, e.g. true for 50% possibility to flip image vertically. # Returns input data after augmentation, whose shape is the same as its original. \"\"\" if inputs.dtype != tf.float32: inputs = tf.image.convert_image_dtype(inputs, dtype=tf.float32) with tf.name_scope('augmentation'): shp = tf.shape(inputs) batch_size, height, width = shp[0], shp[1], shp[2] width = tf.cast(width, tf.float32) height = tf.cast(height, tf.float32) transforms = [] identity = tf.constant([1, 0, 0, 0, 1, 0, 0, 0], dtype=tf.float32) if rotation > 0: angle_rad = rotation * 3.141592653589793 / 180.0 angles = tf.random_uniform([batch_size], -angle_rad, angle_rad) f = tf.contrib.image.angles_to_projective_transforms(angles, height, width) transforms.append(f) if horizontal_flip: coin = tf.less(tf.random_uniform([batch_size], 0, 1.0), 0.5) shape = [-1., 0., width, 0., 1., 0., 0., 0.] flip_transform = tf.convert_to_tensor(shape, dtype=tf.float32) flip = tf.tile(tf.expand_dims(flip_transform, 0), [batch_size, 1]) noflip = tf.tile(tf.expand_dims(identity, 0), [batch_size, 1]) transforms.append(tf.where(coin, flip, noflip)) if vertical_flip: coin = tf.less(tf.random_uniform([batch_size], 0, 1.0), 0.5) shape = [1., 0., 0., 0., -1., height, 0., 0.] flip_transform = tf.convert_to_tensor(shape, dtype=tf.float32) flip = tf.tile(tf.expand_dims(flip_transform, 0), [batch_size, 1]) noflip = tf.tile(tf.expand_dims(identity, 0), [batch_size, 1]) transforms.append(tf.where(coin, flip, noflip)) if transforms: f = tf.contrib.image.compose_transforms(*transforms) inputs = tf.contrib.image.transform(inputs, f, interpolation='BILINEAR') return inputs batch_size = 32 num_classes = 10 epochs = 100 num_predictions = 20 save_dir = '/tmp/saved_models' model_name = 'keras_cifar10_trained_model.h5' # The data, split between train and test sets: (x_train, y_train), (x_test, y_test) = cifar10.load_data() print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') # Convert class vectors to binary class matrices. y_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes) model = Sequential() model.add(Lambda(augment_2d, input_shape=x_train.shape[1:], arguments={'rotation': 8.0, 'horizontal_flip': True})) model.add(Conv2D(32, (3, 3), padding='same')) model.add(Activation('relu')) model.add(Conv2D(32, (3, 3))) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Conv2D(64, (3, 3), padding='same')) model.add(Activation('relu')) model.add(Conv2D(64, (3, 3))) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Flatten()) model.add(Dense(512)) model.add(Activation('relu')) model.add(Dropout(0.5)) model.add(Dense(num_classes)) model.add(Activation('softmax')) # initiate RMSprop optimizer opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6) # Let's train the model using RMSprop model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), shuffle=True) # Save model and weights if not os.path.isdir(save_dir): os.makedirs(save_dir) model_path = os.path.join(save_dir, model_name) model.save(model_path) print('Saved trained model at %s ' % model_path) # Score trained model. scores = model.evaluate(x_test, y_test, verbose=1) print('Test loss:', scores[0]) print('Test accuracy:', scores[1])","title":"Cifar10 cnn tfaugment2d"},{"location":"4-Examples/cifar10_cnn_tfaugment2d/#train-a-simple-deep-cnn-on-the-cifar10-small-images-dataset-using-augmentation","text":"Using TensorFlow internal augmentation APIs by replacing ImageGenerator with an embedded AugmentLayer using LambdaLayer, which is faster on GPU. Benchmark of ImageGenerator (IG) vs AugmentLayer (AL) both using augmentation 2D: (backend = Tensorflow-GPU, Nvidia Tesla P100-SXM2) Epoch no. IG %Accuracy IG Performance AL %Accuracy AL Performance 1 44.84 15 ms/step 45.54 358 us/step 2 52.34 8 ms/step 50.55 285 us/step 8 65.45 8 ms/step 65.59 281 us/step 25 76.74 8 ms/step 76.17 280 us/step 100 78.81 8 ms/step 78.70 285 us/step Settings: horizontal_flip = True Epoch no. IG %Accuracy IG Performance AL %Accuracy AL Performance 1 43.46 15 ms/step 42.21 334 us/step 2 48.95 11 ms/step 48.06 282 us/step 8 63.59 11 ms/step 61.35 290 us/step 25 72.25 12 ms/step 71.08 287 us/step 100 76.35 11 ms/step 74.62 286 us/step Settings: rotation = 30.0 (Corner process and rotation precision by ImageGenerator and AugmentLayer are slightly different.) from __future__ import print_function import keras from keras.datasets import cifar10 from keras.models import Sequential from keras.layers import Dense, Dropout, Activation, Flatten from keras.layers import Conv2D, Lambda, MaxPooling2D from keras import backend as K import os if K.backend() != 'tensorflow': raise RuntimeError('This example can only run with the ' 'TensorFlow backend, ' 'because it requires TF-native augmentation APIs') import tensorflow as tf def augment_2d(inputs, rotation=0, horizontal_flip=False, vertical_flip=False): \"\"\"Apply additive augmentation on 2D data. # Arguments rotation: A float, the degree range for rotation (0 <= rotation < 180), e.g. 3 for random image rotation between (-3.0, 3.0). horizontal_flip: A boolean, whether to allow random horizontal flip, e.g. true for 50% possibility to flip image horizontally. vertical_flip: A boolean, whether to allow random vertical flip, e.g. true for 50% possibility to flip image vertically. # Returns input data after augmentation, whose shape is the same as its original. \"\"\" if inputs.dtype != tf.float32: inputs = tf.image.convert_image_dtype(inputs, dtype=tf.float32) with tf.name_scope('augmentation'): shp = tf.shape(inputs) batch_size, height, width = shp[0], shp[1], shp[2] width = tf.cast(width, tf.float32) height = tf.cast(height, tf.float32) transforms = [] identity = tf.constant([1, 0, 0, 0, 1, 0, 0, 0], dtype=tf.float32) if rotation > 0: angle_rad = rotation * 3.141592653589793 / 180.0 angles = tf.random_uniform([batch_size], -angle_rad, angle_rad) f = tf.contrib.image.angles_to_projective_transforms(angles, height, width) transforms.append(f) if horizontal_flip: coin = tf.less(tf.random_uniform([batch_size], 0, 1.0), 0.5) shape = [-1., 0., width, 0., 1., 0., 0., 0.] flip_transform = tf.convert_to_tensor(shape, dtype=tf.float32) flip = tf.tile(tf.expand_dims(flip_transform, 0), [batch_size, 1]) noflip = tf.tile(tf.expand_dims(identity, 0), [batch_size, 1]) transforms.append(tf.where(coin, flip, noflip)) if vertical_flip: coin = tf.less(tf.random_uniform([batch_size], 0, 1.0), 0.5) shape = [1., 0., 0., 0., -1., height, 0., 0.] flip_transform = tf.convert_to_tensor(shape, dtype=tf.float32) flip = tf.tile(tf.expand_dims(flip_transform, 0), [batch_size, 1]) noflip = tf.tile(tf.expand_dims(identity, 0), [batch_size, 1]) transforms.append(tf.where(coin, flip, noflip)) if transforms: f = tf.contrib.image.compose_transforms(*transforms) inputs = tf.contrib.image.transform(inputs, f, interpolation='BILINEAR') return inputs batch_size = 32 num_classes = 10 epochs = 100 num_predictions = 20 save_dir = '/tmp/saved_models' model_name = 'keras_cifar10_trained_model.h5' # The data, split between train and test sets: (x_train, y_train), (x_test, y_test) = cifar10.load_data() print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') # Convert class vectors to binary class matrices. y_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes) model = Sequential() model.add(Lambda(augment_2d, input_shape=x_train.shape[1:], arguments={'rotation': 8.0, 'horizontal_flip': True})) model.add(Conv2D(32, (3, 3), padding='same')) model.add(Activation('relu')) model.add(Conv2D(32, (3, 3))) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Conv2D(64, (3, 3), padding='same')) model.add(Activation('relu')) model.add(Conv2D(64, (3, 3))) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Flatten()) model.add(Dense(512)) model.add(Activation('relu')) model.add(Dropout(0.5)) model.add(Dense(num_classes)) model.add(Activation('softmax')) # initiate RMSprop optimizer opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6) # Let's train the model using RMSprop model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), shuffle=True) # Save model and weights if not os.path.isdir(save_dir): os.makedirs(save_dir) model_path = os.path.join(save_dir, model_name) model.save(model_path) print('Saved trained model at %s ' % model_path) # Score trained model. scores = model.evaluate(x_test, y_test, verbose=1) print('Test loss:', scores[0]) print('Test accuracy:', scores[1])","title":"Train a simple deep CNN on the CIFAR10 small images dataset using augmentation."},{"location":"4-Examples/cifar10_resnet/","text":"Trains a ResNet on the CIFAR10 dataset. ResNet v1: Deep Residual Learning for Image Recognition ResNet v2: Identity Mappings in Deep Residual Networks Model n 200-epoch accuracy Original paper accuracy sec/epoch GTX1080Ti ResNet20 v1 3 92.16 % 91.25 % 35 ResNet32 v1 5 92.46 % 92.49 % 50 ResNet44 v1 7 92.50 % 92.83 % 70 ResNet56 v1 9 92.71 % 93.03 % 90 ResNet110 v1 18 92.65 % 93.39+-.16 % 165 ResNet164 v1 27 - % 94.07 % - ResNet1001 v1 N/A - % 92.39 % - Model n 200-epoch accuracy Original paper accuracy sec/epoch GTX1080Ti ResNet20 v2 2 - % - % --- ResNet32 v2 N/A NA % NA % NA ResNet44 v2 N/A NA % NA % NA ResNet56 v2 6 93.01 % NA % 100 ResNet110 v2 12 93.15 % 93.63 % 180 ResNet164 v2 18 - % 94.54 % - ResNet1001 v2 111 - % 95.08+-.14 % - from __future__ import print_function import keras from keras.layers import Dense, Conv2D, BatchNormalization, Activation from keras.layers import AveragePooling2D, Input, Flatten from keras.optimizers import Adam from keras.callbacks import ModelCheckpoint, LearningRateScheduler from keras.callbacks import ReduceLROnPlateau from keras.preprocessing.image import ImageDataGenerator from keras.regularizers import l2 from keras import backend as K from keras.models import Model from keras.datasets import cifar10 import numpy as np import os # Training parameters batch_size = 32 # orig paper trained all networks with batch_size=128 epochs = 200 data_augmentation = True num_classes = 10 # Subtracting pixel mean improves accuracy subtract_pixel_mean = True # Model parameter # ---------------------------------------------------------------------------- # | | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch # Model | n | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti # |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2) # ---------------------------------------------------------------------------- # ResNet20 | 3 (2)| 92.16 | 91.25 | ----- | ----- | 35 (---) # ResNet32 | 5(NA)| 92.46 | 92.49 | NA | NA | 50 ( NA) # ResNet44 | 7(NA)| 92.50 | 92.83 | NA | NA | 70 ( NA) # ResNet56 | 9 (6)| 92.71 | 93.03 | 93.01 | NA | 90 (100) # ResNet110 |18(12)| 92.65 | 93.39+-.16| 93.15 | 93.63 | 165(180) # ResNet164 |27(18)| ----- | 94.07 | ----- | 94.54 | ---(---) # ResNet1001| (111)| ----- | 92.39 | ----- | 95.08+-.14| ---(---) # --------------------------------------------------------------------------- n = 3 # Model version # Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2) version = 1 # Computed depth from supplied model parameter n if version == 1: depth = n * 6 + 2 elif version == 2: depth = n * 9 + 2 # Model name, depth and version model_type = 'ResNet%dv%d' % (depth, version) # Load the CIFAR10 data. (x_train, y_train), (x_test, y_test) = cifar10.load_data() # Input image dimensions. input_shape = x_train.shape[1:] # Normalize data. x_train = x_train.astype('float32') / 255 x_test = x_test.astype('float32') / 255 # If subtract pixel mean is enabled if subtract_pixel_mean: x_train_mean = np.mean(x_train, axis=0) x_train -= x_train_mean x_test -= x_train_mean print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') print('y_train shape:', y_train.shape) # Convert class vectors to binary class matrices. y_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes) def lr_schedule(epoch): \"\"\"Learning Rate Schedule Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs. Called automatically every epoch as part of callbacks during training. # Arguments epoch (int): The number of epochs # Returns lr (float32): learning rate \"\"\" lr = 1e-3 if epoch > 180: lr *= 0.5e-3 elif epoch > 160: lr *= 1e-3 elif epoch > 120: lr *= 1e-2 elif epoch > 80: lr *= 1e-1 print('Learning rate: ', lr) return lr def resnet_layer(inputs, num_filters=16, kernel_size=3, strides=1, activation='relu', batch_normalization=True, conv_first=True): \"\"\"2D Convolution-Batch Normalization-Activation stack builder # Arguments inputs (tensor): input tensor from input image or previous layer num_filters (int): Conv2D number of filters kernel_size (int): Conv2D square kernel dimensions strides (int): Conv2D square stride dimensions activation (string): activation name batch_normalization (bool): whether to include batch normalization conv_first (bool): conv-bn-activation (True) or bn-activation-conv (False) # Returns x (tensor): tensor as input to the next layer \"\"\" conv = Conv2D(num_filters, kernel_size=kernel_size, strides=strides, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4)) x = inputs if conv_first: x = conv(x) if batch_normalization: x = BatchNormalization()(x) if activation is not None: x = Activation(activation)(x) else: if batch_normalization: x = BatchNormalization()(x) if activation is not None: x = Activation(activation)(x) x = conv(x) return x def resnet_v1(input_shape, depth, num_classes=10): \"\"\"ResNet Version 1 Model builder [a] Stacks of 2 x (3 x 3) Conv2D-BN-ReLU Last ReLU is after the shortcut connection. At the beginning of each stage, the feature map size is halved (downsampled) by a convolutional layer with strides=2, while the number of filters is doubled. Within each stage, the layers have the same number filters and the same number of filters. Features maps sizes: stage 0: 32x32, 16 stage 1: 16x16, 32 stage 2: 8x8, 64 The Number of parameters is approx the same as Table 6 of [a]: ResNet20 0.27M ResNet32 0.46M ResNet44 0.66M ResNet56 0.85M ResNet110 1.7M # Arguments input_shape (tensor): shape of input image tensor depth (int): number of core convolutional layers num_classes (int): number of classes (CIFAR10 has 10) # Returns model (Model): Keras model instance \"\"\" if (depth - 2) % 6 != 0: raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])') # Start model definition. num_filters = 16 num_res_blocks = int((depth - 2) / 6) inputs = Input(shape=input_shape) x = resnet_layer(inputs=inputs) # Instantiate the stack of residual units for stack in range(3): for res_block in range(num_res_blocks): strides = 1 if stack > 0 and res_block == 0: # first layer but not first stack strides = 2 # downsample y = resnet_layer(inputs=x, num_filters=num_filters, strides=strides) y = resnet_layer(inputs=y, num_filters=num_filters, activation=None) if stack > 0 and res_block == 0: # first layer but not first stack # linear projection residual shortcut connection to match # changed dims x = resnet_layer(inputs=x, num_filters=num_filters, kernel_size=1, strides=strides, activation=None, batch_normalization=False) x = keras.layers.add([x, y]) x = Activation('relu')(x) num_filters *= 2 # Add classifier on top. # v1 does not use BN after last shortcut connection-ReLU x = AveragePooling2D(pool_size=8)(x) y = Flatten()(x) outputs = Dense(num_classes, activation='softmax', kernel_initializer='he_normal')(y) # Instantiate model. model = Model(inputs=inputs, outputs=outputs) return model def resnet_v2(input_shape, depth, num_classes=10): \"\"\"ResNet Version 2 Model builder [b] Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as bottleneck layer First shortcut connection per layer is 1 x 1 Conv2D. Second and onwards shortcut connection is identity. At the beginning of each stage, the feature map size is halved (downsampled) by a convolutional layer with strides=2, while the number of filter maps is doubled. Within each stage, the layers have the same number filters and the same filter map sizes. Features maps sizes: conv1 : 32x32, 16 stage 0: 32x32, 64 stage 1: 16x16, 128 stage 2: 8x8, 256 # Arguments input_shape (tensor): shape of input image tensor depth (int): number of core convolutional layers num_classes (int): number of classes (CIFAR10 has 10) # Returns model (Model): Keras model instance \"\"\" if (depth - 2) % 9 != 0: raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])') # Start model definition. num_filters_in = 16 num_res_blocks = int((depth - 2) / 9) inputs = Input(shape=input_shape) # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths x = resnet_layer(inputs=inputs, num_filters=num_filters_in, conv_first=True) # Instantiate the stack of residual units for stage in range(3): for res_block in range(num_res_blocks): activation = 'relu' batch_normalization = True strides = 1 if stage == 0: num_filters_out = num_filters_in * 4 if res_block == 0: # first layer and first stage activation = None batch_normalization = False else: num_filters_out = num_filters_in * 2 if res_block == 0: # first layer but not first stage strides = 2 # downsample # bottleneck residual unit y = resnet_layer(inputs=x, num_filters=num_filters_in, kernel_size=1, strides=strides, activation=activation, batch_normalization=batch_normalization, conv_first=False) y = resnet_layer(inputs=y, num_filters=num_filters_in, conv_first=False) y = resnet_layer(inputs=y, num_filters=num_filters_out, kernel_size=1, conv_first=False) if res_block == 0: # linear projection residual shortcut connection to match # changed dims x = resnet_layer(inputs=x, num_filters=num_filters_out, kernel_size=1, strides=strides, activation=None, batch_normalization=False) x = keras.layers.add([x, y]) num_filters_in = num_filters_out # Add classifier on top. # v2 has BN-ReLU before Pooling x = BatchNormalization()(x) x = Activation('relu')(x) x = AveragePooling2D(pool_size=8)(x) y = Flatten()(x) outputs = Dense(num_classes, activation='softmax', kernel_initializer='he_normal')(y) # Instantiate model. model = Model(inputs=inputs, outputs=outputs) return model if version == 2: model = resnet_v2(input_shape=input_shape, depth=depth) else: model = resnet_v1(input_shape=input_shape, depth=depth) model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=lr_schedule(0)), metrics=['accuracy']) model.summary() print(model_type) # Prepare model model saving directory. save_dir = os.path.join(os.getcwd(), 'saved_models') model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type if not os.path.isdir(save_dir): os.makedirs(save_dir) filepath = os.path.join(save_dir, model_name) # Prepare callbacks for model saving and for learning rate adjustment. checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_acc', verbose=1, save_best_only=True) lr_scheduler = LearningRateScheduler(lr_schedule) lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6) callbacks = [checkpoint, lr_reducer, lr_scheduler] # Run training, with or without data augmentation. if not data_augmentation: print('Not using data augmentation.') model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), shuffle=True, callbacks=callbacks) else: print('Using real-time data augmentation.') # This will do preprocessing and realtime data augmentation: datagen = ImageDataGenerator( # set input mean to 0 over the dataset featurewise_center=False, # set each sample mean to 0 samplewise_center=False, # divide inputs by std of dataset featurewise_std_normalization=False, # divide each input by its std samplewise_std_normalization=False, # apply ZCA whitening zca_whitening=False, # epsilon for ZCA whitening zca_epsilon=1e-06, # randomly rotate images in the range (deg 0 to 180) rotation_range=0, # randomly shift images horizontally width_shift_range=0.1, # randomly shift images vertically height_shift_range=0.1, # set range for random shear shear_range=0., # set range for random zoom zoom_range=0., # set range for random channel shifts channel_shift_range=0., # set mode for filling points outside the input boundaries fill_mode='nearest', # value used for fill_mode = \"constant\" cval=0., # randomly flip images horizontal_flip=True, # randomly flip images vertical_flip=False, # set rescaling factor (applied before any other transformation) rescale=None, # set function that will be applied on each input preprocessing_function=None, # image data format, either \"channels_first\" or \"channels_last\" data_format=None, # fraction of images reserved for validation (strictly between 0 and 1) validation_split=0.0) # Compute quantities required for featurewise normalization # (std, mean, and principal components if ZCA whitening is applied). datagen.fit(x_train) # Fit the model on the batches generated by datagen.flow(). model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size), validation_data=(x_test, y_test), epochs=epochs, verbose=1, workers=4, callbacks=callbacks) # Score trained model. scores = model.evaluate(x_test, y_test, verbose=1) print('Test loss:', scores[0]) print('Test accuracy:', scores[1])","title":"Cifar10 resnet"},{"location":"4-Examples/cifar10_resnet/#trains-a-resnet-on-the-cifar10-dataset","text":"ResNet v1: Deep Residual Learning for Image Recognition ResNet v2: Identity Mappings in Deep Residual Networks Model n 200-epoch accuracy Original paper accuracy sec/epoch GTX1080Ti ResNet20 v1 3 92.16 % 91.25 % 35 ResNet32 v1 5 92.46 % 92.49 % 50 ResNet44 v1 7 92.50 % 92.83 % 70 ResNet56 v1 9 92.71 % 93.03 % 90 ResNet110 v1 18 92.65 % 93.39+-.16 % 165 ResNet164 v1 27 - % 94.07 % - ResNet1001 v1 N/A - % 92.39 % - Model n 200-epoch accuracy Original paper accuracy sec/epoch GTX1080Ti ResNet20 v2 2 - % - % --- ResNet32 v2 N/A NA % NA % NA ResNet44 v2 N/A NA % NA % NA ResNet56 v2 6 93.01 % NA % 100 ResNet110 v2 12 93.15 % 93.63 % 180 ResNet164 v2 18 - % 94.54 % - ResNet1001 v2 111 - % 95.08+-.14 % - from __future__ import print_function import keras from keras.layers import Dense, Conv2D, BatchNormalization, Activation from keras.layers import AveragePooling2D, Input, Flatten from keras.optimizers import Adam from keras.callbacks import ModelCheckpoint, LearningRateScheduler from keras.callbacks import ReduceLROnPlateau from keras.preprocessing.image import ImageDataGenerator from keras.regularizers import l2 from keras import backend as K from keras.models import Model from keras.datasets import cifar10 import numpy as np import os # Training parameters batch_size = 32 # orig paper trained all networks with batch_size=128 epochs = 200 data_augmentation = True num_classes = 10 # Subtracting pixel mean improves accuracy subtract_pixel_mean = True # Model parameter # ---------------------------------------------------------------------------- # | | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch # Model | n | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti # |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2) # ---------------------------------------------------------------------------- # ResNet20 | 3 (2)| 92.16 | 91.25 | ----- | ----- | 35 (---) # ResNet32 | 5(NA)| 92.46 | 92.49 | NA | NA | 50 ( NA) # ResNet44 | 7(NA)| 92.50 | 92.83 | NA | NA | 70 ( NA) # ResNet56 | 9 (6)| 92.71 | 93.03 | 93.01 | NA | 90 (100) # ResNet110 |18(12)| 92.65 | 93.39+-.16| 93.15 | 93.63 | 165(180) # ResNet164 |27(18)| ----- | 94.07 | ----- | 94.54 | ---(---) # ResNet1001| (111)| ----- | 92.39 | ----- | 95.08+-.14| ---(---) # --------------------------------------------------------------------------- n = 3 # Model version # Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2) version = 1 # Computed depth from supplied model parameter n if version == 1: depth = n * 6 + 2 elif version == 2: depth = n * 9 + 2 # Model name, depth and version model_type = 'ResNet%dv%d' % (depth, version) # Load the CIFAR10 data. (x_train, y_train), (x_test, y_test) = cifar10.load_data() # Input image dimensions. input_shape = x_train.shape[1:] # Normalize data. x_train = x_train.astype('float32') / 255 x_test = x_test.astype('float32') / 255 # If subtract pixel mean is enabled if subtract_pixel_mean: x_train_mean = np.mean(x_train, axis=0) x_train -= x_train_mean x_test -= x_train_mean print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') print('y_train shape:', y_train.shape) # Convert class vectors to binary class matrices. y_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes) def lr_schedule(epoch): \"\"\"Learning Rate Schedule Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs. Called automatically every epoch as part of callbacks during training. # Arguments epoch (int): The number of epochs # Returns lr (float32): learning rate \"\"\" lr = 1e-3 if epoch > 180: lr *= 0.5e-3 elif epoch > 160: lr *= 1e-3 elif epoch > 120: lr *= 1e-2 elif epoch > 80: lr *= 1e-1 print('Learning rate: ', lr) return lr def resnet_layer(inputs, num_filters=16, kernel_size=3, strides=1, activation='relu', batch_normalization=True, conv_first=True): \"\"\"2D Convolution-Batch Normalization-Activation stack builder # Arguments inputs (tensor): input tensor from input image or previous layer num_filters (int): Conv2D number of filters kernel_size (int): Conv2D square kernel dimensions strides (int): Conv2D square stride dimensions activation (string): activation name batch_normalization (bool): whether to include batch normalization conv_first (bool): conv-bn-activation (True) or bn-activation-conv (False) # Returns x (tensor): tensor as input to the next layer \"\"\" conv = Conv2D(num_filters, kernel_size=kernel_size, strides=strides, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4)) x = inputs if conv_first: x = conv(x) if batch_normalization: x = BatchNormalization()(x) if activation is not None: x = Activation(activation)(x) else: if batch_normalization: x = BatchNormalization()(x) if activation is not None: x = Activation(activation)(x) x = conv(x) return x def resnet_v1(input_shape, depth, num_classes=10): \"\"\"ResNet Version 1 Model builder [a] Stacks of 2 x (3 x 3) Conv2D-BN-ReLU Last ReLU is after the shortcut connection. At the beginning of each stage, the feature map size is halved (downsampled) by a convolutional layer with strides=2, while the number of filters is doubled. Within each stage, the layers have the same number filters and the same number of filters. Features maps sizes: stage 0: 32x32, 16 stage 1: 16x16, 32 stage 2: 8x8, 64 The Number of parameters is approx the same as Table 6 of [a]: ResNet20 0.27M ResNet32 0.46M ResNet44 0.66M ResNet56 0.85M ResNet110 1.7M # Arguments input_shape (tensor): shape of input image tensor depth (int): number of core convolutional layers num_classes (int): number of classes (CIFAR10 has 10) # Returns model (Model): Keras model instance \"\"\" if (depth - 2) % 6 != 0: raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])') # Start model definition. num_filters = 16 num_res_blocks = int((depth - 2) / 6) inputs = Input(shape=input_shape) x = resnet_layer(inputs=inputs) # Instantiate the stack of residual units for stack in range(3): for res_block in range(num_res_blocks): strides = 1 if stack > 0 and res_block == 0: # first layer but not first stack strides = 2 # downsample y = resnet_layer(inputs=x, num_filters=num_filters, strides=strides) y = resnet_layer(inputs=y, num_filters=num_filters, activation=None) if stack > 0 and res_block == 0: # first layer but not first stack # linear projection residual shortcut connection to match # changed dims x = resnet_layer(inputs=x, num_filters=num_filters, kernel_size=1, strides=strides, activation=None, batch_normalization=False) x = keras.layers.add([x, y]) x = Activation('relu')(x) num_filters *= 2 # Add classifier on top. # v1 does not use BN after last shortcut connection-ReLU x = AveragePooling2D(pool_size=8)(x) y = Flatten()(x) outputs = Dense(num_classes, activation='softmax', kernel_initializer='he_normal')(y) # Instantiate model. model = Model(inputs=inputs, outputs=outputs) return model def resnet_v2(input_shape, depth, num_classes=10): \"\"\"ResNet Version 2 Model builder [b] Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as bottleneck layer First shortcut connection per layer is 1 x 1 Conv2D. Second and onwards shortcut connection is identity. At the beginning of each stage, the feature map size is halved (downsampled) by a convolutional layer with strides=2, while the number of filter maps is doubled. Within each stage, the layers have the same number filters and the same filter map sizes. Features maps sizes: conv1 : 32x32, 16 stage 0: 32x32, 64 stage 1: 16x16, 128 stage 2: 8x8, 256 # Arguments input_shape (tensor): shape of input image tensor depth (int): number of core convolutional layers num_classes (int): number of classes (CIFAR10 has 10) # Returns model (Model): Keras model instance \"\"\" if (depth - 2) % 9 != 0: raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])') # Start model definition. num_filters_in = 16 num_res_blocks = int((depth - 2) / 9) inputs = Input(shape=input_shape) # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths x = resnet_layer(inputs=inputs, num_filters=num_filters_in, conv_first=True) # Instantiate the stack of residual units for stage in range(3): for res_block in range(num_res_blocks): activation = 'relu' batch_normalization = True strides = 1 if stage == 0: num_filters_out = num_filters_in * 4 if res_block == 0: # first layer and first stage activation = None batch_normalization = False else: num_filters_out = num_filters_in * 2 if res_block == 0: # first layer but not first stage strides = 2 # downsample # bottleneck residual unit y = resnet_layer(inputs=x, num_filters=num_filters_in, kernel_size=1, strides=strides, activation=activation, batch_normalization=batch_normalization, conv_first=False) y = resnet_layer(inputs=y, num_filters=num_filters_in, conv_first=False) y = resnet_layer(inputs=y, num_filters=num_filters_out, kernel_size=1, conv_first=False) if res_block == 0: # linear projection residual shortcut connection to match # changed dims x = resnet_layer(inputs=x, num_filters=num_filters_out, kernel_size=1, strides=strides, activation=None, batch_normalization=False) x = keras.layers.add([x, y]) num_filters_in = num_filters_out # Add classifier on top. # v2 has BN-ReLU before Pooling x = BatchNormalization()(x) x = Activation('relu')(x) x = AveragePooling2D(pool_size=8)(x) y = Flatten()(x) outputs = Dense(num_classes, activation='softmax', kernel_initializer='he_normal')(y) # Instantiate model. model = Model(inputs=inputs, outputs=outputs) return model if version == 2: model = resnet_v2(input_shape=input_shape, depth=depth) else: model = resnet_v1(input_shape=input_shape, depth=depth) model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=lr_schedule(0)), metrics=['accuracy']) model.summary() print(model_type) # Prepare model model saving directory. save_dir = os.path.join(os.getcwd(), 'saved_models') model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type if not os.path.isdir(save_dir): os.makedirs(save_dir) filepath = os.path.join(save_dir, model_name) # Prepare callbacks for model saving and for learning rate adjustment. checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_acc', verbose=1, save_best_only=True) lr_scheduler = LearningRateScheduler(lr_schedule) lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6) callbacks = [checkpoint, lr_reducer, lr_scheduler] # Run training, with or without data augmentation. if not data_augmentation: print('Not using data augmentation.') model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), shuffle=True, callbacks=callbacks) else: print('Using real-time data augmentation.') # This will do preprocessing and realtime data augmentation: datagen = ImageDataGenerator( # set input mean to 0 over the dataset featurewise_center=False, # set each sample mean to 0 samplewise_center=False, # divide inputs by std of dataset featurewise_std_normalization=False, # divide each input by its std samplewise_std_normalization=False, # apply ZCA whitening zca_whitening=False, # epsilon for ZCA whitening zca_epsilon=1e-06, # randomly rotate images in the range (deg 0 to 180) rotation_range=0, # randomly shift images horizontally width_shift_range=0.1, # randomly shift images vertically height_shift_range=0.1, # set range for random shear shear_range=0., # set range for random zoom zoom_range=0., # set range for random channel shifts channel_shift_range=0., # set mode for filling points outside the input boundaries fill_mode='nearest', # value used for fill_mode = \"constant\" cval=0., # randomly flip images horizontal_flip=True, # randomly flip images vertical_flip=False, # set rescaling factor (applied before any other transformation) rescale=None, # set function that will be applied on each input preprocessing_function=None, # image data format, either \"channels_first\" or \"channels_last\" data_format=None, # fraction of images reserved for validation (strictly between 0 and 1) validation_split=0.0) # Compute quantities required for featurewise normalization # (std, mean, and principal components if ZCA whitening is applied). datagen.fit(x_train) # Fit the model on the batches generated by datagen.flow(). model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size), validation_data=(x_test, y_test), epochs=epochs, verbose=1, workers=4, callbacks=callbacks) # Score trained model. scores = model.evaluate(x_test, y_test, verbose=1) print('Test loss:', scores[0]) print('Test accuracy:', scores[1])","title":"Trains a ResNet on the CIFAR10 dataset."},{"location":"4-Examples/class_activation_maps/","text":"import cv2 import matplotlib.pyplot as plt from keras.models import Model import keras.applications.resnet50 as resnet from keras.layers import UpSampling2D, Conv2D # Please set an appropriate image file INPUT_IMG_FILE = \"dog.jpg\" ################################################################ # The following parameters can be changed to other models # that use global average pooling. # e.g.) InceptionResnetV2 / NASNetLarge NETWORK_INPUT_SIZE = 224 MODEL_CLASS = resnet.ResNet50 PREPROCESS_FN = resnet.preprocess_input LAST_CONV_LAYER = \"activation_49\" PRED_LAYER = \"fc1000\" ################################################################ # number of imagenet classes N_CLASSES = 1000 def load_img(fname, input_size, preprocess_fn): original_img = cv2.imread(fname)[:, :, ::-1] original_size = (original_img.shape[1], original_img.shape[0]) img = cv2.resize(original_img, (input_size, input_size)) imgs = np.expand_dims(preprocess_fn(img), axis=0) return imgs, original_img, original_size def get_cam_model(model_class, input_size=224, last_conv_layer=\"activation_49\", pred_layer=\"fc1000\"): model = model_class(input_shape=(input_size, input_size, 3)) final_params = model.get_layer(pred_layer).get_weights() final_params = (final_params[0].reshape( 1, 1, -1, N_CLASSES), final_params[1]) last_conv_output = model.get_layer(last_conv_layer).output x = UpSampling2D(size=(32, 32), interpolation=\"bilinear\")( last_conv_output) x = Conv2D(filters=N_CLASSES, kernel_size=( 1, 1), name=\"predictions_2\")(x) cam_model = Model(inputs=model.input, outputs=[model.output, x]) cam_model.get_layer(\"predictions_2\").set_weights(final_params) return cam_model def postprocess(preds, cams, top_k=1): idxes = np.argsort(preds[0])[-top_k:] class_activation_map = np.zeros_like(cams[0, :, :, 0]) for i in idxes: class_activation_map += cams[0, :, :, i] return class_activation_map # 1. load image imgs, original_img, original_size = load_img(INPUT_IMG_FILE, input_size=NETWORK_INPUT_SIZE, preprocess_fn=resnet.preprocess_input) # 2. prediction model = get_cam_model(resnet.ResNet50, NETWORK_INPUT_SIZE, LAST_CONV_LAYER, PRED_LAYER) preds, cams = model.predict(imgs) # 4. post processing class_activation_map = postprocess(preds, cams) # 5. plot image+cam to original size plt.imshow(original_img, alpha=0.5) plt.imshow(cv2.resize(class_activation_map, original_size), cmap='jet', alpha=0.5) plt.show()","title":"Class activation maps"},{"location":"4-Examples/conv_filter_visualization/","text":"Visualization of the filters of VGG16, via gradient ascent in input space. This script can run on CPU in a few minutes. Results example: from __future__ import print_function import time import numpy as np from PIL import Image as pil_image from keras.preprocessing.image import save_img from keras import layers from keras.applications import vgg16 from keras import backend as K def normalize(x): \"\"\"utility function to normalize a tensor. # Arguments x: An input tensor. # Returns The normalized input tensor. \"\"\" return x / (K.sqrt(K.mean(K.square(x))) + K.epsilon()) def deprocess_image(x): \"\"\"utility function to convert a float array into a valid uint8 image. # Arguments x: A numpy-array representing the generated image. # Returns A processed numpy-array, which could be used in e.g. imshow. \"\"\" # normalize tensor: center on 0., ensure std is 0.25 x -= x.mean() x /= (x.std() + K.epsilon()) x *= 0.25 # clip to [0, 1] x += 0.5 x = np.clip(x, 0, 1) # convert to RGB array x *= 255 if K.image_data_format() == 'channels_first': x = x.transpose((1, 2, 0)) x = np.clip(x, 0, 255).astype('uint8') return x def process_image(x, former): \"\"\"utility function to convert a valid uint8 image back into a float array. Reverses `deprocess_image`. # Arguments x: A numpy-array, which could be used in e.g. imshow. former: The former numpy-array. Need to determine the former mean and variance. # Returns A processed numpy-array representing the generated image. \"\"\" if K.image_data_format() == 'channels_first': x = x.transpose((2, 0, 1)) return (x / 255 - 0.5) * 4 * former.std() + former.mean() def visualize_layer(model, layer_name, step=1., epochs=15, upscaling_steps=9, upscaling_factor=1.2, output_dim=(412, 412), filter_range=(0, None)): \"\"\"Visualizes the most relevant filters of one conv-layer in a certain model. # Arguments model: The model containing layer_name. layer_name: The name of the layer to be visualized. Has to be a part of model. step: step size for gradient ascent. epochs: Number of iterations for gradient ascent. upscaling_steps: Number of upscaling steps. Starting image is in this case (80, 80). upscaling_factor: Factor to which to slowly upgrade the image towards output_dim. output_dim: [img_width, img_height] The output image dimensions. filter_range: Tupel[lower, upper] Determines the to be computed filter numbers. If the second value is `None`, the last filter will be inferred as the upper boundary. \"\"\" def _generate_filter_image(input_img, layer_output, filter_index): \"\"\"Generates image for one particular filter. # Arguments input_img: The input-image Tensor. layer_output: The output-image Tensor. filter_index: The to be processed filter number. Assumed to be valid. #Returns Either None if no image could be generated. or a tuple of the image (array) itself and the last loss. \"\"\" s_time = time.time() # we build a loss function that maximizes the activation # of the nth filter of the layer considered if K.image_data_format() == 'channels_first': loss = K.mean(layer_output[:, filter_index, :, :]) else: loss = K.mean(layer_output[:, :, :, filter_index]) # we compute the gradient of the input picture wrt this loss grads = K.gradients(loss, input_img)[0] # normalization trick: we normalize the gradient grads = normalize(grads) # this function returns the loss and grads given the input picture iterate = K.function([input_img], [loss, grads]) # we start from a gray image with some random noise intermediate_dim = tuple( int(x / (upscaling_factor ** upscaling_steps)) for x in output_dim) if K.image_data_format() == 'channels_first': input_img_data = np.random.random( (1, 3, intermediate_dim[0], intermediate_dim[1])) else: input_img_data = np.random.random( (1, intermediate_dim[0], intermediate_dim[1], 3)) input_img_data = (input_img_data - 0.5) * 20 + 128 # Slowly upscaling towards the original size prevents # a dominating high-frequency of the to visualized structure # as it would occur if we directly compute the 412d-image. # Behaves as a better starting point for each following dimension # and therefore avoids poor local minima for up in reversed(range(upscaling_steps)): # we run gradient ascent for e.g. 20 steps for _ in range(epochs): loss_value, grads_value = iterate([input_img_data]) input_img_data += grads_value * step # some filters get stuck to 0, we can skip them if loss_value <= K.epsilon(): return None # Calulate upscaled dimension intermediate_dim = tuple( int(x / (upscaling_factor ** up)) for x in output_dim) # Upscale img = deprocess_image(input_img_data[0]) img = np.array(pil_image.fromarray(img).resize(intermediate_dim, pil_image.BICUBIC)) input_img_data = [process_image(img, input_img_data[0])] # decode the resulting input image img = deprocess_image(input_img_data[0]) e_time = time.time() print('Costs of filter {:3}: {:5.0f} ( {:4.2f}s )'.format(filter_index, loss_value, e_time - s_time)) return img, loss_value def _draw_filters(filters, n=None): \"\"\"Draw the best filters in a nxn grid. # Arguments filters: A List of generated images and their corresponding losses for each processed filter. n: dimension of the grid. If none, the largest possible square will be used \"\"\" if n is None: n = int(np.floor(np.sqrt(len(filters)))) # the filters that have the highest loss are assumed to be better-looking. # we will only keep the top n*n filters. filters.sort(key=lambda x: x[1], reverse=True) filters = filters[:n * n] # build a black picture with enough space for # e.g. our 8 x 8 filters of size 412 x 412, with a 5px margin in between MARGIN = 5 width = n * output_dim[0] + (n - 1) * MARGIN height = n * output_dim[1] + (n - 1) * MARGIN stitched_filters = np.zeros((width, height, 3), dtype='uint8') # fill the picture with our saved filters for i in range(n): for j in range(n): img, _ = filters[i * n + j] width_margin = (output_dim[0] + MARGIN) * i height_margin = (output_dim[1] + MARGIN) * j stitched_filters[ width_margin: width_margin + output_dim[0], height_margin: height_margin + output_dim[1], :] = img # save the result to disk save_img('vgg_{0:}_{1:}x{1:}.png'.format(layer_name, n), stitched_filters) # this is the placeholder for the input images assert len(model.inputs) == 1 input_img = model.inputs[0] # get the symbolic outputs of each \"key\" layer (we gave them unique names). layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]]) output_layer = layer_dict[layer_name] assert isinstance(output_layer, layers.Conv2D) # Compute to be processed filter range filter_lower = filter_range[0] filter_upper = (filter_range[1] if filter_range[1] is not None else len(output_layer.get_weights()[1])) assert(filter_lower >= 0 and filter_upper <= len(output_layer.get_weights()[1]) and filter_upper > filter_lower) print('Compute filters {:} to {:}'.format(filter_lower, filter_upper)) # iterate through each filter and generate its corresponding image processed_filters = [] for f in range(filter_lower, filter_upper): img_loss = _generate_filter_image(input_img, output_layer.output, f) if img_loss is not None: processed_filters.append(img_loss) print('{} filter processed.'.format(len(processed_filters))) # Finally draw and store the best filters to disk _draw_filters(processed_filters) if __name__ == '__main__': # the name of the layer we want to visualize # (see model definition at keras/applications/vgg16.py) LAYER_NAME = 'block5_conv1' # build the VGG16 network with ImageNet weights vgg = vgg16.VGG16(weights='imagenet', include_top=False) print('Model loaded.') vgg.summary() # example function call visualize_layer(vgg, LAYER_NAME)","title":"Conv filter visualization"},{"location":"4-Examples/conv_filter_visualization/#visualization-of-the-filters-of-vgg16-via-gradient-ascent-in-input-space","text":"This script can run on CPU in a few minutes. Results example: from __future__ import print_function import time import numpy as np from PIL import Image as pil_image from keras.preprocessing.image import save_img from keras import layers from keras.applications import vgg16 from keras import backend as K def normalize(x): \"\"\"utility function to normalize a tensor. # Arguments x: An input tensor. # Returns The normalized input tensor. \"\"\" return x / (K.sqrt(K.mean(K.square(x))) + K.epsilon()) def deprocess_image(x): \"\"\"utility function to convert a float array into a valid uint8 image. # Arguments x: A numpy-array representing the generated image. # Returns A processed numpy-array, which could be used in e.g. imshow. \"\"\" # normalize tensor: center on 0., ensure std is 0.25 x -= x.mean() x /= (x.std() + K.epsilon()) x *= 0.25 # clip to [0, 1] x += 0.5 x = np.clip(x, 0, 1) # convert to RGB array x *= 255 if K.image_data_format() == 'channels_first': x = x.transpose((1, 2, 0)) x = np.clip(x, 0, 255).astype('uint8') return x def process_image(x, former): \"\"\"utility function to convert a valid uint8 image back into a float array. Reverses `deprocess_image`. # Arguments x: A numpy-array, which could be used in e.g. imshow. former: The former numpy-array. Need to determine the former mean and variance. # Returns A processed numpy-array representing the generated image. \"\"\" if K.image_data_format() == 'channels_first': x = x.transpose((2, 0, 1)) return (x / 255 - 0.5) * 4 * former.std() + former.mean() def visualize_layer(model, layer_name, step=1., epochs=15, upscaling_steps=9, upscaling_factor=1.2, output_dim=(412, 412), filter_range=(0, None)): \"\"\"Visualizes the most relevant filters of one conv-layer in a certain model. # Arguments model: The model containing layer_name. layer_name: The name of the layer to be visualized. Has to be a part of model. step: step size for gradient ascent. epochs: Number of iterations for gradient ascent. upscaling_steps: Number of upscaling steps. Starting image is in this case (80, 80). upscaling_factor: Factor to which to slowly upgrade the image towards output_dim. output_dim: [img_width, img_height] The output image dimensions. filter_range: Tupel[lower, upper] Determines the to be computed filter numbers. If the second value is `None`, the last filter will be inferred as the upper boundary. \"\"\" def _generate_filter_image(input_img, layer_output, filter_index): \"\"\"Generates image for one particular filter. # Arguments input_img: The input-image Tensor. layer_output: The output-image Tensor. filter_index: The to be processed filter number. Assumed to be valid. #Returns Either None if no image could be generated. or a tuple of the image (array) itself and the last loss. \"\"\" s_time = time.time() # we build a loss function that maximizes the activation # of the nth filter of the layer considered if K.image_data_format() == 'channels_first': loss = K.mean(layer_output[:, filter_index, :, :]) else: loss = K.mean(layer_output[:, :, :, filter_index]) # we compute the gradient of the input picture wrt this loss grads = K.gradients(loss, input_img)[0] # normalization trick: we normalize the gradient grads = normalize(grads) # this function returns the loss and grads given the input picture iterate = K.function([input_img], [loss, grads]) # we start from a gray image with some random noise intermediate_dim = tuple( int(x / (upscaling_factor ** upscaling_steps)) for x in output_dim) if K.image_data_format() == 'channels_first': input_img_data = np.random.random( (1, 3, intermediate_dim[0], intermediate_dim[1])) else: input_img_data = np.random.random( (1, intermediate_dim[0], intermediate_dim[1], 3)) input_img_data = (input_img_data - 0.5) * 20 + 128 # Slowly upscaling towards the original size prevents # a dominating high-frequency of the to visualized structure # as it would occur if we directly compute the 412d-image. # Behaves as a better starting point for each following dimension # and therefore avoids poor local minima for up in reversed(range(upscaling_steps)): # we run gradient ascent for e.g. 20 steps for _ in range(epochs): loss_value, grads_value = iterate([input_img_data]) input_img_data += grads_value * step # some filters get stuck to 0, we can skip them if loss_value <= K.epsilon(): return None # Calulate upscaled dimension intermediate_dim = tuple( int(x / (upscaling_factor ** up)) for x in output_dim) # Upscale img = deprocess_image(input_img_data[0]) img = np.array(pil_image.fromarray(img).resize(intermediate_dim, pil_image.BICUBIC)) input_img_data = [process_image(img, input_img_data[0])] # decode the resulting input image img = deprocess_image(input_img_data[0]) e_time = time.time() print('Costs of filter {:3}: {:5.0f} ( {:4.2f}s )'.format(filter_index, loss_value, e_time - s_time)) return img, loss_value def _draw_filters(filters, n=None): \"\"\"Draw the best filters in a nxn grid. # Arguments filters: A List of generated images and their corresponding losses for each processed filter. n: dimension of the grid. If none, the largest possible square will be used \"\"\" if n is None: n = int(np.floor(np.sqrt(len(filters)))) # the filters that have the highest loss are assumed to be better-looking. # we will only keep the top n*n filters. filters.sort(key=lambda x: x[1], reverse=True) filters = filters[:n * n] # build a black picture with enough space for # e.g. our 8 x 8 filters of size 412 x 412, with a 5px margin in between MARGIN = 5 width = n * output_dim[0] + (n - 1) * MARGIN height = n * output_dim[1] + (n - 1) * MARGIN stitched_filters = np.zeros((width, height, 3), dtype='uint8') # fill the picture with our saved filters for i in range(n): for j in range(n): img, _ = filters[i * n + j] width_margin = (output_dim[0] + MARGIN) * i height_margin = (output_dim[1] + MARGIN) * j stitched_filters[ width_margin: width_margin + output_dim[0], height_margin: height_margin + output_dim[1], :] = img # save the result to disk save_img('vgg_{0:}_{1:}x{1:}.png'.format(layer_name, n), stitched_filters) # this is the placeholder for the input images assert len(model.inputs) == 1 input_img = model.inputs[0] # get the symbolic outputs of each \"key\" layer (we gave them unique names). layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]]) output_layer = layer_dict[layer_name] assert isinstance(output_layer, layers.Conv2D) # Compute to be processed filter range filter_lower = filter_range[0] filter_upper = (filter_range[1] if filter_range[1] is not None else len(output_layer.get_weights()[1])) assert(filter_lower >= 0 and filter_upper <= len(output_layer.get_weights()[1]) and filter_upper > filter_lower) print('Compute filters {:} to {:}'.format(filter_lower, filter_upper)) # iterate through each filter and generate its corresponding image processed_filters = [] for f in range(filter_lower, filter_upper): img_loss = _generate_filter_image(input_img, output_layer.output, f) if img_loss is not None: processed_filters.append(img_loss) print('{} filter processed.'.format(len(processed_filters))) # Finally draw and store the best filters to disk _draw_filters(processed_filters) if __name__ == '__main__': # the name of the layer we want to visualize # (see model definition at keras/applications/vgg16.py) LAYER_NAME = 'block5_conv1' # build the VGG16 network with ImageNet weights vgg = vgg16.VGG16(weights='imagenet', include_top=False) print('Model loaded.') vgg.summary() # example function call visualize_layer(vgg, LAYER_NAME)","title":"Visualization of the filters of VGG16, via gradient ascent in input space."},{"location":"4-Examples/conv_lstm/","text":"This script demonstrates the use of a convolutional LSTM network. This network is used to predict the next frame of an artificially generated movie which contains moving squares. from keras.models import Sequential from keras.layers.convolutional import Conv3D from keras.layers.convolutional_recurrent import ConvLSTM2D from keras.layers.normalization import BatchNormalization import numpy as np import pylab as plt # We create a layer which take as input movies of shape # (n_frames, width, height, channels) and returns a movie # of identical shape. seq = Sequential() seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3), input_shape=(None, 40, 40, 1), padding='same', return_sequences=True)) seq.add(BatchNormalization()) seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3), padding='same', return_sequences=True)) seq.add(BatchNormalization()) seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3), padding='same', return_sequences=True)) seq.add(BatchNormalization()) seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3), padding='same', return_sequences=True)) seq.add(BatchNormalization()) seq.add(Conv3D(filters=1, kernel_size=(3, 3, 3), activation='sigmoid', padding='same', data_format='channels_last')) seq.compile(loss='binary_crossentropy', optimizer='adadelta') # Artificial data generation: # Generate movies with 3 to 7 moving squares inside. # The squares are of shape 1x1 or 2x2 pixels, # which move linearly over time. # For convenience we first create movies with bigger width and height (80x80) # and at the end we select a 40x40 window. def generate_movies(n_samples=1200, n_frames=15): row = 80 col = 80 noisy_movies = np.zeros((n_samples, n_frames, row, col, 1), dtype=np.float) shifted_movies = np.zeros((n_samples, n_frames, row, col, 1), dtype=np.float) for i in range(n_samples): # Add 3 to 7 moving squares n = np.random.randint(3, 8) for j in range(n): # Initial position xstart = np.random.randint(20, 60) ystart = np.random.randint(20, 60) # Direction of motion directionx = np.random.randint(0, 3) - 1 directiony = np.random.randint(0, 3) - 1 # Size of the square w = np.random.randint(2, 4) for t in range(n_frames): x_shift = xstart + directionx * t y_shift = ystart + directiony * t noisy_movies[i, t, x_shift - w: x_shift + w, y_shift - w: y_shift + w, 0] += 1 # Make it more robust by adding noise. # The idea is that if during inference, # the value of the pixel is not exactly one, # we need to train the network to be robust and still # consider it as a pixel belonging to a square. if np.random.randint(0, 2): noise_f = (-1)**np.random.randint(0, 2) noisy_movies[i, t, x_shift - w - 1: x_shift + w + 1, y_shift - w - 1: y_shift + w + 1, 0] += noise_f * 0.1 # Shift the ground truth by 1 x_shift = xstart + directionx * (t + 1) y_shift = ystart + directiony * (t + 1) shifted_movies[i, t, x_shift - w: x_shift + w, y_shift - w: y_shift + w, 0] += 1 # Cut to a 40x40 window noisy_movies = noisy_movies[::, ::, 20:60, 20:60, ::] shifted_movies = shifted_movies[::, ::, 20:60, 20:60, ::] noisy_movies[noisy_movies >= 1] = 1 shifted_movies[shifted_movies >= 1] = 1 return noisy_movies, shifted_movies # Train the network noisy_movies, shifted_movies = generate_movies(n_samples=1200) seq.fit(noisy_movies[:1000], shifted_movies[:1000], batch_size=10, epochs=300, validation_split=0.05) # Testing the network on one movie # feed it with the first 7 positions and then # predict the new positions which = 1004 track = noisy_movies[which][:7, ::, ::, ::] for j in range(16): new_pos = seq.predict(track[np.newaxis, ::, ::, ::, ::]) new = new_pos[::, -1, ::, ::, ::] track = np.concatenate((track, new), axis=0) # And then compare the predictions # to the ground truth track2 = noisy_movies[which][::, ::, ::, ::] for i in range(15): fig = plt.figure(figsize=(10, 5)) ax = fig.add_subplot(121) if i >= 7: ax.text(1, 3, 'Predictions !', fontsize=20, color='w') else: ax.text(1, 3, 'Initial trajectory', fontsize=20) toplot = track[i, ::, ::, 0] plt.imshow(toplot) ax = fig.add_subplot(122) plt.text(1, 3, 'Ground truth', fontsize=20) toplot = track2[i, ::, ::, 0] if i >= 2: toplot = shifted_movies[which][i - 1, ::, ::, 0] plt.imshow(toplot) plt.savefig('%i_animate.png' % (i + 1))","title":"Conv lstm"},{"location":"4-Examples/deep_dream/","text":"Deep Dreaming in Keras. Run the script with: python deep_dream.py path_to_your_base_image.jpg prefix_for_results e.g.: python deep_dream.py img/mypic.jpg results/dream from __future__ import print_function from keras.preprocessing.image import load_img, save_img, img_to_array import numpy as np import scipy import argparse from keras.applications import inception_v3 from keras import backend as K parser = argparse.ArgumentParser(description='Deep Dreams with Keras.') parser.add_argument('base_image_path', metavar='base', type=str, help='Path to the image to transform.') parser.add_argument('result_prefix', metavar='res_prefix', type=str, help='Prefix for the saved results.') args = parser.parse_args() base_image_path = args.base_image_path result_prefix = args.result_prefix # These are the names of the layers # for which we try to maximize activation, # as well as their weight in the final loss # we try to maximize. # You can tweak these setting to obtain new visual effects. settings = { 'features': { 'mixed2': 0.2, 'mixed3': 0.5, 'mixed4': 2., 'mixed5': 1.5, }, } def preprocess_image(image_path): # Util function to open, resize and format pictures # into appropriate tensors. img = load_img(image_path) img = img_to_array(img) img = np.expand_dims(img, axis=0) img = inception_v3.preprocess_input(img) return img def deprocess_image(x): # Util function to convert a tensor into a valid image. if K.image_data_format() == 'channels_first': x = x.reshape((3, x.shape[2], x.shape[3])) x = x.transpose((1, 2, 0)) else: x = x.reshape((x.shape[1], x.shape[2], 3)) x /= 2. x += 0.5 x *= 255. x = np.clip(x, 0, 255).astype('uint8') return x K.set_learning_phase(0) # Build the InceptionV3 network with our placeholder. # The model will be loaded with pre-trained ImageNet weights. model = inception_v3.InceptionV3(weights='imagenet', include_top=False) dream = model.input print('Model loaded.') # Get the symbolic outputs of each \"key\" layer (we gave them unique names). layer_dict = dict([(layer.name, layer) for layer in model.layers]) # Define the loss. loss = K.variable(0.) for layer_name in settings['features']: # Add the L2 norm of the features of a layer to the loss. if layer_name not in layer_dict: raise ValueError('Layer ' + layer_name + ' not found in model.') coeff = settings['features'][layer_name] x = layer_dict[layer_name].output # We avoid border artifacts by only involving non-border pixels in the loss. scaling = K.prod(K.cast(K.shape(x), 'float32')) if K.image_data_format() == 'channels_first': loss += coeff * K.sum(K.square(x[:, :, 2: -2, 2: -2])) / scaling else: loss += coeff * K.sum(K.square(x[:, 2: -2, 2: -2, :])) / scaling # Compute the gradients of the dream wrt the loss. grads = K.gradients(loss, dream)[0] # Normalize gradients. grads /= K.maximum(K.mean(K.abs(grads)), K.epsilon()) # Set up function to retrieve the value # of the loss and gradients given an input image. outputs = [loss, grads] fetch_loss_and_grads = K.function([dream], outputs) def eval_loss_and_grads(x): outs = fetch_loss_and_grads([x]) loss_value = outs[0] grad_values = outs[1] return loss_value, grad_values def resize_img(img, size): img = np.copy(img) if K.image_data_format() == 'channels_first': factors = (1, 1, float(size[0]) / img.shape[2], float(size[1]) / img.shape[3]) else: factors = (1, float(size[0]) / img.shape[1], float(size[1]) / img.shape[2], 1) return scipy.ndimage.zoom(img, factors, order=1) def gradient_ascent(x, iterations, step, max_loss=None): for i in range(iterations): loss_value, grad_values = eval_loss_and_grads(x) if max_loss is not None and loss_value > max_loss: break print('..Loss value at', i, ':', loss_value) x += step * grad_values return x \"\"\"Process: - Load the original image. - Define a number of processing scales (i.e. image shapes), from smallest to largest. - Resize the original image to the smallest scale. - For every scale, starting with the smallest (i.e. current one): - Run gradient ascent - Upscale image to the next scale - Reinject the detail that was lost at upscaling time - Stop when we are back to the original size. To obtain the detail lost during upscaling, we simply take the original image, shrink it down, upscale it, and compare the result to the (resized) original image. \"\"\" # Playing with these hyperparameters will also allow you to achieve new effects step = 0.01 # Gradient ascent step size num_octave = 3 # Number of scales at which to run gradient ascent octave_scale = 1.4 # Size ratio between scales iterations = 20 # Number of ascent steps per scale max_loss = 10. img = preprocess_image(base_image_path) if K.image_data_format() == 'channels_first': original_shape = img.shape[2:] else: original_shape = img.shape[1:3] successive_shapes = [original_shape] for i in range(1, num_octave): shape = tuple([int(dim / (octave_scale ** i)) for dim in original_shape]) successive_shapes.append(shape) successive_shapes = successive_shapes[::-1] original_img = np.copy(img) shrunk_original_img = resize_img(img, successive_shapes[0]) for shape in successive_shapes: print('Processing image shape', shape) img = resize_img(img, shape) img = gradient_ascent(img, iterations=iterations, step=step, max_loss=max_loss) upscaled_shrunk_original_img = resize_img(shrunk_original_img, shape) same_size_original = resize_img(original_img, shape) lost_detail = same_size_original - upscaled_shrunk_original_img img += lost_detail shrunk_original_img = resize_img(original_img, shape) save_img(result_prefix + '.png', deprocess_image(np.copy(img)))","title":"Deep dream"},{"location":"4-Examples/image_ocr/","text":"Optical character recognition This example uses a convolutional stack followed by a recurrent stack and a CTC logloss function to perform optical character recognition of generated text images. I have no evidence of whether it actually learns general shapes of text, or just is able to recognize all the different fonts thrown at it...the purpose is more to demonstrate CTC inside of Keras. Note that the font list may need to be updated for the particular OS in use. This starts off with 4 letter words. For the first 12 epochs, the difficulty is gradually increased using the TextImageGenerator class which is both a generator class for test/train data and a Keras callback class. After 20 epochs, longer sequences are thrown at it by recompiling the model to handle a wider image and rebuilding the word list to include two words separated by a space. The table below shows normalized edit distance values. Theano uses a slightly different CTC implementation, hence the different results. Epoch TF TH 10 0.027 0.064 15 0.038 0.035 20 0.043 0.045 25 0.014 0.019 This requires cairo and editdistance packages: pip install cairocffi pip install editdistance Created by Mike Henry https://github.com/mbhenry/ import os import itertools import codecs import re import datetime import cairocffi as cairo import editdistance import numpy as np from scipy import ndimage import pylab from keras import backend as K from keras.layers.convolutional import Conv2D, MaxPooling2D from keras.layers import Input, Dense, Activation from keras.layers import Reshape, Lambda from keras.layers.merge import add, concatenate from keras.models import Model from keras.layers.recurrent import GRU from keras.optimizers import SGD from keras.utils.data_utils import get_file from keras.preprocessing import image import keras.callbacks OUTPUT_DIR = 'image_ocr' # character classes and matching regex filter regex = r'^[a-z ]+$' alphabet = u'abcdefghijklmnopqrstuvwxyz ' np.random.seed(55) # this creates larger \"blotches\" of noise which look # more realistic than just adding gaussian noise # assumes greyscale with pixels ranging from 0 to 1 def speckle(img): severity = np.random.uniform(0, 0.6) blur = ndimage.gaussian_filter(np.random.randn(*img.shape) * severity, 1) img_speck = (img + blur) img_speck[img_speck > 1] = 1 img_speck[img_speck <= 0] = 0 return img_speck # paints the string in a random location the bounding box # also uses a random font, a slight random rotation, # and a random amount of speckle noise def paint_text(text, w, h, rotate=False, ud=False, multi_fonts=False): surface = cairo.ImageSurface(cairo.FORMAT_RGB24, w, h) with cairo.Context(surface) as context: context.set_source_rgb(1, 1, 1) # White context.paint() # this font list works in CentOS 7 if multi_fonts: fonts = [ 'Century Schoolbook', 'Courier', 'STIX', 'URW Chancery L', 'FreeMono'] context.select_font_face( np.random.choice(fonts), cairo.FONT_SLANT_NORMAL, np.random.choice([cairo.FONT_WEIGHT_BOLD, cairo.FONT_WEIGHT_NORMAL])) else: context.select_font_face('Courier', cairo.FONT_SLANT_NORMAL, cairo.FONT_WEIGHT_BOLD) context.set_font_size(25) box = context.text_extents(text) border_w_h = (4, 4) if box[2] > (w - 2 * border_w_h[1]) or box[3] > (h - 2 * border_w_h[0]): raise IOError(('Could not fit string into image.' 'Max char count is too large for given image width.')) # teach the RNN translational invariance by # fitting text box randomly on canvas, with some room to rotate max_shift_x = w - box[2] - border_w_h[0] max_shift_y = h - box[3] - border_w_h[1] top_left_x = np.random.randint(0, int(max_shift_x)) if ud: top_left_y = np.random.randint(0, int(max_shift_y)) else: top_left_y = h // 2 context.move_to(top_left_x - int(box[0]), top_left_y - int(box[1])) context.set_source_rgb(0, 0, 0) context.show_text(text) buf = surface.get_data() a = np.frombuffer(buf, np.uint8) a.shape = (h, w, 4) a = a[:, :, 0] # grab single channel a = a.astype(np.float32) / 255 a = np.expand_dims(a, 0) if rotate: a = image.random_rotation(a, 3 * (w - top_left_x) / w + 1) a = speckle(a) return a def shuffle_mats_or_lists(matrix_list, stop_ind=None): ret = [] assert all([len(i) == len(matrix_list[0]) for i in matrix_list]) len_val = len(matrix_list[0]) if stop_ind is None: stop_ind = len_val assert stop_ind <= len_val a = list(range(stop_ind)) np.random.shuffle(a) a += list(range(stop_ind, len_val)) for mat in matrix_list: if isinstance(mat, np.ndarray): ret.append(mat[a]) elif isinstance(mat, list): ret.append([mat[i] for i in a]) else: raise TypeError('`shuffle_mats_or_lists` only supports ' 'numpy.array and list objects.') return ret # Translation of characters to unique integer values def text_to_labels(text): ret = [] for char in text: ret.append(alphabet.find(char)) return ret # Reverse translation of numerical classes back to characters def labels_to_text(labels): ret = [] for c in labels: if c == len(alphabet): # CTC Blank ret.append(\"\") else: ret.append(alphabet[c]) return \"\".join(ret) # only a-z and space..probably not to difficult # to expand to uppercase and symbols def is_valid_str(in_str): search = re.compile(regex, re.UNICODE).search return bool(search(in_str)) # Uses generator functions to supply train/test with # data. Image renderings and text are created on the fly # each time with random perturbations class TextImageGenerator(keras.callbacks.Callback): def __init__(self, monogram_file, bigram_file, minibatch_size, img_w, img_h, downsample_factor, val_split, absolute_max_string_len=16): self.minibatch_size = minibatch_size self.img_w = img_w self.img_h = img_h self.monogram_file = monogram_file self.bigram_file = bigram_file self.downsample_factor = downsample_factor self.val_split = val_split self.blank_label = self.get_output_size() - 1 self.absolute_max_string_len = absolute_max_string_len def get_output_size(self): return len(alphabet) + 1 # num_words can be independent of the epoch size due to the use of generators # as max_string_len grows, num_words can grow def build_word_list(self, num_words, max_string_len=None, mono_fraction=0.5): assert max_string_len <= self.absolute_max_string_len assert num_words % self.minibatch_size == 0 assert (self.val_split * num_words) % self.minibatch_size == 0 self.num_words = num_words self.string_list = [''] * self.num_words tmp_string_list = [] self.max_string_len = max_string_len self.Y_data = np.ones([self.num_words, self.absolute_max_string_len]) * -1 self.X_text = [] self.Y_len = [0] * self.num_words def _is_length_of_word_valid(word): return (max_string_len == -1 or max_string_len is None or len(word) <= max_string_len) # monogram file is sorted by frequency in english speech with codecs.open(self.monogram_file, mode='r', encoding='utf-8') as f: for line in f: if len(tmp_string_list) == int(self.num_words * mono_fraction): break word = line.rstrip() if _is_length_of_word_valid(word): tmp_string_list.append(word) # bigram file contains common word pairings in english speech with codecs.open(self.bigram_file, mode='r', encoding='utf-8') as f: lines = f.readlines() for line in lines: if len(tmp_string_list) == self.num_words: break columns = line.lower().split() word = columns[0] + ' ' + columns[1] if is_valid_str(word) and _is_length_of_word_valid(word): tmp_string_list.append(word) if len(tmp_string_list) != self.num_words: raise IOError('Could not pull enough words' 'from supplied monogram and bigram files.') # interlace to mix up the easy and hard words self.string_list[::2] = tmp_string_list[:self.num_words // 2] self.string_list[1::2] = tmp_string_list[self.num_words // 2:] for i, word in enumerate(self.string_list): self.Y_len[i] = len(word) self.Y_data[i, 0:len(word)] = text_to_labels(word) self.X_text.append(word) self.Y_len = np.expand_dims(np.array(self.Y_len), 1) self.cur_val_index = self.val_split self.cur_train_index = 0 # each time an image is requested from train/val/test, a new random # painting of the text is performed def get_batch(self, index, size, train): # width and height are backwards from typical Keras convention # because width is the time dimension when it gets fed into the RNN if K.image_data_format() == 'channels_first': X_data = np.ones([size, 1, self.img_w, self.img_h]) else: X_data = np.ones([size, self.img_w, self.img_h, 1]) labels = np.ones([size, self.absolute_max_string_len]) input_length = np.zeros([size, 1]) label_length = np.zeros([size, 1]) source_str = [] for i in range(size): # Mix in some blank inputs. This seems to be important for # achieving translational invariance if train and i > size - 4: if K.image_data_format() == 'channels_first': X_data[i, 0, 0:self.img_w, :] = self.paint_func('')[0, :, :].T else: X_data[i, 0:self.img_w, :, 0] = self.paint_func('',)[0, :, :].T labels[i, 0] = self.blank_label input_length[i] = self.img_w // self.downsample_factor - 2 label_length[i] = 1 source_str.append('') else: if K.image_data_format() == 'channels_first': X_data[i, 0, 0:self.img_w, :] = ( self.paint_func(self.X_text[index + i])[0, :, :].T) else: X_data[i, 0:self.img_w, :, 0] = ( self.paint_func(self.X_text[index + i])[0, :, :].T) labels[i, :] = self.Y_data[index + i] input_length[i] = self.img_w // self.downsample_factor - 2 label_length[i] = self.Y_len[index + i] source_str.append(self.X_text[index + i]) inputs = {'the_input': X_data, 'the_labels': labels, 'input_length': input_length, 'label_length': label_length, 'source_str': source_str # used for visualization only } outputs = {'ctc': np.zeros([size])} # dummy data for dummy loss function return (inputs, outputs) def next_train(self): while 1: ret = self.get_batch(self.cur_train_index, self.minibatch_size, train=True) self.cur_train_index += self.minibatch_size if self.cur_train_index >= self.val_split: self.cur_train_index = self.cur_train_index % 32 (self.X_text, self.Y_data, self.Y_len) = shuffle_mats_or_lists( [self.X_text, self.Y_data, self.Y_len], self.val_split) yield ret def next_val(self): while 1: ret = self.get_batch(self.cur_val_index, self.minibatch_size, train=False) self.cur_val_index += self.minibatch_size if self.cur_val_index >= self.num_words: self.cur_val_index = self.val_split + self.cur_val_index % 32 yield ret def on_train_begin(self, logs={}): self.build_word_list(16000, 4, 1) self.paint_func = lambda text: paint_text( text, self.img_w, self.img_h, rotate=False, ud=False, multi_fonts=False) def on_epoch_begin(self, epoch, logs={}): # rebind the paint function to implement curriculum learning if 3 <= epoch < 6: self.paint_func = lambda text: paint_text( text, self.img_w, self.img_h, rotate=False, ud=True, multi_fonts=False) elif 6 <= epoch < 9: self.paint_func = lambda text: paint_text( text, self.img_w, self.img_h, rotate=False, ud=True, multi_fonts=True) elif epoch >= 9: self.paint_func = lambda text: paint_text( text, self.img_w, self.img_h, rotate=True, ud=True, multi_fonts=True) if epoch >= 21 and self.max_string_len < 12: self.build_word_list(32000, 12, 0.5) # the actual loss calc occurs here despite it not being # an internal Keras loss function def ctc_lambda_func(args): y_pred, labels, input_length, label_length = args # the 2 is critical here since the first couple outputs of the RNN # tend to be garbage: y_pred = y_pred[:, 2:, :] return K.ctc_batch_cost(labels, y_pred, input_length, label_length) # For a real OCR application, this should be beam search with a dictionary # and language model. For this example, best path is sufficient. def decode_batch(test_func, word_batch): out = test_func([word_batch])[0] ret = [] for j in range(out.shape[0]): out_best = list(np.argmax(out[j, 2:], 1)) out_best = [k for k, g in itertools.groupby(out_best)] outstr = labels_to_text(out_best) ret.append(outstr) return ret class VizCallback(keras.callbacks.Callback): def __init__(self, run_name, test_func, text_img_gen, num_display_words=6): self.test_func = test_func self.output_dir = os.path.join( OUTPUT_DIR, run_name) self.text_img_gen = text_img_gen self.num_display_words = num_display_words if not os.path.exists(self.output_dir): os.makedirs(self.output_dir) def show_edit_distance(self, num): num_left = num mean_norm_ed = 0.0 mean_ed = 0.0 while num_left > 0: word_batch = next(self.text_img_gen)[0] num_proc = min(word_batch['the_input'].shape[0], num_left) decoded_res = decode_batch(self.test_func, word_batch['the_input'][0:num_proc]) for j in range(num_proc): edit_dist = editdistance.eval(decoded_res[j], word_batch['source_str'][j]) mean_ed += float(edit_dist) mean_norm_ed += float(edit_dist) / len(word_batch['source_str'][j]) num_left -= num_proc mean_norm_ed = mean_norm_ed / num mean_ed = mean_ed / num print('\\nOut of %d samples: Mean edit distance:' '%.3f Mean normalized edit distance: %0.3f' % (num, mean_ed, mean_norm_ed)) def on_epoch_end(self, epoch, logs={}): self.model.save_weights( os.path.join(self.output_dir, 'weights%02d.h5' % (epoch))) self.show_edit_distance(256) word_batch = next(self.text_img_gen)[0] res = decode_batch(self.test_func, word_batch['the_input'][0:self.num_display_words]) if word_batch['the_input'][0].shape[0] < 256: cols = 2 else: cols = 1 for i in range(self.num_display_words): pylab.subplot(self.num_display_words // cols, cols, i + 1) if K.image_data_format() == 'channels_first': the_input = word_batch['the_input'][i, 0, :, :] else: the_input = word_batch['the_input'][i, :, :, 0] pylab.imshow(the_input.T, cmap='Greys_r') pylab.xlabel( 'Truth = \\'%s\\'\\nDecoded = \\'%s\\'' % (word_batch['source_str'][i], res[i])) fig = pylab.gcf() fig.set_size_inches(10, 13) pylab.savefig(os.path.join(self.output_dir, 'e%02d.png' % (epoch))) pylab.close() def train(run_name, start_epoch, stop_epoch, img_w): # Input Parameters img_h = 64 words_per_epoch = 16000 val_split = 0.2 val_words = int(words_per_epoch * (val_split)) # Network parameters conv_filters = 16 kernel_size = (3, 3) pool_size = 2 time_dense_size = 32 rnn_size = 512 minibatch_size = 32 if K.image_data_format() == 'channels_first': input_shape = (1, img_w, img_h) else: input_shape = (img_w, img_h, 1) fdir = os.path.dirname( get_file('wordlists.tgz', origin='http://www.mythic-ai.com/datasets/wordlists.tgz', untar=True)) img_gen = TextImageGenerator( monogram_file=os.path.join(fdir, 'wordlist_mono_clean.txt'), bigram_file=os.path.join(fdir, 'wordlist_bi_clean.txt'), minibatch_size=minibatch_size, img_w=img_w, img_h=img_h, downsample_factor=(pool_size ** 2), val_split=words_per_epoch - val_words) act = 'relu' input_data = Input(name='the_input', shape=input_shape, dtype='float32') inner = Conv2D(conv_filters, kernel_size, padding='same', activation=act, kernel_initializer='he_normal', name='conv1')(input_data) inner = MaxPooling2D(pool_size=(pool_size, pool_size), name='max1')(inner) inner = Conv2D(conv_filters, kernel_size, padding='same', activation=act, kernel_initializer='he_normal', name='conv2')(inner) inner = MaxPooling2D(pool_size=(pool_size, pool_size), name='max2')(inner) conv_to_rnn_dims = (img_w // (pool_size ** 2), (img_h // (pool_size ** 2)) * conv_filters) inner = Reshape(target_shape=conv_to_rnn_dims, name='reshape')(inner) # cuts down input size going into RNN: inner = Dense(time_dense_size, activation=act, name='dense1')(inner) # Two layers of bidirectional GRUs # GRU seems to work as well, if not better than LSTM: gru_1 = GRU(rnn_size, return_sequences=True, kernel_initializer='he_normal', name='gru1')(inner) gru_1b = GRU(rnn_size, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='gru1_b')(inner) gru1_merged = add([gru_1, gru_1b]) gru_2 = GRU(rnn_size, return_sequences=True, kernel_initializer='he_normal', name='gru2')(gru1_merged) gru_2b = GRU(rnn_size, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='gru2_b')(gru1_merged) # transforms RNN output to character activations: inner = Dense(img_gen.get_output_size(), kernel_initializer='he_normal', name='dense2')(concatenate([gru_2, gru_2b])) y_pred = Activation('softmax', name='softmax')(inner) Model(inputs=input_data, outputs=y_pred).summary() labels = Input(name='the_labels', shape=[img_gen.absolute_max_string_len], dtype='float32') input_length = Input(name='input_length', shape=[1], dtype='int64') label_length = Input(name='label_length', shape=[1], dtype='int64') # Keras doesn't currently support loss funcs with extra parameters # so CTC loss is implemented in a lambda layer loss_out = Lambda( ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length]) # clipnorm seems to speeds up convergence sgd = SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5) model = Model(inputs=[input_data, labels, input_length, label_length], outputs=loss_out) # the loss calc occurs elsewhere, so use a dummy lambda func for the loss model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=sgd) if start_epoch > 0: weight_file = os.path.join( OUTPUT_DIR, os.path.join(run_name, 'weights%02d.h5' % (start_epoch - 1))) model.load_weights(weight_file) # captures output of softmax so we can decode the output during visualization test_func = K.function([input_data], [y_pred]) viz_cb = VizCallback(run_name, test_func, img_gen.next_val()) model.fit_generator( generator=img_gen.next_train(), steps_per_epoch=(words_per_epoch - val_words) // minibatch_size, epochs=stop_epoch, validation_data=img_gen.next_val(), validation_steps=val_words // minibatch_size, callbacks=[viz_cb, img_gen], initial_epoch=start_epoch) if __name__ == '__main__': run_name = datetime.datetime.now().strftime('%Y:%m:%d:%H:%M:%S') train(run_name, 0, 20, 128) # increase to wider images and start at epoch 20. # The learned weights are reloaded train(run_name, 20, 25, 512)","title":"Optical character recognition"},{"location":"4-Examples/image_ocr/#optical-character-recognition","text":"This example uses a convolutional stack followed by a recurrent stack and a CTC logloss function to perform optical character recognition of generated text images. I have no evidence of whether it actually learns general shapes of text, or just is able to recognize all the different fonts thrown at it...the purpose is more to demonstrate CTC inside of Keras. Note that the font list may need to be updated for the particular OS in use. This starts off with 4 letter words. For the first 12 epochs, the difficulty is gradually increased using the TextImageGenerator class which is both a generator class for test/train data and a Keras callback class. After 20 epochs, longer sequences are thrown at it by recompiling the model to handle a wider image and rebuilding the word list to include two words separated by a space. The table below shows normalized edit distance values. Theano uses a slightly different CTC implementation, hence the different results. Epoch TF TH 10 0.027 0.064 15 0.038 0.035 20 0.043 0.045 25 0.014 0.019 This requires cairo and editdistance packages: pip install cairocffi pip install editdistance Created by Mike Henry https://github.com/mbhenry/ import os import itertools import codecs import re import datetime import cairocffi as cairo import editdistance import numpy as np from scipy import ndimage import pylab from keras import backend as K from keras.layers.convolutional import Conv2D, MaxPooling2D from keras.layers import Input, Dense, Activation from keras.layers import Reshape, Lambda from keras.layers.merge import add, concatenate from keras.models import Model from keras.layers.recurrent import GRU from keras.optimizers import SGD from keras.utils.data_utils import get_file from keras.preprocessing import image import keras.callbacks OUTPUT_DIR = 'image_ocr' # character classes and matching regex filter regex = r'^[a-z ]+$' alphabet = u'abcdefghijklmnopqrstuvwxyz ' np.random.seed(55) # this creates larger \"blotches\" of noise which look # more realistic than just adding gaussian noise # assumes greyscale with pixels ranging from 0 to 1 def speckle(img): severity = np.random.uniform(0, 0.6) blur = ndimage.gaussian_filter(np.random.randn(*img.shape) * severity, 1) img_speck = (img + blur) img_speck[img_speck > 1] = 1 img_speck[img_speck <= 0] = 0 return img_speck # paints the string in a random location the bounding box # also uses a random font, a slight random rotation, # and a random amount of speckle noise def paint_text(text, w, h, rotate=False, ud=False, multi_fonts=False): surface = cairo.ImageSurface(cairo.FORMAT_RGB24, w, h) with cairo.Context(surface) as context: context.set_source_rgb(1, 1, 1) # White context.paint() # this font list works in CentOS 7 if multi_fonts: fonts = [ 'Century Schoolbook', 'Courier', 'STIX', 'URW Chancery L', 'FreeMono'] context.select_font_face( np.random.choice(fonts), cairo.FONT_SLANT_NORMAL, np.random.choice([cairo.FONT_WEIGHT_BOLD, cairo.FONT_WEIGHT_NORMAL])) else: context.select_font_face('Courier', cairo.FONT_SLANT_NORMAL, cairo.FONT_WEIGHT_BOLD) context.set_font_size(25) box = context.text_extents(text) border_w_h = (4, 4) if box[2] > (w - 2 * border_w_h[1]) or box[3] > (h - 2 * border_w_h[0]): raise IOError(('Could not fit string into image.' 'Max char count is too large for given image width.')) # teach the RNN translational invariance by # fitting text box randomly on canvas, with some room to rotate max_shift_x = w - box[2] - border_w_h[0] max_shift_y = h - box[3] - border_w_h[1] top_left_x = np.random.randint(0, int(max_shift_x)) if ud: top_left_y = np.random.randint(0, int(max_shift_y)) else: top_left_y = h // 2 context.move_to(top_left_x - int(box[0]), top_left_y - int(box[1])) context.set_source_rgb(0, 0, 0) context.show_text(text) buf = surface.get_data() a = np.frombuffer(buf, np.uint8) a.shape = (h, w, 4) a = a[:, :, 0] # grab single channel a = a.astype(np.float32) / 255 a = np.expand_dims(a, 0) if rotate: a = image.random_rotation(a, 3 * (w - top_left_x) / w + 1) a = speckle(a) return a def shuffle_mats_or_lists(matrix_list, stop_ind=None): ret = [] assert all([len(i) == len(matrix_list[0]) for i in matrix_list]) len_val = len(matrix_list[0]) if stop_ind is None: stop_ind = len_val assert stop_ind <= len_val a = list(range(stop_ind)) np.random.shuffle(a) a += list(range(stop_ind, len_val)) for mat in matrix_list: if isinstance(mat, np.ndarray): ret.append(mat[a]) elif isinstance(mat, list): ret.append([mat[i] for i in a]) else: raise TypeError('`shuffle_mats_or_lists` only supports ' 'numpy.array and list objects.') return ret # Translation of characters to unique integer values def text_to_labels(text): ret = [] for char in text: ret.append(alphabet.find(char)) return ret # Reverse translation of numerical classes back to characters def labels_to_text(labels): ret = [] for c in labels: if c == len(alphabet): # CTC Blank ret.append(\"\") else: ret.append(alphabet[c]) return \"\".join(ret) # only a-z and space..probably not to difficult # to expand to uppercase and symbols def is_valid_str(in_str): search = re.compile(regex, re.UNICODE).search return bool(search(in_str)) # Uses generator functions to supply train/test with # data. Image renderings and text are created on the fly # each time with random perturbations class TextImageGenerator(keras.callbacks.Callback): def __init__(self, monogram_file, bigram_file, minibatch_size, img_w, img_h, downsample_factor, val_split, absolute_max_string_len=16): self.minibatch_size = minibatch_size self.img_w = img_w self.img_h = img_h self.monogram_file = monogram_file self.bigram_file = bigram_file self.downsample_factor = downsample_factor self.val_split = val_split self.blank_label = self.get_output_size() - 1 self.absolute_max_string_len = absolute_max_string_len def get_output_size(self): return len(alphabet) + 1 # num_words can be independent of the epoch size due to the use of generators # as max_string_len grows, num_words can grow def build_word_list(self, num_words, max_string_len=None, mono_fraction=0.5): assert max_string_len <= self.absolute_max_string_len assert num_words % self.minibatch_size == 0 assert (self.val_split * num_words) % self.minibatch_size == 0 self.num_words = num_words self.string_list = [''] * self.num_words tmp_string_list = [] self.max_string_len = max_string_len self.Y_data = np.ones([self.num_words, self.absolute_max_string_len]) * -1 self.X_text = [] self.Y_len = [0] * self.num_words def _is_length_of_word_valid(word): return (max_string_len == -1 or max_string_len is None or len(word) <= max_string_len) # monogram file is sorted by frequency in english speech with codecs.open(self.monogram_file, mode='r', encoding='utf-8') as f: for line in f: if len(tmp_string_list) == int(self.num_words * mono_fraction): break word = line.rstrip() if _is_length_of_word_valid(word): tmp_string_list.append(word) # bigram file contains common word pairings in english speech with codecs.open(self.bigram_file, mode='r', encoding='utf-8') as f: lines = f.readlines() for line in lines: if len(tmp_string_list) == self.num_words: break columns = line.lower().split() word = columns[0] + ' ' + columns[1] if is_valid_str(word) and _is_length_of_word_valid(word): tmp_string_list.append(word) if len(tmp_string_list) != self.num_words: raise IOError('Could not pull enough words' 'from supplied monogram and bigram files.') # interlace to mix up the easy and hard words self.string_list[::2] = tmp_string_list[:self.num_words // 2] self.string_list[1::2] = tmp_string_list[self.num_words // 2:] for i, word in enumerate(self.string_list): self.Y_len[i] = len(word) self.Y_data[i, 0:len(word)] = text_to_labels(word) self.X_text.append(word) self.Y_len = np.expand_dims(np.array(self.Y_len), 1) self.cur_val_index = self.val_split self.cur_train_index = 0 # each time an image is requested from train/val/test, a new random # painting of the text is performed def get_batch(self, index, size, train): # width and height are backwards from typical Keras convention # because width is the time dimension when it gets fed into the RNN if K.image_data_format() == 'channels_first': X_data = np.ones([size, 1, self.img_w, self.img_h]) else: X_data = np.ones([size, self.img_w, self.img_h, 1]) labels = np.ones([size, self.absolute_max_string_len]) input_length = np.zeros([size, 1]) label_length = np.zeros([size, 1]) source_str = [] for i in range(size): # Mix in some blank inputs. This seems to be important for # achieving translational invariance if train and i > size - 4: if K.image_data_format() == 'channels_first': X_data[i, 0, 0:self.img_w, :] = self.paint_func('')[0, :, :].T else: X_data[i, 0:self.img_w, :, 0] = self.paint_func('',)[0, :, :].T labels[i, 0] = self.blank_label input_length[i] = self.img_w // self.downsample_factor - 2 label_length[i] = 1 source_str.append('') else: if K.image_data_format() == 'channels_first': X_data[i, 0, 0:self.img_w, :] = ( self.paint_func(self.X_text[index + i])[0, :, :].T) else: X_data[i, 0:self.img_w, :, 0] = ( self.paint_func(self.X_text[index + i])[0, :, :].T) labels[i, :] = self.Y_data[index + i] input_length[i] = self.img_w // self.downsample_factor - 2 label_length[i] = self.Y_len[index + i] source_str.append(self.X_text[index + i]) inputs = {'the_input': X_data, 'the_labels': labels, 'input_length': input_length, 'label_length': label_length, 'source_str': source_str # used for visualization only } outputs = {'ctc': np.zeros([size])} # dummy data for dummy loss function return (inputs, outputs) def next_train(self): while 1: ret = self.get_batch(self.cur_train_index, self.minibatch_size, train=True) self.cur_train_index += self.minibatch_size if self.cur_train_index >= self.val_split: self.cur_train_index = self.cur_train_index % 32 (self.X_text, self.Y_data, self.Y_len) = shuffle_mats_or_lists( [self.X_text, self.Y_data, self.Y_len], self.val_split) yield ret def next_val(self): while 1: ret = self.get_batch(self.cur_val_index, self.minibatch_size, train=False) self.cur_val_index += self.minibatch_size if self.cur_val_index >= self.num_words: self.cur_val_index = self.val_split + self.cur_val_index % 32 yield ret def on_train_begin(self, logs={}): self.build_word_list(16000, 4, 1) self.paint_func = lambda text: paint_text( text, self.img_w, self.img_h, rotate=False, ud=False, multi_fonts=False) def on_epoch_begin(self, epoch, logs={}): # rebind the paint function to implement curriculum learning if 3 <= epoch < 6: self.paint_func = lambda text: paint_text( text, self.img_w, self.img_h, rotate=False, ud=True, multi_fonts=False) elif 6 <= epoch < 9: self.paint_func = lambda text: paint_text( text, self.img_w, self.img_h, rotate=False, ud=True, multi_fonts=True) elif epoch >= 9: self.paint_func = lambda text: paint_text( text, self.img_w, self.img_h, rotate=True, ud=True, multi_fonts=True) if epoch >= 21 and self.max_string_len < 12: self.build_word_list(32000, 12, 0.5) # the actual loss calc occurs here despite it not being # an internal Keras loss function def ctc_lambda_func(args): y_pred, labels, input_length, label_length = args # the 2 is critical here since the first couple outputs of the RNN # tend to be garbage: y_pred = y_pred[:, 2:, :] return K.ctc_batch_cost(labels, y_pred, input_length, label_length) # For a real OCR application, this should be beam search with a dictionary # and language model. For this example, best path is sufficient. def decode_batch(test_func, word_batch): out = test_func([word_batch])[0] ret = [] for j in range(out.shape[0]): out_best = list(np.argmax(out[j, 2:], 1)) out_best = [k for k, g in itertools.groupby(out_best)] outstr = labels_to_text(out_best) ret.append(outstr) return ret class VizCallback(keras.callbacks.Callback): def __init__(self, run_name, test_func, text_img_gen, num_display_words=6): self.test_func = test_func self.output_dir = os.path.join( OUTPUT_DIR, run_name) self.text_img_gen = text_img_gen self.num_display_words = num_display_words if not os.path.exists(self.output_dir): os.makedirs(self.output_dir) def show_edit_distance(self, num): num_left = num mean_norm_ed = 0.0 mean_ed = 0.0 while num_left > 0: word_batch = next(self.text_img_gen)[0] num_proc = min(word_batch['the_input'].shape[0], num_left) decoded_res = decode_batch(self.test_func, word_batch['the_input'][0:num_proc]) for j in range(num_proc): edit_dist = editdistance.eval(decoded_res[j], word_batch['source_str'][j]) mean_ed += float(edit_dist) mean_norm_ed += float(edit_dist) / len(word_batch['source_str'][j]) num_left -= num_proc mean_norm_ed = mean_norm_ed / num mean_ed = mean_ed / num print('\\nOut of %d samples: Mean edit distance:' '%.3f Mean normalized edit distance: %0.3f' % (num, mean_ed, mean_norm_ed)) def on_epoch_end(self, epoch, logs={}): self.model.save_weights( os.path.join(self.output_dir, 'weights%02d.h5' % (epoch))) self.show_edit_distance(256) word_batch = next(self.text_img_gen)[0] res = decode_batch(self.test_func, word_batch['the_input'][0:self.num_display_words]) if word_batch['the_input'][0].shape[0] < 256: cols = 2 else: cols = 1 for i in range(self.num_display_words): pylab.subplot(self.num_display_words // cols, cols, i + 1) if K.image_data_format() == 'channels_first': the_input = word_batch['the_input'][i, 0, :, :] else: the_input = word_batch['the_input'][i, :, :, 0] pylab.imshow(the_input.T, cmap='Greys_r') pylab.xlabel( 'Truth = \\'%s\\'\\nDecoded = \\'%s\\'' % (word_batch['source_str'][i], res[i])) fig = pylab.gcf() fig.set_size_inches(10, 13) pylab.savefig(os.path.join(self.output_dir, 'e%02d.png' % (epoch))) pylab.close() def train(run_name, start_epoch, stop_epoch, img_w): # Input Parameters img_h = 64 words_per_epoch = 16000 val_split = 0.2 val_words = int(words_per_epoch * (val_split)) # Network parameters conv_filters = 16 kernel_size = (3, 3) pool_size = 2 time_dense_size = 32 rnn_size = 512 minibatch_size = 32 if K.image_data_format() == 'channels_first': input_shape = (1, img_w, img_h) else: input_shape = (img_w, img_h, 1) fdir = os.path.dirname( get_file('wordlists.tgz', origin='http://www.mythic-ai.com/datasets/wordlists.tgz', untar=True)) img_gen = TextImageGenerator( monogram_file=os.path.join(fdir, 'wordlist_mono_clean.txt'), bigram_file=os.path.join(fdir, 'wordlist_bi_clean.txt'), minibatch_size=minibatch_size, img_w=img_w, img_h=img_h, downsample_factor=(pool_size ** 2), val_split=words_per_epoch - val_words) act = 'relu' input_data = Input(name='the_input', shape=input_shape, dtype='float32') inner = Conv2D(conv_filters, kernel_size, padding='same', activation=act, kernel_initializer='he_normal', name='conv1')(input_data) inner = MaxPooling2D(pool_size=(pool_size, pool_size), name='max1')(inner) inner = Conv2D(conv_filters, kernel_size, padding='same', activation=act, kernel_initializer='he_normal', name='conv2')(inner) inner = MaxPooling2D(pool_size=(pool_size, pool_size), name='max2')(inner) conv_to_rnn_dims = (img_w // (pool_size ** 2), (img_h // (pool_size ** 2)) * conv_filters) inner = Reshape(target_shape=conv_to_rnn_dims, name='reshape')(inner) # cuts down input size going into RNN: inner = Dense(time_dense_size, activation=act, name='dense1')(inner) # Two layers of bidirectional GRUs # GRU seems to work as well, if not better than LSTM: gru_1 = GRU(rnn_size, return_sequences=True, kernel_initializer='he_normal', name='gru1')(inner) gru_1b = GRU(rnn_size, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='gru1_b')(inner) gru1_merged = add([gru_1, gru_1b]) gru_2 = GRU(rnn_size, return_sequences=True, kernel_initializer='he_normal', name='gru2')(gru1_merged) gru_2b = GRU(rnn_size, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='gru2_b')(gru1_merged) # transforms RNN output to character activations: inner = Dense(img_gen.get_output_size(), kernel_initializer='he_normal', name='dense2')(concatenate([gru_2, gru_2b])) y_pred = Activation('softmax', name='softmax')(inner) Model(inputs=input_data, outputs=y_pred).summary() labels = Input(name='the_labels', shape=[img_gen.absolute_max_string_len], dtype='float32') input_length = Input(name='input_length', shape=[1], dtype='int64') label_length = Input(name='label_length', shape=[1], dtype='int64') # Keras doesn't currently support loss funcs with extra parameters # so CTC loss is implemented in a lambda layer loss_out = Lambda( ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length]) # clipnorm seems to speeds up convergence sgd = SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5) model = Model(inputs=[input_data, labels, input_length, label_length], outputs=loss_out) # the loss calc occurs elsewhere, so use a dummy lambda func for the loss model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=sgd) if start_epoch > 0: weight_file = os.path.join( OUTPUT_DIR, os.path.join(run_name, 'weights%02d.h5' % (start_epoch - 1))) model.load_weights(weight_file) # captures output of softmax so we can decode the output during visualization test_func = K.function([input_data], [y_pred]) viz_cb = VizCallback(run_name, test_func, img_gen.next_val()) model.fit_generator( generator=img_gen.next_train(), steps_per_epoch=(words_per_epoch - val_words) // minibatch_size, epochs=stop_epoch, validation_data=img_gen.next_val(), validation_steps=val_words // minibatch_size, callbacks=[viz_cb, img_gen], initial_epoch=start_epoch) if __name__ == '__main__': run_name = datetime.datetime.now().strftime('%Y:%m:%d:%H:%M:%S') train(run_name, 0, 20, 128) # increase to wider images and start at epoch 20. # The learned weights are reloaded train(run_name, 20, 25, 512)","title":"Optical character recognition"},{"location":"4-Examples/imdb_bidirectional_lstm/","text":"Trains a Bidirectional LSTM on the IMDB sentiment classification task. Output after 4 epochs on CPU: ~0.8146 Time per epoch on CPU (Core i7): ~150s. from __future__ import print_function import numpy as np from keras.preprocessing import sequence from keras.models import Sequential from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional from keras.datasets import imdb max_features = 20000 # cut texts after this number of words # (among top max_features most common words) maxlen = 100 batch_size = 32 print('Loading data...') (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) print(len(x_train), 'train sequences') print(len(x_test), 'test sequences') print('Pad sequences (samples x time)') x_train = sequence.pad_sequences(x_train, maxlen=maxlen) x_test = sequence.pad_sequences(x_test, maxlen=maxlen) print('x_train shape:', x_train.shape) print('x_test shape:', x_test.shape) y_train = np.array(y_train) y_test = np.array(y_test) model = Sequential() model.add(Embedding(max_features, 128, input_length=maxlen)) model.add(Bidirectional(LSTM(64))) model.add(Dropout(0.5)) model.add(Dense(1, activation='sigmoid')) # try using different optimizers and different optimizer configs model.compile('adam', 'binary_crossentropy', metrics=['accuracy']) print('Train...') model.fit(x_train, y_train, batch_size=batch_size, epochs=4, validation_data=[x_test, y_test])","title":"Imdb bidirectional lstm"},{"location":"4-Examples/imdb_bidirectional_lstm/#trains-a-bidirectional-lstm-on-the-imdb-sentiment-classification-task","text":"Output after 4 epochs on CPU: ~0.8146 Time per epoch on CPU (Core i7): ~150s. from __future__ import print_function import numpy as np from keras.preprocessing import sequence from keras.models import Sequential from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional from keras.datasets import imdb max_features = 20000 # cut texts after this number of words # (among top max_features most common words) maxlen = 100 batch_size = 32 print('Loading data...') (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) print(len(x_train), 'train sequences') print(len(x_test), 'test sequences') print('Pad sequences (samples x time)') x_train = sequence.pad_sequences(x_train, maxlen=maxlen) x_test = sequence.pad_sequences(x_test, maxlen=maxlen) print('x_train shape:', x_train.shape) print('x_test shape:', x_test.shape) y_train = np.array(y_train) y_test = np.array(y_test) model = Sequential() model.add(Embedding(max_features, 128, input_length=maxlen)) model.add(Bidirectional(LSTM(64))) model.add(Dropout(0.5)) model.add(Dense(1, activation='sigmoid')) # try using different optimizers and different optimizer configs model.compile('adam', 'binary_crossentropy', metrics=['accuracy']) print('Train...') model.fit(x_train, y_train, batch_size=batch_size, epochs=4, validation_data=[x_test, y_test])","title":"Trains a Bidirectional LSTM on the IMDB sentiment classification task."},{"location":"4-Examples/imdb_cnn/","text":"This example demonstrates the use of Convolution1D for text classification. Gets to 0.89 test accuracy after 2 epochs. 90s/epoch on Intel i5 2.4Ghz CPU. 10s/epoch on Tesla K40 GPU. from __future__ import print_function from keras.preprocessing import sequence from keras.models import Sequential from keras.layers import Dense, Dropout, Activation from keras.layers import Embedding from keras.layers import Conv1D, GlobalMaxPooling1D from keras.datasets import imdb # set parameters: max_features = 5000 maxlen = 400 batch_size = 32 embedding_dims = 50 filters = 250 kernel_size = 3 hidden_dims = 250 epochs = 2 print('Loading data...') (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) print(len(x_train), 'train sequences') print(len(x_test), 'test sequences') print('Pad sequences (samples x time)') x_train = sequence.pad_sequences(x_train, maxlen=maxlen) x_test = sequence.pad_sequences(x_test, maxlen=maxlen) print('x_train shape:', x_train.shape) print('x_test shape:', x_test.shape) print('Build model...') model = Sequential() # we start off with an efficient embedding layer which maps # our vocab indices into embedding_dims dimensions model.add(Embedding(max_features, embedding_dims, input_length=maxlen)) model.add(Dropout(0.2)) # we add a Convolution1D, which will learn filters # word group filters of size filter_length: model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1)) # we use max pooling: model.add(GlobalMaxPooling1D()) # We add a vanilla hidden layer: model.add(Dense(hidden_dims)) model.add(Dropout(0.2)) model.add(Activation('relu')) # We project onto a single unit output layer, and squash it with a sigmoid: model.add(Dense(1)) model.add(Activation('sigmoid')) model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))","title":"Imdb cnn"},{"location":"4-Examples/imdb_cnn_lstm/","text":"Train a recurrent convolutional network on the IMDB sentiment classification task. Gets to 0.8498 test accuracy after 2 epochs. 41s/epoch on K520 GPU. from __future__ import print_function from keras.preprocessing import sequence from keras.models import Sequential from keras.layers import Dense, Dropout, Activation from keras.layers import Embedding from keras.layers import LSTM from keras.layers import Conv1D, MaxPooling1D from keras.datasets import imdb # Embedding max_features = 20000 maxlen = 100 embedding_size = 128 # Convolution kernel_size = 5 filters = 64 pool_size = 4 # LSTM lstm_output_size = 70 # Training batch_size = 30 epochs = 2 ''' Note: batch_size is highly sensitive. Only 2 epochs are needed as the dataset is very small. ''' print('Loading data...') (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) print(len(x_train), 'train sequences') print(len(x_test), 'test sequences') print('Pad sequences (samples x time)') x_train = sequence.pad_sequences(x_train, maxlen=maxlen) x_test = sequence.pad_sequences(x_test, maxlen=maxlen) print('x_train shape:', x_train.shape) print('x_test shape:', x_test.shape) print('Build model...') model = Sequential() model.add(Embedding(max_features, embedding_size, input_length=maxlen)) model.add(Dropout(0.25)) model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1)) model.add(MaxPooling1D(pool_size=pool_size)) model.add(LSTM(lstm_output_size)) model.add(Dense(1)) model.add(Activation('sigmoid')) model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) print('Train...') model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test)) score, acc = model.evaluate(x_test, y_test, batch_size=batch_size) print('Test score:', score) print('Test accuracy:', acc)","title":"Imdb cnn lstm"},{"location":"4-Examples/imdb_fasttext/","text":"This example demonstrates the use of fasttext for text classification Based on Joulin et al's paper: Bags of Tricks for Efficient Text Classification https://arxiv.org/abs/1607.01759 Results on IMDB datasets with uni and bi-gram embeddings: Uni-gram: 0.8813 test accuracy after 5 epochs. 8s/epoch on i7 cpu. Bi-gram : 0.9056 test accuracy after 5 epochs. 2s/epoch on GTx 980M gpu. from __future__ import print_function import numpy as np from keras.preprocessing import sequence from keras.models import Sequential from keras.layers import Dense from keras.layers import Embedding from keras.layers import GlobalAveragePooling1D from keras.datasets import imdb def create_ngram_set(input_list, ngram_value=2): \"\"\" Extract a set of n-grams from a list of integers. >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2) {(4, 9), (4, 1), (1, 4), (9, 4)} >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3) [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)] \"\"\" return set(zip(*[input_list[i:] for i in range(ngram_value)])) def add_ngram(sequences, token_indice, ngram_range=2): \"\"\" Augment the input list of list (sequences) by appending n-grams values. Example: adding bi-gram >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]] >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017} >>> add_ngram(sequences, token_indice, ngram_range=2) [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]] Example: adding tri-gram >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]] >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018} >>> add_ngram(sequences, token_indice, ngram_range=3) [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42, 2018]] \"\"\" new_sequences = [] for input_list in sequences: new_list = input_list[:] for ngram_value in range(2, ngram_range + 1): for i in range(len(new_list) - ngram_value + 1): ngram = tuple(new_list[i:i + ngram_value]) if ngram in token_indice: new_list.append(token_indice[ngram]) new_sequences.append(new_list) return new_sequences # Set parameters: # ngram_range = 2 will add bi-grams features ngram_range = 1 max_features = 20000 maxlen = 400 batch_size = 32 embedding_dims = 50 epochs = 5 print('Loading data...') (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) print(len(x_train), 'train sequences') print(len(x_test), 'test sequences') print('Average train sequence length: {}'.format( np.mean(list(map(len, x_train)), dtype=int))) print('Average test sequence length: {}'.format( np.mean(list(map(len, x_test)), dtype=int))) if ngram_range > 1: print('Adding {}-gram features'.format(ngram_range)) # Create set of unique n-gram from the training set. ngram_set = set() for input_list in x_train: for i in range(2, ngram_range + 1): set_of_ngram = create_ngram_set(input_list, ngram_value=i) ngram_set.update(set_of_ngram) # Dictionary mapping n-gram token to a unique integer. # Integer values are greater than max_features in order # to avoid collision with existing features. start_index = max_features + 1 token_indice = {v: k + start_index for k, v in enumerate(ngram_set)} indice_token = {token_indice[k]: k for k in token_indice} # max_features is the highest integer that could be found in the dataset. max_features = np.max(list(indice_token.keys())) + 1 # Augmenting x_train and x_test with n-grams features x_train = add_ngram(x_train, token_indice, ngram_range) x_test = add_ngram(x_test, token_indice, ngram_range) print('Average train sequence length: {}'.format( np.mean(list(map(len, x_train)), dtype=int))) print('Average test sequence length: {}'.format( np.mean(list(map(len, x_test)), dtype=int))) print('Pad sequences (samples x time)') x_train = sequence.pad_sequences(x_train, maxlen=maxlen) x_test = sequence.pad_sequences(x_test, maxlen=maxlen) print('x_train shape:', x_train.shape) print('x_test shape:', x_test.shape) print('Build model...') model = Sequential() # we start off with an efficient embedding layer which maps # our vocab indices into embedding_dims dimensions model.add(Embedding(max_features, embedding_dims, input_length=maxlen)) # we add a GlobalAveragePooling1D, which will average the embeddings # of all words in the document model.add(GlobalAveragePooling1D()) # We project onto a single unit output layer, and squash it with a sigmoid: model.add(Dense(1, activation='sigmoid')) model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))","title":"Imdb fasttext"},{"location":"4-Examples/imdb_lstm/","text":"Trains an LSTM model on the IMDB sentiment classification task. The dataset is actually too small for LSTM to be of any advantage compared to simpler, much faster methods such as TF-IDF + LogReg. Notes RNNs are tricky. Choice of batch size is important, choice of loss and optimizer is critical, etc. Some configurations won't converge. LSTM loss decrease patterns during training can be quite different from what you see with CNNs/MLPs/etc. from __future__ import print_function from keras.preprocessing import sequence from keras.models import Sequential from keras.layers import Dense, Embedding from keras.layers import LSTM from keras.datasets import imdb max_features = 20000 # cut texts after this number of words (among top max_features most common words) maxlen = 80 batch_size = 32 print('Loading data...') (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) print(len(x_train), 'train sequences') print(len(x_test), 'test sequences') print('Pad sequences (samples x time)') x_train = sequence.pad_sequences(x_train, maxlen=maxlen) x_test = sequence.pad_sequences(x_test, maxlen=maxlen) print('x_train shape:', x_train.shape) print('x_test shape:', x_test.shape) print('Build model...') model = Sequential() model.add(Embedding(max_features, 128)) model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2)) model.add(Dense(1, activation='sigmoid')) # try using different optimizers and different optimizer configs model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) print('Train...') model.fit(x_train, y_train, batch_size=batch_size, epochs=15, validation_data=(x_test, y_test)) score, acc = model.evaluate(x_test, y_test, batch_size=batch_size) print('Test score:', score) print('Test accuracy:', acc)","title":"Imdb lstm"},{"location":"4-Examples/imdb_lstm/#notes","text":"RNNs are tricky. Choice of batch size is important, choice of loss and optimizer is critical, etc. Some configurations won't converge. LSTM loss decrease patterns during training can be quite different from what you see with CNNs/MLPs/etc. from __future__ import print_function from keras.preprocessing import sequence from keras.models import Sequential from keras.layers import Dense, Embedding from keras.layers import LSTM from keras.datasets import imdb max_features = 20000 # cut texts after this number of words (among top max_features most common words) maxlen = 80 batch_size = 32 print('Loading data...') (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) print(len(x_train), 'train sequences') print(len(x_test), 'test sequences') print('Pad sequences (samples x time)') x_train = sequence.pad_sequences(x_train, maxlen=maxlen) x_test = sequence.pad_sequences(x_test, maxlen=maxlen) print('x_train shape:', x_train.shape) print('x_test shape:', x_test.shape) print('Build model...') model = Sequential() model.add(Embedding(max_features, 128)) model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2)) model.add(Dense(1, activation='sigmoid')) # try using different optimizers and different optimizer configs model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) print('Train...') model.fit(x_train, y_train, batch_size=batch_size, epochs=15, validation_data=(x_test, y_test)) score, acc = model.evaluate(x_test, y_test, batch_size=batch_size) print('Test score:', score) print('Test accuracy:', acc)","title":"Notes"},{"location":"4-Examples/lstm_seq2seq/","text":"Sequence to sequence example in Keras (character-level). This script demonstrates how to implement a basic character-level sequence-to-sequence model. We apply it to translating short English sentences into short French sentences, character-by-character. Note that it is fairly unusual to do character-level machine translation, as word-level models are more common in this domain. Summary of the algorithm We start with input sequences from a domain (e.g. English sentences) and corresponding target sequences from another domain (e.g. French sentences). An encoder LSTM turns input sequences to 2 state vectors (we keep the last LSTM state and discard the outputs). A decoder LSTM is trained to turn the target sequences into the same sequence but offset by one timestep in the future, a training process called \"teacher forcing\" in this context. It uses as initial state the state vectors from the encoder. Effectively, the decoder learns to generate targets[t+1...] given targets[...t] , conditioned on the input sequence. In inference mode, when we want to decode unknown input sequences, we: Encode the input sequence into state vectors Start with a target sequence of size 1 (just the start-of-sequence character) Feed the state vectors and 1-char target sequence to the decoder to produce predictions for the next character Sample the next character using these predictions (we simply use argmax). Append the sampled character to the target sequence Repeat until we generate the end-of-sequence character or we hit the character limit. Data download English to French sentence pairs. http://www.manythings.org/anki/fra-eng.zip Lots of neat sentence pairs datasets can be found at: http://www.manythings.org/anki/ References Sequence to Sequence Learning with Neural Networks https://arxiv.org/abs/1409.3215 Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation https://arxiv.org/abs/1406.1078 from __future__ import print_function from keras.models import Model from keras.layers import Input, LSTM, Dense import numpy as np batch_size = 64 # Batch size for training. epochs = 100 # Number of epochs to train for. latent_dim = 256 # Latent dimensionality of the encoding space. num_samples = 10000 # Number of samples to train on. # Path to the data txt file on disk. data_path = 'fra-eng/fra.txt' # Vectorize the data. input_texts = [] target_texts = [] input_characters = set() target_characters = set() with open(data_path, 'r', encoding='utf-8') as f: lines = f.read().split('\\n') for line in lines[: min(num_samples, len(lines) - 1)]: input_text, target_text = line.split('\\t') # We use \"tab\" as the \"start sequence\" character # for the targets, and \"\\n\" as \"end sequence\" character. target_text = '\\t' + target_text + '\\n' input_texts.append(input_text) target_texts.append(target_text) for char in input_text: if char not in input_characters: input_characters.add(char) for char in target_text: if char not in target_characters: target_characters.add(char) input_characters = sorted(list(input_characters)) target_characters = sorted(list(target_characters)) num_encoder_tokens = len(input_characters) num_decoder_tokens = len(target_characters) max_encoder_seq_length = max([len(txt) for txt in input_texts]) max_decoder_seq_length = max([len(txt) for txt in target_texts]) print('Number of samples:', len(input_texts)) print('Number of unique input tokens:', num_encoder_tokens) print('Number of unique output tokens:', num_decoder_tokens) print('Max sequence length for inputs:', max_encoder_seq_length) print('Max sequence length for outputs:', max_decoder_seq_length) input_token_index = dict( [(char, i) for i, char in enumerate(input_characters)]) target_token_index = dict( [(char, i) for i, char in enumerate(target_characters)]) encoder_input_data = np.zeros( (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32') decoder_input_data = np.zeros( (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32') decoder_target_data = np.zeros( (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32') for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)): for t, char in enumerate(input_text): encoder_input_data[i, t, input_token_index[char]] = 1. for t, char in enumerate(target_text): # decoder_target_data is ahead of decoder_input_data by one timestep decoder_input_data[i, t, target_token_index[char]] = 1. if t > 0: # decoder_target_data will be ahead by one timestep # and will not include the start character. decoder_target_data[i, t - 1, target_token_index[char]] = 1. # Define an input sequence and process it. encoder_inputs = Input(shape=(None, num_encoder_tokens)) encoder = LSTM(latent_dim, return_state=True) encoder_outputs, state_h, state_c = encoder(encoder_inputs) # We discard `encoder_outputs` and only keep the states. encoder_states = [state_h, state_c] # Set up the decoder, using `encoder_states` as initial state. decoder_inputs = Input(shape=(None, num_decoder_tokens)) # We set up our decoder to return full output sequences, # and to return internal states as well. We don't use the # return states in the training model, but we will use them in inference. decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states) decoder_dense = Dense(num_decoder_tokens, activation='softmax') decoder_outputs = decoder_dense(decoder_outputs) # Define the model that will turn # `encoder_input_data` & `decoder_input_data` into `decoder_target_data` model = Model([encoder_inputs, decoder_inputs], decoder_outputs) # Run training model.compile(optimizer='rmsprop', loss='categorical_crossentropy') model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=epochs, validation_split=0.2) # Save model model.save('s2s.h5') # Next: inference mode (sampling). # Here's the drill: # 1) encode input and retrieve initial decoder state # 2) run one step of decoder with this initial state # and a \"start of sequence\" token as target. # Output will be the next target token # 3) Repeat with the current target token and current states # Define sampling models encoder_model = Model(encoder_inputs, encoder_states) decoder_state_input_h = Input(shape=(latent_dim,)) decoder_state_input_c = Input(shape=(latent_dim,)) decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c] decoder_outputs, state_h, state_c = decoder_lstm( decoder_inputs, initial_state=decoder_states_inputs) decoder_states = [state_h, state_c] decoder_outputs = decoder_dense(decoder_outputs) decoder_model = Model( [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states) # Reverse-lookup token index to decode sequences back to # something readable. reverse_input_char_index = dict( (i, char) for char, i in input_token_index.items()) reverse_target_char_index = dict( (i, char) for char, i in target_token_index.items()) def decode_sequence(input_seq): # Encode the input as state vectors. states_value = encoder_model.predict(input_seq) # Generate empty target sequence of length 1. target_seq = np.zeros((1, 1, num_decoder_tokens)) # Populate the first character of target sequence with the start character. target_seq[0, 0, target_token_index['\\t']] = 1. # Sampling loop for a batch of sequences # (to simplify, here we assume a batch of size 1). stop_condition = False decoded_sentence = '' while not stop_condition: output_tokens, h, c = decoder_model.predict( [target_seq] + states_value) # Sample a token sampled_token_index = np.argmax(output_tokens[0, -1, :]) sampled_char = reverse_target_char_index[sampled_token_index] decoded_sentence += sampled_char # Exit condition: either hit max length # or find stop character. if (sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length): stop_condition = True # Update the target sequence (of length 1). target_seq = np.zeros((1, 1, num_decoder_tokens)) target_seq[0, 0, sampled_token_index] = 1. # Update states states_value = [h, c] return decoded_sentence for seq_index in range(100): # Take one sequence (part of the training set) # for trying out decoding. input_seq = encoder_input_data[seq_index: seq_index + 1] decoded_sentence = decode_sequence(input_seq) print('-') print('Input sentence:', input_texts[seq_index]) print('Decoded sentence:', decoded_sentence)","title":"Lstm seq2seq"},{"location":"4-Examples/lstm_seq2seq/#summary-of-the-algorithm","text":"We start with input sequences from a domain (e.g. English sentences) and corresponding target sequences from another domain (e.g. French sentences). An encoder LSTM turns input sequences to 2 state vectors (we keep the last LSTM state and discard the outputs). A decoder LSTM is trained to turn the target sequences into the same sequence but offset by one timestep in the future, a training process called \"teacher forcing\" in this context. It uses as initial state the state vectors from the encoder. Effectively, the decoder learns to generate targets[t+1...] given targets[...t] , conditioned on the input sequence. In inference mode, when we want to decode unknown input sequences, we: Encode the input sequence into state vectors Start with a target sequence of size 1 (just the start-of-sequence character) Feed the state vectors and 1-char target sequence to the decoder to produce predictions for the next character Sample the next character using these predictions (we simply use argmax). Append the sampled character to the target sequence Repeat until we generate the end-of-sequence character or we hit the character limit.","title":"Summary of the algorithm"},{"location":"4-Examples/lstm_seq2seq/#data-download","text":"English to French sentence pairs. http://www.manythings.org/anki/fra-eng.zip Lots of neat sentence pairs datasets can be found at: http://www.manythings.org/anki/","title":"Data download"},{"location":"4-Examples/lstm_seq2seq/#references","text":"Sequence to Sequence Learning with Neural Networks https://arxiv.org/abs/1409.3215 Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation https://arxiv.org/abs/1406.1078 from __future__ import print_function from keras.models import Model from keras.layers import Input, LSTM, Dense import numpy as np batch_size = 64 # Batch size for training. epochs = 100 # Number of epochs to train for. latent_dim = 256 # Latent dimensionality of the encoding space. num_samples = 10000 # Number of samples to train on. # Path to the data txt file on disk. data_path = 'fra-eng/fra.txt' # Vectorize the data. input_texts = [] target_texts = [] input_characters = set() target_characters = set() with open(data_path, 'r', encoding='utf-8') as f: lines = f.read().split('\\n') for line in lines[: min(num_samples, len(lines) - 1)]: input_text, target_text = line.split('\\t') # We use \"tab\" as the \"start sequence\" character # for the targets, and \"\\n\" as \"end sequence\" character. target_text = '\\t' + target_text + '\\n' input_texts.append(input_text) target_texts.append(target_text) for char in input_text: if char not in input_characters: input_characters.add(char) for char in target_text: if char not in target_characters: target_characters.add(char) input_characters = sorted(list(input_characters)) target_characters = sorted(list(target_characters)) num_encoder_tokens = len(input_characters) num_decoder_tokens = len(target_characters) max_encoder_seq_length = max([len(txt) for txt in input_texts]) max_decoder_seq_length = max([len(txt) for txt in target_texts]) print('Number of samples:', len(input_texts)) print('Number of unique input tokens:', num_encoder_tokens) print('Number of unique output tokens:', num_decoder_tokens) print('Max sequence length for inputs:', max_encoder_seq_length) print('Max sequence length for outputs:', max_decoder_seq_length) input_token_index = dict( [(char, i) for i, char in enumerate(input_characters)]) target_token_index = dict( [(char, i) for i, char in enumerate(target_characters)]) encoder_input_data = np.zeros( (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32') decoder_input_data = np.zeros( (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32') decoder_target_data = np.zeros( (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32') for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)): for t, char in enumerate(input_text): encoder_input_data[i, t, input_token_index[char]] = 1. for t, char in enumerate(target_text): # decoder_target_data is ahead of decoder_input_data by one timestep decoder_input_data[i, t, target_token_index[char]] = 1. if t > 0: # decoder_target_data will be ahead by one timestep # and will not include the start character. decoder_target_data[i, t - 1, target_token_index[char]] = 1. # Define an input sequence and process it. encoder_inputs = Input(shape=(None, num_encoder_tokens)) encoder = LSTM(latent_dim, return_state=True) encoder_outputs, state_h, state_c = encoder(encoder_inputs) # We discard `encoder_outputs` and only keep the states. encoder_states = [state_h, state_c] # Set up the decoder, using `encoder_states` as initial state. decoder_inputs = Input(shape=(None, num_decoder_tokens)) # We set up our decoder to return full output sequences, # and to return internal states as well. We don't use the # return states in the training model, but we will use them in inference. decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states) decoder_dense = Dense(num_decoder_tokens, activation='softmax') decoder_outputs = decoder_dense(decoder_outputs) # Define the model that will turn # `encoder_input_data` & `decoder_input_data` into `decoder_target_data` model = Model([encoder_inputs, decoder_inputs], decoder_outputs) # Run training model.compile(optimizer='rmsprop', loss='categorical_crossentropy') model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=epochs, validation_split=0.2) # Save model model.save('s2s.h5') # Next: inference mode (sampling). # Here's the drill: # 1) encode input and retrieve initial decoder state # 2) run one step of decoder with this initial state # and a \"start of sequence\" token as target. # Output will be the next target token # 3) Repeat with the current target token and current states # Define sampling models encoder_model = Model(encoder_inputs, encoder_states) decoder_state_input_h = Input(shape=(latent_dim,)) decoder_state_input_c = Input(shape=(latent_dim,)) decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c] decoder_outputs, state_h, state_c = decoder_lstm( decoder_inputs, initial_state=decoder_states_inputs) decoder_states = [state_h, state_c] decoder_outputs = decoder_dense(decoder_outputs) decoder_model = Model( [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states) # Reverse-lookup token index to decode sequences back to # something readable. reverse_input_char_index = dict( (i, char) for char, i in input_token_index.items()) reverse_target_char_index = dict( (i, char) for char, i in target_token_index.items()) def decode_sequence(input_seq): # Encode the input as state vectors. states_value = encoder_model.predict(input_seq) # Generate empty target sequence of length 1. target_seq = np.zeros((1, 1, num_decoder_tokens)) # Populate the first character of target sequence with the start character. target_seq[0, 0, target_token_index['\\t']] = 1. # Sampling loop for a batch of sequences # (to simplify, here we assume a batch of size 1). stop_condition = False decoded_sentence = '' while not stop_condition: output_tokens, h, c = decoder_model.predict( [target_seq] + states_value) # Sample a token sampled_token_index = np.argmax(output_tokens[0, -1, :]) sampled_char = reverse_target_char_index[sampled_token_index] decoded_sentence += sampled_char # Exit condition: either hit max length # or find stop character. if (sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length): stop_condition = True # Update the target sequence (of length 1). target_seq = np.zeros((1, 1, num_decoder_tokens)) target_seq[0, 0, sampled_token_index] = 1. # Update states states_value = [h, c] return decoded_sentence for seq_index in range(100): # Take one sequence (part of the training set) # for trying out decoding. input_seq = encoder_input_data[seq_index: seq_index + 1] decoded_sentence = decode_sequence(input_seq) print('-') print('Input sentence:', input_texts[seq_index]) print('Decoded sentence:', decoded_sentence)","title":"References"},{"location":"4-Examples/lstm_seq2seq_restore/","text":"Restore a character-level sequence to sequence model from disk and use it to generate predictions. This script loads the s2s.h5 model saved by lstm_seq2seq.py and generates sequences from it. It assumes that no changes have been made (for example: latent_dim is unchanged, and the input data and model architecture are unchanged). See lstm_seq2seq.py for more details on the model architecture and how it is trained. from __future__ import print_function from keras.models import Model, load_model from keras.layers import Input import numpy as np batch_size = 64 # Batch size for training. epochs = 100 # Number of epochs to train for. latent_dim = 256 # Latent dimensionality of the encoding space. num_samples = 10000 # Number of samples to train on. # Path to the data txt file on disk. data_path = 'fra-eng/fra.txt' # Vectorize the data. We use the same approach as the training script. # NOTE: the data must be identical, in order for the character -> integer # mappings to be consistent. # We omit encoding target_texts since they are not needed. input_texts = [] target_texts = [] input_characters = set() target_characters = set() with open(data_path, 'r', encoding='utf-8') as f: lines = f.read().split('\\n') for line in lines[: min(num_samples, len(lines) - 1)]: input_text, target_text = line.split('\\t') # We use \"tab\" as the \"start sequence\" character # for the targets, and \"\\n\" as \"end sequence\" character. target_text = '\\t' + target_text + '\\n' input_texts.append(input_text) target_texts.append(target_text) for char in input_text: if char not in input_characters: input_characters.add(char) for char in target_text: if char not in target_characters: target_characters.add(char) input_characters = sorted(list(input_characters)) target_characters = sorted(list(target_characters)) num_encoder_tokens = len(input_characters) num_decoder_tokens = len(target_characters) max_encoder_seq_length = max([len(txt) for txt in input_texts]) max_decoder_seq_length = max([len(txt) for txt in target_texts]) print('Number of samples:', len(input_texts)) print('Number of unique input tokens:', num_encoder_tokens) print('Number of unique output tokens:', num_decoder_tokens) print('Max sequence length for inputs:', max_encoder_seq_length) print('Max sequence length for outputs:', max_decoder_seq_length) input_token_index = dict( [(char, i) for i, char in enumerate(input_characters)]) target_token_index = dict( [(char, i) for i, char in enumerate(target_characters)]) encoder_input_data = np.zeros( (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32') for i, input_text in enumerate(input_texts): for t, char in enumerate(input_text): encoder_input_data[i, t, input_token_index[char]] = 1. # Restore the model and construct the encoder and decoder. model = load_model('s2s.h5') encoder_inputs = model.input[0] # input_1 encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output # lstm_1 encoder_states = [state_h_enc, state_c_enc] encoder_model = Model(encoder_inputs, encoder_states) decoder_inputs = model.input[1] # input_2 decoder_state_input_h = Input(shape=(latent_dim,), name='input_3') decoder_state_input_c = Input(shape=(latent_dim,), name='input_4') decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c] decoder_lstm = model.layers[3] decoder_outputs, state_h_dec, state_c_dec = decoder_lstm( decoder_inputs, initial_state=decoder_states_inputs) decoder_states = [state_h_dec, state_c_dec] decoder_dense = model.layers[4] decoder_outputs = decoder_dense(decoder_outputs) decoder_model = Model( [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states) # Reverse-lookup token index to decode sequences back to # something readable. reverse_input_char_index = dict( (i, char) for char, i in input_token_index.items()) reverse_target_char_index = dict( (i, char) for char, i in target_token_index.items()) # Decodes an input sequence. Future work should support beam search. def decode_sequence(input_seq): # Encode the input as state vectors. states_value = encoder_model.predict(input_seq) # Generate empty target sequence of length 1. target_seq = np.zeros((1, 1, num_decoder_tokens)) # Populate the first character of target sequence with the start character. target_seq[0, 0, target_token_index['\\t']] = 1. # Sampling loop for a batch of sequences # (to simplify, here we assume a batch of size 1). stop_condition = False decoded_sentence = '' while not stop_condition: output_tokens, h, c = decoder_model.predict( [target_seq] + states_value) # Sample a token sampled_token_index = np.argmax(output_tokens[0, -1, :]) sampled_char = reverse_target_char_index[sampled_token_index] decoded_sentence += sampled_char # Exit condition: either hit max length # or find stop character. if (sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length): stop_condition = True # Update the target sequence (of length 1). target_seq = np.zeros((1, 1, num_decoder_tokens)) target_seq[0, 0, sampled_token_index] = 1. # Update states states_value = [h, c] return decoded_sentence for seq_index in range(100): # Take one sequence (part of the training set) # for trying out decoding. input_seq = encoder_input_data[seq_index: seq_index + 1] decoded_sentence = decode_sequence(input_seq) print('-') print('Input sentence:', input_texts[seq_index]) print('Decoded sentence:', decoded_sentence)","title":"Lstm seq2seq restore"},{"location":"4-Examples/lstm_stateful/","text":"Example script showing how to use a stateful LSTM model and how its stateless counterpart performs. More documentation about the Keras LSTM model can be found at https://keras.io/layers/recurrent/#lstm The models are trained on an input/output pair, where the input is a generated uniformly distributed random sequence of length = \"input_len\", and the output is a moving average of the input with window length = \"tsteps\". Both \"input_len\" and \"tsteps\" are defined in the \"editable parameters\" section. A larger \"tsteps\" value means that the LSTM will need more memory to figure out the input-output relationship. This memory length is controlled by the \"lahead\" variable (more details below). The rest of the parameters are: - input_len: the length of the generated input sequence - lahead: the input sequence length that the LSTM is trained on for each output point - batch_size, epochs: same parameters as in the model.fit(...) function When lahead > 1, the model input is preprocessed to a \"rolling window view\" of the data, with the window length = \"lahead\". This is similar to sklearn's \"view_as_windows\" with \"window_shape\" being a single number Ref: http://scikit-image.org/docs/0.10.x/api/skimage.util.html#view-as-windows When lahead < tsteps, only the stateful LSTM converges because its statefulness allows it to see beyond the capability that lahead gave it to fit the n-point average. The stateless LSTM does not have this capability, and hence is limited by its \"lahead\" parameter, which is not sufficient to see the n-point average. When lahead >= tsteps, both the stateful and stateless LSTM converge. from __future__ import print_function import numpy as np import matplotlib.pyplot as plt import pandas as pd from keras.models import Sequential from keras.layers import Dense, LSTM # ---------------------------------------------------------- # EDITABLE PARAMETERS # Read the documentation in the script head for more details # ---------------------------------------------------------- # length of input input_len = 1000 # The window length of the moving average used to generate # the output from the input in the input/output pair used # to train the LSTM # e.g. if tsteps=2 and input=[1, 2, 3, 4, 5], # then output=[1.5, 2.5, 3.5, 4.5] tsteps = 2 # The input sequence length that the LSTM is trained on for each output point lahead = 1 # training parameters passed to \"model.fit(...)\" batch_size = 1 epochs = 10 # ------------ # MAIN PROGRAM # ------------ print(\"*\" * 33) if lahead >= tsteps: print(\"STATELESS LSTM WILL ALSO CONVERGE\") else: print(\"STATELESS LSTM WILL NOT CONVERGE\") print(\"*\" * 33) np.random.seed(1986) print('Generating Data...') def gen_uniform_amp(amp=1, xn=10000): \"\"\"Generates uniform random data between -amp and +amp and of length xn Arguments: amp: maximum/minimum range of uniform data xn: length of series \"\"\" data_input = np.random.uniform(-1 * amp, +1 * amp, xn) data_input = pd.DataFrame(data_input) return data_input # Since the output is a moving average of the input, # the first few points of output will be NaN # and will be dropped from the generated data # before training the LSTM. # Also, when lahead > 1, # the preprocessing step later of \"rolling window view\" # will also cause some points to be lost. # For aesthetic reasons, # in order to maintain generated data length = input_len after pre-processing, # add a few points to account for the values that will be lost. to_drop = max(tsteps - 1, lahead - 1) data_input = gen_uniform_amp(amp=0.1, xn=input_len + to_drop) # set the target to be a N-point average of the input expected_output = data_input.rolling(window=tsteps, center=False).mean() # when lahead > 1, need to convert the input to \"rolling window view\" # https://docs.scipy.org/doc/numpy/reference/generated/numpy.repeat.html if lahead > 1: data_input = np.repeat(data_input.values, repeats=lahead, axis=1) data_input = pd.DataFrame(data_input) for i, c in enumerate(data_input.columns): data_input[c] = data_input[c].shift(i) # drop the nan expected_output = expected_output[to_drop:] data_input = data_input[to_drop:] print('Input shape:', data_input.shape) print('Output shape:', expected_output.shape) print('Input head: ') print(data_input.head()) print('Output head: ') print(expected_output.head()) print('Input tail: ') print(data_input.tail()) print('Output tail: ') print(expected_output.tail()) print('Plotting input and expected output') plt.plot(data_input[0][:10], '.') plt.plot(expected_output[0][:10], '-') plt.legend(['Input', 'Expected output']) plt.title('Input') plt.show() def create_model(stateful): model = Sequential() model.add(LSTM(20, input_shape=(lahead, 1), batch_size=batch_size, stateful=stateful)) model.add(Dense(1)) model.compile(loss='mse', optimizer='adam') return model print('Creating Stateful Model...') model_stateful = create_model(stateful=True) # split train/test data def split_data(x, y, ratio=0.8): to_train = int(input_len * ratio) # tweak to match with batch_size to_train -= to_train % batch_size x_train = x[:to_train] y_train = y[:to_train] x_test = x[to_train:] y_test = y[to_train:] # tweak to match with batch_size to_drop = x.shape[0] % batch_size if to_drop > 0: x_test = x_test[:-1 * to_drop] y_test = y_test[:-1 * to_drop] # some reshaping reshape_3 = lambda x: x.values.reshape((x.shape[0], x.shape[1], 1)) x_train = reshape_3(x_train) x_test = reshape_3(x_test) reshape_2 = lambda x: x.values.reshape((x.shape[0], 1)) y_train = reshape_2(y_train) y_test = reshape_2(y_test) return (x_train, y_train), (x_test, y_test) (x_train, y_train), (x_test, y_test) = split_data(data_input, expected_output) print('x_train.shape: ', x_train.shape) print('y_train.shape: ', y_train.shape) print('x_test.shape: ', x_test.shape) print('y_test.shape: ', y_test.shape) print('Training') for i in range(epochs): print('Epoch', i + 1, '/', epochs) # Note that the last state for sample i in a batch will # be used as initial state for sample i in the next batch. # Thus we are simultaneously training on batch_size series with # lower resolution than the original series contained in data_input. # Each of these series are offset by one step and can be # extracted with data_input[i::batch_size]. model_stateful.fit(x_train, y_train, batch_size=batch_size, epochs=1, verbose=1, validation_data=(x_test, y_test), shuffle=False) model_stateful.reset_states() print('Predicting') predicted_stateful = model_stateful.predict(x_test, batch_size=batch_size) print('Creating Stateless Model...') model_stateless = create_model(stateful=False) print('Training') model_stateless.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test), shuffle=False) print('Predicting') predicted_stateless = model_stateless.predict(x_test, batch_size=batch_size) # ---------------------------- print('Plotting Results') plt.subplot(3, 1, 1) plt.plot(y_test) plt.title('Expected') plt.subplot(3, 1, 2) # drop the first \"tsteps-1\" because it is not possible to predict them # since the \"previous\" timesteps to use do not exist plt.plot((y_test - predicted_stateful).flatten()[tsteps - 1:]) plt.title('Stateful: Expected - Predicted') plt.subplot(3, 1, 3) plt.plot((y_test - predicted_stateless).flatten()) plt.title('Stateless: Expected - Predicted') plt.show()","title":"Lstm stateful"},{"location":"4-Examples/lstm_text_generation/","text":"Example script to generate text from Nietzsche's writings. At least 20 epochs are required before the generated text starts sounding coherent. It is recommended to run this script on GPU, as recurrent networks are quite computationally intensive. If you try this script on new data, make sure your corpus has at least ~100k characters. ~1M is better. from __future__ import print_function from keras.callbacks import LambdaCallback from keras.models import Sequential from keras.layers import Dense from keras.layers import LSTM from keras.optimizers import RMSprop from keras.utils.data_utils import get_file import numpy as np import random import sys import io path = get_file( 'nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt') with io.open(path, encoding='utf-8') as f: text = f.read().lower() print('corpus length:', len(text)) chars = sorted(list(set(text))) print('total chars:', len(chars)) char_indices = dict((c, i) for i, c in enumerate(chars)) indices_char = dict((i, c) for i, c in enumerate(chars)) # cut the text in semi-redundant sequences of maxlen characters maxlen = 40 step = 3 sentences = [] next_chars = [] for i in range(0, len(text) - maxlen, step): sentences.append(text[i: i + maxlen]) next_chars.append(text[i + maxlen]) print('nb sequences:', len(sentences)) print('Vectorization...') x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool) y = np.zeros((len(sentences), len(chars)), dtype=np.bool) for i, sentence in enumerate(sentences): for t, char in enumerate(sentence): x[i, t, char_indices[char]] = 1 y[i, char_indices[next_chars[i]]] = 1 # build the model: a single LSTM print('Build model...') model = Sequential() model.add(LSTM(128, input_shape=(maxlen, len(chars)))) model.add(Dense(len(chars), activation='softmax')) optimizer = RMSprop(lr=0.01) model.compile(loss='categorical_crossentropy', optimizer=optimizer) def sample(preds, temperature=1.0): # helper function to sample an index from a probability array preds = np.asarray(preds).astype('float64') preds = np.log(preds) / temperature exp_preds = np.exp(preds) preds = exp_preds / np.sum(exp_preds) probas = np.random.multinomial(1, preds, 1) return np.argmax(probas) def on_epoch_end(epoch, _): # Function invoked at end of each epoch. Prints generated text. print() print('----- Generating text after Epoch: %d' % epoch) start_index = random.randint(0, len(text) - maxlen - 1) for diversity in [0.2, 0.5, 1.0, 1.2]: print('----- diversity:', diversity) generated = '' sentence = text[start_index: start_index + maxlen] generated += sentence print('----- Generating with seed: \"' + sentence + '\"') sys.stdout.write(generated) for i in range(400): x_pred = np.zeros((1, maxlen, len(chars))) for t, char in enumerate(sentence): x_pred[0, t, char_indices[char]] = 1. preds = model.predict(x_pred, verbose=0)[0] next_index = sample(preds, diversity) next_char = indices_char[next_index] generated += next_char sentence = sentence[1:] + next_char sys.stdout.write(next_char) sys.stdout.flush() print() print_callback = LambdaCallback(on_epoch_end=on_epoch_end) model.fit(x, y, batch_size=128, epochs=60, callbacks=[print_callback])","title":"Lstm text generation"},{"location":"4-Examples/mnist_acgan/","text":"Train an Auxiliary Classifier Generative Adversarial Network (ACGAN) on the MNIST dataset. See https://arxiv.org/abs/1610.09585 for more details. You should start to see reasonable images after ~5 epochs, and good images by ~15 epochs. You should use a GPU, as the convolution-heavy operations are very slow on the CPU. Prefer the TensorFlow backend if you plan on iterating, as the compilation time can be a blocker using Theano. Timings: Hardware | Backend | Time / Epoch CPU | TF | 3 hrs Titan X (maxwell) | TF | 4 min Titan X (maxwell) | TH | 7 min Consult https://github.com/lukedeo/keras-acgan for more information and example output from __future__ import print_function from collections import defaultdict try: import cPickle as pickle except ImportError: import pickle from PIL import Image from six.moves import range from keras.datasets import mnist from keras import layers from keras.layers import Input, Dense, Reshape, Flatten, Embedding, Dropout from keras.layers import BatchNormalization from keras.layers.advanced_activations import LeakyReLU from keras.layers.convolutional import Conv2DTranspose, Conv2D from keras.models import Sequential, Model from keras.optimizers import Adam from keras.utils.generic_utils import Progbar import numpy as np np.random.seed(1337) num_classes = 10 def build_generator(latent_size): # we will map a pair of (z, L), where z is a latent vector and L is a # label drawn from P_c, to image space (..., 28, 28, 1) cnn = Sequential() cnn.add(Dense(3 * 3 * 384, input_dim=latent_size, activation='relu')) cnn.add(Reshape((3, 3, 384))) # upsample to (7, 7, ...) cnn.add(Conv2DTranspose(192, 5, strides=1, padding='valid', activation='relu', kernel_initializer='glorot_normal')) cnn.add(BatchNormalization()) # upsample to (14, 14, ...) cnn.add(Conv2DTranspose(96, 5, strides=2, padding='same', activation='relu', kernel_initializer='glorot_normal')) cnn.add(BatchNormalization()) # upsample to (28, 28, ...) cnn.add(Conv2DTranspose(1, 5, strides=2, padding='same', activation='tanh', kernel_initializer='glorot_normal')) # this is the z space commonly referred to in GAN papers latent = Input(shape=(latent_size, )) # this will be our label image_class = Input(shape=(1,), dtype='int32') cls = Embedding(num_classes, latent_size, embeddings_initializer='glorot_normal')(image_class) # hadamard product between z-space and a class conditional embedding h = layers.multiply([latent, cls]) fake_image = cnn(h) return Model([latent, image_class], fake_image) def build_discriminator(): # build a relatively standard conv net, with LeakyReLUs as suggested in # the reference paper cnn = Sequential() cnn.add(Conv2D(32, 3, padding='same', strides=2, input_shape=(28, 28, 1))) cnn.add(LeakyReLU(0.2)) cnn.add(Dropout(0.3)) cnn.add(Conv2D(64, 3, padding='same', strides=1)) cnn.add(LeakyReLU(0.2)) cnn.add(Dropout(0.3)) cnn.add(Conv2D(128, 3, padding='same', strides=2)) cnn.add(LeakyReLU(0.2)) cnn.add(Dropout(0.3)) cnn.add(Conv2D(256, 3, padding='same', strides=1)) cnn.add(LeakyReLU(0.2)) cnn.add(Dropout(0.3)) cnn.add(Flatten()) image = Input(shape=(28, 28, 1)) features = cnn(image) # first output (name=generation) is whether or not the discriminator # thinks the image that is being shown is fake, and the second output # (name=auxiliary) is the class that the discriminator thinks the image # belongs to. fake = Dense(1, activation='sigmoid', name='generation')(features) aux = Dense(num_classes, activation='softmax', name='auxiliary')(features) return Model(image, [fake, aux]) if __name__ == '__main__': # batch and latent size taken from the paper epochs = 100 batch_size = 100 latent_size = 100 # Adam parameters suggested in https://arxiv.org/abs/1511.06434 adam_lr = 0.0002 adam_beta_1 = 0.5 # build the discriminator print('Discriminator model:') discriminator = build_discriminator() discriminator.compile( optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1), loss=['binary_crossentropy', 'sparse_categorical_crossentropy'] ) discriminator.summary() # build the generator generator = build_generator(latent_size) latent = Input(shape=(latent_size, )) image_class = Input(shape=(1,), dtype='int32') # get a fake image fake = generator([latent, image_class]) # we only want to be able to train generation for the combined model discriminator.trainable = False fake, aux = discriminator(fake) combined = Model([latent, image_class], [fake, aux]) print('Combined model:') combined.compile( optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1), loss=['binary_crossentropy', 'sparse_categorical_crossentropy'] ) combined.summary() # get our mnist data, and force it to be of shape (..., 28, 28, 1) with # range [-1, 1] (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = (x_train.astype(np.float32) - 127.5) / 127.5 x_train = np.expand_dims(x_train, axis=-1) x_test = (x_test.astype(np.float32) - 127.5) / 127.5 x_test = np.expand_dims(x_test, axis=-1) num_train, num_test = x_train.shape[0], x_test.shape[0] train_history = defaultdict(list) test_history = defaultdict(list) for epoch in range(1, epochs + 1): print('Epoch {}/{}'.format(epoch, epochs)) num_batches = int(np.ceil(x_train.shape[0] / float(batch_size))) progress_bar = Progbar(target=num_batches) epoch_gen_loss = [] epoch_disc_loss = [] for index in range(num_batches): # get a batch of real images image_batch = x_train[index * batch_size:(index + 1) * batch_size] label_batch = y_train[index * batch_size:(index + 1) * batch_size] # generate a new batch of noise noise = np.random.uniform(-1, 1, (len(image_batch), latent_size)) # sample some labels from p_c sampled_labels = np.random.randint(0, num_classes, len(image_batch)) # generate a batch of fake images, using the generated labels as a # conditioner. We reshape the sampled labels to be # (len(image_batch), 1) so that we can feed them into the embedding # layer as a length one sequence generated_images = generator.predict( [noise, sampled_labels.reshape((-1, 1))], verbose=0) x = np.concatenate((image_batch, generated_images)) # use one-sided soft real/fake labels # Salimans et al., 2016 # https://arxiv.org/pdf/1606.03498.pdf (Section 3.4) soft_zero, soft_one = 0, 0.95 y = np.array( [soft_one] * len(image_batch) + [soft_zero] * len(image_batch)) aux_y = np.concatenate((label_batch, sampled_labels), axis=0) # we don't want the discriminator to also maximize the classification # accuracy of the auxiliary classifier on generated images, so we # don't train discriminator to produce class labels for generated # images (see https://openreview.net/forum?id=rJXTf9Bxg). # To preserve sum of sample weights for the auxiliary classifier, # we assign sample weight of 2 to the real images. disc_sample_weight = [np.ones(2 * len(image_batch)), np.concatenate((np.ones(len(image_batch)) * 2, np.zeros(len(image_batch))))] # see if the discriminator can figure itself out... epoch_disc_loss.append(discriminator.train_on_batch( x, [y, aux_y], sample_weight=disc_sample_weight)) # make new noise. we generate 2 * batch size here such that we have # the generator optimize over an identical number of images as the # discriminator noise = np.random.uniform(-1, 1, (2 * len(image_batch), latent_size)) sampled_labels = np.random.randint(0, num_classes, 2 * len(image_batch)) # we want to train the generator to trick the discriminator # For the generator, we want all the {fake, not-fake} labels to say # not-fake trick = np.ones(2 * len(image_batch)) * soft_one epoch_gen_loss.append(combined.train_on_batch( [noise, sampled_labels.reshape((-1, 1))], [trick, sampled_labels])) progress_bar.update(index + 1) print('Testing for epoch {}:'.format(epoch)) # evaluate the testing loss here # generate a new batch of noise noise = np.random.uniform(-1, 1, (num_test, latent_size)) # sample some labels from p_c and generate images from them sampled_labels = np.random.randint(0, num_classes, num_test) generated_images = generator.predict( [noise, sampled_labels.reshape((-1, 1))], verbose=False) x = np.concatenate((x_test, generated_images)) y = np.array([1] * num_test + [0] * num_test) aux_y = np.concatenate((y_test, sampled_labels), axis=0) # see if the discriminator can figure itself out... discriminator_test_loss = discriminator.evaluate( x, [y, aux_y], verbose=False) discriminator_train_loss = np.mean(np.array(epoch_disc_loss), axis=0) # make new noise noise = np.random.uniform(-1, 1, (2 * num_test, latent_size)) sampled_labels = np.random.randint(0, num_classes, 2 * num_test) trick = np.ones(2 * num_test) generator_test_loss = combined.evaluate( [noise, sampled_labels.reshape((-1, 1))], [trick, sampled_labels], verbose=False) generator_train_loss = np.mean(np.array(epoch_gen_loss), axis=0) # generate an epoch report on performance train_history['generator'].append(generator_train_loss) train_history['discriminator'].append(discriminator_train_loss) test_history['generator'].append(generator_test_loss) test_history['discriminator'].append(discriminator_test_loss) print('{0:<22s} | {1:4s} | {2:15s} | {3:5s}'.format( 'component', *discriminator.metrics_names)) print('-' * 65) ROW_FMT = '{0:<22s} | {1:<4.2f} | {2:<15.4f} | {3:<5.4f}' print(ROW_FMT.format('generator (train)', *train_history['generator'][-1])) print(ROW_FMT.format('generator (test)', *test_history['generator'][-1])) print(ROW_FMT.format('discriminator (train)', *train_history['discriminator'][-1])) print(ROW_FMT.format('discriminator (test)', *test_history['discriminator'][-1])) # save weights every epoch generator.save_weights( 'params_generator_epoch_{0:03d}.hdf5'.format(epoch), True) discriminator.save_weights( 'params_discriminator_epoch_{0:03d}.hdf5'.format(epoch), True) # generate some digits to display num_rows = 40 noise = np.tile(np.random.uniform(-1, 1, (num_rows, latent_size)), (num_classes, 1)) sampled_labels = np.array([ [i] * num_rows for i in range(num_classes) ]).reshape(-1, 1) # get a batch to display generated_images = generator.predict( [noise, sampled_labels], verbose=0) # prepare real images sorted by class label real_labels = y_train[(epoch - 1) * num_rows * num_classes: epoch * num_rows * num_classes] indices = np.argsort(real_labels, axis=0) real_images = x_train[(epoch - 1) * num_rows * num_classes: epoch * num_rows * num_classes][indices] # display generated images, white separator, real images img = np.concatenate( (generated_images, np.repeat(np.ones_like(x_train[:1]), num_rows, axis=0), real_images)) # arrange them into a grid img = (np.concatenate([r.reshape(-1, 28) for r in np.split(img, 2 * num_classes + 1) ], axis=-1) * 127.5 + 127.5).astype(np.uint8) Image.fromarray(img).save( 'plot_epoch_{0:03d}_generated.png'.format(epoch)) with open('acgan-history.pkl', 'wb') as f: pickle.dump({'train': train_history, 'test': test_history}, f)","title":"Mnist acgan"},{"location":"4-Examples/mnist_acgan/#hardware-backend-time-epoch","text":"CPU | TF | 3 hrs Titan X (maxwell) | TF | 4 min Titan X (maxwell) | TH | 7 min Consult https://github.com/lukedeo/keras-acgan for more information and example output from __future__ import print_function from collections import defaultdict try: import cPickle as pickle except ImportError: import pickle from PIL import Image from six.moves import range from keras.datasets import mnist from keras import layers from keras.layers import Input, Dense, Reshape, Flatten, Embedding, Dropout from keras.layers import BatchNormalization from keras.layers.advanced_activations import LeakyReLU from keras.layers.convolutional import Conv2DTranspose, Conv2D from keras.models import Sequential, Model from keras.optimizers import Adam from keras.utils.generic_utils import Progbar import numpy as np np.random.seed(1337) num_classes = 10 def build_generator(latent_size): # we will map a pair of (z, L), where z is a latent vector and L is a # label drawn from P_c, to image space (..., 28, 28, 1) cnn = Sequential() cnn.add(Dense(3 * 3 * 384, input_dim=latent_size, activation='relu')) cnn.add(Reshape((3, 3, 384))) # upsample to (7, 7, ...) cnn.add(Conv2DTranspose(192, 5, strides=1, padding='valid', activation='relu', kernel_initializer='glorot_normal')) cnn.add(BatchNormalization()) # upsample to (14, 14, ...) cnn.add(Conv2DTranspose(96, 5, strides=2, padding='same', activation='relu', kernel_initializer='glorot_normal')) cnn.add(BatchNormalization()) # upsample to (28, 28, ...) cnn.add(Conv2DTranspose(1, 5, strides=2, padding='same', activation='tanh', kernel_initializer='glorot_normal')) # this is the z space commonly referred to in GAN papers latent = Input(shape=(latent_size, )) # this will be our label image_class = Input(shape=(1,), dtype='int32') cls = Embedding(num_classes, latent_size, embeddings_initializer='glorot_normal')(image_class) # hadamard product between z-space and a class conditional embedding h = layers.multiply([latent, cls]) fake_image = cnn(h) return Model([latent, image_class], fake_image) def build_discriminator(): # build a relatively standard conv net, with LeakyReLUs as suggested in # the reference paper cnn = Sequential() cnn.add(Conv2D(32, 3, padding='same', strides=2, input_shape=(28, 28, 1))) cnn.add(LeakyReLU(0.2)) cnn.add(Dropout(0.3)) cnn.add(Conv2D(64, 3, padding='same', strides=1)) cnn.add(LeakyReLU(0.2)) cnn.add(Dropout(0.3)) cnn.add(Conv2D(128, 3, padding='same', strides=2)) cnn.add(LeakyReLU(0.2)) cnn.add(Dropout(0.3)) cnn.add(Conv2D(256, 3, padding='same', strides=1)) cnn.add(LeakyReLU(0.2)) cnn.add(Dropout(0.3)) cnn.add(Flatten()) image = Input(shape=(28, 28, 1)) features = cnn(image) # first output (name=generation) is whether or not the discriminator # thinks the image that is being shown is fake, and the second output # (name=auxiliary) is the class that the discriminator thinks the image # belongs to. fake = Dense(1, activation='sigmoid', name='generation')(features) aux = Dense(num_classes, activation='softmax', name='auxiliary')(features) return Model(image, [fake, aux]) if __name__ == '__main__': # batch and latent size taken from the paper epochs = 100 batch_size = 100 latent_size = 100 # Adam parameters suggested in https://arxiv.org/abs/1511.06434 adam_lr = 0.0002 adam_beta_1 = 0.5 # build the discriminator print('Discriminator model:') discriminator = build_discriminator() discriminator.compile( optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1), loss=['binary_crossentropy', 'sparse_categorical_crossentropy'] ) discriminator.summary() # build the generator generator = build_generator(latent_size) latent = Input(shape=(latent_size, )) image_class = Input(shape=(1,), dtype='int32') # get a fake image fake = generator([latent, image_class]) # we only want to be able to train generation for the combined model discriminator.trainable = False fake, aux = discriminator(fake) combined = Model([latent, image_class], [fake, aux]) print('Combined model:') combined.compile( optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1), loss=['binary_crossentropy', 'sparse_categorical_crossentropy'] ) combined.summary() # get our mnist data, and force it to be of shape (..., 28, 28, 1) with # range [-1, 1] (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = (x_train.astype(np.float32) - 127.5) / 127.5 x_train = np.expand_dims(x_train, axis=-1) x_test = (x_test.astype(np.float32) - 127.5) / 127.5 x_test = np.expand_dims(x_test, axis=-1) num_train, num_test = x_train.shape[0], x_test.shape[0] train_history = defaultdict(list) test_history = defaultdict(list) for epoch in range(1, epochs + 1): print('Epoch {}/{}'.format(epoch, epochs)) num_batches = int(np.ceil(x_train.shape[0] / float(batch_size))) progress_bar = Progbar(target=num_batches) epoch_gen_loss = [] epoch_disc_loss = [] for index in range(num_batches): # get a batch of real images image_batch = x_train[index * batch_size:(index + 1) * batch_size] label_batch = y_train[index * batch_size:(index + 1) * batch_size] # generate a new batch of noise noise = np.random.uniform(-1, 1, (len(image_batch), latent_size)) # sample some labels from p_c sampled_labels = np.random.randint(0, num_classes, len(image_batch)) # generate a batch of fake images, using the generated labels as a # conditioner. We reshape the sampled labels to be # (len(image_batch), 1) so that we can feed them into the embedding # layer as a length one sequence generated_images = generator.predict( [noise, sampled_labels.reshape((-1, 1))], verbose=0) x = np.concatenate((image_batch, generated_images)) # use one-sided soft real/fake labels # Salimans et al., 2016 # https://arxiv.org/pdf/1606.03498.pdf (Section 3.4) soft_zero, soft_one = 0, 0.95 y = np.array( [soft_one] * len(image_batch) + [soft_zero] * len(image_batch)) aux_y = np.concatenate((label_batch, sampled_labels), axis=0) # we don't want the discriminator to also maximize the classification # accuracy of the auxiliary classifier on generated images, so we # don't train discriminator to produce class labels for generated # images (see https://openreview.net/forum?id=rJXTf9Bxg). # To preserve sum of sample weights for the auxiliary classifier, # we assign sample weight of 2 to the real images. disc_sample_weight = [np.ones(2 * len(image_batch)), np.concatenate((np.ones(len(image_batch)) * 2, np.zeros(len(image_batch))))] # see if the discriminator can figure itself out... epoch_disc_loss.append(discriminator.train_on_batch( x, [y, aux_y], sample_weight=disc_sample_weight)) # make new noise. we generate 2 * batch size here such that we have # the generator optimize over an identical number of images as the # discriminator noise = np.random.uniform(-1, 1, (2 * len(image_batch), latent_size)) sampled_labels = np.random.randint(0, num_classes, 2 * len(image_batch)) # we want to train the generator to trick the discriminator # For the generator, we want all the {fake, not-fake} labels to say # not-fake trick = np.ones(2 * len(image_batch)) * soft_one epoch_gen_loss.append(combined.train_on_batch( [noise, sampled_labels.reshape((-1, 1))], [trick, sampled_labels])) progress_bar.update(index + 1) print('Testing for epoch {}:'.format(epoch)) # evaluate the testing loss here # generate a new batch of noise noise = np.random.uniform(-1, 1, (num_test, latent_size)) # sample some labels from p_c and generate images from them sampled_labels = np.random.randint(0, num_classes, num_test) generated_images = generator.predict( [noise, sampled_labels.reshape((-1, 1))], verbose=False) x = np.concatenate((x_test, generated_images)) y = np.array([1] * num_test + [0] * num_test) aux_y = np.concatenate((y_test, sampled_labels), axis=0) # see if the discriminator can figure itself out... discriminator_test_loss = discriminator.evaluate( x, [y, aux_y], verbose=False) discriminator_train_loss = np.mean(np.array(epoch_disc_loss), axis=0) # make new noise noise = np.random.uniform(-1, 1, (2 * num_test, latent_size)) sampled_labels = np.random.randint(0, num_classes, 2 * num_test) trick = np.ones(2 * num_test) generator_test_loss = combined.evaluate( [noise, sampled_labels.reshape((-1, 1))], [trick, sampled_labels], verbose=False) generator_train_loss = np.mean(np.array(epoch_gen_loss), axis=0) # generate an epoch report on performance train_history['generator'].append(generator_train_loss) train_history['discriminator'].append(discriminator_train_loss) test_history['generator'].append(generator_test_loss) test_history['discriminator'].append(discriminator_test_loss) print('{0:<22s} | {1:4s} | {2:15s} | {3:5s}'.format( 'component', *discriminator.metrics_names)) print('-' * 65) ROW_FMT = '{0:<22s} | {1:<4.2f} | {2:<15.4f} | {3:<5.4f}' print(ROW_FMT.format('generator (train)', *train_history['generator'][-1])) print(ROW_FMT.format('generator (test)', *test_history['generator'][-1])) print(ROW_FMT.format('discriminator (train)', *train_history['discriminator'][-1])) print(ROW_FMT.format('discriminator (test)', *test_history['discriminator'][-1])) # save weights every epoch generator.save_weights( 'params_generator_epoch_{0:03d}.hdf5'.format(epoch), True) discriminator.save_weights( 'params_discriminator_epoch_{0:03d}.hdf5'.format(epoch), True) # generate some digits to display num_rows = 40 noise = np.tile(np.random.uniform(-1, 1, (num_rows, latent_size)), (num_classes, 1)) sampled_labels = np.array([ [i] * num_rows for i in range(num_classes) ]).reshape(-1, 1) # get a batch to display generated_images = generator.predict( [noise, sampled_labels], verbose=0) # prepare real images sorted by class label real_labels = y_train[(epoch - 1) * num_rows * num_classes: epoch * num_rows * num_classes] indices = np.argsort(real_labels, axis=0) real_images = x_train[(epoch - 1) * num_rows * num_classes: epoch * num_rows * num_classes][indices] # display generated images, white separator, real images img = np.concatenate( (generated_images, np.repeat(np.ones_like(x_train[:1]), num_rows, axis=0), real_images)) # arrange them into a grid img = (np.concatenate([r.reshape(-1, 28) for r in np.split(img, 2 * num_classes + 1) ], axis=-1) * 127.5 + 127.5).astype(np.uint8) Image.fromarray(img).save( 'plot_epoch_{0:03d}_generated.png'.format(epoch)) with open('acgan-history.pkl', 'wb') as f: pickle.dump({'train': train_history, 'test': test_history}, f)","title":"Hardware           | Backend | Time / Epoch"},{"location":"4-Examples/mnist_cnn/","text":"Trains a simple convnet on the MNIST dataset. Gets to 99.25% test accuracy after 12 epochs (there is still a lot of margin for parameter tuning). 16 seconds per epoch on a GRID K520 GPU. from __future__ import print_function import keras from keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense, Dropout, Flatten from keras.layers import Conv2D, MaxPooling2D from keras import backend as K batch_size = 128 num_classes = 10 epochs = 12 # input image dimensions img_rows, img_cols = 28, 28 # the data, split between train and test sets (x_train, y_train), (x_test, y_test) = mnist.load_data() if K.image_data_format() == 'channels_first': x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols) x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols) input_shape = (1, img_rows, img_cols) else: x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1) x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1) input_shape = (img_rows, img_cols, 1) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') # convert class vectors to binary class matrices y_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes) model = Sequential() model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape)) model.add(Conv2D(64, (3, 3), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Flatten()) model.add(Dense(128, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(num_classes, activation='softmax')) model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy']) model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test)) score = model.evaluate(x_test, y_test, verbose=0) print('Test loss:', score[0]) print('Test accuracy:', score[1])","title":"Mnist cnn"},{"location":"4-Examples/mnist_dataset_api/","text":"MNIST classification with TensorFlow's Dataset API. Introduced in TensorFlow 1.3, the Dataset API is now the standard method for loading data into TensorFlow models. A Dataset is a sequence of elements, which are themselves composed of tf.Tensor components. For more details, see: https://www.tensorflow.org/programmers_guide/datasets To use this with Keras, we make a dataset out of elements of the form (input batch, output batch). From there, we create a one-shot iterator and a graph node corresponding to its get_next() method. Its components are then provided to the network's Input layer and the Model.compile() method, respectively. This example is intended to closely follow the mnist_tfrecord.py example. import numpy as np import os import tempfile import keras from keras import backend as K from keras import layers from keras.datasets import mnist import tensorflow as tf if K.backend() != 'tensorflow': raise RuntimeError('This example can only run with the TensorFlow backend,' ' because it requires the Datset API, which is not' ' supported on other platforms.') def cnn_layers(inputs): x = layers.Conv2D(32, (3, 3), activation='relu', padding='valid')(inputs) x = layers.MaxPooling2D(pool_size=(2, 2))(x) x = layers.Conv2D(64, (3, 3), activation='relu')(x) x = layers.MaxPooling2D(pool_size=(2, 2))(x) x = layers.Flatten()(x) x = layers.Dense(512, activation='relu')(x) x = layers.Dropout(0.5)(x) predictions = layers.Dense(num_classes, activation='softmax', name='x_train_out')(x) return predictions batch_size = 128 buffer_size = 10000 steps_per_epoch = int(np.ceil(60000 / float(batch_size))) # = 469 epochs = 5 num_classes = 10 (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.astype(np.float32) / 255 x_train = np.expand_dims(x_train, -1) y_train = tf.one_hot(y_train, num_classes) # Create the dataset and its associated one-shot iterator. dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)) dataset = dataset.repeat() dataset = dataset.shuffle(buffer_size) dataset = dataset.batch(batch_size) iterator = dataset.make_one_shot_iterator() # Model creation using tensors from the get_next() graph node. inputs, targets = iterator.get_next() model_input = layers.Input(tensor=inputs) model_output = cnn_layers(model_input) train_model = keras.models.Model(inputs=model_input, outputs=model_output) train_model.compile(optimizer=keras.optimizers.RMSprop(lr=2e-3, decay=1e-5), loss='categorical_crossentropy', metrics=['accuracy'], target_tensors=[targets]) train_model.summary() train_model.fit(epochs=epochs, steps_per_epoch=steps_per_epoch) # Save the model weights. weight_path = os.path.join(tempfile.gettempdir(), 'saved_wt.h5') train_model.save_weights(weight_path) # Clean up the TF session. K.clear_session() # Second session to test loading trained model without tensors. x_test = x_test.astype(np.float32) x_test = np.expand_dims(x_test, -1) x_test_inp = layers.Input(shape=x_test.shape[1:]) test_out = cnn_layers(x_test_inp) test_model = keras.models.Model(inputs=x_test_inp, outputs=test_out) test_model.load_weights(weight_path) test_model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy']) test_model.summary() loss, acc = test_model.evaluate(x_test, y_test, num_classes) print('\\nTest accuracy: {0}'.format(acc))","title":"Mnist dataset api"},{"location":"4-Examples/mnist_denoising_autoencoder/","text":"Trains a denoising autoencoder on MNIST dataset. Denoising is one of the classic applications of autoencoders. The denoising process removes unwanted noise that corrupted the true signal. Noise + Data ---> Denoising Autoencoder ---> Data Given a training dataset of corrupted data as input and true signal as output, a denoising autoencoder can recover the hidden structure to generate clean data. This example has modular design. The encoder, decoder and autoencoder are 3 models that share weights. For example, after training the autoencoder, the encoder can be used to generate latent vectors of input data for low-dim visualization like PCA or TSNE. from __future__ import absolute_import from __future__ import division from __future__ import print_function import keras from keras.layers import Activation, Dense, Input from keras.layers import Conv2D, Flatten from keras.layers import Reshape, Conv2DTranspose from keras.models import Model from keras import backend as K from keras.datasets import mnist import numpy as np import matplotlib.pyplot as plt from PIL import Image np.random.seed(1337) # MNIST dataset (x_train, _), (x_test, _) = mnist.load_data() image_size = x_train.shape[1] x_train = np.reshape(x_train, [-1, image_size, image_size, 1]) x_test = np.reshape(x_test, [-1, image_size, image_size, 1]) x_train = x_train.astype('float32') / 255 x_test = x_test.astype('float32') / 255 # Generate corrupted MNIST images by adding noise with normal dist # centered at 0.5 and std=0.5 noise = np.random.normal(loc=0.5, scale=0.5, size=x_train.shape) x_train_noisy = x_train + noise noise = np.random.normal(loc=0.5, scale=0.5, size=x_test.shape) x_test_noisy = x_test + noise x_train_noisy = np.clip(x_train_noisy, 0., 1.) x_test_noisy = np.clip(x_test_noisy, 0., 1.) # Network parameters input_shape = (image_size, image_size, 1) batch_size = 128 kernel_size = 3 latent_dim = 16 # Encoder/Decoder number of CNN layers and filters per layer layer_filters = [32, 64] # Build the Autoencoder Model # First build the Encoder Model inputs = Input(shape=input_shape, name='encoder_input') x = inputs # Stack of Conv2D blocks # Notes: # 1) Use Batch Normalization before ReLU on deep networks # 2) Use MaxPooling2D as alternative to strides>1 # - faster but not as good as strides>1 for filters in layer_filters: x = Conv2D(filters=filters, kernel_size=kernel_size, strides=2, activation='relu', padding='same')(x) # Shape info needed to build Decoder Model shape = K.int_shape(x) # Generate the latent vector x = Flatten()(x) latent = Dense(latent_dim, name='latent_vector')(x) # Instantiate Encoder Model encoder = Model(inputs, latent, name='encoder') encoder.summary() # Build the Decoder Model latent_inputs = Input(shape=(latent_dim,), name='decoder_input') x = Dense(shape[1] * shape[2] * shape[3])(latent_inputs) x = Reshape((shape[1], shape[2], shape[3]))(x) # Stack of Transposed Conv2D blocks # Notes: # 1) Use Batch Normalization before ReLU on deep networks # 2) Use UpSampling2D as alternative to strides>1 # - faster but not as good as strides>1 for filters in layer_filters[::-1]: x = Conv2DTranspose(filters=filters, kernel_size=kernel_size, strides=2, activation='relu', padding='same')(x) x = Conv2DTranspose(filters=1, kernel_size=kernel_size, padding='same')(x) outputs = Activation('sigmoid', name='decoder_output')(x) # Instantiate Decoder Model decoder = Model(latent_inputs, outputs, name='decoder') decoder.summary() # Autoencoder = Encoder + Decoder # Instantiate Autoencoder Model autoencoder = Model(inputs, decoder(encoder(inputs)), name='autoencoder') autoencoder.summary() autoencoder.compile(loss='mse', optimizer='adam') # Train the autoencoder autoencoder.fit(x_train_noisy, x_train, validation_data=(x_test_noisy, x_test), epochs=30, batch_size=batch_size) # Predict the Autoencoder output from corrupted test images x_decoded = autoencoder.predict(x_test_noisy) # Display the 1st 8 corrupted and denoised images rows, cols = 10, 30 num = rows * cols imgs = np.concatenate([x_test[:num], x_test_noisy[:num], x_decoded[:num]]) imgs = imgs.reshape((rows * 3, cols, image_size, image_size)) imgs = np.vstack(np.split(imgs, rows, axis=1)) imgs = imgs.reshape((rows * 3, -1, image_size, image_size)) imgs = np.vstack([np.hstack(i) for i in imgs]) imgs = (imgs * 255).astype(np.uint8) plt.figure() plt.axis('off') plt.title('Original images: top rows, ' 'Corrupted Input: middle rows, ' 'Denoised Input: third rows') plt.imshow(imgs, interpolation='none', cmap='gray') Image.fromarray(imgs).save('corrupted_and_denoised.png') plt.show()","title":"Mnist denoising autoencoder"},{"location":"4-Examples/mnist_hierarchical_rnn/","text":"Example of using Hierarchical RNN (HRNN) to classify MNIST digits. HRNNs can learn across multiple levels of temporal hierarchy over a complex sequence. Usually, the first recurrent layer of an HRNN encodes a sentence (e.g. of word vectors) into a sentence vector. The second recurrent layer then encodes a sequence of such vectors (encoded by the first layer) into a document vector. This document vector is considered to preserve both the word-level and sentence-level structure of the context. References [A Hierarchical Neural Autoencoder for Paragraphs and Documents] (https://arxiv.org/abs/1506.01057) Encodes paragraphs and documents with HRNN. Results have shown that HRNN outperforms standard RNNs and may play some role in more sophisticated generation tasks like summarization or question answering. [Hierarchical recurrent neural network for skeleton based action recognition] (http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7298714) Achieved state-of-the-art results on skeleton based action recognition with 3 levels of bidirectional HRNN combined with fully connected layers. In the below MNIST example the first LSTM layer first encodes every column of pixels of shape (28, 1) to a column vector of shape (128,). The second LSTM layer encodes then these 28 column vectors of shape (28, 128) to a image vector representing the whole image. A final Dense layer is added for prediction. After 5 epochs: train acc: 0.9858, val acc: 0.9864 from __future__ import print_function import keras from keras.datasets import mnist from keras.models import Model from keras.layers import Input, Dense, TimeDistributed from keras.layers import LSTM # Training parameters. batch_size = 32 num_classes = 10 epochs = 5 # Embedding dimensions. row_hidden = 128 col_hidden = 128 # The data, split between train and test sets. (x_train, y_train), (x_test, y_test) = mnist.load_data() # Reshapes data to 4D for Hierarchical RNN. x_train = x_train.reshape(x_train.shape[0], 28, 28, 1) x_test = x_test.reshape(x_test.shape[0], 28, 28, 1) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') # Converts class vectors to binary class matrices. y_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes) row, col, pixel = x_train.shape[1:] # 4D input. x = Input(shape=(row, col, pixel)) # Encodes a row of pixels using TimeDistributed Wrapper. encoded_rows = TimeDistributed(LSTM(row_hidden))(x) # Encodes columns of encoded rows. encoded_columns = LSTM(col_hidden)(encoded_rows) # Final predictions and model. prediction = Dense(num_classes, activation='softmax')(encoded_columns) model = Model(x, prediction) model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) # Training. model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test)) # Evaluation. scores = model.evaluate(x_test, y_test, verbose=0) print('Test loss:', scores[0]) print('Test accuracy:', scores[1])","title":"Mnist hierarchical rnn"},{"location":"4-Examples/mnist_hierarchical_rnn/#references","text":"[A Hierarchical Neural Autoencoder for Paragraphs and Documents] (https://arxiv.org/abs/1506.01057) Encodes paragraphs and documents with HRNN. Results have shown that HRNN outperforms standard RNNs and may play some role in more sophisticated generation tasks like summarization or question answering. [Hierarchical recurrent neural network for skeleton based action recognition] (http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7298714) Achieved state-of-the-art results on skeleton based action recognition with 3 levels of bidirectional HRNN combined with fully connected layers. In the below MNIST example the first LSTM layer first encodes every column of pixels of shape (28, 1) to a column vector of shape (128,). The second LSTM layer encodes then these 28 column vectors of shape (28, 128) to a image vector representing the whole image. A final Dense layer is added for prediction. After 5 epochs: train acc: 0.9858, val acc: 0.9864 from __future__ import print_function import keras from keras.datasets import mnist from keras.models import Model from keras.layers import Input, Dense, TimeDistributed from keras.layers import LSTM # Training parameters. batch_size = 32 num_classes = 10 epochs = 5 # Embedding dimensions. row_hidden = 128 col_hidden = 128 # The data, split between train and test sets. (x_train, y_train), (x_test, y_test) = mnist.load_data() # Reshapes data to 4D for Hierarchical RNN. x_train = x_train.reshape(x_train.shape[0], 28, 28, 1) x_test = x_test.reshape(x_test.shape[0], 28, 28, 1) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') # Converts class vectors to binary class matrices. y_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes) row, col, pixel = x_train.shape[1:] # 4D input. x = Input(shape=(row, col, pixel)) # Encodes a row of pixels using TimeDistributed Wrapper. encoded_rows = TimeDistributed(LSTM(row_hidden))(x) # Encodes columns of encoded rows. encoded_columns = LSTM(col_hidden)(encoded_rows) # Final predictions and model. prediction = Dense(num_classes, activation='softmax')(encoded_columns) model = Model(x, prediction) model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) # Training. model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test)) # Evaluation. scores = model.evaluate(x_test, y_test, verbose=0) print('Test loss:', scores[0]) print('Test accuracy:', scores[1])","title":"References"},{"location":"4-Examples/mnist_irnn/","text":"This is a reproduction of the IRNN experiment with pixel-by-pixel sequential MNIST in \"A Simple Way to Initialize Recurrent Networks of Rectified Linear Units\" by Quoc V. Le, Navdeep Jaitly, Geoffrey E. Hinton arxiv:1504.00941v2 [cs.NE] 7 Apr 2015 http://arxiv.org/pdf/1504.00941v2.pdf Optimizer is replaced with RMSprop which yields more stable and steady improvement. Reaches 0.93 train/test accuracy after 900 epochs (which roughly corresponds to 1687500 steps in the original paper.) from __future__ import print_function import keras from keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense, Activation from keras.layers import SimpleRNN from keras import initializers from keras.optimizers import RMSprop batch_size = 32 num_classes = 10 epochs = 200 hidden_units = 100 learning_rate = 1e-6 clip_norm = 1.0 # the data, split between train and test sets (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.reshape(x_train.shape[0], -1, 1) x_test = x_test.reshape(x_test.shape[0], -1, 1) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') # convert class vectors to binary class matrices y_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes) print('Evaluate IRNN...') model = Sequential() model.add(SimpleRNN(hidden_units, kernel_initializer=initializers.RandomNormal(stddev=0.001), recurrent_initializer=initializers.Identity(gain=1.0), activation='relu', input_shape=x_train.shape[1:])) model.add(Dense(num_classes)) model.add(Activation('softmax')) rmsprop = RMSprop(lr=learning_rate) model.compile(loss='categorical_crossentropy', optimizer=rmsprop, metrics=['accuracy']) model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test)) scores = model.evaluate(x_test, y_test, verbose=0) print('IRNN test score:', scores[0]) print('IRNN test accuracy:', scores[1])","title":"Mnist irnn"},{"location":"4-Examples/mnist_mlp/","text":"Trains a simple deep NN on the MNIST dataset. Gets to 98.40% test accuracy after 20 epochs (there is a lot of margin for parameter tuning). 2 seconds per epoch on a K520 GPU. from __future__ import print_function import keras from keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense, Dropout from keras.optimizers import RMSprop batch_size = 128 num_classes = 10 epochs = 20 # the data, split between train and test sets (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.reshape(60000, 784) x_test = x_test.reshape(10000, 784) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') # convert class vectors to binary class matrices y_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes) model = Sequential() model.add(Dense(512, activation='relu', input_shape=(784,))) model.add(Dropout(0.2)) model.add(Dense(512, activation='relu')) model.add(Dropout(0.2)) model.add(Dense(num_classes, activation='softmax')) model.summary() model.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy']) history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test)) score = model.evaluate(x_test, y_test, verbose=0) print('Test loss:', score[0]) print('Test accuracy:', score[1])","title":"Mnist mlp"},{"location":"4-Examples/mnist_net2net/","text":"This is an implementation of Net2Net experiment with MNIST in 'Net2Net: Accelerating Learning via Knowledge Transfer' by Tianqi Chen, Ian Goodfellow, and Jonathon Shlens arXiv:1511.05641v4 [cs.LG] 23 Apr 2016 http://arxiv.org/abs/1511.05641 Notes What: Net2Net is a group of methods to transfer knowledge from a teacher neural net to a student net,so that the student net can be trained faster than from scratch. The paper discussed two specific methods of Net2Net, i.e. Net2WiderNet and Net2DeeperNet. Net2WiderNet replaces a model with an equivalent wider model that has more units in each hidden layer. Net2DeeperNet replaces a model with an equivalent deeper model. Both are based on the idea of 'function-preserving transformations of neural nets'. Why: Enable fast exploration of multiple neural nets in experimentation and design process,by creating a series of wider and deeper models with transferable knowledge. Enable 'lifelong learning system' by gradually adjusting model complexity to data availability,and reusing transferable knowledge. Experiments Teacher model: a basic CNN model trained on MNIST for 3 epochs. Net2WiderNet experiment: Student model has a wider Conv2D layer and a wider FC layer. Comparison of 'random-padding' vs 'net2wider' weight initialization. With both methods, after 1 epoch, student model should perform as well as teacher model, but 'net2wider' is slightly better. Net2DeeperNet experiment: Student model has an extra Conv2D layer and an extra FC layer. Comparison of 'random-init' vs 'net2deeper' weight initialization. After 1 epoch, performance of 'net2deeper' is better than 'random-init'. Hyper-parameters: SGD with momentum=0.9 is used for training teacher and student models. Learning rate adjustment: it's suggested to reduce learning rate to 1/10 for student model. Addition of noise in 'net2wider' is used to break weight symmetry and thus enable full capacity of student models. It is optional when a Dropout layer is used. Results Tested with TF backend and 'channels_last' image_data_format. Running on GPU GeForce GTX Titan X Maxwell Performance Comparisons - validation loss values during first 3 epochs: Teacher model ... (0) teacher_model: 0.0537 0.0354 0.0356 Experiment of Net2WiderNet ... (1) wider_random_pad: 0.0320 0.0317 0.0289 (2) wider_net2wider: 0.0271 0.0274 0.0270 Experiment of Net2DeeperNet ... (3) deeper_random_init: 0.0682 0.0506 0.0468 (4) deeper_net2deeper: 0.0292 0.0294 0.0286 from __future__ import print_function import numpy as np import keras from keras import backend as K from keras.models import Sequential from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten from keras.optimizers import SGD from keras.datasets import mnist if K.image_data_format() == 'channels_first': input_shape = (1, 28, 28) # image shape else: input_shape = (28, 28, 1) # image shape num_classes = 10 # number of classes epochs = 3 # load and pre-process data def preprocess_input(x): return x.astype('float32').reshape((-1,) + input_shape) / 255 def preprocess_output(y): return keras.utils.to_categorical(y) (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = map(preprocess_input, [x_train, x_test]) y_train, y_test = map(preprocess_output, [y_train, y_test]) print('Loading MNIST data...') print('x_train shape:', x_train.shape, 'y_train shape:', y_train.shape) print('x_test shape:', x_test.shape, 'y_test shape', y_test.shape) # knowledge transfer algorithms def wider2net_conv2d(teacher_w1, teacher_b1, teacher_w2, new_width, init): '''Get initial weights for a wider conv2d layer with a bigger filters, by 'random-padding' or 'net2wider'. # Arguments teacher_w1: `weight` of conv2d layer to become wider, of shape (filters1, num_channel1, kh1, kw1) teacher_b1: `bias` of conv2d layer to become wider, of shape (filters1, ) teacher_w2: `weight` of next connected conv2d layer, of shape (filters2, num_channel2, kh2, kw2) new_width: new `filters` for the wider conv2d layer init: initialization algorithm for new weights, either 'random-pad' or 'net2wider' ''' assert teacher_w1.shape[0] == teacher_w2.shape[1], ( 'successive layers from teacher model should have compatible shapes') assert teacher_w1.shape[3] == teacher_b1.shape[0], ( 'weight and bias from same layer should have compatible shapes') assert new_width > teacher_w1.shape[3], ( 'new width (filters) should be bigger than the existing one') n = new_width - teacher_w1.shape[3] if init == 'random-pad': new_w1 = np.random.normal(0, 0.1, size=teacher_w1.shape[:3] + (n,)) new_b1 = np.ones(n) * 0.1 new_w2 = np.random.normal( 0, 0.1, size=teacher_w2.shape[:2] + (n, teacher_w2.shape[3])) elif init == 'net2wider': index = np.random.randint(teacher_w1.shape[3], size=n) factors = np.bincount(index)[index] + 1. new_w1 = teacher_w1[:, :, :, index] new_b1 = teacher_b1[index] new_w2 = teacher_w2[:, :, index, :] / factors.reshape((1, 1, -1, 1)) else: raise ValueError('Unsupported weight initializer: %s' % init) student_w1 = np.concatenate((teacher_w1, new_w1), axis=3) if init == 'random-pad': student_w2 = np.concatenate((teacher_w2, new_w2), axis=2) elif init == 'net2wider': # add small noise to break symmetry, so that student model will have # full capacity later noise = np.random.normal(0, 5e-2 * new_w2.std(), size=new_w2.shape) student_w2 = np.concatenate((teacher_w2, new_w2 + noise), axis=2) student_w2[:, :, index, :] = new_w2 student_b1 = np.concatenate((teacher_b1, new_b1), axis=0) return student_w1, student_b1, student_w2 def wider2net_fc(teacher_w1, teacher_b1, teacher_w2, new_width, init): '''Get initial weights for a wider fully connected (dense) layer with a bigger nout, by 'random-padding' or 'net2wider'. # Arguments teacher_w1: `weight` of fc layer to become wider, of shape (nin1, nout1) teacher_b1: `bias` of fc layer to become wider, of shape (nout1, ) teacher_w2: `weight` of next connected fc layer, of shape (nin2, nout2) new_width: new `nout` for the wider fc layer init: initialization algorithm for new weights, either 'random-pad' or 'net2wider' ''' assert teacher_w1.shape[1] == teacher_w2.shape[0], ( 'successive layers from teacher model should have compatible shapes') assert teacher_w1.shape[1] == teacher_b1.shape[0], ( 'weight and bias from same layer should have compatible shapes') assert new_width > teacher_w1.shape[1], ( 'new width (nout) should be bigger than the existing one') n = new_width - teacher_w1.shape[1] if init == 'random-pad': new_w1 = np.random.normal(0, 0.1, size=(teacher_w1.shape[0], n)) new_b1 = np.ones(n) * 0.1 new_w2 = np.random.normal(0, 0.1, size=(n, teacher_w2.shape[1])) elif init == 'net2wider': index = np.random.randint(teacher_w1.shape[1], size=n) factors = np.bincount(index)[index] + 1. new_w1 = teacher_w1[:, index] new_b1 = teacher_b1[index] new_w2 = teacher_w2[index, :] / factors[:, np.newaxis] else: raise ValueError('Unsupported weight initializer: %s' % init) student_w1 = np.concatenate((teacher_w1, new_w1), axis=1) if init == 'random-pad': student_w2 = np.concatenate((teacher_w2, new_w2), axis=0) elif init == 'net2wider': # add small noise to break symmetry, so that student model will have # full capacity later noise = np.random.normal(0, 5e-2 * new_w2.std(), size=new_w2.shape) student_w2 = np.concatenate((teacher_w2, new_w2 + noise), axis=0) student_w2[index, :] = new_w2 student_b1 = np.concatenate((teacher_b1, new_b1), axis=0) return student_w1, student_b1, student_w2 def deeper2net_conv2d(teacher_w): '''Get initial weights for a deeper conv2d layer by net2deeper'. # Arguments teacher_w: `weight` of previous conv2d layer, of shape (kh, kw, num_channel, filters) ''' kh, kw, num_channel, filters = teacher_w.shape student_w = np.zeros_like(teacher_w) for i in range(filters): student_w[(kh - 1) // 2, (kw - 1) // 2, i, i] = 1. student_b = np.zeros(filters) return student_w, student_b def copy_weights(teacher_model, student_model, layer_names): '''Copy weights from teacher_model to student_model, for layers with names listed in layer_names ''' for name in layer_names: weights = teacher_model.get_layer(name=name).get_weights() student_model.get_layer(name=name).set_weights(weights) # methods to construct teacher_model and student_models def make_teacher_model(x_train, y_train, x_test, y_test, epochs): '''Train and benchmark performance of a simple CNN. (0) Teacher model ''' model = Sequential() model.add(Conv2D(64, 3, input_shape=input_shape, padding='same', name='conv1')) model.add(MaxPooling2D(2, name='pool1')) model.add(Conv2D(64, 3, padding='same', name='conv2')) model.add(MaxPooling2D(2, name='pool2')) model.add(Flatten(name='flatten')) model.add(Dense(64, activation='relu', name='fc1')) model.add(Dense(num_classes, activation='softmax', name='fc2')) model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9), metrics=['accuracy']) model.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test)) return model def make_wider_student_model(teacher_model, x_train, y_train, x_test, y_test, init, epochs): '''Train a wider student model based on teacher_model, with either 'random-pad' (baseline) or 'net2wider' ''' new_conv1_width = 128 new_fc1_width = 128 model = Sequential() # a wider conv1 compared to teacher_model model.add(Conv2D(new_conv1_width, 3, input_shape=input_shape, padding='same', name='conv1')) model.add(MaxPooling2D(2, name='pool1')) model.add(Conv2D(64, 3, padding='same', name='conv2')) model.add(MaxPooling2D(2, name='pool2')) model.add(Flatten(name='flatten')) # a wider fc1 compared to teacher model model.add(Dense(new_fc1_width, activation='relu', name='fc1')) model.add(Dense(num_classes, activation='softmax', name='fc2')) # The weights for other layers need to be copied from teacher_model # to student_model, except for widened layers # and their immediate downstreams, which will be initialized separately. # For this example there are no other layers that need to be copied. w_conv1, b_conv1 = teacher_model.get_layer('conv1').get_weights() w_conv2, b_conv2 = teacher_model.get_layer('conv2').get_weights() new_w_conv1, new_b_conv1, new_w_conv2 = wider2net_conv2d( w_conv1, b_conv1, w_conv2, new_conv1_width, init) model.get_layer('conv1').set_weights([new_w_conv1, new_b_conv1]) model.get_layer('conv2').set_weights([new_w_conv2, b_conv2]) w_fc1, b_fc1 = teacher_model.get_layer('fc1').get_weights() w_fc2, b_fc2 = teacher_model.get_layer('fc2').get_weights() new_w_fc1, new_b_fc1, new_w_fc2 = wider2net_fc( w_fc1, b_fc1, w_fc2, new_fc1_width, init) model.get_layer('fc1').set_weights([new_w_fc1, new_b_fc1]) model.get_layer('fc2').set_weights([new_w_fc2, b_fc2]) model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.001, momentum=0.9), metrics=['accuracy']) model.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test)) def make_deeper_student_model(teacher_model, x_train, y_train, x_test, y_test, init, epochs): '''Train a deeper student model based on teacher_model, with either 'random-init' (baseline) or 'net2deeper' ''' model = Sequential() model.add(Conv2D(64, 3, input_shape=input_shape, padding='same', name='conv1')) model.add(MaxPooling2D(2, name='pool1')) model.add(Conv2D(64, 3, padding='same', name='conv2')) # add another conv2d layer to make original conv2 deeper if init == 'net2deeper': prev_w, _ = model.get_layer('conv2').get_weights() new_weights = deeper2net_conv2d(prev_w) model.add(Conv2D(64, 3, padding='same', name='conv2-deeper', weights=new_weights)) elif init == 'random-init': model.add(Conv2D(64, 3, padding='same', name='conv2-deeper')) else: raise ValueError('Unsupported weight initializer: %s' % init) model.add(MaxPooling2D(2, name='pool2')) model.add(Flatten(name='flatten')) model.add(Dense(64, activation='relu', name='fc1')) # add another fc layer to make original fc1 deeper if init == 'net2deeper': # net2deeper for fc layer with relu, is just an identity initializer model.add(Dense(64, kernel_initializer='identity', activation='relu', name='fc1-deeper')) elif init == 'random-init': model.add(Dense(64, activation='relu', name='fc1-deeper')) else: raise ValueError('Unsupported weight initializer: %s' % init) model.add(Dense(num_classes, activation='softmax', name='fc2')) # copy weights for other layers copy_weights(teacher_model, model, layer_names=[ 'conv1', 'conv2', 'fc1', 'fc2']) model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.001, momentum=0.9), metrics=['accuracy']) model.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test)) # experiments setup def net2wider_experiment(): '''Benchmark performances of (1) a wider student model with `random_pad` initializer (2) a wider student model with `Net2WiderNet` initializer ''' print('\\nExperiment of Net2WiderNet ...') print('\\n(1) building wider student model by random padding ...') make_wider_student_model(teacher_model, x_train, y_train, x_test, y_test, init='random-pad', epochs=epochs) print('\\n(2) building wider student model by net2wider ...') make_wider_student_model(teacher_model, x_train, y_train, x_test, y_test, init='net2wider', epochs=epochs) def net2deeper_experiment(): '''Benchmark performances of (3) a deeper student model with `random_init` initializer (4) a deeper student model with `Net2DeeperNet` initializer ''' print('\\nExperiment of Net2DeeperNet ...') print('\\n(3) building deeper student model by random init ...') make_deeper_student_model(teacher_model, x_train, y_train, x_test, y_test, init='random-init', epochs=epochs) print('\\n(4) building deeper student model by net2deeper ...') make_deeper_student_model(teacher_model, x_train, y_train, x_test, y_test, init='net2deeper', epochs=epochs) print('\\n(0) building teacher model ...') teacher_model = make_teacher_model(x_train, y_train, x_test, y_test, epochs=epochs) # run the experiments net2wider_experiment() net2deeper_experiment()","title":"Mnist net2net"},{"location":"4-Examples/mnist_net2net/#notes","text":"What: Net2Net is a group of methods to transfer knowledge from a teacher neural net to a student net,so that the student net can be trained faster than from scratch. The paper discussed two specific methods of Net2Net, i.e. Net2WiderNet and Net2DeeperNet. Net2WiderNet replaces a model with an equivalent wider model that has more units in each hidden layer. Net2DeeperNet replaces a model with an equivalent deeper model. Both are based on the idea of 'function-preserving transformations of neural nets'. Why: Enable fast exploration of multiple neural nets in experimentation and design process,by creating a series of wider and deeper models with transferable knowledge. Enable 'lifelong learning system' by gradually adjusting model complexity to data availability,and reusing transferable knowledge.","title":"Notes"},{"location":"4-Examples/mnist_net2net/#experiments","text":"Teacher model: a basic CNN model trained on MNIST for 3 epochs. Net2WiderNet experiment: Student model has a wider Conv2D layer and a wider FC layer. Comparison of 'random-padding' vs 'net2wider' weight initialization. With both methods, after 1 epoch, student model should perform as well as teacher model, but 'net2wider' is slightly better. Net2DeeperNet experiment: Student model has an extra Conv2D layer and an extra FC layer. Comparison of 'random-init' vs 'net2deeper' weight initialization. After 1 epoch, performance of 'net2deeper' is better than 'random-init'. Hyper-parameters: SGD with momentum=0.9 is used for training teacher and student models. Learning rate adjustment: it's suggested to reduce learning rate to 1/10 for student model. Addition of noise in 'net2wider' is used to break weight symmetry and thus enable full capacity of student models. It is optional when a Dropout layer is used.","title":"Experiments"},{"location":"4-Examples/mnist_net2net/#results","text":"Tested with TF backend and 'channels_last' image_data_format. Running on GPU GeForce GTX Titan X Maxwell Performance Comparisons - validation loss values during first 3 epochs: Teacher model ... (0) teacher_model: 0.0537 0.0354 0.0356 Experiment of Net2WiderNet ... (1) wider_random_pad: 0.0320 0.0317 0.0289 (2) wider_net2wider: 0.0271 0.0274 0.0270 Experiment of Net2DeeperNet ... (3) deeper_random_init: 0.0682 0.0506 0.0468 (4) deeper_net2deeper: 0.0292 0.0294 0.0286 from __future__ import print_function import numpy as np import keras from keras import backend as K from keras.models import Sequential from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten from keras.optimizers import SGD from keras.datasets import mnist if K.image_data_format() == 'channels_first': input_shape = (1, 28, 28) # image shape else: input_shape = (28, 28, 1) # image shape num_classes = 10 # number of classes epochs = 3 # load and pre-process data def preprocess_input(x): return x.astype('float32').reshape((-1,) + input_shape) / 255 def preprocess_output(y): return keras.utils.to_categorical(y) (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = map(preprocess_input, [x_train, x_test]) y_train, y_test = map(preprocess_output, [y_train, y_test]) print('Loading MNIST data...') print('x_train shape:', x_train.shape, 'y_train shape:', y_train.shape) print('x_test shape:', x_test.shape, 'y_test shape', y_test.shape) # knowledge transfer algorithms def wider2net_conv2d(teacher_w1, teacher_b1, teacher_w2, new_width, init): '''Get initial weights for a wider conv2d layer with a bigger filters, by 'random-padding' or 'net2wider'. # Arguments teacher_w1: `weight` of conv2d layer to become wider, of shape (filters1, num_channel1, kh1, kw1) teacher_b1: `bias` of conv2d layer to become wider, of shape (filters1, ) teacher_w2: `weight` of next connected conv2d layer, of shape (filters2, num_channel2, kh2, kw2) new_width: new `filters` for the wider conv2d layer init: initialization algorithm for new weights, either 'random-pad' or 'net2wider' ''' assert teacher_w1.shape[0] == teacher_w2.shape[1], ( 'successive layers from teacher model should have compatible shapes') assert teacher_w1.shape[3] == teacher_b1.shape[0], ( 'weight and bias from same layer should have compatible shapes') assert new_width > teacher_w1.shape[3], ( 'new width (filters) should be bigger than the existing one') n = new_width - teacher_w1.shape[3] if init == 'random-pad': new_w1 = np.random.normal(0, 0.1, size=teacher_w1.shape[:3] + (n,)) new_b1 = np.ones(n) * 0.1 new_w2 = np.random.normal( 0, 0.1, size=teacher_w2.shape[:2] + (n, teacher_w2.shape[3])) elif init == 'net2wider': index = np.random.randint(teacher_w1.shape[3], size=n) factors = np.bincount(index)[index] + 1. new_w1 = teacher_w1[:, :, :, index] new_b1 = teacher_b1[index] new_w2 = teacher_w2[:, :, index, :] / factors.reshape((1, 1, -1, 1)) else: raise ValueError('Unsupported weight initializer: %s' % init) student_w1 = np.concatenate((teacher_w1, new_w1), axis=3) if init == 'random-pad': student_w2 = np.concatenate((teacher_w2, new_w2), axis=2) elif init == 'net2wider': # add small noise to break symmetry, so that student model will have # full capacity later noise = np.random.normal(0, 5e-2 * new_w2.std(), size=new_w2.shape) student_w2 = np.concatenate((teacher_w2, new_w2 + noise), axis=2) student_w2[:, :, index, :] = new_w2 student_b1 = np.concatenate((teacher_b1, new_b1), axis=0) return student_w1, student_b1, student_w2 def wider2net_fc(teacher_w1, teacher_b1, teacher_w2, new_width, init): '''Get initial weights for a wider fully connected (dense) layer with a bigger nout, by 'random-padding' or 'net2wider'. # Arguments teacher_w1: `weight` of fc layer to become wider, of shape (nin1, nout1) teacher_b1: `bias` of fc layer to become wider, of shape (nout1, ) teacher_w2: `weight` of next connected fc layer, of shape (nin2, nout2) new_width: new `nout` for the wider fc layer init: initialization algorithm for new weights, either 'random-pad' or 'net2wider' ''' assert teacher_w1.shape[1] == teacher_w2.shape[0], ( 'successive layers from teacher model should have compatible shapes') assert teacher_w1.shape[1] == teacher_b1.shape[0], ( 'weight and bias from same layer should have compatible shapes') assert new_width > teacher_w1.shape[1], ( 'new width (nout) should be bigger than the existing one') n = new_width - teacher_w1.shape[1] if init == 'random-pad': new_w1 = np.random.normal(0, 0.1, size=(teacher_w1.shape[0], n)) new_b1 = np.ones(n) * 0.1 new_w2 = np.random.normal(0, 0.1, size=(n, teacher_w2.shape[1])) elif init == 'net2wider': index = np.random.randint(teacher_w1.shape[1], size=n) factors = np.bincount(index)[index] + 1. new_w1 = teacher_w1[:, index] new_b1 = teacher_b1[index] new_w2 = teacher_w2[index, :] / factors[:, np.newaxis] else: raise ValueError('Unsupported weight initializer: %s' % init) student_w1 = np.concatenate((teacher_w1, new_w1), axis=1) if init == 'random-pad': student_w2 = np.concatenate((teacher_w2, new_w2), axis=0) elif init == 'net2wider': # add small noise to break symmetry, so that student model will have # full capacity later noise = np.random.normal(0, 5e-2 * new_w2.std(), size=new_w2.shape) student_w2 = np.concatenate((teacher_w2, new_w2 + noise), axis=0) student_w2[index, :] = new_w2 student_b1 = np.concatenate((teacher_b1, new_b1), axis=0) return student_w1, student_b1, student_w2 def deeper2net_conv2d(teacher_w): '''Get initial weights for a deeper conv2d layer by net2deeper'. # Arguments teacher_w: `weight` of previous conv2d layer, of shape (kh, kw, num_channel, filters) ''' kh, kw, num_channel, filters = teacher_w.shape student_w = np.zeros_like(teacher_w) for i in range(filters): student_w[(kh - 1) // 2, (kw - 1) // 2, i, i] = 1. student_b = np.zeros(filters) return student_w, student_b def copy_weights(teacher_model, student_model, layer_names): '''Copy weights from teacher_model to student_model, for layers with names listed in layer_names ''' for name in layer_names: weights = teacher_model.get_layer(name=name).get_weights() student_model.get_layer(name=name).set_weights(weights) # methods to construct teacher_model and student_models def make_teacher_model(x_train, y_train, x_test, y_test, epochs): '''Train and benchmark performance of a simple CNN. (0) Teacher model ''' model = Sequential() model.add(Conv2D(64, 3, input_shape=input_shape, padding='same', name='conv1')) model.add(MaxPooling2D(2, name='pool1')) model.add(Conv2D(64, 3, padding='same', name='conv2')) model.add(MaxPooling2D(2, name='pool2')) model.add(Flatten(name='flatten')) model.add(Dense(64, activation='relu', name='fc1')) model.add(Dense(num_classes, activation='softmax', name='fc2')) model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9), metrics=['accuracy']) model.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test)) return model def make_wider_student_model(teacher_model, x_train, y_train, x_test, y_test, init, epochs): '''Train a wider student model based on teacher_model, with either 'random-pad' (baseline) or 'net2wider' ''' new_conv1_width = 128 new_fc1_width = 128 model = Sequential() # a wider conv1 compared to teacher_model model.add(Conv2D(new_conv1_width, 3, input_shape=input_shape, padding='same', name='conv1')) model.add(MaxPooling2D(2, name='pool1')) model.add(Conv2D(64, 3, padding='same', name='conv2')) model.add(MaxPooling2D(2, name='pool2')) model.add(Flatten(name='flatten')) # a wider fc1 compared to teacher model model.add(Dense(new_fc1_width, activation='relu', name='fc1')) model.add(Dense(num_classes, activation='softmax', name='fc2')) # The weights for other layers need to be copied from teacher_model # to student_model, except for widened layers # and their immediate downstreams, which will be initialized separately. # For this example there are no other layers that need to be copied. w_conv1, b_conv1 = teacher_model.get_layer('conv1').get_weights() w_conv2, b_conv2 = teacher_model.get_layer('conv2').get_weights() new_w_conv1, new_b_conv1, new_w_conv2 = wider2net_conv2d( w_conv1, b_conv1, w_conv2, new_conv1_width, init) model.get_layer('conv1').set_weights([new_w_conv1, new_b_conv1]) model.get_layer('conv2').set_weights([new_w_conv2, b_conv2]) w_fc1, b_fc1 = teacher_model.get_layer('fc1').get_weights() w_fc2, b_fc2 = teacher_model.get_layer('fc2').get_weights() new_w_fc1, new_b_fc1, new_w_fc2 = wider2net_fc( w_fc1, b_fc1, w_fc2, new_fc1_width, init) model.get_layer('fc1').set_weights([new_w_fc1, new_b_fc1]) model.get_layer('fc2').set_weights([new_w_fc2, b_fc2]) model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.001, momentum=0.9), metrics=['accuracy']) model.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test)) def make_deeper_student_model(teacher_model, x_train, y_train, x_test, y_test, init, epochs): '''Train a deeper student model based on teacher_model, with either 'random-init' (baseline) or 'net2deeper' ''' model = Sequential() model.add(Conv2D(64, 3, input_shape=input_shape, padding='same', name='conv1')) model.add(MaxPooling2D(2, name='pool1')) model.add(Conv2D(64, 3, padding='same', name='conv2')) # add another conv2d layer to make original conv2 deeper if init == 'net2deeper': prev_w, _ = model.get_layer('conv2').get_weights() new_weights = deeper2net_conv2d(prev_w) model.add(Conv2D(64, 3, padding='same', name='conv2-deeper', weights=new_weights)) elif init == 'random-init': model.add(Conv2D(64, 3, padding='same', name='conv2-deeper')) else: raise ValueError('Unsupported weight initializer: %s' % init) model.add(MaxPooling2D(2, name='pool2')) model.add(Flatten(name='flatten')) model.add(Dense(64, activation='relu', name='fc1')) # add another fc layer to make original fc1 deeper if init == 'net2deeper': # net2deeper for fc layer with relu, is just an identity initializer model.add(Dense(64, kernel_initializer='identity', activation='relu', name='fc1-deeper')) elif init == 'random-init': model.add(Dense(64, activation='relu', name='fc1-deeper')) else: raise ValueError('Unsupported weight initializer: %s' % init) model.add(Dense(num_classes, activation='softmax', name='fc2')) # copy weights for other layers copy_weights(teacher_model, model, layer_names=[ 'conv1', 'conv2', 'fc1', 'fc2']) model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.001, momentum=0.9), metrics=['accuracy']) model.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test)) # experiments setup def net2wider_experiment(): '''Benchmark performances of (1) a wider student model with `random_pad` initializer (2) a wider student model with `Net2WiderNet` initializer ''' print('\\nExperiment of Net2WiderNet ...') print('\\n(1) building wider student model by random padding ...') make_wider_student_model(teacher_model, x_train, y_train, x_test, y_test, init='random-pad', epochs=epochs) print('\\n(2) building wider student model by net2wider ...') make_wider_student_model(teacher_model, x_train, y_train, x_test, y_test, init='net2wider', epochs=epochs) def net2deeper_experiment(): '''Benchmark performances of (3) a deeper student model with `random_init` initializer (4) a deeper student model with `Net2DeeperNet` initializer ''' print('\\nExperiment of Net2DeeperNet ...') print('\\n(3) building deeper student model by random init ...') make_deeper_student_model(teacher_model, x_train, y_train, x_test, y_test, init='random-init', epochs=epochs) print('\\n(4) building deeper student model by net2deeper ...') make_deeper_student_model(teacher_model, x_train, y_train, x_test, y_test, init='net2deeper', epochs=epochs) print('\\n(0) building teacher model ...') teacher_model = make_teacher_model(x_train, y_train, x_test, y_test, epochs=epochs) # run the experiments net2wider_experiment() net2deeper_experiment()","title":"Results"},{"location":"4-Examples/mnist_siamese/","text":"Trains a Siamese MLP on pairs of digits from the MNIST dataset. It follows Hadsell-et-al.'06 [1] by computing the Euclidean distance on the output of the shared network and by optimizing the contrastive loss (see paper for more details). References Dimensionality Reduction by Learning an Invariant Mapping http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf Gets to 97.2% test accuracy after 20 epochs. 2 seconds per epoch on a Titan X Maxwell GPU from __future__ import absolute_import from __future__ import print_function import numpy as np import random from keras.datasets import mnist from keras.models import Model from keras.layers import Input, Flatten, Dense, Dropout, Lambda from keras.optimizers import RMSprop from keras import backend as K num_classes = 10 epochs = 20 def euclidean_distance(vects): x, y = vects sum_square = K.sum(K.square(x - y), axis=1, keepdims=True) return K.sqrt(K.maximum(sum_square, K.epsilon())) def eucl_dist_output_shape(shapes): shape1, shape2 = shapes return (shape1[0], 1) def contrastive_loss(y_true, y_pred): '''Contrastive loss from Hadsell-et-al.'06 http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf ''' margin = 1 sqaure_pred = K.square(y_pred) margin_square = K.square(K.maximum(margin - y_pred, 0)) return K.mean(y_true * sqaure_pred + (1 - y_true) * margin_square) def create_pairs(x, digit_indices): '''Positive and negative pair creation. Alternates between positive and negative pairs. ''' pairs = [] labels = [] n = min([len(digit_indices[d]) for d in range(num_classes)]) - 1 for d in range(num_classes): for i in range(n): z1, z2 = digit_indices[d][i], digit_indices[d][i + 1] pairs += [[x[z1], x[z2]]] inc = random.randrange(1, num_classes) dn = (d + inc) % num_classes z1, z2 = digit_indices[d][i], digit_indices[dn][i] pairs += [[x[z1], x[z2]]] labels += [1, 0] return np.array(pairs), np.array(labels) def create_base_network(input_shape): '''Base network to be shared (eq. to feature extraction). ''' input = Input(shape=input_shape) x = Flatten()(input) x = Dense(128, activation='relu')(x) x = Dropout(0.1)(x) x = Dense(128, activation='relu')(x) x = Dropout(0.1)(x) x = Dense(128, activation='relu')(x) return Model(input, x) def compute_accuracy(y_true, y_pred): '''Compute classification accuracy with a fixed threshold on distances. ''' pred = y_pred.ravel() < 0.5 return np.mean(pred == y_true) def accuracy(y_true, y_pred): '''Compute classification accuracy with a fixed threshold on distances. ''' return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype))) # the data, split between train and test sets (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 input_shape = x_train.shape[1:] # create training+test positive and negative pairs digit_indices = [np.where(y_train == i)[0] for i in range(num_classes)] tr_pairs, tr_y = create_pairs(x_train, digit_indices) digit_indices = [np.where(y_test == i)[0] for i in range(num_classes)] te_pairs, te_y = create_pairs(x_test, digit_indices) # network definition base_network = create_base_network(input_shape) input_a = Input(shape=input_shape) input_b = Input(shape=input_shape) # because we re-use the same instance `base_network`, # the weights of the network # will be shared across the two branches processed_a = base_network(input_a) processed_b = base_network(input_b) distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b]) model = Model([input_a, input_b], distance) # train rms = RMSprop() model.compile(loss=contrastive_loss, optimizer=rms, metrics=[accuracy]) model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y, batch_size=128, epochs=epochs, validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y)) # compute final accuracy on training and test sets y_pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]]) tr_acc = compute_accuracy(tr_y, y_pred) y_pred = model.predict([te_pairs[:, 0], te_pairs[:, 1]]) te_acc = compute_accuracy(te_y, y_pred) print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc)) print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))","title":"Mnist siamese"},{"location":"4-Examples/mnist_siamese/#references","text":"Dimensionality Reduction by Learning an Invariant Mapping http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf Gets to 97.2% test accuracy after 20 epochs. 2 seconds per epoch on a Titan X Maxwell GPU from __future__ import absolute_import from __future__ import print_function import numpy as np import random from keras.datasets import mnist from keras.models import Model from keras.layers import Input, Flatten, Dense, Dropout, Lambda from keras.optimizers import RMSprop from keras import backend as K num_classes = 10 epochs = 20 def euclidean_distance(vects): x, y = vects sum_square = K.sum(K.square(x - y), axis=1, keepdims=True) return K.sqrt(K.maximum(sum_square, K.epsilon())) def eucl_dist_output_shape(shapes): shape1, shape2 = shapes return (shape1[0], 1) def contrastive_loss(y_true, y_pred): '''Contrastive loss from Hadsell-et-al.'06 http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf ''' margin = 1 sqaure_pred = K.square(y_pred) margin_square = K.square(K.maximum(margin - y_pred, 0)) return K.mean(y_true * sqaure_pred + (1 - y_true) * margin_square) def create_pairs(x, digit_indices): '''Positive and negative pair creation. Alternates between positive and negative pairs. ''' pairs = [] labels = [] n = min([len(digit_indices[d]) for d in range(num_classes)]) - 1 for d in range(num_classes): for i in range(n): z1, z2 = digit_indices[d][i], digit_indices[d][i + 1] pairs += [[x[z1], x[z2]]] inc = random.randrange(1, num_classes) dn = (d + inc) % num_classes z1, z2 = digit_indices[d][i], digit_indices[dn][i] pairs += [[x[z1], x[z2]]] labels += [1, 0] return np.array(pairs), np.array(labels) def create_base_network(input_shape): '''Base network to be shared (eq. to feature extraction). ''' input = Input(shape=input_shape) x = Flatten()(input) x = Dense(128, activation='relu')(x) x = Dropout(0.1)(x) x = Dense(128, activation='relu')(x) x = Dropout(0.1)(x) x = Dense(128, activation='relu')(x) return Model(input, x) def compute_accuracy(y_true, y_pred): '''Compute classification accuracy with a fixed threshold on distances. ''' pred = y_pred.ravel() < 0.5 return np.mean(pred == y_true) def accuracy(y_true, y_pred): '''Compute classification accuracy with a fixed threshold on distances. ''' return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype))) # the data, split between train and test sets (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 input_shape = x_train.shape[1:] # create training+test positive and negative pairs digit_indices = [np.where(y_train == i)[0] for i in range(num_classes)] tr_pairs, tr_y = create_pairs(x_train, digit_indices) digit_indices = [np.where(y_test == i)[0] for i in range(num_classes)] te_pairs, te_y = create_pairs(x_test, digit_indices) # network definition base_network = create_base_network(input_shape) input_a = Input(shape=input_shape) input_b = Input(shape=input_shape) # because we re-use the same instance `base_network`, # the weights of the network # will be shared across the two branches processed_a = base_network(input_a) processed_b = base_network(input_b) distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b]) model = Model([input_a, input_b], distance) # train rms = RMSprop() model.compile(loss=contrastive_loss, optimizer=rms, metrics=[accuracy]) model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y, batch_size=128, epochs=epochs, validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y)) # compute final accuracy on training and test sets y_pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]]) tr_acc = compute_accuracy(tr_y, y_pred) y_pred = model.predict([te_pairs[:, 0], te_pairs[:, 1]]) te_acc = compute_accuracy(te_y, y_pred) print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc)) print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))","title":"References"},{"location":"4-Examples/mnist_sklearn_wrapper/","text":"Example of how to use sklearn wrapper Builds simple CNN models on MNIST and uses sklearn's GridSearchCV to find best model from __future__ import print_function import keras from keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense, Dropout, Activation, Flatten from keras.layers import Conv2D, MaxPooling2D from keras.wrappers.scikit_learn import KerasClassifier from keras import backend as K from sklearn.model_selection import GridSearchCV num_classes = 10 # input image dimensions img_rows, img_cols = 28, 28 # load training data and do basic data normalization (x_train, y_train), (x_test, y_test) = mnist.load_data() if K.image_data_format() == 'channels_first': x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols) x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols) input_shape = (1, img_rows, img_cols) else: x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1) x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1) input_shape = (img_rows, img_cols, 1) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 # convert class vectors to binary class matrices y_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes) def make_model(dense_layer_sizes, filters, kernel_size, pool_size): '''Creates model comprised of 2 convolutional layers followed by dense layers dense_layer_sizes: List of layer sizes. This list has one number for each layer filters: Number of convolutional filters in each convolutional layer kernel_size: Convolutional kernel size pool_size: Size of pooling area for max pooling ''' model = Sequential() model.add(Conv2D(filters, kernel_size, padding='valid', input_shape=input_shape)) model.add(Activation('relu')) model.add(Conv2D(filters, kernel_size)) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=pool_size)) model.add(Dropout(0.25)) model.add(Flatten()) for layer_size in dense_layer_sizes: model.add(Dense(layer_size)) model.add(Activation('relu')) model.add(Dropout(0.5)) model.add(Dense(num_classes)) model.add(Activation('softmax')) model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy']) return model dense_size_candidates = [[32], [64], [32, 32], [64, 64]] my_classifier = KerasClassifier(make_model, batch_size=32) validator = GridSearchCV(my_classifier, param_grid={'dense_layer_sizes': dense_size_candidates, # epochs is avail for tuning even when not # an argument to model building function 'epochs': [3, 6], 'filters': [8], 'kernel_size': [3], 'pool_size': [2]}, scoring='neg_log_loss', n_jobs=1) validator.fit(x_train, y_train) print('The parameters of the best model are: ') print(validator.best_params_) # validator.best_estimator_ returns sklearn-wrapped version of best model. # validator.best_estimator_.model returns the (unwrapped) keras model best_model = validator.best_estimator_.model metric_names = best_model.metrics_names metric_values = best_model.evaluate(x_test, y_test) for metric, value in zip(metric_names, metric_values): print(metric, ': ', value)","title":"Mnist sklearn wrapper"},{"location":"4-Examples/mnist_swwae/","text":"Trains a stacked what-where autoencoder built on residual blocks on the MNIST dataset. It exemplifies two influential methods that have been developed in the past few years. The first is the idea of properly 'unpooling.' During any max pool, the exact location (the 'where') of the maximal value in a pooled receptive field is lost, however it can be very useful in the overall reconstruction of an input image. Therefore, if the 'where' is handed from the encoder to the corresponding decoder layer, features being decoded can be 'placed' in the right location, allowing for reconstructions of much higher fidelity. References Visualizing and Understanding Convolutional Networks Matthew D Zeiler, Rob Fergus https://arxiv.org/abs/1311.2901v3 Stacked What-Where Auto-encoders Junbo Zhao, Michael Mathieu, Ross Goroshin, Yann LeCun https://arxiv.org/abs/1506.02351v8 The second idea exploited here is that of residual learning. Residual blocks ease the training process by allowing skip connections that give the network the ability to be as linear (or non-linear) as the data sees fit. This allows for much deep networks to be easily trained. The residual element seems to be advantageous in the context of this example as it allows a nice symmetry between the encoder and decoder. Normally, in the decoder, the final projection to the space where the image is reconstructed is linear, however this does not have to be the case for a residual block as the degree to which its output is linear or non-linear is determined by the data it is fed. However, in order to cap the reconstruction in this example, a hard softmax is applied as a bias because we know the MNIST digits are mapped to [0, 1]. References Deep Residual Learning for Image Recognition Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun https://arxiv.org/abs/1512.03385v1 Identity Mappings in Deep Residual Networks Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun https://arxiv.org/abs/1603.05027v3 from __future__ import print_function import numpy as np from keras.datasets import mnist from keras.models import Model from keras.layers import Activation from keras.layers import UpSampling2D, Conv2D, MaxPooling2D from keras.layers import Input, BatchNormalization, ELU import matplotlib.pyplot as plt import keras.backend as K from keras import layers def convresblock(x, nfeats=8, ksize=3, nskipped=2, elu=True): \"\"\"The proposed residual block from [4]. Running with elu=True will use ELU nonlinearity and running with elu=False will use BatchNorm + RELU nonlinearity. While ELU's are fast due to the fact they do not suffer from BatchNorm overhead, they may overfit because they do not offer the stochastic element of the batch formation process of BatchNorm, which acts as a good regularizer. # Arguments x: 4D tensor, the tensor to feed through the block nfeats: Integer, number of feature maps for conv layers. ksize: Integer, width and height of conv kernels in first convolution. nskipped: Integer, number of conv layers for the residual function. elu: Boolean, whether to use ELU or BN+RELU. # Input shape 4D tensor with shape: `(batch, channels, rows, cols)` # Output shape 4D tensor with shape: `(batch, filters, rows, cols)` \"\"\" y0 = Conv2D(nfeats, ksize, padding='same')(x) y = y0 for i in range(nskipped): if elu: y = ELU()(y) else: y = BatchNormalization(axis=1)(y) y = Activation('relu')(y) y = Conv2D(nfeats, 1, padding='same')(y) return layers.add([y0, y]) def getwhere(x): ''' Calculate the 'where' mask that contains switches indicating which index contained the max value when MaxPool2D was applied. Using the gradient of the sum is a nice trick to keep everything high level.''' y_prepool, y_postpool = x return K.gradients(K.sum(y_postpool), y_prepool) if K.backend() == 'tensorflow': raise RuntimeError('This example can only run with the ' 'Theano backend for the time being, ' 'because it requires taking the gradient ' 'of a gradient, which isn\\'t ' 'supported for all TensorFlow ops.') # This example assume 'channels_first' data format. K.set_image_data_format('channels_first') # input image dimensions img_rows, img_cols = 28, 28 # the data, split between train and test sets (x_train, _), (x_test, _) = mnist.load_data() x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols) x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') # The size of the kernel used for the MaxPooling2D pool_size = 2 # The total number of feature maps at each layer nfeats = [8, 16, 32, 64, 128] # The sizes of the pooling kernel at each layer pool_sizes = np.array([1, 1, 1, 1, 1]) * pool_size # The convolution kernel size ksize = 3 # Number of epochs to train for epochs = 5 # Batch size during training batch_size = 128 if pool_size == 2: # if using a 5 layer net of pool_size = 2 x_train = np.pad(x_train, [[0, 0], [0, 0], [2, 2], [2, 2]], mode='constant') x_test = np.pad(x_test, [[0, 0], [0, 0], [2, 2], [2, 2]], mode='constant') nlayers = 5 elif pool_size == 3: # if using a 3 layer net of pool_size = 3 x_train = x_train[:, :, :-1, :-1] x_test = x_test[:, :, :-1, :-1] nlayers = 3 else: import sys sys.exit('Script supports pool_size of 2 and 3.') # Shape of input to train on (note that model is fully convolutional however) input_shape = x_train.shape[1:] # The final list of the size of axis=1 for all layers, including input nfeats_all = [input_shape[0]] + nfeats # First build the encoder, all the while keeping track of the 'where' masks img_input = Input(shape=input_shape) # We push the 'where' masks to the following list wheres = [None] * nlayers y = img_input for i in range(nlayers): y_prepool = convresblock(y, nfeats=nfeats_all[i + 1], ksize=ksize) y = MaxPooling2D(pool_size=(pool_sizes[i], pool_sizes[i]))(y_prepool) wheres[i] = layers.Lambda( getwhere, output_shape=lambda x: x[0])([y_prepool, y]) # Now build the decoder, and use the stored 'where' masks to place the features for i in range(nlayers): ind = nlayers - 1 - i y = UpSampling2D(size=(pool_sizes[ind], pool_sizes[ind]))(y) y = layers.multiply([y, wheres[ind]]) y = convresblock(y, nfeats=nfeats_all[ind], ksize=ksize) # Use hard_simgoid to clip range of reconstruction y = Activation('hard_sigmoid')(y) # Define the model and it's mean square error loss, and compile it with Adam model = Model(img_input, y) model.compile('adam', 'mse') # Fit the model model.fit(x_train, x_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, x_test)) # Plot x_recon = model.predict(x_test[:25]) x_plot = np.concatenate((x_test[:25], x_recon), axis=1) x_plot = x_plot.reshape((5, 10, input_shape[-2], input_shape[-1])) x_plot = np.vstack([np.hstack(x) for x in x_plot]) plt.figure() plt.axis('off') plt.title('Test Samples: Originals/Reconstructions') plt.imshow(x_plot, interpolation='none', cmap='gray') plt.savefig('reconstructions.png')","title":"Mnist swwae"},{"location":"4-Examples/mnist_swwae/#references","text":"Visualizing and Understanding Convolutional Networks Matthew D Zeiler, Rob Fergus https://arxiv.org/abs/1311.2901v3 Stacked What-Where Auto-encoders Junbo Zhao, Michael Mathieu, Ross Goroshin, Yann LeCun https://arxiv.org/abs/1506.02351v8 The second idea exploited here is that of residual learning. Residual blocks ease the training process by allowing skip connections that give the network the ability to be as linear (or non-linear) as the data sees fit. This allows for much deep networks to be easily trained. The residual element seems to be advantageous in the context of this example as it allows a nice symmetry between the encoder and decoder. Normally, in the decoder, the final projection to the space where the image is reconstructed is linear, however this does not have to be the case for a residual block as the degree to which its output is linear or non-linear is determined by the data it is fed. However, in order to cap the reconstruction in this example, a hard softmax is applied as a bias because we know the MNIST digits are mapped to [0, 1].","title":"References"},{"location":"4-Examples/mnist_swwae/#references_1","text":"Deep Residual Learning for Image Recognition Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun https://arxiv.org/abs/1512.03385v1 Identity Mappings in Deep Residual Networks Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun https://arxiv.org/abs/1603.05027v3 from __future__ import print_function import numpy as np from keras.datasets import mnist from keras.models import Model from keras.layers import Activation from keras.layers import UpSampling2D, Conv2D, MaxPooling2D from keras.layers import Input, BatchNormalization, ELU import matplotlib.pyplot as plt import keras.backend as K from keras import layers def convresblock(x, nfeats=8, ksize=3, nskipped=2, elu=True): \"\"\"The proposed residual block from [4]. Running with elu=True will use ELU nonlinearity and running with elu=False will use BatchNorm + RELU nonlinearity. While ELU's are fast due to the fact they do not suffer from BatchNorm overhead, they may overfit because they do not offer the stochastic element of the batch formation process of BatchNorm, which acts as a good regularizer. # Arguments x: 4D tensor, the tensor to feed through the block nfeats: Integer, number of feature maps for conv layers. ksize: Integer, width and height of conv kernels in first convolution. nskipped: Integer, number of conv layers for the residual function. elu: Boolean, whether to use ELU or BN+RELU. # Input shape 4D tensor with shape: `(batch, channels, rows, cols)` # Output shape 4D tensor with shape: `(batch, filters, rows, cols)` \"\"\" y0 = Conv2D(nfeats, ksize, padding='same')(x) y = y0 for i in range(nskipped): if elu: y = ELU()(y) else: y = BatchNormalization(axis=1)(y) y = Activation('relu')(y) y = Conv2D(nfeats, 1, padding='same')(y) return layers.add([y0, y]) def getwhere(x): ''' Calculate the 'where' mask that contains switches indicating which index contained the max value when MaxPool2D was applied. Using the gradient of the sum is a nice trick to keep everything high level.''' y_prepool, y_postpool = x return K.gradients(K.sum(y_postpool), y_prepool) if K.backend() == 'tensorflow': raise RuntimeError('This example can only run with the ' 'Theano backend for the time being, ' 'because it requires taking the gradient ' 'of a gradient, which isn\\'t ' 'supported for all TensorFlow ops.') # This example assume 'channels_first' data format. K.set_image_data_format('channels_first') # input image dimensions img_rows, img_cols = 28, 28 # the data, split between train and test sets (x_train, _), (x_test, _) = mnist.load_data() x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols) x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') # The size of the kernel used for the MaxPooling2D pool_size = 2 # The total number of feature maps at each layer nfeats = [8, 16, 32, 64, 128] # The sizes of the pooling kernel at each layer pool_sizes = np.array([1, 1, 1, 1, 1]) * pool_size # The convolution kernel size ksize = 3 # Number of epochs to train for epochs = 5 # Batch size during training batch_size = 128 if pool_size == 2: # if using a 5 layer net of pool_size = 2 x_train = np.pad(x_train, [[0, 0], [0, 0], [2, 2], [2, 2]], mode='constant') x_test = np.pad(x_test, [[0, 0], [0, 0], [2, 2], [2, 2]], mode='constant') nlayers = 5 elif pool_size == 3: # if using a 3 layer net of pool_size = 3 x_train = x_train[:, :, :-1, :-1] x_test = x_test[:, :, :-1, :-1] nlayers = 3 else: import sys sys.exit('Script supports pool_size of 2 and 3.') # Shape of input to train on (note that model is fully convolutional however) input_shape = x_train.shape[1:] # The final list of the size of axis=1 for all layers, including input nfeats_all = [input_shape[0]] + nfeats # First build the encoder, all the while keeping track of the 'where' masks img_input = Input(shape=input_shape) # We push the 'where' masks to the following list wheres = [None] * nlayers y = img_input for i in range(nlayers): y_prepool = convresblock(y, nfeats=nfeats_all[i + 1], ksize=ksize) y = MaxPooling2D(pool_size=(pool_sizes[i], pool_sizes[i]))(y_prepool) wheres[i] = layers.Lambda( getwhere, output_shape=lambda x: x[0])([y_prepool, y]) # Now build the decoder, and use the stored 'where' masks to place the features for i in range(nlayers): ind = nlayers - 1 - i y = UpSampling2D(size=(pool_sizes[ind], pool_sizes[ind]))(y) y = layers.multiply([y, wheres[ind]]) y = convresblock(y, nfeats=nfeats_all[ind], ksize=ksize) # Use hard_simgoid to clip range of reconstruction y = Activation('hard_sigmoid')(y) # Define the model and it's mean square error loss, and compile it with Adam model = Model(img_input, y) model.compile('adam', 'mse') # Fit the model model.fit(x_train, x_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, x_test)) # Plot x_recon = model.predict(x_test[:25]) x_plot = np.concatenate((x_test[:25], x_recon), axis=1) x_plot = x_plot.reshape((5, 10, input_shape[-2], input_shape[-1])) x_plot = np.vstack([np.hstack(x) for x in x_plot]) plt.figure() plt.axis('off') plt.title('Test Samples: Originals/Reconstructions') plt.imshow(x_plot, interpolation='none', cmap='gray') plt.savefig('reconstructions.png')","title":"References"},{"location":"4-Examples/mnist_tfrecord/","text":"MNIST dataset with TFRecords, the standard TensorFlow data format. TFRecord is a data format supported throughout TensorFlow. This example demonstrates how to load TFRecord data using Input Tensors. Input Tensors differ from the normal Keras workflow because instead of fitting to data loaded into a a numpy array, data is supplied via a special tensor that reads data from nodes that are wired directly into model graph with the Input(tensor=input_tensor) parameter. There are several advantages to using Input Tensors. First, if a dataset is already in TFRecord format you can load and train on that data directly in Keras. Second, extended backend API capabilities such as TensorFlow data augmentation is easy to integrate directly into your Keras training scripts via input tensors. Third, TensorFlow implements several data APIs for TFRecords, some of which provide significantly faster training performance than numpy arrays can provide because they run via the C++ backend. Please note that this example is tailored for brevity and clarity and not to demonstrate performance or augmentation capabilities. Input Tensors also have important disadvantages. In particular, Input Tensors are fixed at model construction because rewiring networks is not yet supported. For this reason, changing the data input source means model weights must be saved and the model rebuilt from scratch to connect the new input data. validation cannot currently be performed as training progresses, and must be performed after training completes. This example demonstrates how to train with input tensors, save the model weights, and then evaluate the model using the numpy based Keras API. Gets to ~99.1% test accuracy after 5 epochs (high variance from run to run: 98.9-99.3). import numpy as np import os import tensorflow as tf import keras from keras import backend as K from keras import layers from keras.callbacks import Callback from tensorflow.contrib.learn.python.learn.datasets import mnist if K.backend() != 'tensorflow': raise RuntimeError('This example can only run with the ' 'TensorFlow backend, ' 'because it requires TFRecords, which ' 'are not supported on other platforms.') class EvaluateInputTensor(Callback): \"\"\" Validate a model which does not expect external numpy data during training. Keras does not expect external numpy data at training time, and thus cannot accept numpy arrays for validation when all of a Keras Model's `Input(input_tensor)` layers are provided an `input_tensor` parameter, and the call to `Model.compile(target_tensors)` defines all `target_tensors`. Instead, create a second model for validation which is also configured with input tensors and add it to the `EvaluateInputTensor` callback to perform validation. It is recommended that this callback be the first in the list of callbacks because it defines the validation variables required by many other callbacks, and Callbacks are made in order. # Arguments model: Keras model on which to call model.evaluate(). steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. \"\"\" def __init__(self, model, steps, metrics_prefix='val', verbose=1): # parameter of callbacks passed during initialization # pass evalation mode directly super(EvaluateInputTensor, self).__init__() self.val_model = model self.num_steps = steps self.verbose = verbose self.metrics_prefix = metrics_prefix def on_epoch_end(self, epoch, logs={}): self.val_model.set_weights(self.model.get_weights()) results = self.val_model.evaluate(None, None, steps=int(self.num_steps), verbose=self.verbose) metrics_str = '\\n' for result, name in zip(results, self.val_model.metrics_names): metric_name = self.metrics_prefix + '_' + name logs[metric_name] = result if self.verbose > 0: metrics_str = metrics_str + metric_name + ': ' + str(result) + ' ' if self.verbose > 0: print(metrics_str) def cnn_layers(x_train_input): x = layers.Conv2D(32, (3, 3), activation='relu', padding='valid')(x_train_input) x = layers.MaxPooling2D(pool_size=(2, 2))(x) x = layers.Conv2D(64, (3, 3), activation='relu')(x) x = layers.MaxPooling2D(pool_size=(2, 2))(x) x = layers.Flatten()(x) x = layers.Dense(512, activation='relu')(x) x = layers.Dropout(0.5)(x) x_train_out = layers.Dense(num_classes, activation='softmax', name='x_train_out')(x) return x_train_out sess = K.get_session() batch_size = 100 batch_shape = (batch_size, 28, 28, 1) epochs = 5 num_classes = 10 # The capacity variable controls the maximum queue size # allowed when prefetching data for training. capacity = 10000 # min_after_dequeue is the minimum number elements in the queue # after a dequeue, which ensures sufficient mixing of elements. min_after_dequeue = 3000 # If `enqueue_many` is `False`, `tensors` is assumed to represent a # single example. An input tensor with shape `[x, y, z]` will be output # as a tensor with shape `[batch_size, x, y, z]`. # # If `enqueue_many` is `True`, `tensors` is assumed to represent a # batch of examples, where the first dimension is indexed by example, # and all members of `tensors` should have the same size in the # first dimension. If an input tensor has shape `[*, x, y, z]`, the # output will have shape `[batch_size, x, y, z]`. enqueue_many = True cache_dir = os.path.expanduser( os.path.join('~', '.keras', 'datasets', 'MNIST-data')) data = mnist.read_data_sets(cache_dir, validation_size=0) x_train_batch, y_train_batch = tf.train.shuffle_batch( tensors=[data.train.images, data.train.labels.astype(np.int32)], batch_size=batch_size, capacity=capacity, min_after_dequeue=min_after_dequeue, enqueue_many=enqueue_many, num_threads=8) x_train_batch = tf.cast(x_train_batch, tf.float32) x_train_batch = tf.reshape(x_train_batch, shape=batch_shape) y_train_batch = tf.cast(y_train_batch, tf.int32) y_train_batch = tf.one_hot(y_train_batch, num_classes) x_batch_shape = x_train_batch.get_shape().as_list() y_batch_shape = y_train_batch.get_shape().as_list() model_input = layers.Input(tensor=x_train_batch) model_output = cnn_layers(model_input) train_model = keras.models.Model(inputs=model_input, outputs=model_output) # Pass the target tensor `y_train_batch` to `compile` # via the `target_tensors` keyword argument: train_model.compile(optimizer=keras.optimizers.RMSprop(lr=2e-3, decay=1e-5), loss='categorical_crossentropy', metrics=['accuracy'], target_tensors=[y_train_batch]) train_model.summary() x_test_batch, y_test_batch = tf.train.batch( tensors=[data.test.images, data.test.labels.astype(np.int32)], batch_size=batch_size, capacity=capacity, enqueue_many=enqueue_many, num_threads=8) # Create a separate test model # to perform validation during training x_test_batch = tf.cast(x_test_batch, tf.float32) x_test_batch = tf.reshape(x_test_batch, shape=batch_shape) y_test_batch = tf.cast(y_test_batch, tf.int32) y_test_batch = tf.one_hot(y_test_batch, num_classes) x_test_batch_shape = x_test_batch.get_shape().as_list() y_test_batch_shape = y_test_batch.get_shape().as_list() test_model_input = layers.Input(tensor=x_test_batch) test_model_output = cnn_layers(test_model_input) test_model = keras.models.Model(inputs=test_model_input, outputs=test_model_output) # Pass the target tensor `y_test_batch` to `compile` # via the `target_tensors` keyword argument: test_model.compile(optimizer=keras.optimizers.RMSprop(lr=2e-3, decay=1e-5), loss='categorical_crossentropy', metrics=['accuracy'], target_tensors=[y_test_batch]) # Fit the model using data from the TFRecord data tensors. coord = tf.train.Coordinator() threads = tf.train.start_queue_runners(sess, coord) train_model.fit( epochs=epochs, steps_per_epoch=int(np.ceil(data.train.num_examples / float(batch_size))), callbacks=[EvaluateInputTensor(test_model, steps=100)]) # Save the model weights. train_model.save_weights('saved_wt.h5') # Clean up the TF session. coord.request_stop() coord.join(threads) K.clear_session() # Second Session to test loading trained model without tensors x_test = np.reshape(data.test.images, (data.test.images.shape[0], 28, 28, 1)) y_test = data.test.labels x_test_inp = layers.Input(shape=(x_test.shape[1:])) test_out = cnn_layers(x_test_inp) test_model = keras.models.Model(inputs=x_test_inp, outputs=test_out) test_model.load_weights('saved_wt.h5') test_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) test_model.summary() loss, acc = test_model.evaluate(x_test, keras.utils.to_categorical(y_test), batch_size=batch_size) print('\\nTest accuracy: {0}'.format(acc))","title":"Mnist tfrecord"},{"location":"4-Examples/mnist_transfer_cnn/","text":"Transfer learning toy example. 1 - Train a simple convnet on the MNIST dataset the first 5 digits [0..4]. 2 - Freeze convolutional layers and fine-tune dense layers for the classification of digits [5..9]. Get to 99.8% test accuracy after 5 epochs for the first five digits classifier and 99.2% for the last five digits after transfer + fine-tuning. from __future__ import print_function import datetime import keras from keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense, Dropout, Activation, Flatten from keras.layers import Conv2D, MaxPooling2D from keras import backend as K now = datetime.datetime.now batch_size = 128 num_classes = 5 epochs = 5 # input image dimensions img_rows, img_cols = 28, 28 # number of convolutional filters to use filters = 32 # size of pooling area for max pooling pool_size = 2 # convolution kernel size kernel_size = 3 if K.image_data_format() == 'channels_first': input_shape = (1, img_rows, img_cols) else: input_shape = (img_rows, img_cols, 1) def train_model(model, train, test, num_classes): x_train = train[0].reshape((train[0].shape[0],) + input_shape) x_test = test[0].reshape((test[0].shape[0],) + input_shape) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') # convert class vectors to binary class matrices y_train = keras.utils.to_categorical(train[1], num_classes) y_test = keras.utils.to_categorical(test[1], num_classes) model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy']) t = now() model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test)) print('Training time: %s' % (now() - t)) score = model.evaluate(x_test, y_test, verbose=0) print('Test score:', score[0]) print('Test accuracy:', score[1]) # the data, split between train and test sets (x_train, y_train), (x_test, y_test) = mnist.load_data() # create two datasets one with digits below 5 and one with 5 and above x_train_lt5 = x_train[y_train < 5] y_train_lt5 = y_train[y_train < 5] x_test_lt5 = x_test[y_test < 5] y_test_lt5 = y_test[y_test < 5] x_train_gte5 = x_train[y_train >= 5] y_train_gte5 = y_train[y_train >= 5] - 5 x_test_gte5 = x_test[y_test >= 5] y_test_gte5 = y_test[y_test >= 5] - 5 # define two groups of layers: feature (convolutions) and classification (dense) feature_layers = [ Conv2D(filters, kernel_size, padding='valid', input_shape=input_shape), Activation('relu'), Conv2D(filters, kernel_size), Activation('relu'), MaxPooling2D(pool_size=pool_size), Dropout(0.25), Flatten(), ] classification_layers = [ Dense(128), Activation('relu'), Dropout(0.5), Dense(num_classes), Activation('softmax') ] # create complete model model = Sequential(feature_layers + classification_layers) # train model for 5-digit classification [0..4] train_model(model, (x_train_lt5, y_train_lt5), (x_test_lt5, y_test_lt5), num_classes) # freeze feature layers and rebuild model for l in feature_layers: l.trainable = False # transfer: train dense layers for new classification task [5..9] train_model(model, (x_train_gte5, y_train_gte5), (x_test_gte5, y_test_gte5), num_classes)","title":"Mnist transfer cnn"},{"location":"4-Examples/neural_doodle/","text":"Neural doodle with Keras Script Usage Arguments --nlabels: # of regions (colors) in mask images --style-image: image to learn style from --style-mask: semantic labels for style image --target-mask: semantic labels for target image (your doodle) --content-image: optional image to learn content from --target-image-prefix: path prefix for generated target images Example 1: doodle using a style image, style mask and target mask. python neural_doodle.py --nlabels 4 --style-image Monet/style.png --style-mask Monet/style_mask.png --target-mask Monet/target_mask.png --target-image-prefix generated/monet Example 2: doodle using a style image, style mask, target mask and an optional content image. python neural_doodle.py --nlabels 4 --style-image Renoir/style.png --style-mask Renoir/style_mask.png --target-mask Renoir/target_mask.png --content-image Renoir/creek.jpg --target-image-prefix generated/renoir References [Dmitry Ulyanov's blog on fast-neural-doodle] (http://dmitryulyanov.github.io/feed-forward-neural-doodle/) [Torch code for fast-neural-doodle] (https://github.com/DmitryUlyanov/fast-neural-doodle) [Torch code for online-neural-doodle] (https://github.com/DmitryUlyanov/online-neural-doodle) [Paper Texture Networks: Feed-forward Synthesis of Textures and Stylized Images] (http://arxiv.org/abs/1603.03417) [Discussion on parameter tuning] (https://github.com/keras-team/keras/issues/3705) Resources Example images can be downloaded from https://github.com/DmitryUlyanov/fast-neural-doodle/tree/master/data from __future__ import print_function import time import argparse import numpy as np from scipy.optimize import fmin_l_bfgs_b from keras import backend as K from keras.layers import Input, AveragePooling2D from keras.models import Model from keras.preprocessing.image import load_img, save_img, img_to_array from keras.applications import vgg19 # Command line arguments parser = argparse.ArgumentParser(description='Keras neural doodle example') parser.add_argument('--nlabels', type=int, help='number of semantic labels' ' (regions in differnet colors)' ' in style_mask/target_mask') parser.add_argument('--style-image', type=str, help='path to image to learn style from') parser.add_argument('--style-mask', type=str, help='path to semantic mask of style image') parser.add_argument('--target-mask', type=str, help='path to semantic mask of target image') parser.add_argument('--content-image', type=str, default=None, help='path to optional content image') parser.add_argument('--target-image-prefix', type=str, help='path prefix for generated results') args = parser.parse_args() style_img_path = args.style_image style_mask_path = args.style_mask target_mask_path = args.target_mask content_img_path = args.content_image target_img_prefix = args.target_image_prefix use_content_img = content_img_path is not None num_labels = args.nlabels num_colors = 3 # RGB # determine image sizes based on target_mask ref_img = img_to_array(load_img(target_mask_path)) img_nrows, img_ncols = ref_img.shape[:2] total_variation_weight = 50. style_weight = 1. content_weight = 0.1 if use_content_img else 0 content_feature_layers = ['block5_conv2'] # To get better generation qualities, use more conv layers for style features style_feature_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1'] # helper functions for reading/processing images def preprocess_image(image_path): img = load_img(image_path, target_size=(img_nrows, img_ncols)) img = img_to_array(img) img = np.expand_dims(img, axis=0) img = vgg19.preprocess_input(img) return img def deprocess_image(x): if K.image_data_format() == 'channels_first': x = x.reshape((3, img_nrows, img_ncols)) x = x.transpose((1, 2, 0)) else: x = x.reshape((img_nrows, img_ncols, 3)) # Remove zero-center by mean pixel x[:, :, 0] += 103.939 x[:, :, 1] += 116.779 x[:, :, 2] += 123.68 # 'BGR'->'RGB' x = x[:, :, ::-1] x = np.clip(x, 0, 255).astype('uint8') return x def kmeans(xs, k): assert xs.ndim == 2 try: from sklearn.cluster import k_means _, labels, _ = k_means(xs.astype('float64'), k) except ImportError: from scipy.cluster.vq import kmeans2 _, labels = kmeans2(xs, k, missing='raise') return labels def load_mask_labels(): '''Load both target and style masks. A mask image (nr x nc) with m labels/colors will be loaded as a 4D boolean tensor: (1, m, nr, nc) for 'channels_first' or (1, nr, nc, m) for 'channels_last' ''' target_mask_img = load_img(target_mask_path, target_size=(img_nrows, img_ncols)) target_mask_img = img_to_array(target_mask_img) style_mask_img = load_img(style_mask_path, target_size=(img_nrows, img_ncols)) style_mask_img = img_to_array(style_mask_img) if K.image_data_format() == 'channels_first': mask_vecs = np.vstack([style_mask_img.reshape((3, -1)).T, target_mask_img.reshape((3, -1)).T]) else: mask_vecs = np.vstack([style_mask_img.reshape((-1, 3)), target_mask_img.reshape((-1, 3))]) labels = kmeans(mask_vecs, num_labels) style_mask_label = labels[:img_nrows * img_ncols].reshape((img_nrows, img_ncols)) target_mask_label = labels[img_nrows * img_ncols:].reshape((img_nrows, img_ncols)) stack_axis = 0 if K.image_data_format() == 'channels_first' else -1 style_mask = np.stack([style_mask_label == r for r in range(num_labels)], axis=stack_axis) target_mask = np.stack([target_mask_label == r for r in range(num_labels)], axis=stack_axis) return (np.expand_dims(style_mask, axis=0), np.expand_dims(target_mask, axis=0)) # Create tensor variables for images if K.image_data_format() == 'channels_first': shape = (1, num_colors, img_nrows, img_ncols) else: shape = (1, img_nrows, img_ncols, num_colors) style_image = K.variable(preprocess_image(style_img_path)) target_image = K.placeholder(shape=shape) if use_content_img: content_image = K.variable(preprocess_image(content_img_path)) else: content_image = K.zeros(shape=shape) images = K.concatenate([style_image, target_image, content_image], axis=0) # Create tensor variables for masks raw_style_mask, raw_target_mask = load_mask_labels() style_mask = K.variable(raw_style_mask.astype('float32')) target_mask = K.variable(raw_target_mask.astype('float32')) masks = K.concatenate([style_mask, target_mask], axis=0) # index constants for images and tasks variables STYLE, TARGET, CONTENT = 0, 1, 2 # Build image model, mask model and use layer outputs as features # image model as VGG19 image_model = vgg19.VGG19(include_top=False, input_tensor=images) # mask model as a series of pooling mask_input = Input(tensor=masks, shape=(None, None, None), name='mask_input') x = mask_input for layer in image_model.layers[1:]: name = 'mask_%s' % layer.name if 'conv' in layer.name: x = AveragePooling2D((3, 3), padding='same', strides=( 1, 1), name=name)(x) elif 'pool' in layer.name: x = AveragePooling2D((2, 2), name=name)(x) mask_model = Model(mask_input, x) # Collect features from image_model and task_model image_features = {} mask_features = {} for img_layer, mask_layer in zip(image_model.layers, mask_model.layers): if 'conv' in img_layer.name: assert 'mask_' + img_layer.name == mask_layer.name layer_name = img_layer.name img_feat, mask_feat = img_layer.output, mask_layer.output image_features[layer_name] = img_feat mask_features[layer_name] = mask_feat # Define loss functions def gram_matrix(x): assert K.ndim(x) == 3 features = K.batch_flatten(x) gram = K.dot(features, K.transpose(features)) return gram def region_style_loss(style_image, target_image, style_mask, target_mask): '''Calculate style loss between style_image and target_image, for one common region specified by their (boolean) masks ''' assert 3 == K.ndim(style_image) == K.ndim(target_image) assert 2 == K.ndim(style_mask) == K.ndim(target_mask) if K.image_data_format() == 'channels_first': masked_style = style_image * style_mask masked_target = target_image * target_mask num_channels = K.shape(style_image)[0] else: masked_style = K.permute_dimensions( style_image, (2, 0, 1)) * style_mask masked_target = K.permute_dimensions( target_image, (2, 0, 1)) * target_mask num_channels = K.shape(style_image)[-1] num_channels = K.cast(num_channels, dtype='float32') s = gram_matrix(masked_style) / K.mean(style_mask) / num_channels c = gram_matrix(masked_target) / K.mean(target_mask) / num_channels return K.mean(K.square(s - c)) def style_loss(style_image, target_image, style_masks, target_masks): '''Calculate style loss between style_image and target_image, in all regions. ''' assert 3 == K.ndim(style_image) == K.ndim(target_image) assert 3 == K.ndim(style_masks) == K.ndim(target_masks) loss = K.variable(0) for i in range(num_labels): if K.image_data_format() == 'channels_first': style_mask = style_masks[i, :, :] target_mask = target_masks[i, :, :] else: style_mask = style_masks[:, :, i] target_mask = target_masks[:, :, i] loss += region_style_loss(style_image, target_image, style_mask, target_mask) return loss def content_loss(content_image, target_image): return K.sum(K.square(target_image - content_image)) def total_variation_loss(x): assert 4 == K.ndim(x) if K.image_data_format() == 'channels_first': a = K.square(x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, 1:, :img_ncols - 1]) b = K.square(x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, :img_nrows - 1, 1:]) else: a = K.square(x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :]) b = K.square(x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :]) return K.sum(K.pow(a + b, 1.25)) # Overall loss is the weighted sum of content_loss, style_loss and tv_loss # Each individual loss uses features from image/mask models. loss = K.variable(0) for layer in content_feature_layers: content_feat = image_features[layer][CONTENT, :, :, :] target_feat = image_features[layer][TARGET, :, :, :] loss += content_weight * content_loss(content_feat, target_feat) for layer in style_feature_layers: style_feat = image_features[layer][STYLE, :, :, :] target_feat = image_features[layer][TARGET, :, :, :] style_masks = mask_features[layer][STYLE, :, :, :] target_masks = mask_features[layer][TARGET, :, :, :] sl = style_loss(style_feat, target_feat, style_masks, target_masks) loss += (style_weight / len(style_feature_layers)) * sl loss += total_variation_weight * total_variation_loss(target_image) loss_grads = K.gradients(loss, target_image) # Evaluator class for computing efficiency outputs = [loss] if isinstance(loss_grads, (list, tuple)): outputs += loss_grads else: outputs.append(loss_grads) f_outputs = K.function([target_image], outputs) def eval_loss_and_grads(x): if K.image_data_format() == 'channels_first': x = x.reshape((1, 3, img_nrows, img_ncols)) else: x = x.reshape((1, img_nrows, img_ncols, 3)) outs = f_outputs([x]) loss_value = outs[0] if len(outs[1:]) == 1: grad_values = outs[1].flatten().astype('float64') else: grad_values = np.array(outs[1:]).flatten().astype('float64') return loss_value, grad_values class Evaluator(object): def __init__(self): self.loss_value = None self.grads_values = None def loss(self, x): assert self.loss_value is None loss_value, grad_values = eval_loss_and_grads(x) self.loss_value = loss_value self.grad_values = grad_values return self.loss_value def grads(self, x): assert self.loss_value is not None grad_values = np.copy(self.grad_values) self.loss_value = None self.grad_values = None return grad_values evaluator = Evaluator() # Generate images by iterative optimization if K.image_data_format() == 'channels_first': x = np.random.uniform(0, 255, (1, 3, img_nrows, img_ncols)) - 128. else: x = np.random.uniform(0, 255, (1, img_nrows, img_ncols, 3)) - 128. for i in range(50): print('Start of iteration', i) start_time = time.time() x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(), fprime=evaluator.grads, maxfun=20) print('Current loss value:', min_val) # save current generated image img = deprocess_image(x.copy()) fname = target_img_prefix + '_at_iteration_%d.png' % i save_img(fname, img) end_time = time.time() print('Image saved as', fname) print('Iteration %d completed in %ds' % (i, end_time - start_time))","title":"Neural doodle"},{"location":"4-Examples/neural_doodle/#script-usage","text":"","title":"Script Usage"},{"location":"4-Examples/neural_doodle/#arguments","text":"--nlabels: # of regions (colors) in mask images --style-image: image to learn style from --style-mask: semantic labels for style image --target-mask: semantic labels for target image (your doodle) --content-image: optional image to learn content from --target-image-prefix: path prefix for generated target images","title":"Arguments"},{"location":"4-Examples/neural_doodle/#example-1-doodle-using-a-style-image-style-mask","text":"and target mask. python neural_doodle.py --nlabels 4 --style-image Monet/style.png --style-mask Monet/style_mask.png --target-mask Monet/target_mask.png --target-image-prefix generated/monet","title":"Example 1: doodle using a style image, style mask"},{"location":"4-Examples/neural_doodle/#example-2-doodle-using-a-style-image-style-mask","text":"target mask and an optional content image. python neural_doodle.py --nlabels 4 --style-image Renoir/style.png --style-mask Renoir/style_mask.png --target-mask Renoir/target_mask.png --content-image Renoir/creek.jpg --target-image-prefix generated/renoir","title":"Example 2: doodle using a style image, style mask,"},{"location":"4-Examples/neural_doodle/#references","text":"[Dmitry Ulyanov's blog on fast-neural-doodle] (http://dmitryulyanov.github.io/feed-forward-neural-doodle/) [Torch code for fast-neural-doodle] (https://github.com/DmitryUlyanov/fast-neural-doodle) [Torch code for online-neural-doodle] (https://github.com/DmitryUlyanov/online-neural-doodle) [Paper Texture Networks: Feed-forward Synthesis of Textures and Stylized Images] (http://arxiv.org/abs/1603.03417) [Discussion on parameter tuning] (https://github.com/keras-team/keras/issues/3705)","title":"References"},{"location":"4-Examples/neural_doodle/#resources","text":"Example images can be downloaded from https://github.com/DmitryUlyanov/fast-neural-doodle/tree/master/data from __future__ import print_function import time import argparse import numpy as np from scipy.optimize import fmin_l_bfgs_b from keras import backend as K from keras.layers import Input, AveragePooling2D from keras.models import Model from keras.preprocessing.image import load_img, save_img, img_to_array from keras.applications import vgg19 # Command line arguments parser = argparse.ArgumentParser(description='Keras neural doodle example') parser.add_argument('--nlabels', type=int, help='number of semantic labels' ' (regions in differnet colors)' ' in style_mask/target_mask') parser.add_argument('--style-image', type=str, help='path to image to learn style from') parser.add_argument('--style-mask', type=str, help='path to semantic mask of style image') parser.add_argument('--target-mask', type=str, help='path to semantic mask of target image') parser.add_argument('--content-image', type=str, default=None, help='path to optional content image') parser.add_argument('--target-image-prefix', type=str, help='path prefix for generated results') args = parser.parse_args() style_img_path = args.style_image style_mask_path = args.style_mask target_mask_path = args.target_mask content_img_path = args.content_image target_img_prefix = args.target_image_prefix use_content_img = content_img_path is not None num_labels = args.nlabels num_colors = 3 # RGB # determine image sizes based on target_mask ref_img = img_to_array(load_img(target_mask_path)) img_nrows, img_ncols = ref_img.shape[:2] total_variation_weight = 50. style_weight = 1. content_weight = 0.1 if use_content_img else 0 content_feature_layers = ['block5_conv2'] # To get better generation qualities, use more conv layers for style features style_feature_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1'] # helper functions for reading/processing images def preprocess_image(image_path): img = load_img(image_path, target_size=(img_nrows, img_ncols)) img = img_to_array(img) img = np.expand_dims(img, axis=0) img = vgg19.preprocess_input(img) return img def deprocess_image(x): if K.image_data_format() == 'channels_first': x = x.reshape((3, img_nrows, img_ncols)) x = x.transpose((1, 2, 0)) else: x = x.reshape((img_nrows, img_ncols, 3)) # Remove zero-center by mean pixel x[:, :, 0] += 103.939 x[:, :, 1] += 116.779 x[:, :, 2] += 123.68 # 'BGR'->'RGB' x = x[:, :, ::-1] x = np.clip(x, 0, 255).astype('uint8') return x def kmeans(xs, k): assert xs.ndim == 2 try: from sklearn.cluster import k_means _, labels, _ = k_means(xs.astype('float64'), k) except ImportError: from scipy.cluster.vq import kmeans2 _, labels = kmeans2(xs, k, missing='raise') return labels def load_mask_labels(): '''Load both target and style masks. A mask image (nr x nc) with m labels/colors will be loaded as a 4D boolean tensor: (1, m, nr, nc) for 'channels_first' or (1, nr, nc, m) for 'channels_last' ''' target_mask_img = load_img(target_mask_path, target_size=(img_nrows, img_ncols)) target_mask_img = img_to_array(target_mask_img) style_mask_img = load_img(style_mask_path, target_size=(img_nrows, img_ncols)) style_mask_img = img_to_array(style_mask_img) if K.image_data_format() == 'channels_first': mask_vecs = np.vstack([style_mask_img.reshape((3, -1)).T, target_mask_img.reshape((3, -1)).T]) else: mask_vecs = np.vstack([style_mask_img.reshape((-1, 3)), target_mask_img.reshape((-1, 3))]) labels = kmeans(mask_vecs, num_labels) style_mask_label = labels[:img_nrows * img_ncols].reshape((img_nrows, img_ncols)) target_mask_label = labels[img_nrows * img_ncols:].reshape((img_nrows, img_ncols)) stack_axis = 0 if K.image_data_format() == 'channels_first' else -1 style_mask = np.stack([style_mask_label == r for r in range(num_labels)], axis=stack_axis) target_mask = np.stack([target_mask_label == r for r in range(num_labels)], axis=stack_axis) return (np.expand_dims(style_mask, axis=0), np.expand_dims(target_mask, axis=0)) # Create tensor variables for images if K.image_data_format() == 'channels_first': shape = (1, num_colors, img_nrows, img_ncols) else: shape = (1, img_nrows, img_ncols, num_colors) style_image = K.variable(preprocess_image(style_img_path)) target_image = K.placeholder(shape=shape) if use_content_img: content_image = K.variable(preprocess_image(content_img_path)) else: content_image = K.zeros(shape=shape) images = K.concatenate([style_image, target_image, content_image], axis=0) # Create tensor variables for masks raw_style_mask, raw_target_mask = load_mask_labels() style_mask = K.variable(raw_style_mask.astype('float32')) target_mask = K.variable(raw_target_mask.astype('float32')) masks = K.concatenate([style_mask, target_mask], axis=0) # index constants for images and tasks variables STYLE, TARGET, CONTENT = 0, 1, 2 # Build image model, mask model and use layer outputs as features # image model as VGG19 image_model = vgg19.VGG19(include_top=False, input_tensor=images) # mask model as a series of pooling mask_input = Input(tensor=masks, shape=(None, None, None), name='mask_input') x = mask_input for layer in image_model.layers[1:]: name = 'mask_%s' % layer.name if 'conv' in layer.name: x = AveragePooling2D((3, 3), padding='same', strides=( 1, 1), name=name)(x) elif 'pool' in layer.name: x = AveragePooling2D((2, 2), name=name)(x) mask_model = Model(mask_input, x) # Collect features from image_model and task_model image_features = {} mask_features = {} for img_layer, mask_layer in zip(image_model.layers, mask_model.layers): if 'conv' in img_layer.name: assert 'mask_' + img_layer.name == mask_layer.name layer_name = img_layer.name img_feat, mask_feat = img_layer.output, mask_layer.output image_features[layer_name] = img_feat mask_features[layer_name] = mask_feat # Define loss functions def gram_matrix(x): assert K.ndim(x) == 3 features = K.batch_flatten(x) gram = K.dot(features, K.transpose(features)) return gram def region_style_loss(style_image, target_image, style_mask, target_mask): '''Calculate style loss between style_image and target_image, for one common region specified by their (boolean) masks ''' assert 3 == K.ndim(style_image) == K.ndim(target_image) assert 2 == K.ndim(style_mask) == K.ndim(target_mask) if K.image_data_format() == 'channels_first': masked_style = style_image * style_mask masked_target = target_image * target_mask num_channels = K.shape(style_image)[0] else: masked_style = K.permute_dimensions( style_image, (2, 0, 1)) * style_mask masked_target = K.permute_dimensions( target_image, (2, 0, 1)) * target_mask num_channels = K.shape(style_image)[-1] num_channels = K.cast(num_channels, dtype='float32') s = gram_matrix(masked_style) / K.mean(style_mask) / num_channels c = gram_matrix(masked_target) / K.mean(target_mask) / num_channels return K.mean(K.square(s - c)) def style_loss(style_image, target_image, style_masks, target_masks): '''Calculate style loss between style_image and target_image, in all regions. ''' assert 3 == K.ndim(style_image) == K.ndim(target_image) assert 3 == K.ndim(style_masks) == K.ndim(target_masks) loss = K.variable(0) for i in range(num_labels): if K.image_data_format() == 'channels_first': style_mask = style_masks[i, :, :] target_mask = target_masks[i, :, :] else: style_mask = style_masks[:, :, i] target_mask = target_masks[:, :, i] loss += region_style_loss(style_image, target_image, style_mask, target_mask) return loss def content_loss(content_image, target_image): return K.sum(K.square(target_image - content_image)) def total_variation_loss(x): assert 4 == K.ndim(x) if K.image_data_format() == 'channels_first': a = K.square(x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, 1:, :img_ncols - 1]) b = K.square(x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, :img_nrows - 1, 1:]) else: a = K.square(x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :]) b = K.square(x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :]) return K.sum(K.pow(a + b, 1.25)) # Overall loss is the weighted sum of content_loss, style_loss and tv_loss # Each individual loss uses features from image/mask models. loss = K.variable(0) for layer in content_feature_layers: content_feat = image_features[layer][CONTENT, :, :, :] target_feat = image_features[layer][TARGET, :, :, :] loss += content_weight * content_loss(content_feat, target_feat) for layer in style_feature_layers: style_feat = image_features[layer][STYLE, :, :, :] target_feat = image_features[layer][TARGET, :, :, :] style_masks = mask_features[layer][STYLE, :, :, :] target_masks = mask_features[layer][TARGET, :, :, :] sl = style_loss(style_feat, target_feat, style_masks, target_masks) loss += (style_weight / len(style_feature_layers)) * sl loss += total_variation_weight * total_variation_loss(target_image) loss_grads = K.gradients(loss, target_image) # Evaluator class for computing efficiency outputs = [loss] if isinstance(loss_grads, (list, tuple)): outputs += loss_grads else: outputs.append(loss_grads) f_outputs = K.function([target_image], outputs) def eval_loss_and_grads(x): if K.image_data_format() == 'channels_first': x = x.reshape((1, 3, img_nrows, img_ncols)) else: x = x.reshape((1, img_nrows, img_ncols, 3)) outs = f_outputs([x]) loss_value = outs[0] if len(outs[1:]) == 1: grad_values = outs[1].flatten().astype('float64') else: grad_values = np.array(outs[1:]).flatten().astype('float64') return loss_value, grad_values class Evaluator(object): def __init__(self): self.loss_value = None self.grads_values = None def loss(self, x): assert self.loss_value is None loss_value, grad_values = eval_loss_and_grads(x) self.loss_value = loss_value self.grad_values = grad_values return self.loss_value def grads(self, x): assert self.loss_value is not None grad_values = np.copy(self.grad_values) self.loss_value = None self.grad_values = None return grad_values evaluator = Evaluator() # Generate images by iterative optimization if K.image_data_format() == 'channels_first': x = np.random.uniform(0, 255, (1, 3, img_nrows, img_ncols)) - 128. else: x = np.random.uniform(0, 255, (1, img_nrows, img_ncols, 3)) - 128. for i in range(50): print('Start of iteration', i) start_time = time.time() x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(), fprime=evaluator.grads, maxfun=20) print('Current loss value:', min_val) # save current generated image img = deprocess_image(x.copy()) fname = target_img_prefix + '_at_iteration_%d.png' % i save_img(fname, img) end_time = time.time() print('Image saved as', fname) print('Iteration %d completed in %ds' % (i, end_time - start_time))","title":"Resources"},{"location":"4-Examples/neural_style_transfer/","text":"Neural style transfer with Keras. Run the script with: python neural_style_transfer.py path_to_your_base_image.jpg path_to_your_reference.jpg prefix_for_results e.g.: python neural_style_transfer.py img/tuebingen.jpg img/starry_night.jpg results/my_result Optional parameters: --iter, To specify the number of iterations the style transfer takes place (Default is 10) --content_weight, The weight given to the content loss (Default is 0.025) --style_weight, The weight given to the style loss (Default is 1.0) --tv_weight, The weight given to the total variation loss (Default is 1.0) It is preferable to run this script on GPU, for speed. Example result: https://twitter.com/fchollet/status/686631033085677568 Details Style transfer consists in generating an image with the same \"content\" as a base image, but with the \"style\" of a different picture (typically artistic). This is achieved through the optimization of a loss function that has 3 components: \"style loss\", \"content loss\", and \"total variation loss\": The total variation loss imposes local spatial continuity between the pixels of the combination image, giving it visual coherence. The style loss is where the deep learning keeps in --that one is defined using a deep convolutional neural network. Precisely, it consists in a sum of L2 distances between the Gram matrices of the representations of the base image and the style reference image, extracted from different layers of a convnet (trained on ImageNet). The general idea is to capture color/texture information at different spatial scales (fairly large scales --defined by the depth of the layer considered). The content loss is a L2 distance between the features of the base image (extracted from a deep layer) and the features of the combination image, keeping the generated image close enough to the original one. References - [A Neural Algorithm of Artistic Style](http://arxiv.org/abs/1508.06576) from __future__ import print_function from keras.preprocessing.image import load_img, save_img, img_to_array import numpy as np from scipy.optimize import fmin_l_bfgs_b import time import argparse from keras.applications import vgg19 from keras import backend as K parser = argparse.ArgumentParser(description='Neural style transfer with Keras.') parser.add_argument('base_image_path', metavar='base', type=str, help='Path to the image to transform.') parser.add_argument('style_reference_image_path', metavar='ref', type=str, help='Path to the style reference image.') parser.add_argument('result_prefix', metavar='res_prefix', type=str, help='Prefix for the saved results.') parser.add_argument('--iter', type=int, default=10, required=False, help='Number of iterations to run.') parser.add_argument('--content_weight', type=float, default=0.025, required=False, help='Content weight.') parser.add_argument('--style_weight', type=float, default=1.0, required=False, help='Style weight.') parser.add_argument('--tv_weight', type=float, default=1.0, required=False, help='Total Variation weight.') args = parser.parse_args() base_image_path = args.base_image_path style_reference_image_path = args.style_reference_image_path result_prefix = args.result_prefix iterations = args.iter # these are the weights of the different loss components total_variation_weight = args.tv_weight style_weight = args.style_weight content_weight = args.content_weight # dimensions of the generated picture. width, height = load_img(base_image_path).size img_nrows = 400 img_ncols = int(width * img_nrows / height) # util function to open, resize and format pictures into appropriate tensors def preprocess_image(image_path): img = load_img(image_path, target_size=(img_nrows, img_ncols)) img = img_to_array(img) img = np.expand_dims(img, axis=0) img = vgg19.preprocess_input(img) return img # util function to convert a tensor into a valid image def deprocess_image(x): if K.image_data_format() == 'channels_first': x = x.reshape((3, img_nrows, img_ncols)) x = x.transpose((1, 2, 0)) else: x = x.reshape((img_nrows, img_ncols, 3)) # Remove zero-center by mean pixel x[:, :, 0] += 103.939 x[:, :, 1] += 116.779 x[:, :, 2] += 123.68 # 'BGR'->'RGB' x = x[:, :, ::-1] x = np.clip(x, 0, 255).astype('uint8') return x # get tensor representations of our images base_image = K.variable(preprocess_image(base_image_path)) style_reference_image = K.variable(preprocess_image(style_reference_image_path)) # this will contain our generated image if K.image_data_format() == 'channels_first': combination_image = K.placeholder((1, 3, img_nrows, img_ncols)) else: combination_image = K.placeholder((1, img_nrows, img_ncols, 3)) # combine the 3 images into a single Keras tensor input_tensor = K.concatenate([base_image, style_reference_image, combination_image], axis=0) # build the VGG19 network with our 3 images as input # the model will be loaded with pre-trained ImageNet weights model = vgg19.VGG19(input_tensor=input_tensor, weights='imagenet', include_top=False) print('Model loaded.') # get the symbolic outputs of each \"key\" layer (we gave them unique names). outputs_dict = dict([(layer.name, layer.output) for layer in model.layers]) # compute the neural style loss # first we need to define 4 util functions # the gram matrix of an image tensor (feature-wise outer product) def gram_matrix(x): assert K.ndim(x) == 3 if K.image_data_format() == 'channels_first': features = K.batch_flatten(x) else: features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1))) gram = K.dot(features, K.transpose(features)) return gram # the \"style loss\" is designed to maintain # the style of the reference image in the generated image. # It is based on the gram matrices (which capture style) of # feature maps from the style reference image # and from the generated image def style_loss(style, combination): assert K.ndim(style) == 3 assert K.ndim(combination) == 3 S = gram_matrix(style) C = gram_matrix(combination) channels = 3 size = img_nrows * img_ncols return K.sum(K.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2)) # an auxiliary loss function # designed to maintain the \"content\" of the # base image in the generated image def content_loss(base, combination): return K.sum(K.square(combination - base)) # the 3rd loss function, total variation loss, # designed to keep the generated image locally coherent def total_variation_loss(x): assert K.ndim(x) == 4 if K.image_data_format() == 'channels_first': a = K.square( x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, 1:, :img_ncols - 1]) b = K.square( x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, :img_nrows - 1, 1:]) else: a = K.square( x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :]) b = K.square( x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :]) return K.sum(K.pow(a + b, 1.25)) # combine these loss functions into a single scalar loss = K.variable(0.0) layer_features = outputs_dict['block5_conv2'] base_image_features = layer_features[0, :, :, :] combination_features = layer_features[2, :, :, :] loss += content_weight * content_loss(base_image_features, combination_features) feature_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1'] for layer_name in feature_layers: layer_features = outputs_dict[layer_name] style_reference_features = layer_features[1, :, :, :] combination_features = layer_features[2, :, :, :] sl = style_loss(style_reference_features, combination_features) loss += (style_weight / len(feature_layers)) * sl loss += total_variation_weight * total_variation_loss(combination_image) # get the gradients of the generated image wrt the loss grads = K.gradients(loss, combination_image) outputs = [loss] if isinstance(grads, (list, tuple)): outputs += grads else: outputs.append(grads) f_outputs = K.function([combination_image], outputs) def eval_loss_and_grads(x): if K.image_data_format() == 'channels_first': x = x.reshape((1, 3, img_nrows, img_ncols)) else: x = x.reshape((1, img_nrows, img_ncols, 3)) outs = f_outputs([x]) loss_value = outs[0] if len(outs[1:]) == 1: grad_values = outs[1].flatten().astype('float64') else: grad_values = np.array(outs[1:]).flatten().astype('float64') return loss_value, grad_values # this Evaluator class makes it possible # to compute loss and gradients in one pass # while retrieving them via two separate functions, # \"loss\" and \"grads\". This is done because scipy.optimize # requires separate functions for loss and gradients, # but computing them separately would be inefficient. class Evaluator(object): def __init__(self): self.loss_value = None self.grads_values = None def loss(self, x): assert self.loss_value is None loss_value, grad_values = eval_loss_and_grads(x) self.loss_value = loss_value self.grad_values = grad_values return self.loss_value def grads(self, x): assert self.loss_value is not None grad_values = np.copy(self.grad_values) self.loss_value = None self.grad_values = None return grad_values evaluator = Evaluator() # run scipy-based optimization (L-BFGS) over the pixels of the generated image # so as to minimize the neural style loss x = preprocess_image(base_image_path) for i in range(iterations): print('Start of iteration', i) start_time = time.time() x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(), fprime=evaluator.grads, maxfun=20) print('Current loss value:', min_val) # save current generated image img = deprocess_image(x.copy()) fname = result_prefix + '_at_iteration_%d.png' % i save_img(fname, img) end_time = time.time() print('Image saved as', fname) print('Iteration %d completed in %ds' % (i, end_time - start_time))","title":"Neural style transfer"},{"location":"4-Examples/neural_style_transfer/#details","text":"Style transfer consists in generating an image with the same \"content\" as a base image, but with the \"style\" of a different picture (typically artistic). This is achieved through the optimization of a loss function that has 3 components: \"style loss\", \"content loss\", and \"total variation loss\": The total variation loss imposes local spatial continuity between the pixels of the combination image, giving it visual coherence. The style loss is where the deep learning keeps in --that one is defined using a deep convolutional neural network. Precisely, it consists in a sum of L2 distances between the Gram matrices of the representations of the base image and the style reference image, extracted from different layers of a convnet (trained on ImageNet). The general idea is to capture color/texture information at different spatial scales (fairly large scales --defined by the depth of the layer considered). The content loss is a L2 distance between the features of the base image (extracted from a deep layer) and the features of the combination image, keeping the generated image close enough to the original one.","title":"Details"},{"location":"4-Examples/neural_style_transfer/#references","text":"- [A Neural Algorithm of Artistic Style](http://arxiv.org/abs/1508.06576) from __future__ import print_function from keras.preprocessing.image import load_img, save_img, img_to_array import numpy as np from scipy.optimize import fmin_l_bfgs_b import time import argparse from keras.applications import vgg19 from keras import backend as K parser = argparse.ArgumentParser(description='Neural style transfer with Keras.') parser.add_argument('base_image_path', metavar='base', type=str, help='Path to the image to transform.') parser.add_argument('style_reference_image_path', metavar='ref', type=str, help='Path to the style reference image.') parser.add_argument('result_prefix', metavar='res_prefix', type=str, help='Prefix for the saved results.') parser.add_argument('--iter', type=int, default=10, required=False, help='Number of iterations to run.') parser.add_argument('--content_weight', type=float, default=0.025, required=False, help='Content weight.') parser.add_argument('--style_weight', type=float, default=1.0, required=False, help='Style weight.') parser.add_argument('--tv_weight', type=float, default=1.0, required=False, help='Total Variation weight.') args = parser.parse_args() base_image_path = args.base_image_path style_reference_image_path = args.style_reference_image_path result_prefix = args.result_prefix iterations = args.iter # these are the weights of the different loss components total_variation_weight = args.tv_weight style_weight = args.style_weight content_weight = args.content_weight # dimensions of the generated picture. width, height = load_img(base_image_path).size img_nrows = 400 img_ncols = int(width * img_nrows / height) # util function to open, resize and format pictures into appropriate tensors def preprocess_image(image_path): img = load_img(image_path, target_size=(img_nrows, img_ncols)) img = img_to_array(img) img = np.expand_dims(img, axis=0) img = vgg19.preprocess_input(img) return img # util function to convert a tensor into a valid image def deprocess_image(x): if K.image_data_format() == 'channels_first': x = x.reshape((3, img_nrows, img_ncols)) x = x.transpose((1, 2, 0)) else: x = x.reshape((img_nrows, img_ncols, 3)) # Remove zero-center by mean pixel x[:, :, 0] += 103.939 x[:, :, 1] += 116.779 x[:, :, 2] += 123.68 # 'BGR'->'RGB' x = x[:, :, ::-1] x = np.clip(x, 0, 255).astype('uint8') return x # get tensor representations of our images base_image = K.variable(preprocess_image(base_image_path)) style_reference_image = K.variable(preprocess_image(style_reference_image_path)) # this will contain our generated image if K.image_data_format() == 'channels_first': combination_image = K.placeholder((1, 3, img_nrows, img_ncols)) else: combination_image = K.placeholder((1, img_nrows, img_ncols, 3)) # combine the 3 images into a single Keras tensor input_tensor = K.concatenate([base_image, style_reference_image, combination_image], axis=0) # build the VGG19 network with our 3 images as input # the model will be loaded with pre-trained ImageNet weights model = vgg19.VGG19(input_tensor=input_tensor, weights='imagenet', include_top=False) print('Model loaded.') # get the symbolic outputs of each \"key\" layer (we gave them unique names). outputs_dict = dict([(layer.name, layer.output) for layer in model.layers]) # compute the neural style loss # first we need to define 4 util functions # the gram matrix of an image tensor (feature-wise outer product) def gram_matrix(x): assert K.ndim(x) == 3 if K.image_data_format() == 'channels_first': features = K.batch_flatten(x) else: features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1))) gram = K.dot(features, K.transpose(features)) return gram # the \"style loss\" is designed to maintain # the style of the reference image in the generated image. # It is based on the gram matrices (which capture style) of # feature maps from the style reference image # and from the generated image def style_loss(style, combination): assert K.ndim(style) == 3 assert K.ndim(combination) == 3 S = gram_matrix(style) C = gram_matrix(combination) channels = 3 size = img_nrows * img_ncols return K.sum(K.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2)) # an auxiliary loss function # designed to maintain the \"content\" of the # base image in the generated image def content_loss(base, combination): return K.sum(K.square(combination - base)) # the 3rd loss function, total variation loss, # designed to keep the generated image locally coherent def total_variation_loss(x): assert K.ndim(x) == 4 if K.image_data_format() == 'channels_first': a = K.square( x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, 1:, :img_ncols - 1]) b = K.square( x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, :img_nrows - 1, 1:]) else: a = K.square( x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :]) b = K.square( x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :]) return K.sum(K.pow(a + b, 1.25)) # combine these loss functions into a single scalar loss = K.variable(0.0) layer_features = outputs_dict['block5_conv2'] base_image_features = layer_features[0, :, :, :] combination_features = layer_features[2, :, :, :] loss += content_weight * content_loss(base_image_features, combination_features) feature_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1'] for layer_name in feature_layers: layer_features = outputs_dict[layer_name] style_reference_features = layer_features[1, :, :, :] combination_features = layer_features[2, :, :, :] sl = style_loss(style_reference_features, combination_features) loss += (style_weight / len(feature_layers)) * sl loss += total_variation_weight * total_variation_loss(combination_image) # get the gradients of the generated image wrt the loss grads = K.gradients(loss, combination_image) outputs = [loss] if isinstance(grads, (list, tuple)): outputs += grads else: outputs.append(grads) f_outputs = K.function([combination_image], outputs) def eval_loss_and_grads(x): if K.image_data_format() == 'channels_first': x = x.reshape((1, 3, img_nrows, img_ncols)) else: x = x.reshape((1, img_nrows, img_ncols, 3)) outs = f_outputs([x]) loss_value = outs[0] if len(outs[1:]) == 1: grad_values = outs[1].flatten().astype('float64') else: grad_values = np.array(outs[1:]).flatten().astype('float64') return loss_value, grad_values # this Evaluator class makes it possible # to compute loss and gradients in one pass # while retrieving them via two separate functions, # \"loss\" and \"grads\". This is done because scipy.optimize # requires separate functions for loss and gradients, # but computing them separately would be inefficient. class Evaluator(object): def __init__(self): self.loss_value = None self.grads_values = None def loss(self, x): assert self.loss_value is None loss_value, grad_values = eval_loss_and_grads(x) self.loss_value = loss_value self.grad_values = grad_values return self.loss_value def grads(self, x): assert self.loss_value is not None grad_values = np.copy(self.grad_values) self.loss_value = None self.grad_values = None return grad_values evaluator = Evaluator() # run scipy-based optimization (L-BFGS) over the pixels of the generated image # so as to minimize the neural style loss x = preprocess_image(base_image_path) for i in range(iterations): print('Start of iteration', i) start_time = time.time() x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(), fprime=evaluator.grads, maxfun=20) print('Current loss value:', min_val) # save current generated image img = deprocess_image(x.copy()) fname = result_prefix + '_at_iteration_%d.png' % i save_img(fname, img) end_time = time.time() print('Image saved as', fname) print('Iteration %d completed in %ds' % (i, end_time - start_time))","title":"References"},{"location":"4-Examples/reuters_mlp/","text":"Trains and evaluate a simple MLP on the Reuters newswire topic classification task. from __future__ import print_function import numpy as np import keras from keras.datasets import reuters from keras.models import Sequential from keras.layers import Dense, Dropout, Activation from keras.preprocessing.text import Tokenizer max_words = 1000 batch_size = 32 epochs = 5 print('Loading data...') (x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words, test_split=0.2) print(len(x_train), 'train sequences') print(len(x_test), 'test sequences') num_classes = np.max(y_train) + 1 print(num_classes, 'classes') print('Vectorizing sequence data...') tokenizer = Tokenizer(num_words=max_words) x_train = tokenizer.sequences_to_matrix(x_train, mode='binary') x_test = tokenizer.sequences_to_matrix(x_test, mode='binary') print('x_train shape:', x_train.shape) print('x_test shape:', x_test.shape) print('Convert class vector to binary class matrix ' '(for use with categorical_crossentropy)') y_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes) print('y_train shape:', y_train.shape) print('y_test shape:', y_test.shape) print('Building model...') model = Sequential() model.add(Dense(512, input_shape=(max_words,))) model.add(Activation('relu')) model.add(Dropout(0.5)) model.add(Dense(num_classes)) model.add(Activation('softmax')) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1) score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1) print('Test score:', score[0]) print('Test accuracy:', score[1])","title":"Reuters mlp"},{"location":"4-Examples/reuters_mlp_relu_vs_selu/","text":"Compares self-normalizing MLPs with regular MLPs. Compares the performance of a simple MLP using two different activation functions: RELU and SELU on the Reuters newswire topic classification task. Reference Klambauer, G., Unterthiner, T., Mayr, A., & Hochreiter, S. (2017). Self-Normalizing Neural Networks. arXiv preprint arXiv:1706.02515. https://arxiv.org/abs/1706.02515 from __future__ import print_function import numpy as np import matplotlib.pyplot as plt import keras from keras.datasets import reuters from keras.models import Sequential from keras.layers import Dense, Activation, Dropout from keras.layers.noise import AlphaDropout from keras.preprocessing.text import Tokenizer max_words = 1000 batch_size = 16 epochs = 40 plot = True def create_network(n_dense=6, dense_units=16, activation='selu', dropout=AlphaDropout, dropout_rate=0.1, kernel_initializer='lecun_normal', optimizer='adam', num_classes=1, max_words=max_words): \"\"\"Generic function to create a fully-connected neural network. # Arguments n_dense: int > 0. Number of dense layers. dense_units: int > 0. Number of dense units per layer. dropout: keras.layers.Layer. A dropout layer to apply. dropout_rate: 0 <= float <= 1. The rate of dropout. kernel_initializer: str. The initializer for the weights. optimizer: str/keras.optimizers.Optimizer. The optimizer to use. num_classes: int > 0. The number of classes to predict. max_words: int > 0. The maximum number of words per data point. # Returns A Keras model instance (compiled). \"\"\" model = Sequential() model.add(Dense(dense_units, input_shape=(max_words,), kernel_initializer=kernel_initializer)) model.add(Activation(activation)) model.add(dropout(dropout_rate)) for i in range(n_dense - 1): model.add(Dense(dense_units, kernel_initializer=kernel_initializer)) model.add(Activation(activation)) model.add(dropout(dropout_rate)) model.add(Dense(num_classes)) model.add(Activation('softmax')) model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) return model network1 = { 'n_dense': 6, 'dense_units': 16, 'activation': 'relu', 'dropout': Dropout, 'dropout_rate': 0.5, 'kernel_initializer': 'glorot_uniform', 'optimizer': 'sgd' } network2 = { 'n_dense': 6, 'dense_units': 16, 'activation': 'selu', 'dropout': AlphaDropout, 'dropout_rate': 0.1, 'kernel_initializer': 'lecun_normal', 'optimizer': 'sgd' } print('Loading data...') (x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words, test_split=0.2) print(len(x_train), 'train sequences') print(len(x_test), 'test sequences') num_classes = np.max(y_train) + 1 print(num_classes, 'classes') print('Vectorizing sequence data...') tokenizer = Tokenizer(num_words=max_words) x_train = tokenizer.sequences_to_matrix(x_train, mode='binary') x_test = tokenizer.sequences_to_matrix(x_test, mode='binary') print('x_train shape:', x_train.shape) print('x_test shape:', x_test.shape) print('Convert class vector to binary class matrix ' '(for use with categorical_crossentropy)') y_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes) print('y_train shape:', y_train.shape) print('y_test shape:', y_test.shape) print('\\nBuilding network 1...') model1 = create_network(num_classes=num_classes, **network1) history_model1 = model1.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1) score_model1 = model1.evaluate(x_test, y_test, batch_size=batch_size, verbose=1) print('\\nBuilding network 2...') model2 = create_network(num_classes=num_classes, **network2) history_model2 = model2.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1) score_model2 = model2.evaluate(x_test, y_test, batch_size=batch_size, verbose=1) print('\\nNetwork 1 results') print('Hyperparameters:', network1) print('Test score:', score_model1[0]) print('Test accuracy:', score_model1[1]) print('Network 2 results') print('Hyperparameters:', network2) print('Test score:', score_model2[0]) print('Test accuracy:', score_model2[1]) plt.plot(range(epochs), history_model1.history['val_loss'], 'g-', label='Network 1 Val Loss') plt.plot(range(epochs), history_model2.history['val_loss'], 'r-', label='Network 2 Val Loss') plt.plot(range(epochs), history_model1.history['loss'], 'g--', label='Network 1 Loss') plt.plot(range(epochs), history_model2.history['loss'], 'r--', label='Network 2 Loss') plt.xlabel('Epochs') plt.ylabel('Loss') plt.legend() plt.savefig('comparison_of_networks.png')","title":"Reuters mlp relu vs selu"},{"location":"4-Examples/reuters_mlp_relu_vs_selu/#reference","text":"Klambauer, G., Unterthiner, T., Mayr, A., & Hochreiter, S. (2017). Self-Normalizing Neural Networks. arXiv preprint arXiv:1706.02515. https://arxiv.org/abs/1706.02515 from __future__ import print_function import numpy as np import matplotlib.pyplot as plt import keras from keras.datasets import reuters from keras.models import Sequential from keras.layers import Dense, Activation, Dropout from keras.layers.noise import AlphaDropout from keras.preprocessing.text import Tokenizer max_words = 1000 batch_size = 16 epochs = 40 plot = True def create_network(n_dense=6, dense_units=16, activation='selu', dropout=AlphaDropout, dropout_rate=0.1, kernel_initializer='lecun_normal', optimizer='adam', num_classes=1, max_words=max_words): \"\"\"Generic function to create a fully-connected neural network. # Arguments n_dense: int > 0. Number of dense layers. dense_units: int > 0. Number of dense units per layer. dropout: keras.layers.Layer. A dropout layer to apply. dropout_rate: 0 <= float <= 1. The rate of dropout. kernel_initializer: str. The initializer for the weights. optimizer: str/keras.optimizers.Optimizer. The optimizer to use. num_classes: int > 0. The number of classes to predict. max_words: int > 0. The maximum number of words per data point. # Returns A Keras model instance (compiled). \"\"\" model = Sequential() model.add(Dense(dense_units, input_shape=(max_words,), kernel_initializer=kernel_initializer)) model.add(Activation(activation)) model.add(dropout(dropout_rate)) for i in range(n_dense - 1): model.add(Dense(dense_units, kernel_initializer=kernel_initializer)) model.add(Activation(activation)) model.add(dropout(dropout_rate)) model.add(Dense(num_classes)) model.add(Activation('softmax')) model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) return model network1 = { 'n_dense': 6, 'dense_units': 16, 'activation': 'relu', 'dropout': Dropout, 'dropout_rate': 0.5, 'kernel_initializer': 'glorot_uniform', 'optimizer': 'sgd' } network2 = { 'n_dense': 6, 'dense_units': 16, 'activation': 'selu', 'dropout': AlphaDropout, 'dropout_rate': 0.1, 'kernel_initializer': 'lecun_normal', 'optimizer': 'sgd' } print('Loading data...') (x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words, test_split=0.2) print(len(x_train), 'train sequences') print(len(x_test), 'test sequences') num_classes = np.max(y_train) + 1 print(num_classes, 'classes') print('Vectorizing sequence data...') tokenizer = Tokenizer(num_words=max_words) x_train = tokenizer.sequences_to_matrix(x_train, mode='binary') x_test = tokenizer.sequences_to_matrix(x_test, mode='binary') print('x_train shape:', x_train.shape) print('x_test shape:', x_test.shape) print('Convert class vector to binary class matrix ' '(for use with categorical_crossentropy)') y_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes) print('y_train shape:', y_train.shape) print('y_test shape:', y_test.shape) print('\\nBuilding network 1...') model1 = create_network(num_classes=num_classes, **network1) history_model1 = model1.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1) score_model1 = model1.evaluate(x_test, y_test, batch_size=batch_size, verbose=1) print('\\nBuilding network 2...') model2 = create_network(num_classes=num_classes, **network2) history_model2 = model2.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1) score_model2 = model2.evaluate(x_test, y_test, batch_size=batch_size, verbose=1) print('\\nNetwork 1 results') print('Hyperparameters:', network1) print('Test score:', score_model1[0]) print('Test accuracy:', score_model1[1]) print('Network 2 results') print('Hyperparameters:', network2) print('Test score:', score_model2[0]) print('Test accuracy:', score_model2[1]) plt.plot(range(epochs), history_model1.history['val_loss'], 'g-', label='Network 1 Val Loss') plt.plot(range(epochs), history_model2.history['val_loss'], 'r-', label='Network 2 Val Loss') plt.plot(range(epochs), history_model1.history['loss'], 'g--', label='Network 1 Loss') plt.plot(range(epochs), history_model2.history['loss'], 'r--', label='Network 2 Loss') plt.xlabel('Epochs') plt.ylabel('Loss') plt.legend() plt.savefig('comparison_of_networks.png')","title":"Reference"},{"location":"4-Examples/tensorboard_embeddings_mnist/","text":"Trains a simple convnet on the MNIST dataset and embeds test data. The test data is embedded using the weights of the final dense layer, just before the classification head. This embedding can then be visualized using TensorBoard's Embedding Projector. from __future__ import print_function from os import makedirs from os.path import exists, join import keras from keras.callbacks import TensorBoard from keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense, Dropout, Flatten from keras.layers import Conv2D, MaxPooling2D from keras import backend as K import numpy as np batch_size = 128 num_classes = 10 epochs = 12 log_dir = './logs' if not exists(log_dir): makedirs(log_dir) # input image dimensions img_rows, img_cols = 28, 28 # the data, split between train and test sets (x_train, y_train), (x_test, y_test) = mnist.load_data() if K.image_data_format() == 'channels_first': x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols) x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols) input_shape = (1, img_rows, img_cols) else: x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1) x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1) input_shape = (img_rows, img_cols, 1) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') # save class labels to disk to color data points in TensorBoard accordingly with open(join(log_dir, 'metadata.tsv'), 'w') as f: np.savetxt(f, y_test) # convert class vectors to binary class matrices y_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes) tensorboard = TensorBoard(batch_size=batch_size, embeddings_freq=1, embeddings_layer_names=['features'], embeddings_metadata='metadata.tsv', embeddings_data=x_test) model = Sequential() model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape)) model.add(Conv2D(64, (3, 3), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Flatten()) model.add(Dense(128, activation='relu', name='features')) model.add(Dropout(0.5)) model.add(Dense(num_classes, activation='softmax')) model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy']) model.fit(x_train, y_train, batch_size=batch_size, callbacks=[tensorboard], epochs=epochs, verbose=1, validation_data=(x_test, y_test)) score = model.evaluate(x_test, y_test, verbose=0) print('Test loss:', score[0]) print('Test accuracy:', score[1]) # You can now launch tensorboard with `tensorboard --logdir=./logs` on your # command line and then go to http://localhost:6006/#projector to view the # embeddings","title":"Tensorboard embeddings mnist"},{"location":"4-Examples/variational_autoencoder/","text":"Example of VAE on MNIST dataset using MLP The VAE has a modular design. The encoder, decoder and VAE are 3 models that share weights. After training the VAE model, the encoder can be used to generate latent vectors. The decoder can be used to generate MNIST digits by sampling the latent vector from a Gaussian distribution with mean = 0 and std = 1. Reference [1] Kingma, Diederik P., and Max Welling. \"Auto-Encoding Variational Bayes.\" https://arxiv.org/abs/1312.6114 from __future__ import absolute_import from __future__ import division from __future__ import print_function from keras.layers import Lambda, Input, Dense from keras.models import Model from keras.datasets import mnist from keras.losses import mse, binary_crossentropy from keras.utils import plot_model from keras import backend as K import numpy as np import matplotlib.pyplot as plt import argparse import os # reparameterization trick # instead of sampling from Q(z|X), sample epsilon = N(0,I) # z = z_mean + sqrt(var) * epsilon def sampling(args): \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian. # Arguments args (tensor): mean and log of variance of Q(z|X) # Returns z (tensor): sampled latent vector \"\"\" z_mean, z_log_var = args batch = K.shape(z_mean)[0] dim = K.int_shape(z_mean)[1] # by default, random_normal has mean = 0 and std = 1.0 epsilon = K.random_normal(shape=(batch, dim)) return z_mean + K.exp(0.5 * z_log_var) * epsilon def plot_results(models, data, batch_size=128, model_name=\"vae_mnist\"): \"\"\"Plots labels and MNIST digits as a function of the 2D latent vector # Arguments models (tuple): encoder and decoder models data (tuple): test data and label batch_size (int): prediction batch size model_name (string): which model is using this function \"\"\" encoder, decoder = models x_test, y_test = data os.makedirs(model_name, exist_ok=True) filename = os.path.join(model_name, \"vae_mean.png\") # display a 2D plot of the digit classes in the latent space z_mean, _, _ = encoder.predict(x_test, batch_size=batch_size) plt.figure(figsize=(12, 10)) plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test) plt.colorbar() plt.xlabel(\"z[0]\") plt.ylabel(\"z[1]\") plt.savefig(filename) plt.show() filename = os.path.join(model_name, \"digits_over_latent.png\") # display a 30x30 2D manifold of digits n = 30 digit_size = 28 figure = np.zeros((digit_size * n, digit_size * n)) # linearly spaced coordinates corresponding to the 2D plot # of digit classes in the latent space grid_x = np.linspace(-4, 4, n) grid_y = np.linspace(-4, 4, n)[::-1] for i, yi in enumerate(grid_y): for j, xi in enumerate(grid_x): z_sample = np.array([[xi, yi]]) x_decoded = decoder.predict(z_sample) digit = x_decoded[0].reshape(digit_size, digit_size) figure[i * digit_size: (i + 1) * digit_size, j * digit_size: (j + 1) * digit_size] = digit plt.figure(figsize=(10, 10)) start_range = digit_size // 2 end_range = n * digit_size + start_range + 1 pixel_range = np.arange(start_range, end_range, digit_size) sample_range_x = np.round(grid_x, 1) sample_range_y = np.round(grid_y, 1) plt.xticks(pixel_range, sample_range_x) plt.yticks(pixel_range, sample_range_y) plt.xlabel(\"z[0]\") plt.ylabel(\"z[1]\") plt.imshow(figure, cmap='Greys_r') plt.savefig(filename) plt.show() # MNIST dataset (x_train, y_train), (x_test, y_test) = mnist.load_data() image_size = x_train.shape[1] original_dim = image_size * image_size x_train = np.reshape(x_train, [-1, original_dim]) x_test = np.reshape(x_test, [-1, original_dim]) x_train = x_train.astype('float32') / 255 x_test = x_test.astype('float32') / 255 # network parameters input_shape = (original_dim, ) intermediate_dim = 512 batch_size = 128 latent_dim = 2 epochs = 50 # VAE model = encoder + decoder # build encoder model inputs = Input(shape=input_shape, name='encoder_input') x = Dense(intermediate_dim, activation='relu')(inputs) z_mean = Dense(latent_dim, name='z_mean')(x) z_log_var = Dense(latent_dim, name='z_log_var')(x) # use reparameterization trick to push the sampling out as input # note that \"output_shape\" isn't necessary with the TensorFlow backend z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var]) # instantiate encoder model encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder') encoder.summary() plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True) # build decoder model latent_inputs = Input(shape=(latent_dim,), name='z_sampling') x = Dense(intermediate_dim, activation='relu')(latent_inputs) outputs = Dense(original_dim, activation='sigmoid')(x) # instantiate decoder model decoder = Model(latent_inputs, outputs, name='decoder') decoder.summary() plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True) # instantiate VAE model outputs = decoder(encoder(inputs)[2]) vae = Model(inputs, outputs, name='vae_mlp') if __name__ == '__main__': parser = argparse.ArgumentParser() help_ = \"Load h5 model trained weights\" parser.add_argument(\"-w\", \"--weights\", help=help_) help_ = \"Use mse loss instead of binary cross entropy (default)\" parser.add_argument(\"-m\", \"--mse\", help=help_, action='store_true') args = parser.parse_args() models = (encoder, decoder) data = (x_test, y_test) # VAE loss = mse_loss or xent_loss + kl_loss if args.mse: reconstruction_loss = mse(inputs, outputs) else: reconstruction_loss = binary_crossentropy(inputs, outputs) reconstruction_loss *= original_dim kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var) kl_loss = K.sum(kl_loss, axis=-1) kl_loss *= -0.5 vae_loss = K.mean(reconstruction_loss + kl_loss) vae.add_loss(vae_loss) vae.compile(optimizer='adam') vae.summary() plot_model(vae, to_file='vae_mlp.png', show_shapes=True) if args.weights: vae.load_weights(args.weights) else: # train the autoencoder vae.fit(x_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, None)) vae.save_weights('vae_mlp_mnist.h5') plot_results(models, data, batch_size=batch_size, model_name=\"vae_mlp\")","title":"Variational autoencoder"},{"location":"4-Examples/variational_autoencoder/#reference","text":"[1] Kingma, Diederik P., and Max Welling. \"Auto-Encoding Variational Bayes.\" https://arxiv.org/abs/1312.6114 from __future__ import absolute_import from __future__ import division from __future__ import print_function from keras.layers import Lambda, Input, Dense from keras.models import Model from keras.datasets import mnist from keras.losses import mse, binary_crossentropy from keras.utils import plot_model from keras import backend as K import numpy as np import matplotlib.pyplot as plt import argparse import os # reparameterization trick # instead of sampling from Q(z|X), sample epsilon = N(0,I) # z = z_mean + sqrt(var) * epsilon def sampling(args): \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian. # Arguments args (tensor): mean and log of variance of Q(z|X) # Returns z (tensor): sampled latent vector \"\"\" z_mean, z_log_var = args batch = K.shape(z_mean)[0] dim = K.int_shape(z_mean)[1] # by default, random_normal has mean = 0 and std = 1.0 epsilon = K.random_normal(shape=(batch, dim)) return z_mean + K.exp(0.5 * z_log_var) * epsilon def plot_results(models, data, batch_size=128, model_name=\"vae_mnist\"): \"\"\"Plots labels and MNIST digits as a function of the 2D latent vector # Arguments models (tuple): encoder and decoder models data (tuple): test data and label batch_size (int): prediction batch size model_name (string): which model is using this function \"\"\" encoder, decoder = models x_test, y_test = data os.makedirs(model_name, exist_ok=True) filename = os.path.join(model_name, \"vae_mean.png\") # display a 2D plot of the digit classes in the latent space z_mean, _, _ = encoder.predict(x_test, batch_size=batch_size) plt.figure(figsize=(12, 10)) plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test) plt.colorbar() plt.xlabel(\"z[0]\") plt.ylabel(\"z[1]\") plt.savefig(filename) plt.show() filename = os.path.join(model_name, \"digits_over_latent.png\") # display a 30x30 2D manifold of digits n = 30 digit_size = 28 figure = np.zeros((digit_size * n, digit_size * n)) # linearly spaced coordinates corresponding to the 2D plot # of digit classes in the latent space grid_x = np.linspace(-4, 4, n) grid_y = np.linspace(-4, 4, n)[::-1] for i, yi in enumerate(grid_y): for j, xi in enumerate(grid_x): z_sample = np.array([[xi, yi]]) x_decoded = decoder.predict(z_sample) digit = x_decoded[0].reshape(digit_size, digit_size) figure[i * digit_size: (i + 1) * digit_size, j * digit_size: (j + 1) * digit_size] = digit plt.figure(figsize=(10, 10)) start_range = digit_size // 2 end_range = n * digit_size + start_range + 1 pixel_range = np.arange(start_range, end_range, digit_size) sample_range_x = np.round(grid_x, 1) sample_range_y = np.round(grid_y, 1) plt.xticks(pixel_range, sample_range_x) plt.yticks(pixel_range, sample_range_y) plt.xlabel(\"z[0]\") plt.ylabel(\"z[1]\") plt.imshow(figure, cmap='Greys_r') plt.savefig(filename) plt.show() # MNIST dataset (x_train, y_train), (x_test, y_test) = mnist.load_data() image_size = x_train.shape[1] original_dim = image_size * image_size x_train = np.reshape(x_train, [-1, original_dim]) x_test = np.reshape(x_test, [-1, original_dim]) x_train = x_train.astype('float32') / 255 x_test = x_test.astype('float32') / 255 # network parameters input_shape = (original_dim, ) intermediate_dim = 512 batch_size = 128 latent_dim = 2 epochs = 50 # VAE model = encoder + decoder # build encoder model inputs = Input(shape=input_shape, name='encoder_input') x = Dense(intermediate_dim, activation='relu')(inputs) z_mean = Dense(latent_dim, name='z_mean')(x) z_log_var = Dense(latent_dim, name='z_log_var')(x) # use reparameterization trick to push the sampling out as input # note that \"output_shape\" isn't necessary with the TensorFlow backend z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var]) # instantiate encoder model encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder') encoder.summary() plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True) # build decoder model latent_inputs = Input(shape=(latent_dim,), name='z_sampling') x = Dense(intermediate_dim, activation='relu')(latent_inputs) outputs = Dense(original_dim, activation='sigmoid')(x) # instantiate decoder model decoder = Model(latent_inputs, outputs, name='decoder') decoder.summary() plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True) # instantiate VAE model outputs = decoder(encoder(inputs)[2]) vae = Model(inputs, outputs, name='vae_mlp') if __name__ == '__main__': parser = argparse.ArgumentParser() help_ = \"Load h5 model trained weights\" parser.add_argument(\"-w\", \"--weights\", help=help_) help_ = \"Use mse loss instead of binary cross entropy (default)\" parser.add_argument(\"-m\", \"--mse\", help=help_, action='store_true') args = parser.parse_args() models = (encoder, decoder) data = (x_test, y_test) # VAE loss = mse_loss or xent_loss + kl_loss if args.mse: reconstruction_loss = mse(inputs, outputs) else: reconstruction_loss = binary_crossentropy(inputs, outputs) reconstruction_loss *= original_dim kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var) kl_loss = K.sum(kl_loss, axis=-1) kl_loss *= -0.5 vae_loss = K.mean(reconstruction_loss + kl_loss) vae.add_loss(vae_loss) vae.compile(optimizer='adam') vae.summary() plot_model(vae, to_file='vae_mlp.png', show_shapes=True) if args.weights: vae.load_weights(args.weights) else: # train the autoencoder vae.fit(x_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, None)) vae.save_weights('vae_mlp_mnist.h5') plot_results(models, data, batch_size=batch_size, model_name=\"vae_mlp\")","title":"Reference"},{"location":"4-Examples/variational_autoencoder_deconv/","text":"Example of VAE on MNIST dataset using CNN The VAE has a modular design. The encoder, decoder and VAE are 3 models that share weights. After training the VAE model, the encoder can be used to generate latent vectors. The decoder can be used to generate MNIST digits by sampling the latent vector from a Gaussian distribution with mean=0 and std=1. Reference [1] Kingma, Diederik P., and Max Welling. \"Auto-encoding variational bayes.\" https://arxiv.org/abs/1312.6114 from __future__ import absolute_import from __future__ import division from __future__ import print_function from keras.layers import Dense, Input from keras.layers import Conv2D, Flatten, Lambda from keras.layers import Reshape, Conv2DTranspose from keras.models import Model from keras.datasets import mnist from keras.losses import mse, binary_crossentropy from keras.utils import plot_model from keras import backend as K import numpy as np import matplotlib.pyplot as plt import argparse import os # reparameterization trick # instead of sampling from Q(z|X), sample eps = N(0,I) # then z = z_mean + sqrt(var)*eps def sampling(args): \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian. # Arguments args (tensor): mean and log of variance of Q(z|X) # Returns z (tensor): sampled latent vector \"\"\" z_mean, z_log_var = args batch = K.shape(z_mean)[0] dim = K.int_shape(z_mean)[1] # by default, random_normal has mean=0 and std=1.0 epsilon = K.random_normal(shape=(batch, dim)) return z_mean + K.exp(0.5 * z_log_var) * epsilon def plot_results(models, data, batch_size=128, model_name=\"vae_mnist\"): \"\"\"Plots labels and MNIST digits as function of 2-dim latent vector # Arguments models (tuple): encoder and decoder models data (tuple): test data and label batch_size (int): prediction batch size model_name (string): which model is using this function \"\"\" encoder, decoder = models x_test, y_test = data os.makedirs(model_name, exist_ok=True) filename = os.path.join(model_name, \"vae_mean.png\") # display a 2D plot of the digit classes in the latent space z_mean, _, _ = encoder.predict(x_test, batch_size=batch_size) plt.figure(figsize=(12, 10)) plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test) plt.colorbar() plt.xlabel(\"z[0]\") plt.ylabel(\"z[1]\") plt.savefig(filename) plt.show() filename = os.path.join(model_name, \"digits_over_latent.png\") # display a 30x30 2D manifold of digits n = 30 digit_size = 28 figure = np.zeros((digit_size * n, digit_size * n)) # linearly spaced coordinates corresponding to the 2D plot # of digit classes in the latent space grid_x = np.linspace(-4, 4, n) grid_y = np.linspace(-4, 4, n)[::-1] for i, yi in enumerate(grid_y): for j, xi in enumerate(grid_x): z_sample = np.array([[xi, yi]]) x_decoded = decoder.predict(z_sample) digit = x_decoded[0].reshape(digit_size, digit_size) figure[i * digit_size: (i + 1) * digit_size, j * digit_size: (j + 1) * digit_size] = digit plt.figure(figsize=(10, 10)) start_range = digit_size // 2 end_range = n * digit_size + start_range + 1 pixel_range = np.arange(start_range, end_range, digit_size) sample_range_x = np.round(grid_x, 1) sample_range_y = np.round(grid_y, 1) plt.xticks(pixel_range, sample_range_x) plt.yticks(pixel_range, sample_range_y) plt.xlabel(\"z[0]\") plt.ylabel(\"z[1]\") plt.imshow(figure, cmap='Greys_r') plt.savefig(filename) plt.show() # MNIST dataset (x_train, y_train), (x_test, y_test) = mnist.load_data() image_size = x_train.shape[1] x_train = np.reshape(x_train, [-1, image_size, image_size, 1]) x_test = np.reshape(x_test, [-1, image_size, image_size, 1]) x_train = x_train.astype('float32') / 255 x_test = x_test.astype('float32') / 255 # network parameters input_shape = (image_size, image_size, 1) batch_size = 128 kernel_size = 3 filters = 16 latent_dim = 2 epochs = 30 # VAE model = encoder + decoder # build encoder model inputs = Input(shape=input_shape, name='encoder_input') x = inputs for i in range(2): filters *= 2 x = Conv2D(filters=filters, kernel_size=kernel_size, activation='relu', strides=2, padding='same')(x) # shape info needed to build decoder model shape = K.int_shape(x) # generate latent vector Q(z|X) x = Flatten()(x) x = Dense(16, activation='relu')(x) z_mean = Dense(latent_dim, name='z_mean')(x) z_log_var = Dense(latent_dim, name='z_log_var')(x) # use reparameterization trick to push the sampling out as input # note that \"output_shape\" isn't necessary with the TensorFlow backend z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var]) # instantiate encoder model encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder') encoder.summary() plot_model(encoder, to_file='vae_cnn_encoder.png', show_shapes=True) # build decoder model latent_inputs = Input(shape=(latent_dim,), name='z_sampling') x = Dense(shape[1] * shape[2] * shape[3], activation='relu')(latent_inputs) x = Reshape((shape[1], shape[2], shape[3]))(x) for i in range(2): x = Conv2DTranspose(filters=filters, kernel_size=kernel_size, activation='relu', strides=2, padding='same')(x) filters //= 2 outputs = Conv2DTranspose(filters=1, kernel_size=kernel_size, activation='sigmoid', padding='same', name='decoder_output')(x) # instantiate decoder model decoder = Model(latent_inputs, outputs, name='decoder') decoder.summary() plot_model(decoder, to_file='vae_cnn_decoder.png', show_shapes=True) # instantiate VAE model outputs = decoder(encoder(inputs)[2]) vae = Model(inputs, outputs, name='vae') if __name__ == '__main__': parser = argparse.ArgumentParser() help_ = \"Load h5 model trained weights\" parser.add_argument(\"-w\", \"--weights\", help=help_) help_ = \"Use mse loss instead of binary cross entropy (default)\" parser.add_argument(\"-m\", \"--mse\", help=help_, action='store_true') args = parser.parse_args() models = (encoder, decoder) data = (x_test, y_test) # VAE loss = mse_loss or xent_loss + kl_loss if args.mse: reconstruction_loss = mse(K.flatten(inputs), K.flatten(outputs)) else: reconstruction_loss = binary_crossentropy(K.flatten(inputs), K.flatten(outputs)) reconstruction_loss *= image_size * image_size kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var) kl_loss = K.sum(kl_loss, axis=-1) kl_loss *= -0.5 vae_loss = K.mean(reconstruction_loss + kl_loss) vae.add_loss(vae_loss) vae.compile(optimizer='rmsprop') vae.summary() plot_model(vae, to_file='vae_cnn.png', show_shapes=True) if args.weights: vae.load_weights(args.weights) else: # train the autoencoder vae.fit(x_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, None)) vae.save_weights('vae_cnn_mnist.h5') plot_results(models, data, batch_size=batch_size, model_name=\"vae_cnn\")","title":"Variational autoencoder deconv"},{"location":"4-Examples/variational_autoencoder_deconv/#reference","text":"[1] Kingma, Diederik P., and Max Welling. \"Auto-encoding variational bayes.\" https://arxiv.org/abs/1312.6114 from __future__ import absolute_import from __future__ import division from __future__ import print_function from keras.layers import Dense, Input from keras.layers import Conv2D, Flatten, Lambda from keras.layers import Reshape, Conv2DTranspose from keras.models import Model from keras.datasets import mnist from keras.losses import mse, binary_crossentropy from keras.utils import plot_model from keras import backend as K import numpy as np import matplotlib.pyplot as plt import argparse import os # reparameterization trick # instead of sampling from Q(z|X), sample eps = N(0,I) # then z = z_mean + sqrt(var)*eps def sampling(args): \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian. # Arguments args (tensor): mean and log of variance of Q(z|X) # Returns z (tensor): sampled latent vector \"\"\" z_mean, z_log_var = args batch = K.shape(z_mean)[0] dim = K.int_shape(z_mean)[1] # by default, random_normal has mean=0 and std=1.0 epsilon = K.random_normal(shape=(batch, dim)) return z_mean + K.exp(0.5 * z_log_var) * epsilon def plot_results(models, data, batch_size=128, model_name=\"vae_mnist\"): \"\"\"Plots labels and MNIST digits as function of 2-dim latent vector # Arguments models (tuple): encoder and decoder models data (tuple): test data and label batch_size (int): prediction batch size model_name (string): which model is using this function \"\"\" encoder, decoder = models x_test, y_test = data os.makedirs(model_name, exist_ok=True) filename = os.path.join(model_name, \"vae_mean.png\") # display a 2D plot of the digit classes in the latent space z_mean, _, _ = encoder.predict(x_test, batch_size=batch_size) plt.figure(figsize=(12, 10)) plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test) plt.colorbar() plt.xlabel(\"z[0]\") plt.ylabel(\"z[1]\") plt.savefig(filename) plt.show() filename = os.path.join(model_name, \"digits_over_latent.png\") # display a 30x30 2D manifold of digits n = 30 digit_size = 28 figure = np.zeros((digit_size * n, digit_size * n)) # linearly spaced coordinates corresponding to the 2D plot # of digit classes in the latent space grid_x = np.linspace(-4, 4, n) grid_y = np.linspace(-4, 4, n)[::-1] for i, yi in enumerate(grid_y): for j, xi in enumerate(grid_x): z_sample = np.array([[xi, yi]]) x_decoded = decoder.predict(z_sample) digit = x_decoded[0].reshape(digit_size, digit_size) figure[i * digit_size: (i + 1) * digit_size, j * digit_size: (j + 1) * digit_size] = digit plt.figure(figsize=(10, 10)) start_range = digit_size // 2 end_range = n * digit_size + start_range + 1 pixel_range = np.arange(start_range, end_range, digit_size) sample_range_x = np.round(grid_x, 1) sample_range_y = np.round(grid_y, 1) plt.xticks(pixel_range, sample_range_x) plt.yticks(pixel_range, sample_range_y) plt.xlabel(\"z[0]\") plt.ylabel(\"z[1]\") plt.imshow(figure, cmap='Greys_r') plt.savefig(filename) plt.show() # MNIST dataset (x_train, y_train), (x_test, y_test) = mnist.load_data() image_size = x_train.shape[1] x_train = np.reshape(x_train, [-1, image_size, image_size, 1]) x_test = np.reshape(x_test, [-1, image_size, image_size, 1]) x_train = x_train.astype('float32') / 255 x_test = x_test.astype('float32') / 255 # network parameters input_shape = (image_size, image_size, 1) batch_size = 128 kernel_size = 3 filters = 16 latent_dim = 2 epochs = 30 # VAE model = encoder + decoder # build encoder model inputs = Input(shape=input_shape, name='encoder_input') x = inputs for i in range(2): filters *= 2 x = Conv2D(filters=filters, kernel_size=kernel_size, activation='relu', strides=2, padding='same')(x) # shape info needed to build decoder model shape = K.int_shape(x) # generate latent vector Q(z|X) x = Flatten()(x) x = Dense(16, activation='relu')(x) z_mean = Dense(latent_dim, name='z_mean')(x) z_log_var = Dense(latent_dim, name='z_log_var')(x) # use reparameterization trick to push the sampling out as input # note that \"output_shape\" isn't necessary with the TensorFlow backend z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var]) # instantiate encoder model encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder') encoder.summary() plot_model(encoder, to_file='vae_cnn_encoder.png', show_shapes=True) # build decoder model latent_inputs = Input(shape=(latent_dim,), name='z_sampling') x = Dense(shape[1] * shape[2] * shape[3], activation='relu')(latent_inputs) x = Reshape((shape[1], shape[2], shape[3]))(x) for i in range(2): x = Conv2DTranspose(filters=filters, kernel_size=kernel_size, activation='relu', strides=2, padding='same')(x) filters //= 2 outputs = Conv2DTranspose(filters=1, kernel_size=kernel_size, activation='sigmoid', padding='same', name='decoder_output')(x) # instantiate decoder model decoder = Model(latent_inputs, outputs, name='decoder') decoder.summary() plot_model(decoder, to_file='vae_cnn_decoder.png', show_shapes=True) # instantiate VAE model outputs = decoder(encoder(inputs)[2]) vae = Model(inputs, outputs, name='vae') if __name__ == '__main__': parser = argparse.ArgumentParser() help_ = \"Load h5 model trained weights\" parser.add_argument(\"-w\", \"--weights\", help=help_) help_ = \"Use mse loss instead of binary cross entropy (default)\" parser.add_argument(\"-m\", \"--mse\", help=help_, action='store_true') args = parser.parse_args() models = (encoder, decoder) data = (x_test, y_test) # VAE loss = mse_loss or xent_loss + kl_loss if args.mse: reconstruction_loss = mse(K.flatten(inputs), K.flatten(outputs)) else: reconstruction_loss = binary_crossentropy(K.flatten(inputs), K.flatten(outputs)) reconstruction_loss *= image_size * image_size kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var) kl_loss = K.sum(kl_loss, axis=-1) kl_loss *= -0.5 vae_loss = K.mean(reconstruction_loss + kl_loss) vae.add_loss(vae_loss) vae.compile(optimizer='rmsprop') vae.summary() plot_model(vae, to_file='vae_cnn.png', show_shapes=True) if args.weights: vae.load_weights(args.weights) else: # train the autoencoder vae.fit(x_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, None)) vae.save_weights('vae_cnn_mnist.h5') plot_results(models, data, batch_size=batch_size, model_name=\"vae_cnn\")","title":"Reference"}]}